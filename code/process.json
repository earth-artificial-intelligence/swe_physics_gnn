[{
  "id" : "dl8xf3",
  "name" : "install_modules",
  "description" : null,
  "code" : "#!/bin/bash\n\n\n#install require libaries\npip install pandas torch torch-geometric scikit-learn scipy numpy\n\n\n# converting data into GNN format\n\n# split the data \n\n#train the model\n\n#test and evaluate the model\n",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "77sgmy",
  "name" : "fetch_data",
  "description" : null,
  "code" : "#!/bin/bash\n\n#fetch data\n# wget -q https://geobrain.csiss.gmu.edu/swe_forecasting/snotel_ghcnd_stations_4yrs_all_cols_log10.csv -P /media/volume1/\n\n\ncd /media/volume1/\n\n\nls\n\n",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "kevt5u",
  "name" : "model_creation_gnn",
  "description" : null,
  "code" : "import numpy as np\nimport torch\nimport warnings\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport pandas as pd\nfrom torch_geometric.nn import SAGEConv, BatchNorm\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch_geometric.utils import add_self_loops\n\nwarnings.filterwarnings(\"ignore\")\n\n# Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Loss Function\nclass CustomLoss(nn.Module):\n    def __init__(self, alpha=0.7):\n        super(CustomLoss, self).__init__()\n        self.mse = nn.MSELoss()\n        self.smooth_l1 = nn.SmoothL1Loss(beta=1.0)\n        self.alpha = alpha\n\n    def forward(self, predictions, targets):\n        return self.alpha * self.mse(predictions, targets) + (1 - self.alpha) * self.smooth_l1(predictions, targets)\n\n# GraphSAGE Model\nclass GraphSAGE(nn.Module):\n    def __init__(self, input_dim, hidden_dim=256, output_dim=1, dropout=0.2):\n        super(GraphSAGE, self).__init__()\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"max\")\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\n        self.conv5 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\n\n        self.bn1 = BatchNorm(hidden_dim)\n        self.bn2 = BatchNorm(hidden_dim)\n        self.bn3 = BatchNorm(hidden_dim)\n        self.bn4 = BatchNorm(hidden_dim)\n        self.bn5 = BatchNorm(hidden_dim)\n\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\n        x = self.dropout(x)\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\n        x = self.dropout(x)\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\n        x = self.dropout(x)\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\n        x = self.dropout(x)\n        x = F.leaky_relu(self.bn5(self.conv5(x, edge_index)))\n        x = F.leaky_relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Load Graph\ndef load_graph(path):\n    from torch_geometric.data import Data\n    from torch.serialization import add_safe_globals\n    add_safe_globals([Data])  \n\n    with open(path, 'rb') as f:\n        g_data = torch.load(f, weights_only=False)  \n    print(f\"✅ Loaded graph from {path}\")\n    print(f\"Nodes: {g_data.num_nodes}, Edges: {g_data.num_edges}, Features: {g_data.x.shape}\")\n    return g_data\n\n\n\n# Train Step\ndef train(model, optimizer, loss_fn, data, mask):\n    model.train()\n    optimizer.zero_grad()\n    preds = model(data)[mask].squeeze()\n    loss = loss_fn(preds, data.y[mask])\n    loss.backward()\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n    optimizer.step()\n    return loss.item()\n\n# Test Step\ndef test(model, loss_fn, data, mask, y_scaler):\n    model.eval()\n    with torch.no_grad():\n        preds = model(data)[mask].squeeze().cpu().numpy()\n        actuals = data.y[mask].cpu().numpy()\n\n        preds_original = y_scaler.inverse_transform(preds.reshape(-1, 1)).flatten()\n        actuals_original = y_scaler.inverse_transform(actuals.reshape(-1, 1)).flatten()\n\n        pd.DataFrame({\n            'predicted_swe': preds_original,\n            'actual_swe': actuals_original\n        }).to_csv(\"predicted_vs_actual.csv\", index=False)\n\n        mse = mean_squared_error(actuals_original, preds_original)\n        r2 = r2_score(actuals_original, preds_original)\n        mae = mean_absolute_error(actuals_original, preds_original)\n        rmse = mean_squared_error(actuals_original, preds_original, squared=False)\n\n    return mse, r2, rmse, mae\n\n# Main\ndef main():\n    g_path = \"/media/volume1/gnn_graph_with_time_all_states_2.pt\"\n    data = load_graph(g_path)\n\n    # Normalize features\n    x_scaler = MinMaxScaler()\n    data.x = torch.tensor(x_scaler.fit_transform(data.x.cpu()), dtype=torch.float).to(device)\n\n    # Normalize labels\n    y_scaler = MinMaxScaler()\n    data.y = torch.tensor(y_scaler.fit_transform(data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\n\n    # Add self-loops\n    data.edge_index, _ = add_self_loops(data.edge_index, num_nodes=data.num_nodes)\n\n    # Train/Test Split\n    indices = torch.randperm(data.num_nodes)\n    train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n    train_mask[indices[:int(0.8 * data.num_nodes)]] = True\n    test_mask = ~train_mask\n\n    print(f\"🔁 Training on {train_mask.sum().item()}, Testing on {test_mask.sum().item()}\")\n\n    model = GraphSAGE(input_dim=data.num_features).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-4)\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=25)\n    criterion = CustomLoss(alpha=0.7)\n\n    best_r2 = -float(\"inf\")\n    wait = 0\n    patience = 200\n\n    print(\"🚀 Starting training...\")\n    for epoch in range(1, 2001):\n        loss = train(model, optimizer, criterion, data, train_mask)\n\n        if epoch % 100 == 0 or epoch == 1:\n            mse, r2, rmse, mae = test(model, criterion, data, test_mask, y_scaler)\n            print(f\"Epoch {epoch} ➤ Train Loss: {loss:.4f} | R²: {r2:.4f} | RMSE: {rmse:.4f} | MAE: {mae:.4f}\")\n\n            if r2 > best_r2:\n                best_r2 = r2\n                wait = 0\n            else:\n                wait += 1\n\n            if wait >= patience:\n                print(f\"🛑 Early stopping at epoch {epoch}\")\n                break\n\n    print(f\"\\n✅ Best R²: {best_r2:.4f}\")\n\nif __name__ == \"__main__\":\n    main()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "p7ar4i",
  "name" : "graph_data_creation",
  "description" : null,
  "code" : "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\nimport pandas as pd\nimport torch\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nfrom torch_geometric.utils import degree\n\n# 📍 File path\nfile_path = '/media/volume1/all_points_final_merged_training.csv'\nchunksize = 500000\n\n# 📋 Intended columns\nbase_columns = [\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness',\n    'SWE', 'fsca', 'air_temperature_tmmx', 'air_temperature_tmmn',\n    'potential_evapotranspiration', 'relative_humidity_rmax', 'relative_humidity_rmin',\n    'mean_vapor_pressure_deficit', 'wind_speed', 'snodas_mask', 'water_year', 'swe_value'\n]\n\ntime_series_cols = [\n    f\"{col}_{i}\" for col in [\n        \"SWE\", \"air_temperature_tmmx\", \"air_temperature_tmmn\", \"potential_evapotranspiration\",\n        \"relative_humidity_rmax\", \"relative_humidity_rmin\", \"mean_vapor_pressure_deficit\",\n        \"precipitation_amount\", \"wind_speed\", \"fsca\"\n    ] for i in range(1, 8)\n]\n\ncumulative_cols = [\n    \"cumulative_SWE\", \"cumulative_air_temperature_tmmn\", \"cumulative_air_temperature_tmmx\",\n    \"cumulative_potential_evapotranspiration\", \"cumulative_mean_vapor_pressure_deficit\",\n    \"cumulative_precipitation_amount\", \"cumulative_relative_humidity_rmax\",\n    \"cumulative_relative_humidity_rmin\", \"cumulative_wind_speed\", \"cumulative_fsca\"\n]\n\nintended_columns = base_columns + time_series_cols + cumulative_cols\n\n# 🔍 Check available columns\nprint(\"🔍 Checking available columns in CSV...\")\npreview = pd.read_csv(file_path, nrows=1)\navailable_columns = set(preview.columns)\nuseful_columns = [col for col in intended_columns if col in available_columns]\nif 'swe_value' not in useful_columns:\n    useful_columns.append('swe_value')\n\nmissing_columns = [col for col in intended_columns if col not in available_columns]\nif missing_columns:\n    print(f\"⚠️ Missing columns skipped: {missing_columns}\")\n\n# 📥 Load dataset\nprint(\"\\n📥 Loading dataset...\")\ndf_list = []\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\n    if 'date' in chunk.columns:\n        chunk['date'] = pd.to_datetime(chunk['date'], errors='coerce')\n        chunk = chunk.dropna(subset=['date'])\n        chunk['day_of_year'] = chunk['date'].dt.dayofyear\n    else:\n        chunk['day_of_year'] = 1\n\n    if 'swe_value' in chunk.columns:\n        chunk = chunk[(chunk['swe_value'] >= 0) & (chunk['swe_value'] < 3000)]\n\n    df_list.append(chunk)\n\ndata = pd.concat(df_list, ignore_index=True)\ndata = data.drop_duplicates()\nprint(f\"✅ Loaded shape: {data.shape}\")\n\n# 🧮 Binning\ngrid_size = 0.5\nnum_time_bins = 8\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\ndata['dayofyear_bin'] = (data['day_of_year'] // (365 / num_time_bins)).astype(int)\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['dayofyear_bin'].astype(str)\n\n# 📦 Aggregation\nagg_cols = {col: 'mean' for col in useful_columns if col not in ['date', 'lat', 'lon']}\nagg_cols.update({'lat': 'mean', 'lon': 'mean', 'day_of_year': 'mean'})\nif 'date' in useful_columns:\n    agg_cols['date'] = 'first'\nagg_cols['swe_value'] = 'mean'\n\nmerged_nodes = data.groupby('grid_id').agg(agg_cols).reset_index()\n\n# ⏳ Temporal encoding\nmerged_nodes['sin_doy'] = np.sin(2 * np.pi * merged_nodes['day_of_year'] / 365)\nmerged_nodes['cos_doy'] = np.cos(2 * np.pi * merged_nodes['day_of_year'] / 365)\n\n# 🌍 Spatial encoding\nmerged_nodes['lat_rad'] = np.radians(merged_nodes['lat'])\nmerged_nodes['lon_rad'] = np.radians(merged_nodes['lon'])\nmerged_nodes['sin_lat'] = np.sin(merged_nodes['lat_rad'])\nmerged_nodes['cos_lat'] = np.cos(merged_nodes['lat_rad'])\nmerged_nodes['sin_lon'] = np.sin(merged_nodes['lon_rad'])\nmerged_nodes['cos_lon'] = np.cos(merged_nodes['lon_rad'])\n\n# 🧪 Final feature selection\nexclude = ['grid_id', 'lat', 'lon', 'lat_rad', 'lon_rad', 'date', 'day_of_year', 'swe_value']\nfeature_columns = [col for col in merged_nodes.columns if col not in exclude]\nfeature_columns += ['sin_lat', 'cos_lat', 'sin_lon', 'cos_lon']\n\n# ⚙️ Normalize & filter correlations\nscaler = StandardScaler()\nnode_features_scaled = scaler.fit_transform(merged_nodes[feature_columns])\ndf_scaled = pd.DataFrame(node_features_scaled, columns=feature_columns)\n\ncorr = df_scaled.corr().abs()\nupper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\nto_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\nprint(f\"🧹 Dropping highly correlated features: {to_drop}\")\ndf_scaled.drop(columns=to_drop, inplace=True)\n\n# Convert to tensor\nnode_features = torch.tensor(df_scaled.values, dtype=torch.float)\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\n\n# 🌐 Build edges with KDTree\ncoordinates = merged_nodes[['lat', 'lon']].values\ntree = KDTree(coordinates)\nk = 6\nthreshold = 0.5\ndistances, neighbors = tree.query(coordinates, k=k+1)\n\nedge_list = []\nfor i in range(len(neighbors)):\n    for j in range(1, k+1):\n        if distances[i][j] < threshold:\n            edge_list.append((i, neighbors[i][j]))\n\nedges = np.array(edge_list).T\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.size > 0 else torch.empty((2, 0), dtype=torch.long)\n\n# 🧵 Create Graph\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\ngraph_data.dates = merged_nodes['date'].astype(str).tolist() if 'date' in merged_nodes.columns else []\n\n# 📊 Graph Summary\nprint(\"\\n📊 Graph Summary\")\nprint(f\"🔹 Nodes: {graph_data.num_nodes}\")\nprint(f\"🔹 Edges: {graph_data.num_edges}\")\nprint(f\"🔹 Features per node: {graph_data.num_node_features}\")\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\nprint(f\"   • Min Degree : {deg.min().item()}\")\nprint(f\"   • Max Degree : {deg.max().item()}\")\nprint(f\"   • Mean Degree: {deg.float().mean():.2f}\")\nprint(f\"   • Isolated   : {(deg == 0).sum().item()} nodes\")\n\n# 💾 Save graph\nsave_path = \"/media/volume1/gnn_graph_with_time_all_states_2.pt\"\ntorch.save(graph_data, save_path)\nprint(f\"\\n✅ Graph data saved at: {save_path}\")\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "ppl3n3",
  "name" : "graph_data_cr",
  "description" : null,
  "code" : "import numpy as np\nimport torch\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nimport pandas as pd\nimport os\nfrom datetime import datetime\n\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\n\nprint(\"Loading dataset...\")\ndata = pd.read_csv(file_path)\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\n\n# Step 1: Filter data for the western USA region\nlat_min, lat_max = 32.0, 49.0  # Covers western states\nlon_min, lon_max = -125.0, -102.0  # Covers western states\nfiltered_data = data[(data['lat'] >= lat_min) & (data['lat'] <= lat_max) &\n                     (data['lon'] >= lon_min) & (data['lon'] <= lon_max)]\nprint(f\"Filtered dataset to western USA region with {filtered_data.shape[0]} rows.\")\n\n# Step 2: Sample fewer data points\nsample_size = 50000  \nsampled_data = filtered_data.sample(n=sample_size, random_state=42).reset_index(drop=True)\nprint(f\"Sampled dataset with {sampled_data.shape[0]} rows.\")\n\n# Step 3: Normalize features\nselected_features = [\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount'\n]\n\nprint(\"Normalizing features...\")\nscaler = StandardScaler()\nsampled_data[selected_features] = scaler.fit_transform(sampled_data[selected_features])\n\noutput_file_path = 'normalized_sampled_snotel_west.csv'\nsampled_data.to_csv(output_file_path, index=False)\nprint(\"Normalized sampled dataset saved as 'normalized_sampled_snotel_west.csv'.\")\n\n# Step 4: Create node features tensor\nnode_features = torch.tensor(sampled_data[selected_features].values, dtype=torch.float)\nprint(f\"Node features tensor created with shape: {node_features.shape}\")\n\n# Step 5: Create labels tensor\nlabels = torch.tensor(sampled_data['swe_value'].values, dtype=torch.float)\nprint(f\"Labels tensor created with shape: {labels.shape}\")\n\n# Step 6: Build KDTree and construct fewer edges\ncoordinates = sampled_data[['lat', 'lon']].values\nthreshold = 0.05  # Adjusted threshold to limit edges\n\nprint(\"Building KDTree and constructing edges...\")\ntree = KDTree(coordinates)\ntedges = list(tree.query_pairs(r=threshold))\n\n# Ensure bidirectional edges\nedges = np.array(tedges)\nedges = np.vstack((edges, edges[:, ::-1]))  # Add reversed edges\nedge_index = torch.tensor(edges.T, dtype=torch.long)\nprint(f\"Edge index tensor created with shape: {edge_index.shape}\")\n\n# Step 7: Create PyTorch Geometric graph data object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# Save the PyTorch Geometric Data object\ngraph_data_output_path = '/media/volume1/gat_training_data_west.pt'\ntorch.save(graph_data, graph_data_output_path)\nprint(f\"PyTorch Geometric Data object saved as '{graph_data_output_path}'.\")\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "ytgin6",
  "name" : "graph_data_prep",
  "description" : null,
  "code" : "import pandas as pd\nimport torch\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nimport numpy as np\n\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\n\nprint(\"Loading dataset...\")\ndata = pd.read_csv(file_path)\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\n\n# Define Bounding Box (Western US region)\nLAT_MAX, LON_MAX = 48.864715, -120.190430\nLAT_MIN, LON_MIN = 45.798170, -124.145508\n\n# Step 1: Filter Data Within the Bounding Box\nprint(\"Filtering data within the bounding box...\")\ndata = data[\n    (data['lat'] <= LAT_MAX) & (data['lat'] >= LAT_MIN) & \n    (data['lon'] <= LON_MAX) & (data['lon'] >= LON_MIN)\n]\nprint(f\"Filtered dataset with {data.shape[0]} rows.\")\n\n# Step 2: Spatially aware sampling\nsample_size = min(50000, len(data))  # Adjust sample size dynamically\ncoordinates = data[['lat', 'lon']].values\n\nprint(\"Building KDTree for spatial sampling...\")\ntree = KDTree(coordinates)\nindices = [np.random.randint(0, len(data))]  # Start with a random point\n\nwhile len(indices) < sample_size:\n    ref_point = coordinates[indices[-1]]  # Last added point\n    neighbors = tree.query_ball_point(ref_point, r=0.01)  # Get neighbors within radius\n    if len(neighbors) > 1:\n        new_idx = np.random.choice(neighbors)\n        if new_idx not in indices:\n            indices.append(new_idx)\n\nsampled_data = data.iloc[indices]\nprint(f\"Spatially sampled dataset with {sampled_data.shape[0]} rows.\")\n\n# Step 3: Normalize Features\nselected_features = [\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount'\n]\n\nprint(\"Normalizing features...\")\nscaler = StandardScaler()\nsampled_data[selected_features] = scaler.fit_transform(sampled_data[selected_features])\n\n# Step 4: Normalize Labels (swe_value)\nprint(\"Normalizing target variable (swe_value)...\")\nlabel_scaler = StandardScaler()\nsampled_data['swe_value'] = label_scaler.fit_transform(sampled_data['swe_value'].values.reshape(-1, 1))\n\n# Save normalized dataset\noutput_file_path = 'normalized_spatial_sampled_snotel.csv'\nsampled_data.to_csv(output_file_path, index=False)\nprint(f\"Normalized sampled dataset saved as '{output_file_path}'.\")\n\n# Step 5: Create Node Features Tensor\nnode_features = torch.tensor(sampled_data[selected_features].values, dtype=torch.float)\nprint(f\"Node features tensor created with shape: {node_features.shape}\")\n\n# Step 6: Create Labels Tensor\nlabels = torch.tensor(sampled_data['swe_value'].values, dtype=torch.float)\nprint(f\"Labels tensor created with shape: {labels.shape}\")\n\n# Step 7: Build KDTree and Construct Edges\nprint(\"Building KDTree and constructing edges...\")\ncoordinates = sampled_data[['lat', 'lon']].values\ntree = KDTree(coordinates)\nthreshold = 0.005  # Adjust as needed\n\nbatch_edges = tree.query_pairs(r=threshold)\ntedges = list(batch_edges)\nprint(f\"Number of edges created: {len(tedges)}\")\n\n# Convert to Edge Index Tensor\nedge_index = torch.tensor(tedges, dtype=torch.long).t().contiguous()\nprint(f\"Edge index tensor created with shape: {edge_index.shape}\")\n\n# Step 8: Create PyTorch Geometric Graph Data Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# Save the PyTorch Geometric Data Object\ngraph_data_output_path = '/media/volume1/gat_spatial_training_data_sampled.pt'\ntorch.save(graph_data, graph_data_output_path)\nprint(f\"PyTorch Geometric Data object saved as '{graph_data_output_path}'.\")\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "bhixzm",
  "name" : "access_terminal",
  "description" : null,
  "code" : "#!/bin/bash\n\ncd /media/volume1/\nls\n# ls -lt\n# df -h\n# cat training_log_20250224_231327.txt\n\n# rm -rf gat_training_data_west.pt gcn_trained_model.pth\n\n\n\n# ls -lhtra /media/volume1/\n\n# rm -rf  /media/volume1/all_points_final_merged_training_snodas_mask_resnet_all_batch.csv\n\n# pip install haversine\n\n# export MKL_THREADING_LAYER=GNU\n\n# whoami\n# hostname -I\n\n# pip install gdown\n# gdown https://drive.google.com/file/d/1OmXjmQVlOi_i-Af9p8lxFxZ5PsQHx4BJ/view?usp=sharing -O /media/volume1/all_points_final_merged_training_snodas_mask_resnet_all_batch.csv\n\n# whoami\n# hostname -I\n# pwd \n# scp /Users/saiannam/GRA/testing/new_training_data.csv geo2021@129.174.130.164:/media/volume1/gw-workspace/B3do6eaS9UlH/\n# du -h /media/volume1/all_points_final_merged_training_snodas_mask_resnet_all_batch.csv\n\n# wget -q  https://geobrain.csiss.gmu.edu/swe_forecasting/all_points_final_merged_training.csv -P /media/volume1/\n\n# cd media/volume1/\n# ls\n\n\n\n",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "3lobvp",
  "name" : "state_data_creation",
  "description" : null,
  "code" : "import pandas as pd\nimport torch\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nimport numpy as np\n\n# Load dataset\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\nprint(\"Loading dataset...\")\ndata = pd.read_csv(file_path)\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\n\n# Define Bounding Box (Western US region)\nLAT_MAX, LON_MAX = 48.864715, -120.190430\nLAT_MIN, LON_MIN = 45.798170, -124.145508\n\n# Step 1: Filter Data Within the Bounding Box\nprint(\"Filtering data within the bounding box...\")\ndata = data[\n    (data['lat'] <= LAT_MAX) & (data['lat'] >= LAT_MIN) & \n    (data['lon'] <= LON_MAX) & (data['lon'] >= LON_MIN)\n]\nprint(f\"Filtered dataset with {data.shape[0]} rows.\")\n\n# Step 2: Apply Grid-Based Sampling\ngrid_size = 0.05  # Reduced grid size to increase node density\nlat_bins = np.arange(LAT_MIN, LAT_MAX, grid_size)\nlon_bins = np.arange(LON_MIN, LON_MAX, grid_size)\n\nsampled_data = []\nfor lat_bin in lat_bins:\n    for lon_bin in lon_bins:\n        subset = data[\n            (data['lat'] >= lat_bin) & (data['lat'] < lat_bin + grid_size) &\n            (data['lon'] >= lon_bin) & (data['lon'] < lon_bin + grid_size)\n        ]\n        if not subset.empty:\n            sampled_data.append(subset.sample(1, random_state=42))  \n\nsampled_data = pd.concat(sampled_data)\nprint(f\"Grid-based sampled dataset with {sampled_data.shape[0]} rows.\")\n\n# Step 3: Normalize Features\nselected_features = [\n    'swe_value', 'SWE', 'fsca', 'air_temperature_tmmx', \n    'air_temperature_tmmn', 'potential_evapotranspiration', \n    'relative_humidity_rmax', 'Elevation', 'Slope', 'Curvature', \n    'Aspect', 'Eastness', 'Northness'\n]\n\nprint(\"Normalizing features...\")\nscaler = StandardScaler()\nsampled_data[selected_features] = scaler.fit_transform(sampled_data[selected_features])\n\n# Step 4: Normalize Labels\nprint(\"Normalizing target variable (swe_value)...\")\nlabel_scaler = StandardScaler()\nsampled_data['swe_value'] = label_scaler.fit_transform(sampled_data['swe_value'].values.reshape(-1, 1))\n\n# Step 5: Save Normalized Dataset\noutput_file_path = 'normalized_grid_sampled_snotel.csv'\nsampled_data.to_csv(output_file_path, index=False)\nprint(f\"Normalized sampled dataset saved as '{output_file_path}'.\")\n\n# Step 6: Create Node Features Tensor\nnode_features = torch.tensor(sampled_data[selected_features].values, dtype=torch.float)\nprint(f\"Node features tensor created with shape: {node_features.shape}\")\n\n# Step 7: Create Labels Tensor\nlabels = torch.tensor(sampled_data['swe_value'].values, dtype=torch.float)\nprint(f\"Labels tensor created with shape: {labels.shape}\")\n\n# Step 8: Build KDTree and Construct Edges Using Distance Threshold\nprint(\"Building KDTree and constructing edges using distance threshold...\")\ncoordinates = sampled_data[['lat', 'lon']].values\ntree = KDTree(coordinates)\n\nk = 30  \nradius = 0.2\nedges = []\nfor i in range(len(coordinates)):\n    knn_neighbors = tree.query(coordinates[i], k=k+1)[1][1:]  \n    radius_neighbors = tree.query_ball_point(coordinates[i], r=radius)  \n    \n    for neighbor in set(knn_neighbors).union(radius_neighbors): \n        if i != neighbor:\n            edges.append((i, neighbor))\n\n\nprint(f\"Number of edges created: {len(edges)}\")\n\n# Convert to Edge Index Tensor\nedge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\nprint(f\"Edge index tensor created with shape: {edge_index.shape}\")\n\n# Step 9: Create PyTorch Geometric Graph Data Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# Save the PyTorch Geometric Data Object\ngraph_data_output_path = '/media/volume1/gat_spatial_training_data_sampled_radius_final_01.pt'\ntorch.save(graph_data, graph_data_output_path)\nprint(f\"PyTorch Geometric Data object saved as '{graph_data_output_path}'.\")",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "0ocdgc",
  "name" : "graph_creation",
  "description" : null,
  "code" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom torch_geometric.utils import degree, to_undirected, add_self_loops\r\n\r\n# Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\nchunksize = 500000\r\n#  latitude & longitude to cover Washington, Oregon, and Idaho\r\nmin_lat, max_lat = 42.365162, 48.981824\r\nmin_lon, max_lon = -123.842534, -111.627646\r\n\r\n\r\nprint(\"Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'SWE', 'swe_value', 'relative_humidity_rmin',\r\n    'potential_evapotranspiration', 'air_temperature_tmmx',\r\n    'relative_humidity_rmax', 'mean_vapor_pressure_deficit',\r\n    'air_temperature_tmmn', 'wind_speed', 'Elevation', 'Aspect',\r\n    'Curvature', 'Northness', 'Eastness', 'fsca', 'Slope', 'SWE_1',\r\n    'air_temperature_tmmn_1', 'potential_evapotranspiration_1',\r\n    'mean_vapor_pressure_deficit_1', 'relative_humidity_rmax_1',\r\n    'relative_humidity_rmin_1', 'air_temperature_tmmx_1', 'wind_speed_1',\r\n    'fsca_1', 'SWE_2', 'air_temperature_tmmn_2',\r\n    'potential_evapotranspiration_2', 'mean_vapor_pressure_deficit_2',\r\n    'relative_humidity_rmax_2', 'relative_humidity_rmin_2',\r\n    'air_temperature_tmmx_2', 'wind_speed_2', 'fsca_2', 'SWE_3',\r\n    'air_temperature_tmmn_3', 'potential_evapotranspiration_3',\r\n    'mean_vapor_pressure_deficit_3', 'relative_humidity_rmax_3',\r\n    'relative_humidity_rmin_3', 'air_temperature_tmmx_3', 'wind_speed_3',\r\n    'fsca_3', 'SWE_4', 'air_temperature_tmmn_4',\r\n    'potential_evapotranspiration_4', 'mean_vapor_pressure_deficit_4',\r\n    'relative_humidity_rmax_4', 'relative_humidity_rmin_4',\r\n    'air_temperature_tmmx_4', 'wind_speed_4', 'fsca_4', 'SWE_5',\r\n    'air_temperature_tmmn_5', 'potential_evapotranspiration_5',\r\n    'mean_vapor_pressure_deficit_5', 'relative_humidity_rmax_5',\r\n    'relative_humidity_rmin_5', 'air_temperature_tmmx_5', 'wind_speed_5',\r\n    'fsca_5', 'SWE_6', 'air_temperature_tmmn_6',\r\n    'potential_evapotranspiration_6', 'mean_vapor_pressure_deficit_6',\r\n    'relative_humidity_rmax_6', 'relative_humidity_rmin_6',\r\n    'air_temperature_tmmx_6', 'wind_speed_6', 'fsca_6', 'SWE_7',\r\n    'air_temperature_tmmn_7', 'potential_evapotranspiration_7',\r\n    'mean_vapor_pressure_deficit_7', 'relative_humidity_rmax_7',\r\n    'relative_humidity_rmin_7', 'air_temperature_tmmx_7', 'wind_speed_7',\r\n    'fsca_7', 'water_year', 'snodas_mask'\r\n]\r\n\r\ndf_list = []\r\ntotal_rows = 0\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, parse_dates=['date'], chunksize=chunksize):\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear  \r\n    total_rows += len(chunk)\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Total rows in raw dataset: {total_rows}\")\r\nprint(f\"Rows remaining after lat/lon filtering: {data.shape[0]}\")\r\n\r\n# Check for missing values\r\nmissing_values = data[['date', 'lat', 'lon', 'swe_value']].isnull().sum()\r\nprint(f\"Missing values count per column:\\n{missing_values}\")\r\n\r\n# Remove duplicates\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\"Found {num_duplicates} duplicate rows.\")\r\ndata = data.drop_duplicates()\r\nprint(f\"Dataset size after deduplication: {data.shape[0]}\")\r\n\r\n# Merge records for the same station on the same day\r\nprint(\"Merging records for the same station on the same day...\")\r\nmerge_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\ndata = data.groupby(['lat', 'lon', 'date'], as_index=False)[merge_columns].mean()\r\ndata['dayofyear'] = pd.to_datetime(data['date']).dt.dayofyear\r\nprint(f\" Merged dataset size: {data.shape[0]}\")\r\n\r\n# Round numerical values\r\nprint(\" Rounding numerical values to 3 decimals...\")\r\nnumerical_cols = list(set(merge_columns) & set(data.columns))  \r\ndata[numerical_cols] = data[numerical_cols].round(3)\r\n\r\n# Merging nodes to reduce graph size\r\ngrid_size = 0.01  \r\ntime_bins = 12  #  Increased from 10 to 12 bins\r\n\r\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\ndata['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\nmerged_nodes = data.groupby('grid_id').agg({\r\n    'Elevation': 'mean', 'Slope': 'mean', 'SWE': 'mean', 'snow_depth': 'mean',\r\n    'air_temperature_observed_f': 'mean', 'precipitation_amount': 'mean',\r\n    'relative_humidity_rmin': 'mean', 'relative_humidity_rmax': 'mean', 'wind_speed': 'mean',\r\n    'cumulative_SWE': 'mean', 'cumulative_precipitation_amount': 'mean', 'swe_value': 'mean',\r\n    'lat': 'mean', 'lon': 'mean', 'dayofyear': 'mean'\r\n}).reset_index()\r\n\r\nprint(f\" Merged nodes: {merged_nodes.shape[0]} from {data.shape[0]} original records.\")\r\n\r\n# Normalizing features\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\n\r\nmerged_nodes[feature_columns] = merged_nodes[feature_columns].fillna(merged_nodes[feature_columns].median())\r\n\r\nscaler = StandardScaler()\r\nmerged_nodes[feature_columns] = scaler.fit_transform(merged_nodes[feature_columns].values)\r\n\r\nnode_features = torch.tensor(merged_nodes[feature_columns].values, dtype=torch.float)\r\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\n\r\nprint(f\" Normalization completed. Node feature shape: {node_features.shape}\")\r\n\r\n# Adaptive spatial threshold based on data distribution\r\ncoordinates = merged_nodes[['lat', 'lon', 'dayofyear']].values  \r\ntree = KDTree(coordinates)\r\n\r\navg_degree = max(5, min(10, int(len(merged_nodes) ** 0.5 / 10)))\r\ndistances, neighbors = tree.query(coordinates, k=avg_degree + 1)\r\n\r\nspatial_threshold = np.percentile(distances[:, 1:], 75)  #  Adaptive threshold\r\ntime_weight = 0.07  \r\n\r\ndef time_aware_distance(p1, p2):\r\n    spatial_dist = np.linalg.norm(p1[:2] - p2[:2])  \r\n    time_dist = abs(p1[2] - p2[2]) * time_weight  \r\n    return spatial_dist + time_dist\r\n\r\nprint(\"Rebuilding KDTree and constructing edges...\")\r\n\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, avg_degree + 1):\r\n        if time_aware_distance(coordinates[i], coordinates[neighbors[i][j]]) < spatial_threshold:\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T\r\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\n# Ensure bidirectional edges\r\nedge_index = to_undirected(edge_index)\r\n\r\n# Graph Pruning: Remove low-degree nodes\r\ndeg = degree(edge_index[0], num_nodes=merged_nodes.shape[0])\r\nvalid_nodes = deg > 1  \r\nfiltered_node_mask = valid_nodes.nonzero(as_tuple=True)[0]\r\n\r\n# Filter node features & labels\r\nnode_features = node_features[filtered_node_mask]\r\nlabels = labels[filtered_node_mask]\r\n\r\n# Reconstruct Edge Index after filtering\r\nnode_map = {old_idx: new_idx for new_idx, old_idx in enumerate(filtered_node_mask.tolist())}\r\nnew_edges = [(node_map[i], node_map[j]) for i, j in edge_index.T.tolist() if i in node_map and j in node_map]\r\nedge_index = torch.tensor(new_edges, dtype=torch.long).T if len(new_edges) > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\nprint(f\"Final Graph: Nodes = {len(filtered_node_mask)}, Edges = {edge_index.shape[1]}\")\r\n\r\n# Save graph\r\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\ntorch.save(graph_data, \"/media/volume1/gnn_3states.pt\")\r\n\r\nprint(\" Graph successfully saved.\")\r\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "o7v206",
  "name" : "graph_model",
  "description" : null,
  "code" : "import numpy as np\r\nimport torch\r\nimport warnings\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv, BatchNorm\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\n\r\n# Load and verify graph\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data = torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\"Number of Edges: {g_data.num_edges}\")\r\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\"Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n    except Exception as e:\r\n        print(f\"Error loading graph: {e}\")\r\n        exit()\r\n\r\n# Define GraphSAGE model with deeper layers & mean aggregation\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv5 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")  # Added extra layer\r\n\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n        self.bn4 = BatchNorm(hidden_dim)\r\n        self.bn5 = BatchNorm(hidden_dim)\r\n\r\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\r\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\r\n\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn5(self.conv5(x, edge_index)))\r\n\r\n        x = F.leaky_relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n\r\n    loss.backward()\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n# Main function\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph_final_reduced_wa_4.pt\"  \r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n    # Normalize node features\r\n    scaler_x = StandardScaler()\r\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\r\n\r\n    # Normalize target variable\r\n    y_scaler = StandardScaler()\r\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\r\n\r\n    # Training split\r\n    indices= torch.randperm(g_data.num_nodes)\r\n    train_size=int(0.8*g_data.num_nodes)\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[indices[:train_size]] = True  \r\n    test_mask = ~train_mask  \r\n\r\n    print(f\"Training on {train_size} nodes & Testing on {g_data.num_nodes - train_size} nodes\")\r\n\r\n    # Define model, optimizer, and loss function\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=512, output_dim=1, dropout=0.4).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-3)\r\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=50)\r\n    criterion = nn.SmoothL1Loss(beta=1.0)\r\n\r\n    best_r2 = -float(\"inf\")\r\n\r\n    print(\"Starting training for 1000 epochs...\")\r\n\r\n    for epoch in range(1, 5001):  \r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch % 100 == 0 or epoch == 5:  \r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\"Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n            if r2 > best_r2:\r\n                best_r2 = r2\r\n\r\n    print(f\"Best R² score: {best_r2:.4f}\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "b5qsrf",
  "name" : "graph_model_sage",
  "description" : null,
  "code" : "import numpy as np\r\nimport torch\r\nimport warnings\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv, BatchNorm\r\nfrom torch_geometric.utils import add_self_loops\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\n\r\n# Load and verify graph\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data = torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\"Number of Edges: {g_data.num_edges}\")\r\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\"Warning: Target variable y is missing or None!\")\r\n\r\n        return g_data\r\n    except Exception as e:\r\n        print(f\"Error loading graph: {e}\")\r\n        exit()\r\n\r\n# Define GraphSAGE model with residual connections\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim):\r\n        super(GraphSAGE, self).__init__()\r\n        \r\n        # Linear layer to project input features to hidden_dim\r\n        self.input_proj = nn.Linear(input_dim, hidden_dim) \r\n        \r\n        self.conv1 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")  \r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\r\n\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n\r\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\r\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\r\n\r\n        self.dropout1 = nn.Dropout(0.2)  # Slightly higher dropout in early layers\r\n        self.dropout2 = nn.Dropout(0.1)  # Lower dropout in final layers\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n\r\n        # Project input features to hidden_dim for residual connections\r\n        x = self.input_proj(x)\r\n\r\n        residual = x  # Keep projected features for residual connection\r\n\r\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\r\n        x = self.dropout1(x)\r\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\r\n        x = self.dropout1(x)\r\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\r\n        x = self.dropout2(x)\r\n        \r\n        x = x + residual  # Residual connection\r\n\r\n        x = F.leaky_relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n\r\n    loss.backward()\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=3.0)  # Improved gradient clipping\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n# Main function\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_2graph.pt\"  \r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n    # Normalize node features\r\n    scaler_x = StandardScaler()\r\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\r\n\r\n    # Normalize target variable using StandardScaler (fixes scaling issues)\r\n    y_scaler = StandardScaler()\r\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\r\n\r\n    # Training split\r\n    indices = np.arange(g_data.num_nodes)\r\n    np.random.shuffle(indices)\r\n    train_size = int(0.8 * len(indices))\r\n    \r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[indices[:train_size]] = True\r\n    test_mask = ~train_mask  \r\n\r\n    print(f\"Training on {train_size} nodes & Testing on {g_data.num_nodes - train_size} nodes\")\r\n\r\n    # Define model, optimizer, and loss function\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=256, output_dim=1).to(device)\r\n    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)  # Slightly lower learning rate\r\n    scheduler = CosineAnnealingLR(optimizer, T_max=300, eta_min=1e-6)  \r\n    criterion = nn.SmoothL1Loss(beta=1.0)\r\n\r\n    best_r2 = -float(\"inf\")\r\n\r\n    print(\"Starting training for 1000 epochs...\")\r\n\r\n    for epoch in range(1, 1001):  \r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch % 100 == 0 or epoch == 5:  \r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\"Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n            # Update best R² score\r\n            if r2 > best_r2:\r\n                best_r2 = r2\r\n\r\n        # Reduce learning rate dynamically\r\n        scheduler.step()\r\n\r\n    print(f\"Best R² score: {best_r2:.4f}\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "u1ypge",
  "name" : "temporal_graph",
  "description" : null,
  "code" : "import os\nos.environ[\"MKL_THREADING_LAYER\"] = \"GNU\"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom sklearn.metrics import r2_score\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\n\n# SET YOUR GRAPH FILE PATH HERE\nGRAPH_DATA_PATH = \"/media/volume1/gnn_graph_pinn_data_new_3.pt\"\nEPOCHS = 2000\nLAMBDA1 = 0.5\nLAMBDA2 = 1.0\nTEST_RATIO = 0.2\n\n# 1️⃣ PINN Model\nclass PINN(nn.Module):\n    def __init__(self, input_dim, hidden_dim,dropout=0.1):\n        super(PINN, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        self.out = nn.Linear(hidden_dim, 1)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        return self.out(x).squeeze()\n\n# 2️⃣ Compute SCA (Snow Covered Area)\ndef compute_sca(swe_pred, threshold=0.01):\n    return (swe_pred > threshold).float().mean()\n\n# 3️⃣ Compute SWE by Elevation Bins\ndef compute_swe_by_elevation(swe_pred, elevation, bins):\n    digitized = torch.bucketize(elevation, bins)\n    means = [swe_pred[digitized == i].mean() if (digitized == i).any()\n             else torch.tensor(0.0, device=swe_pred.device) for i in range(1, len(bins))]\n    return torch.stack(means)\n\n# 4️⃣ Custom Loss\ndef pinn_physics_loss(pred, y_true, sca_pred, sca_ref, swe_model_by_elev, swe_climo_by_elev, λ1=1.0, λ2=1.0):\n    model_loss = F.mse_loss(pred, y_true)\n    sca_loss = F.l1_loss(sca_pred, sca_ref)\n    hypsometric_loss = F.l1_loss(swe_model_by_elev, swe_climo_by_elev)\n    return model_loss + λ1 * sca_loss + λ2 * hypsometric_loss\n\n# 5️⃣ Split Graph Data\n\n\ndef split_graph_data(graph_data, test_ratio=0.2, seed=None):\n    \"\"\"\n    Splits graph nodes into train and test indices with a fixed test ratio.\n\n    Parameters:\n        graph_data: torch_geometric.data.Data\n        test_ratio: float\n        seed: int (optional)\n\n    Returns:\n        train_idx, test_idx: torch.Tensor, torch.Tensor\n    \"\"\"\n    if seed is not None:\n        torch.manual_seed(seed)\n\n    num_nodes = graph_data.num_nodes\n    num_test = int(round(test_ratio * num_nodes))  # round to ensure fixed size\n    indices = torch.randperm(num_nodes)\n\n    test_idx = indices[:num_test]\n    train_idx = indices[num_test:]\n\n    return train_idx, test_idx\n\n\n\n# 6️⃣ Train Function\n\ndef train_pinn_with_improvements(graph_data, epochs=10000, λ1=1.0, λ2=1.0, test_ratio=0.2, patience=300):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    input_dim = graph_data.x.shape[1]\n    model = PINN(input_dim=input_dim, hidden_dim=64, dropout=0.1).to(device)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=50, verbose=True)\n\n    graph_data = graph_data.to(device)\n    elevation = graph_data.x[:, 0]\n    elevation_bins = torch.tensor([0, 250, 500, 750, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000], device=device)\n    sca_ref = torch.tensor(0.6, device=device)\n    swe_climo = torch.tensor(\n    [0.1, 0.15, 0.25, 0.35, 0.45, 0.55, 0.60, 0.55, 0.50, 0.40, 0.30, 0.20],\n    device=device)\n\n    \n    \n\n    train_idx, val_idx = split_graph_data(graph_data, test_ratio)\n\n    best_val_loss = float('inf')\n    best_r2 = -1\n    best_model_state = None\n    no_improve_epochs = 0\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        pred_train = model(graph_data.x[train_idx])\n        y_train = graph_data.y[train_idx]\n        elev_train = elevation[train_idx]\n\n        sca_pred = compute_sca(pred_train)\n        swe_model_hypso = compute_swe_by_elevation(pred_train, elev_train, elevation_bins)\n\n        loss = pinn_physics_loss(pred_train, y_train, sca_pred, sca_ref, swe_model_hypso, swe_climo, λ1, λ2)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        model.eval()\n        with torch.no_grad():\n            pred_val = model(graph_data.x[val_idx])\n            y_val = graph_data.y[val_idx]\n            val_loss = F.mse_loss(pred_val, y_val)\n            r2 = r2_score(y_val.cpu().numpy(), pred_val.cpu().numpy())\n\n        scheduler.step(val_loss)\n\n        if epoch % 100 == 0 or epoch == 1 or epoch == epochs:\n            print(f\"Epoch {epoch}/{epochs} | Train Loss: {loss.item():.4f} | Val Loss: {val_loss.item():.4f} | R²: {r2:.4f}\")\n\n        scheduler.step(val_loss)\n\n         # ✅ Track best R² and best model\n        if val_loss.item() < best_val_loss:\n            best_val_loss = val_loss.item()\n            best_model_state = model.state_dict()\n            best_r2 = r2\n            no_improve_epochs = 0\n        else:\n            no_improve_epochs += 1\n\n        if no_improve_epochs >= patience:\n            print(f\"\\n⏹️ Early stopping at epoch {epoch} — Best R²: {best_r2:.4f}\")\n            break\n\n    model.load_state_dict(best_model_state)\n\n    # Final Eval\n    model.eval()\n    with torch.no_grad():\n        pred = model(graph_data.x[val_idx]).cpu().numpy()\n        pred = np.clip(pred, 0, None)  # ⛔ Prevent negative SWE predictions\n        true = graph_data.y[val_idx].cpu().numpy()\n\n        final_mse = F.mse_loss(torch.tensor(pred), torch.tensor(true)).item()\n        final_mae = F.l1_loss(torch.tensor(pred), torch.tensor(true)).item()\n        final_r2 = r2_score(true, pred)\n\n\n    print(\"\\n✅ Final Evaluation:\")\n    print(f\"📉 Final MSE      : {final_mse:.4f}\")\n    print(f\"📊 Final MAE      : {final_mae:.4f}\")\n    print(f\"📈 Final R² Score : {final_r2:.4f}\")\n    print(\"🏁 Model training & evaluation complete.\\n\")\n\n    # 🔍 Show a few prediction vs actual pairs\n    print(\"🔍 Sample predictions vs actual:\")\n    for i in range(5):\n        error = abs(pred[i] - true[i])\n        print(f\"   Predicted: {pred[i]:.3f} | Actual: {true[i]:.3f} | Error: {error:.3f}\")\n\n\n    return model\n\n# evaulate the model \ndef evaluate_pinn(model, graph_data, test_ratio=0.2):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    graph_data = graph_data.to(device)\n\n    _, val_idx = split_graph_data(graph_data, test_ratio)\n    model.eval()\n\n    with torch.no_grad():\n        pred = model(graph_data.x[val_idx]).cpu().numpy()\n        pred = np.clip(pred, 0, None)\n        true = graph_data.y[val_idx].cpu().numpy()\n\n        mse = F.mse_loss(torch.tensor(pred), torch.tensor(true)).item()\n        mae = F.l1_loss(torch.tensor(pred), torch.tensor(true)).item()\n        r2 = r2_score(true, pred)\n\n        print(\"\\n🔍 Test Set Evaluation:\")\n        print(f\"📉 MSE : {mse:.4f}\")\n        print(f\"📊 MAE : {mae:.4f}\")\n        print(f\"📈 R² Score : {r2:.4f}\")\n\n        print(\"\\n🔍 Sample predictions vs actual:\")\n        for i in range(10):\n            error = abs(pred[i] - true[i])\n            print(f\"   Predicted: {pred[i]:.3f} | Actual: {true[i]:.3f} | Error: {error:.3f}\")\n\n\n\n\n# 7️⃣ Just Run This\n# 7️⃣ Just Run This\nif __name__ == \"__main__\":\n    print(f\"📥 Loading graph from: {GRAPH_DATA_PATH}\")\n    graph_data = torch.load(GRAPH_DATA_PATH, weights_only=False)\n\n    # 📊 Inspect target SWE values\n    print(\"\\n📊 Inspecting graph_data.y:\")\n    print(\"   Min :\", graph_data.y.min().item())\n    print(\"   Max :\", graph_data.y.max().item())\n    print(\"   Mean:\", graph_data.y.mean().item())\n    print(\"   Std :\", graph_data.y.std().item())\n    print(\"   Sample values:\", graph_data.y[:10])\n\n    print(f\"🚀 Training PINN for {EPOCHS} epochs...\")\n    trained_model = train_pinn_with_improvements(\n        graph_data=graph_data,\n        epochs=EPOCHS,\n        λ1=LAMBDA1,\n        λ2=LAMBDA2,\n        test_ratio=TEST_RATIO\n    )\n\n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
}]