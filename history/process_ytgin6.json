[{
  "history_id" : "fx44z9ciw9q",
  "history_input" : "import pandas as pd\nimport torch\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nimport numpy as np\n\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\n\nprint(\"Loading dataset...\")\ndata = pd.read_csv(file_path)\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\n\n# Define Bounding Box (Western US region)\nLAT_MAX, LON_MAX = 48.864715, -120.190430\nLAT_MIN, LON_MIN = 45.798170, -124.145508\n\n# Step 1: Filter Data Within the Bounding Box\nprint(\"Filtering data within the bounding box...\")\ndata = data[\n    (data['lat'] <= LAT_MAX) & (data['lat'] >= LAT_MIN) & \n    (data['lon'] <= LON_MAX) & (data['lon'] >= LON_MIN)\n]\nprint(f\"Filtered dataset with {data.shape[0]} rows.\")\n\n# Step 2: Spatially aware sampling\nsample_size = min(50000, len(data))  # Adjust sample size dynamically\ncoordinates = data[['lat', 'lon']].values\n\nprint(\"Building KDTree for spatial sampling...\")\ntree = KDTree(coordinates)\nindices = [np.random.randint(0, len(data))]  # Start with a random point\n\nwhile len(indices) < sample_size:\n    ref_point = coordinates[indices[-1]]  # Last added point\n    neighbors = tree.query_ball_point(ref_point, r=0.01)  # Get neighbors within radius\n    if len(neighbors) > 1:\n        new_idx = np.random.choice(neighbors)\n        if new_idx not in indices:\n            indices.append(new_idx)\n\nsampled_data = data.iloc[indices]\nprint(f\"Spatially sampled dataset with {sampled_data.shape[0]} rows.\")\n\n# Step 3: Normalize Features\nselected_features = [\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount'\n]\n\nprint(\"Normalizing features...\")\nscaler = StandardScaler()\nsampled_data[selected_features] = scaler.fit_transform(sampled_data[selected_features])\n\n# Step 4: Normalize Labels (swe_value)\nprint(\"Normalizing target variable (swe_value)...\")\nlabel_scaler = StandardScaler()\nsampled_data['swe_value'] = label_scaler.fit_transform(sampled_data['swe_value'].values.reshape(-1, 1))\n\n# Save normalized dataset\noutput_file_path = 'normalized_spatial_sampled_snotel.csv'\nsampled_data.to_csv(output_file_path, index=False)\nprint(f\"Normalized sampled dataset saved as '{output_file_path}'.\")\n\n# Step 5: Create Node Features Tensor\nnode_features = torch.tensor(sampled_data[selected_features].values, dtype=torch.float)\nprint(f\"Node features tensor created with shape: {node_features.shape}\")\n\n# Step 6: Create Labels Tensor\nlabels = torch.tensor(sampled_data['swe_value'].values, dtype=torch.float)\nprint(f\"Labels tensor created with shape: {labels.shape}\")\n\n# Step 7: Build KDTree and Construct Edges\nprint(\"Building KDTree and constructing edges...\")\ncoordinates = sampled_data[['lat', 'lon']].values\ntree = KDTree(coordinates)\nthreshold = 0.005  # Adjust as needed\n\nbatch_edges = tree.query_pairs(r=threshold)\ntedges = list(batch_edges)\nprint(f\"Number of edges created: {len(tedges)}\")\n\n# Convert to Edge Index Tensor\nedge_index = torch.tensor(tedges, dtype=torch.long).t().contiguous()\nprint(f\"Edge index tensor created with shape: {edge_index.shape}\")\n\n# Step 8: Create PyTorch Geometric Graph Data Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# Save the PyTorch Geometric Data Object\ngraph_data_output_path = '/media/volume1/gat_spatial_training_data_sampled.pt'\ntorch.save(graph_data, graph_data_output_path)\nprint(f\"PyTorch Geometric Data object saved as '{graph_data_output_path}'.\")\n",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 105 columns.\nFiltering data within the bounding box...\nFiltered dataset with 360859 rows.\n",
  "history_begin_time" : 1741409322786,
  "history_end_time" : 1741409322759,
  "history_notes" : null,
  "history_process" : "ytgin6",
  "host_id" : "100001",
  "indicator" : "Running"
},{
  "history_id" : "5lyry463mg0",
  "history_input" : "import pandas as pd\nimport torch\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nimport numpy as np\n\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\n\nprint(\"Loading dataset...\")\ndata = pd.read_csv(file_path)\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\n\n# Define Bounding Box (Western US region)\nLAT_MAX, LON_MAX = 48.864715, -120.190430\nLAT_MIN, LON_MIN = 45.798170, -124.145508\n\n# Step 1: Filter Data Within the Bounding Box\nprint(\"Filtering data within the bounding box...\")\ndata = data[\n    (data['lat'] <= LAT_MAX) & (data['lat'] >= LAT_MIN) & \n    (data['lon'] <= LON_MAX) & (data['lon'] >= LON_MIN)\n]\nprint(f\"Filtered dataset with {data.shape[0]} rows.\")\n\n# Step 2: Spatially aware sampling\nsample_size = min(50000, len(data))  # Adjust sample size dynamically\ncoordinates = data[['lat', 'lon']].values\n\nprint(\"Building KDTree for spatial sampling...\")\ntree = KDTree(coordinates)\nindices = [np.random.randint(0, len(data))]  # Start with a random point\n\nwhile len(indices) < sample_size:\n    ref_point = coordinates[indices[-1]]  # Last added point\n    neighbors = tree.query_ball_point(ref_point, r=0.01)  # Get neighbors within radius\n    if len(neighbors) > 1:\n        new_idx = np.random.choice(neighbors)\n        if new_idx not in indices:\n            indices.append(new_idx)\n\nsampled_data = data.iloc[indices]\nprint(f\"Spatially sampled dataset with {sampled_data.shape[0]} rows.\")\n\n# Step 3: Normalize Features\nselected_features = [\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount'\n]\n\nprint(\"Normalizing features...\")\nscaler = StandardScaler()\nsampled_data[selected_features] = scaler.fit_transform(sampled_data[selected_features])\n\n# Step 4: Normalize Labels (swe_value)\nprint(\"Normalizing target variable (swe_value)...\")\nlabel_scaler = StandardScaler()\nsampled_data['swe_value'] = label_scaler.fit_transform(sampled_data['swe_value'].values.reshape(-1, 1))\n\n# Save normalized dataset\noutput_file_path = 'normalized_spatial_sampled_snotel.csv'\nsampled_data.to_csv(output_file_path, index=False)\nprint(f\"Normalized sampled dataset saved as '{output_file_path}'.\")\n\n# Step 5: Create Node Features Tensor\nnode_features = torch.tensor(sampled_data[selected_features].values, dtype=torch.float)\nprint(f\"Node features tensor created with shape: {node_features.shape}\")\n\n# Step 6: Create Labels Tensor\nlabels = torch.tensor(sampled_data['swe_value'].values, dtype=torch.float)\nprint(f\"Labels tensor created with shape: {labels.shape}\")\n\n# Step 7: Build KDTree and Construct Edges\nprint(\"Building KDTree and constructing edges...\")\ncoordinates = sampled_data[['lat', 'lon']].values\ntree = KDTree(coordinates)\nthreshold = 0.005  # Adjust as needed\n\nbatch_edges = tree.query_pairs(r=threshold)\ntedges = list(batch_edges)\nprint(f\"Number of edges created: {len(tedges)}\")\n\n# Convert to Edge Index Tensor\nedge_index = torch.tensor(tedges, dtype=torch.long).t().contiguous()\nprint(f\"Edge index tensor created with shape: {edge_index.shape}\")\n\n# Step 8: Create PyTorch Geometric Graph Data Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# Save the PyTorch Geometric Data Object\ngraph_data_output_path = '/media/volume1/gat_spatial_training_data_sampled.pt'\ntorch.save(graph_data, graph_data_output_path)\nprint(f\"PyTorch Geometric Data object saved as '{graph_data_output_path}'.\")\n",
  "history_output" : "Loading dataset...\n",
  "history_begin_time" : 1741409299768,
  "history_end_time" : 1741409321256,
  "history_notes" : null,
  "history_process" : "ytgin6",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "gyc0yzca3vz",
  "history_input" : "import pandas as pd\nimport torch\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nimport numpy as np\n\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\n\nprint(\"Loading dataset...\")\ndata = pd.read_csv(file_path)\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\n\n# Define Bounding Box (Western US region)\nLAT_MAX, LON_MAX = 48.864715, -120.190430\nLAT_MIN, LON_MIN = 45.798170, -124.145508\n\n# Step 1: Filter Data Within the Bounding Box\nprint(\"Filtering data within the bounding box...\")\ndata = data[\n    (data['lat'] <= LAT_MAX) & (data['lat'] >= LAT_MIN) & \n    (data['lon'] <= LON_MAX) & (data['lon'] >= LON_MIN)\n]\nprint(f\"Filtered dataset with {data.shape[0]} rows.\")\n\n# Step 2: Spatially aware sampling\nsample_size = min(50000, len(data))  # Adjust sample size dynamically\ncoordinates = data[['lat', 'lon']].values\n\nprint(\"Building KDTree for spatial sampling...\")\ntree = KDTree(coordinates)\nindices = [np.random.randint(0, len(data))]  # Start with a random point\n\nwhile len(indices) < sample_size:\n    ref_point = coordinates[indices[-1]]  # Last added point\n    neighbors = tree.query_ball_point(ref_point, r=0.01)  # Get neighbors within radius\n    if len(neighbors) > 1:\n        new_idx = np.random.choice(neighbors)\n        if new_idx not in indices:\n            indices.append(new_idx)\n\nsampled_data = data.iloc[indices]\nprint(f\"Spatially sampled dataset with {sampled_data.shape[0]} rows.\")\n\n# Step 3: Normalize Features\nselected_features = [\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount'\n]\n\nprint(\"Normalizing features...\")\nscaler = StandardScaler()\nsampled_data[selected_features] = scaler.fit_transform(sampled_data[selected_features])\n\n# Step 4: Normalize Labels (swe_value)\nprint(\"Normalizing target variable (swe_value)...\")\nlabel_scaler = StandardScaler()\nsampled_data['swe_value'] = label_scaler.fit_transform(sampled_data['swe_value'].values.reshape(-1, 1))\n\n# Save normalized dataset\noutput_file_path = 'normalized_spatial_sampled_snotel.csv'\nsampled_data.to_csv(output_file_path, index=False)\nprint(f\"Normalized sampled dataset saved as '{output_file_path}'.\")\n\n# Step 5: Create Node Features Tensor\nnode_features = torch.tensor(sampled_data[selected_features].values, dtype=torch.float)\nprint(f\"Node features tensor created with shape: {node_features.shape}\")\n\n# Step 6: Create Labels Tensor\nlabels = torch.tensor(sampled_data['swe_value'].values, dtype=torch.float)\nprint(f\"Labels tensor created with shape: {labels.shape}\")\n\n# Step 7: Build KDTree and Construct Edges\nprint(\"Building KDTree and constructing edges...\")\ncoordinates = sampled_data[['lat', 'lon']].values\ntree = KDTree(coordinates)\nthreshold = 0.005  # Adjust as needed\n\nbatch_edges = tree.query_pairs(r=threshold)\ntedges = list(batch_edges)\nprint(f\"Number of edges created: {len(tedges)}\")\n\n# Convert to Edge Index Tensor\nedge_index = torch.tensor(tedges, dtype=torch.long).t().contiguous()\nprint(f\"Edge index tensor created with shape: {edge_index.shape}\")\n\n# Step 8: Create PyTorch Geometric Graph Data Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# Save the PyTorch Geometric Data Object\ngraph_data_output_path = '/media/volume1/gat_spatial_training_data_sampled.pt'\ntorch.save(graph_data, graph_data_output_path)\nprint(f\"PyTorch Geometric Data object saved as '{graph_data_output_path}'.\")\n",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 105 columns.\nFiltering data within the bounding box...\nFiltered dataset with 360859 rows.\n",
  "history_begin_time" : 1741212657995,
  "history_end_time" : 1741212657828,
  "history_notes" : null,
  "history_process" : "ytgin6",
  "host_id" : "100001",
  "indicator" : "Running"
},{
  "history_id" : "32iw0iugf2m",
  "history_input" : "import pandas as pd\nimport torch\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nimport numpy as np\n\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\n\nprint(\"Loading dataset...\")\ndata = pd.read_csv(file_path)\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\n\n# Define Bounding Box (Western US region)\nLAT_MAX, LON_MAX = 48.864715, -120.190430\nLAT_MIN, LON_MIN = 45.798170, -124.145508\n\n# Step 1: Filter Data Within the Bounding Box\nprint(\"Filtering data within the bounding box...\")\ndata = data[\n    (data['lat'] <= LAT_MAX) & (data['lat'] >= LAT_MIN) & \n    (data['lon'] <= LON_MAX) & (data['lon'] >= LON_MIN)\n]\nprint(f\"Filtered dataset with {data.shape[0]} rows.\")\n\n# Step 2: Spatially aware sampling\nsample_size = min(50000, len(data))  # Adjust sample size dynamically\ncoordinates = data[['lat', 'lon']].values\n\nprint(\"Building KDTree for spatial sampling...\")\ntree = KDTree(coordinates)\nindices = [np.random.randint(0, len(data))]  # Start with a random point\n\nwhile len(indices) < sample_size:\n    ref_point = coordinates[indices[-1]]  # Last added point\n    neighbors = tree.query_ball_point(ref_point, r=0.01)  # Get neighbors within radius\n    if len(neighbors) > 1:\n        new_idx = np.random.choice(neighbors)\n        if new_idx not in indices:\n            indices.append(new_idx)\n\nsampled_data = data.iloc[indices]\nprint(f\"Spatially sampled dataset with {sampled_data.shape[0]} rows.\")\n\n# Step 3: Normalize Features\nselected_features = [\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount'\n]\n\nprint(\"Normalizing features...\")\nscaler = StandardScaler()\nsampled_data[selected_features] = scaler.fit_transform(sampled_data[selected_features])\n\n# Step 4: Normalize Labels (swe_value)\nprint(\"Normalizing target variable (swe_value)...\")\nlabel_scaler = StandardScaler()\nsampled_data['swe_value'] = label_scaler.fit_transform(sampled_data['swe_value'].values.reshape(-1, 1))\n\n# Save normalized dataset\noutput_file_path = 'normalized_spatial_sampled_snotel.csv'\nsampled_data.to_csv(output_file_path, index=False)\nprint(f\"Normalized sampled dataset saved as '{output_file_path}'.\")\n\n# Step 5: Create Node Features Tensor\nnode_features = torch.tensor(sampled_data[selected_features].values, dtype=torch.float)\nprint(f\"Node features tensor created with shape: {node_features.shape}\")\n\n# Step 6: Create Labels Tensor\nlabels = torch.tensor(sampled_data['swe_value'].values, dtype=torch.float)\nprint(f\"Labels tensor created with shape: {labels.shape}\")\n\n# Step 7: Build KDTree and Construct Edges\nprint(\"Building KDTree and constructing edges...\")\ncoordinates = sampled_data[['lat', 'lon']].values\ntree = KDTree(coordinates)\nthreshold = 0.005  # Adjust as needed\n\nbatch_edges = tree.query_pairs(r=threshold)\ntedges = list(batch_edges)\nprint(f\"Number of edges created: {len(tedges)}\")\n\n# Convert to Edge Index Tensor\nedge_index = torch.tensor(tedges, dtype=torch.long).t().contiguous()\nprint(f\"Edge index tensor created with shape: {edge_index.shape}\")\n\n# Step 8: Create PyTorch Geometric Graph Data Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# Save the PyTorch Geometric Data Object\ngraph_data_output_path = '/media/volume1/gat_spatial_training_data_sampled.pt'\ntorch.save(graph_data, graph_data_output_path)\nprint(f\"PyTorch Geometric Data object saved as '{graph_data_output_path}'.\")\n",
  "history_output" : "Cannot run program \"python\" (in directory \"/home/geo2021/gw-workspace/32iw0iugf2m\"): error=2, No such file or directory",
  "history_begin_time" : 1740622682724,
  "history_end_time" : 1740622683452,
  "history_notes" : null,
  "history_process" : "ytgin6",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "2wvkvxza9az",
  "history_input" : "import pandas as pd\nimport torch\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nimport numpy as np\n\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\n\nprint(\"Loading dataset...\")\ndata = pd.read_csv(file_path)\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\n\n# Define Bounding Box (Western US region)\nLAT_MAX, LON_MAX = 48.864715, -120.190430\nLAT_MIN, LON_MIN = 45.798170, -124.145508\n\n# Step 1: Filter Data Within the Bounding Box\nprint(\"Filtering data within the bounding box...\")\ndata = data[\n    (data['lat'] <= LAT_MAX) & (data['lat'] >= LAT_MIN) & \n    (data['lon'] <= LON_MAX) & (data['lon'] >= LON_MIN)\n]\nprint(f\"Filtered dataset with {data.shape[0]} rows.\")\n\n# Step 2: Spatially aware sampling\nsample_size = min(50000, len(data))  # Adjust sample size dynamically\ncoordinates = data[['lat', 'lon']].values\n\nprint(\"Building KDTree for spatial sampling...\")\ntree = KDTree(coordinates)\nindices = [np.random.randint(0, len(data))]  # Start with a random point\n\nwhile len(indices) < sample_size:\n    ref_point = coordinates[indices[-1]]  # Last added point\n    neighbors = tree.query_ball_point(ref_point, r=0.01)  # Get neighbors within radius\n    if len(neighbors) > 1:\n        new_idx = np.random.choice(neighbors)\n        if new_idx not in indices:\n            indices.append(new_idx)\n\nsampled_data = data.iloc[indices]\nprint(f\"Spatially sampled dataset with {sampled_data.shape[0]} rows.\")\n\n# Step 3: Normalize Features\nselected_features = [\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount'\n]\n\nprint(\"Normalizing features...\")\nscaler = StandardScaler()\nsampled_data[selected_features] = scaler.fit_transform(sampled_data[selected_features])\n\n# Step 4: Normalize Labels (swe_value)\nprint(\"Normalizing target variable (swe_value)...\")\nlabel_scaler = StandardScaler()\nsampled_data['swe_value'] = label_scaler.fit_transform(sampled_data['swe_value'].values.reshape(-1, 1))\n\n# Save normalized dataset\noutput_file_path = 'normalized_spatial_sampled_snotel.csv'\nsampled_data.to_csv(output_file_path, index=False)\nprint(f\"Normalized sampled dataset saved as '{output_file_path}'.\")\n\n# Step 5: Create Node Features Tensor\nnode_features = torch.tensor(sampled_data[selected_features].values, dtype=torch.float)\nprint(f\"Node features tensor created with shape: {node_features.shape}\")\n\n# Step 6: Create Labels Tensor\nlabels = torch.tensor(sampled_data['swe_value'].values, dtype=torch.float)\nprint(f\"Labels tensor created with shape: {labels.shape}\")\n\n# Step 7: Build KDTree and Construct Edges\nprint(\"Building KDTree and constructing edges...\")\ncoordinates = sampled_data[['lat', 'lon']].values\ntree = KDTree(coordinates)\nthreshold = 0.005  # Adjust as needed\n\nbatch_edges = tree.query_pairs(r=threshold)\ntedges = list(batch_edges)\nprint(f\"Number of edges created: {len(tedges)}\")\n\n# Convert to Edge Index Tensor\nedge_index = torch.tensor(tedges, dtype=torch.long).t().contiguous()\nprint(f\"Edge index tensor created with shape: {edge_index.shape}\")\n\n# Step 8: Create PyTorch Geometric Graph Data Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# Save the PyTorch Geometric Data Object\ngraph_data_output_path = '/media/volume1/gat_spatial_training_data_sampled.pt'\ntorch.save(graph_data, graph_data_output_path)\nprint(f\"PyTorch Geometric Data object saved as '{graph_data_output_path}'.\")\n",
  "history_output" : "Cannot run program \"python\" (in directory \"/home/geo2021/gw-workspace/2wvkvxza9az\"): error=2, No such file or directory",
  "history_begin_time" : 1740620102574,
  "history_end_time" : 1740620102837,
  "history_notes" : null,
  "history_process" : "ytgin6",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "03ye0dvvm7r",
  "history_input" : "import pandas as pd\nimport torch\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nimport numpy as np\n\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\n\nprint(\"Loading dataset...\")\ndata = pd.read_csv(file_path)\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\n\n# Define Bounding Box (Western US region)\nLAT_MAX, LON_MAX = 48.864715, -120.190430\nLAT_MIN, LON_MIN = 45.798170, -124.145508\n\n# Step 1: Filter Data Within the Bounding Box\nprint(\"Filtering data within the bounding box...\")\ndata = data[\n    (data['lat'] <= LAT_MAX) & (data['lat'] >= LAT_MIN) & \n    (data['lon'] <= LON_MAX) & (data['lon'] >= LON_MIN)\n]\nprint(f\"Filtered dataset with {data.shape[0]} rows.\")\n\n# Step 2: Spatially aware sampling\nsample_size = min(50000, len(data))  # Adjust sample size dynamically\ncoordinates = data[['lat', 'lon']].values\n\nprint(\"Building KDTree for spatial sampling...\")\ntree = KDTree(coordinates)\nindices = [np.random.randint(0, len(data))]  # Start with a random point\n\nwhile len(indices) < sample_size:\n    ref_point = coordinates[indices[-1]]  # Last added point\n    neighbors = tree.query_ball_point(ref_point, r=0.01)  # Get neighbors within radius\n    if len(neighbors) > 1:\n        new_idx = np.random.choice(neighbors)\n        if new_idx not in indices:\n            indices.append(new_idx)\n\nsampled_data = data.iloc[indices]\nprint(f\"Spatially sampled dataset with {sampled_data.shape[0]} rows.\")\n\n# Step 3: Normalize Features\nselected_features = [\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount'\n]\n\nprint(\"Normalizing features...\")\nscaler = StandardScaler()\nsampled_data[selected_features] = scaler.fit_transform(sampled_data[selected_features])\n\n# Step 4: Normalize Labels (swe_value)\nprint(\"Normalizing target variable (swe_value)...\")\nlabel_scaler = StandardScaler()\nsampled_data['swe_value'] = label_scaler.fit_transform(sampled_data['swe_value'].values.reshape(-1, 1))\n\n# Save normalized dataset\noutput_file_path = 'normalized_spatial_sampled_snotel.csv'\nsampled_data.to_csv(output_file_path, index=False)\nprint(f\"Normalized sampled dataset saved as '{output_file_path}'.\")\n\n# Step 5: Create Node Features Tensor\nnode_features = torch.tensor(sampled_data[selected_features].values, dtype=torch.float)\nprint(f\"Node features tensor created with shape: {node_features.shape}\")\n\n# Step 6: Create Labels Tensor\nlabels = torch.tensor(sampled_data['swe_value'].values, dtype=torch.float)\nprint(f\"Labels tensor created with shape: {labels.shape}\")\n\n# Step 7: Build KDTree and Construct Edges\nprint(\"Building KDTree and constructing edges...\")\ncoordinates = sampled_data[['lat', 'lon']].values\ntree = KDTree(coordinates)\nthreshold = 0.005  # Adjust as needed\n\nbatch_edges = tree.query_pairs(r=threshold)\ntedges = list(batch_edges)\nprint(f\"Number of edges created: {len(tedges)}\")\n\n# Convert to Edge Index Tensor\nedge_index = torch.tensor(tedges, dtype=torch.long).t().contiguous()\nprint(f\"Edge index tensor created with shape: {edge_index.shape}\")\n\n# Step 8: Create PyTorch Geometric Graph Data Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# Save the PyTorch Geometric Data Object\ngraph_data_output_path = '/media/volume1/gat_spatial_training_data_sampled.pt'\ntorch.save(graph_data, graph_data_output_path)\nprint(f\"PyTorch Geometric Data object saved as '{graph_data_output_path}'.\")\n",
  "history_output" : "Loading dataset...\n",
  "history_begin_time" : 1740620037506,
  "history_end_time" : 1740620048052,
  "history_notes" : null,
  "history_process" : "ytgin6",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "nN2fnbI1oizL",
  "history_input" : "import numpy as np\nimport torch\nimport pandas as pd\nfrom torch_geometric.data import Data\nfrom haversine import haversine\n\n# Mock file path (change this to actual path if needed)\nfile_path = \"/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv\"\n\ntry:\n    # Load dataset\n    df = pd.read_csv(file_path)\n\n    # Define feature and target columns, removing duplicate 'Elevation'\n    feature_columns = [\n        'air_temperature_observed_f', 'precipitation_amount', \n        'relative_humidity_rmin', 'potential_evapotranspiration',\n        'air_temperature_tmmx', 'relative_humidity_rmax', 'mean_vapor_pressure_deficit',\n        'wind_speed', 'Aspect', 'Curvature', 'Slope', \n        'lat', 'lon', 'cumulative_SWE', 'cumulative_precipitation_amount'\n    ]\n    target_column = 'SWE'\n\n    # Ensure all required columns exist\n    missing_features = [col for col in feature_columns if col not in df.columns]\n    if missing_features:\n        raise ValueError(f\"Missing feature columns in dataset: {missing_features}\")\n\n    if target_column not in df.columns:\n        raise ValueError(f\"Missing target column '{target_column}' in dataset\")\n\n    # Fill NaNs and normalize features\n    features = df[feature_columns].fillna(0).values\n    features = (features - np.mean(features, axis=0)) / np.std(features, axis=0)\n\n    # Normalize target variable\n    targets = df[target_column].fillna(0).values\n    targets = (targets - np.mean(targets)) / np.std(targets)\n\n    # Ensure 'station_name' exists\n    if 'station_name' not in df.columns:\n        raise ValueError(\"Column 'station_name' is missing in dataset\")\n\n    # Create station index mapping\n    station_ids = df['station_name'].dropna().unique()\n    station_to_idx = {station: idx for idx, station in enumerate(station_ids)}\n\n    # Create adjacency list based on spatial distance\n    edge_index = []\n    threshold_km = 50  # Connection range\n\n    # Convert lat/lon to tuples for haversine\n    lat_lon_pairs = df[['lat', 'lon']].dropna().values.tolist()\n\n    for i, row1 in df.iterrows():\n        if pd.isna(row1['station_name']) or row1['station_name'] not in station_to_idx:\n            continue  # Skip missing station names\n\n        for j, row2 in df.iterrows():\n            if i != j and pd.notna(row2['station_name']) and row2['station_name'] in station_to_idx:\n                distance = haversine((row1['lat'], row1['lon']), (row2['lat'], row2['lon']))\n                if distance < threshold_km:  # Connect if within 50km\n                    edge_index.append([station_to_idx[row1['station_name']], station_to_idx[row2['station_name']]])\n\n    # Convert to PyTorch tensors\n    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n    x = torch.tensor(features, dtype=torch.float)\n    y = torch.tensor(targets, dtype=torch.float).view(-1, 1)\n\n    # Create PyTorch Geometric Data object\n    graph_data = Data(x=x, edge_index=edge_index, y=y)\n\n    num_edges = graph_data.edge_index.shape[1]\n    print(f\"Number of edges: {num_edges}\")\n\n    # Save graph data\n    torch.save(graph_data, \"/mnt/data/graph_data.pt\")\n    print(\"Graph data saved successfully.\")\n\nexcept Exception as e:\n    print(f\"Error encountered: {e}\")\n",
  "history_output" : "",
  "history_begin_time" : 1739733484080,
  "history_end_time" : 1739733658796,
  "history_notes" : null,
  "history_process" : "ytgin6",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "TIOMTTAnJX0A",
  "history_input" : "# Write your first Python code in Geoweaver\n\nimport numpy as np\nimport torch\nimport pandas as pd\nimport networkx as nx\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import GCNConv\nfrom haversine import haversine\n\n# Load the dataset\nfile_path = \"/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv\"\ndf = pd.read_csv(file_path)\n\n# Select relevant node features (excluding non-numeric columns)\nfeature_columns = [\n    'air_temperature_observed_f', 'precipitation_amount', \n    'relative_humidity_rmin', 'potential_evapotranspiration',\n    'air_temperature_tmmx', 'relative_humidity_rmax', 'mean_vapor_pressure_deficit',\n    'wind_speed', 'Elevation', 'Aspect', 'Curvature', 'Slope', \n    'lat', 'lon', 'Elevation', 'cumulative_SWE', 'cumulative_precipitation_amount'\n]\ntarget_column = 'SWE'\n\n# Normalize Features\nfeatures = df[feature_columns].fillna(0).values\nfeatures = (features - features.mean(axis=0)) / features.std(axis=0)\n\n# Target variable (SWE)\ntargets = df[target_column].fillna(0).values\ntargets = (targets - targets.mean()) / targets.std()\n\n# Create a station-index mapping\nstation_ids = df['station_name'].unique()\nstation_to_idx = {station: idx for idx, station in enumerate(station_ids)}\n\n# Create adjacency list based on spatial distance\nedge_index = []\nthreshold_km = 50  # Define connection range\n\nfor i, row1 in df.iterrows():\n    for j, row2 in df.iterrows():\n        if i != j:\n            distance = haversine((row1['lat'], row1['lon']), (row2['lat'], row2['lon']))\n            if distance < threshold_km:  # Connect if within 50km\n                edge_index.append([station_to_idx[row1['station_name']], station_to_idx[row2['station_name']]])\n\n# Convert to PyTorch tensors\nedge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\nx = torch.tensor(features, dtype=torch.float)\ny = torch.tensor(targets, dtype=torch.float).view(-1, 1)\n\n# Create PyTorch Geometric Data object\ngraph_data = Data(x=x, edge_index=edge_index, y=y)\n\nnum_edges = graph_data.edge_index.shape[1]\nprint(f\"Number of edges: {num_edges}\")\n\n\ntorch.save(graph_data, \"graph_data.pt\")\nprint(\"Graph data saved to graph_data.pt\")\n\n\n",
  "history_output" : "",
  "history_begin_time" : 1739733064303,
  "history_end_time" : 1739733181906,
  "history_notes" : null,
  "history_process" : "ytgin6",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "nf3dhbWoOcWJ",
  "history_input" : "# Write your first Python code in Geoweaver\n\nimport numpy as np\nimport torch\nimport pandas as pd\nimport networkx as nx\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import GCNConv\nfrom haversine import haversine\n\n# Load the dataset\nfile_path = \"/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv\"\ndf = pd.read_csv(file_path)\n\n# Select relevant node features (excluding non-numeric columns)\nfeature_columns = [\n    'air_temperature_observed_f', 'precipitation_amount', \n    'relative_humidity_rmin', 'potential_evapotranspiration',\n    'air_temperature_tmmx', 'relative_humidity_rmax', 'mean_vapor_pressure_deficit',\n    'wind_speed', 'Elevation', 'Aspect', 'Curvature', 'Slope', \n    'lat', 'lon', 'Elevation', 'cumulative_SWE', 'cumulative_precipitation_amount'\n]\ntarget_column = 'SWE'\n\n# Normalize Features\nfeatures = df[feature_columns].fillna(0).values\nfeatures = (features - features.mean(axis=0)) / features.std(axis=0)\n\n# Target variable (SWE)\ntargets = df[target_column].fillna(0).values\ntargets = (targets - targets.mean()) / targets.std()\n\n# Create a station-index mapping\nstation_ids = df['station_name'].unique()\nstation_to_idx = {station: idx for idx, station in enumerate(station_ids)}\n\n# Create adjacency list based on spatial distance\nedge_index = []\nthreshold_km = 50  # Define connection range\n\nfor i, row1 in df.iterrows():\n    for j, row2 in df.iterrows():\n        if i != j:\n            distance = haversine((row1['lat'], row1['lon']), (row2['lat'], row2['lon']))\n            if distance < threshold_km:  # Connect if within 50km\n                edge_index.append([station_to_idx[row1['station_name']], station_to_idx[row2['station_name']]])\n\n# Convert to PyTorch tensors\nedge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\nx = torch.tensor(features, dtype=torch.float)\ny = torch.tensor(targets, dtype=torch.float).view(-1, 1)\n\n# Create PyTorch Geometric Data object\ngraph_data = Data(x=x, edge_index=edge_index, y=y)\n\nnum_edges = graph_data.edge_index.shape[1]\nprint(f\"Number of edges: {num_edges}\")\n\n\ntorch.save(graph_data, \"graph_data.pt\")\nprint(\"Graph data saved to graph_data.pt\")\n\n\n",
  "history_output" : "",
  "history_begin_time" : 1739732820610,
  "history_end_time" : 1739732918448,
  "history_notes" : null,
  "history_process" : "ytgin6",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "yK1QDZKMRa9n",
  "history_input" : "# Write your first Python code in Geoweaver\n\nimport torch\nimport pandas as pd\nimport networkx as nx\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import GCNConv\nfrom haversine import haversine\n\n# Load the dataset\nfile_path = \"/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv\"\ndf = pd.read_csv(file_path)\n\n# Select relevant node features (excluding non-numeric columns)\nfeature_columns = [\n    'air_temperature_observed_f', 'precipitation_amount', \n    'relative_humidity_rmin', 'potential_evapotranspiration',\n    'air_temperature_tmmx', 'relative_humidity_rmax', 'mean_vapor_pressure_deficit',\n    'wind_speed', 'Elevation', 'Aspect', 'Curvature', 'Slope', \n    'lat', 'lon', 'Elevation', 'cumulative_SWE', 'cumulative_precipitation_amount'\n]\ntarget_column = 'SWE'\n\n# Normalize Features\nfeatures = df[feature_columns].fillna(0).values\nfeatures = (features - features.mean(axis=0)) / features.std(axis=0)\n\n# Target variable (SWE)\ntargets = df[target_column].fillna(0).values\ntargets = (targets - targets.mean()) / targets.std()\n\n# Create a station-index mapping\nstation_ids = df['station_name'].unique()\nstation_to_idx = {station: idx for idx, station in enumerate(station_ids)}\n\n# Create adjacency list based on spatial distance\nedge_index = []\nthreshold_km = 50  # Define connection range\n\nfor i, row1 in df.iterrows():\n    for j, row2 in df.iterrows():\n        if i != j:\n            distance = haversine((row1['lat'], row1['lon']), (row2['lat'], row2['lon']))\n            if distance < threshold_km:  # Connect if within 50km\n                edge_index.append([station_to_idx[row1['station_name']], station_to_idx[row2['station_name']]])\n\n# Convert to PyTorch tensors\nedge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\nx = torch.tensor(features, dtype=torch.float)\ny = torch.tensor(targets, dtype=torch.float).view(-1, 1)\n\n# Create PyTorch Geometric Data object\ngraph_data = Data(x=x, edge_index=edge_index, y=y)\n\nnum_edges = graph_data.edge_index.shape[1]\nprint(f\"Number of edges: {num_edges}\")\n\n\ntorch.save(graph_data, \"graph_data.pt\")\nprint(\"Graph data saved to graph_data.pt\")\n\n\n",
  "history_output" : "Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp-a34b3233.so.1 library.\n\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n",
  "history_begin_time" : 1739732755815,
  "history_end_time" : 1739732756675,
  "history_notes" : null,
  "history_process" : "ytgin6",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "ruYX3dlw4Nuq",
  "history_input" : "# Write your first Python code in Geoweaver\n\nimport torch\nimport pandas as pd\nimport networkx as nx\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import GCNConv\nfrom haversine import haversine\n\n# Load the dataset\nfile_path = \"/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv\"\ndf = pd.read_csv(file_path)\n\n# Select relevant node features (excluding non-numeric columns)\nfeature_columns = [\n    'air_temperature_observed_f', 'precipitation_amount', \n    'relative_humidity_rmin', 'potential_evapotranspiration',\n    'air_temperature_tmmx', 'relative_humidity_rmax', 'mean_vapor_pressure_deficit',\n    'wind_speed', 'Elevation', 'Aspect', 'Curvature', 'Slope', \n    'lat', 'lon', 'Elevation', 'cumulative_SWE', 'cumulative_precipitation_amount'\n]\ntarget_column = 'SWE'\n\n# Normalize Features\nfeatures = df[feature_columns].fillna(0).values\nfeatures = (features - features.mean(axis=0)) / features.std(axis=0)\n\n# Target variable (SWE)\ntargets = df[target_column].fillna(0).values\ntargets = (targets - targets.mean()) / targets.std()\n\n# Create a station-index mapping\nstation_ids = df['station_name'].unique()\nstation_to_idx = {station: idx for idx, station in enumerate(station_ids)}\n\n# Create adjacency list based on spatial distance\nedge_index = []\nthreshold_km = 50  # Define connection range\n\nfor i, row1 in df.iterrows():\n    for j, row2 in df.iterrows():\n        if i != j:\n            distance = haversine((row1['lat'], row1['lon']), (row2['lat'], row2['lon']))\n            if distance < threshold_km:  # Connect if within 50km\n                edge_index.append([station_to_idx[row1['station_name']], station_to_idx[row2['station_name']]])\n\n# Convert to PyTorch tensors\nedge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\nx = torch.tensor(features, dtype=torch.float)\ny = torch.tensor(targets, dtype=torch.float).view(-1, 1)\n\n# Create PyTorch Geometric Data object\ngraph_data = Data(x=x, edge_index=edge_index, y=y)\n\nnum_edges = graph_data.edge_index.shape[1]\nprint(f\"Number of edges: {num_edges}\")\n\n\ntorch.save(graph_data, \"graph_data.pt\")\nprint(\"Graph data saved to graph_data.pt\")\n\n\n",
  "history_output" : "Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp-a34b3233.so.1 library.\n\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n",
  "history_begin_time" : 1739732592965,
  "history_end_time" : 1739732593801,
  "history_notes" : null,
  "history_process" : "ytgin6",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "ICcOapfqMw8c",
  "history_input" : "# Write your first Python code in Geoweaver\n\nimport torch\nimport pandas as pd\nimport networkx as nx\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import GCNConv\nfrom haversine import haversine\n\n# Load the dataset\nfile_path = \"/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv\"\ndf = pd.read_csv(file_path)\n\n# Select relevant node features (excluding non-numeric columns)\nfeature_columns = [\n    'air_temperature_observed_f', 'precipitation_amount', \n    'relative_humidity_rmin', 'potential_evapotranspiration',\n    'air_temperature_tmmx', 'relative_humidity_rmax', 'mean_vapor_pressure_deficit',\n    'wind_speed', 'Elevation', 'Aspect', 'Curvature', 'Slope', \n    'lat', 'lon', 'Elevation', 'cumulative_SWE', 'cumulative_precipitation_amount'\n]\ntarget_column = 'SWE'\n\n# Normalize Features\nfeatures = df[feature_columns].fillna(0).values\nfeatures = (features - features.mean(axis=0)) / features.std(axis=0)\n\n# Target variable (SWE)\ntargets = df[target_column].fillna(0).values\ntargets = (targets - targets.mean()) / targets.std()\n\n# Create a station-index mapping\nstation_ids = df['station_name'].unique()\nstation_to_idx = {station: idx for idx, station in enumerate(station_ids)}\n\n# Create adjacency list based on spatial distance\nedge_index = []\nthreshold_km = 50  # Define connection range\n\nfor i, row1 in df.iterrows():\n    for j, row2 in df.iterrows():\n        if i != j:\n            distance = haversine((row1['lat'], row1['lon']), (row2['lat'], row2['lon']))\n            if distance < threshold_km:  # Connect if within 50km\n                edge_index.append([station_to_idx[row1['station_name']], station_to_idx[row2['station_name']]])\n\n# Convert to PyTorch tensors\nedge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\nx = torch.tensor(features, dtype=torch.float)\ny = torch.tensor(targets, dtype=torch.float).view(-1, 1)\n\n# Create PyTorch Geometric Data object\ngraph_data = Data(x=x, edge_index=edge_index, y=y)\n\nnum_edges = graph_data.edge_index.shape[1]\nprint(f\"Number of edges: {num_edges}\")\n\n\ntorch.save(graph_data, \"graph_data.pt\")\nprint(\"Graph data saved to graph_data.pt\")\n\n\n",
  "history_output" : "Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp-a34b3233.so.1 library.\n\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n",
  "history_begin_time" : 1739732520391,
  "history_end_time" : 1739732521319,
  "history_notes" : null,
  "history_process" : "ytgin6",
  "host_id" : null,
  "indicator" : "Failed"
}]