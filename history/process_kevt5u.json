[{
  "history_id" : "XqIfrV41D2Yq",
  "history_input" : "import numpy as np\nimport torch\nimport warnings\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport pandas as pd\nfrom torch_geometric.nn import SAGEConv, BatchNorm\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch_geometric.utils import add_self_loops\n\nwarnings.filterwarnings(\"ignore\")\n\n# Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Loss Function\nclass CustomLoss(nn.Module):\n    def __init__(self, alpha=0.7):\n        super(CustomLoss, self).__init__()\n        self.mse = nn.MSELoss()\n        self.smooth_l1 = nn.SmoothL1Loss(beta=1.0)\n        self.alpha = alpha\n\n    def forward(self, predictions, targets):\n        return self.alpha * self.mse(predictions, targets) + (1 - self.alpha) * self.smooth_l1(predictions, targets)\n\n# GraphSAGE Model\nclass GraphSAGE(nn.Module):\n    def __init__(self, input_dim, hidden_dim=256, output_dim=1, dropout=0.2):\n        super(GraphSAGE, self).__init__()\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"max\")\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\n        self.conv5 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\n\n        self.bn1 = BatchNorm(hidden_dim)\n        self.bn2 = BatchNorm(hidden_dim)\n        self.bn3 = BatchNorm(hidden_dim)\n        self.bn4 = BatchNorm(hidden_dim)\n        self.bn5 = BatchNorm(hidden_dim)\n\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\n        x = self.dropout(x)\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\n        x = self.dropout(x)\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\n        x = self.dropout(x)\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\n        x = self.dropout(x)\n        x = F.leaky_relu(self.bn5(self.conv5(x, edge_index)))\n        x = F.leaky_relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Load Graph\ndef load_graph(path):\n    from torch_geometric.data import Data\n    from torch.serialization import add_safe_globals\n    add_safe_globals([Data])  \n\n    with open(path, 'rb') as f:\n        g_data = torch.load(f, weights_only=False)  \n    print(f\"‚úÖ Loaded graph from {path}\")\n    print(f\"Nodes: {g_data.num_nodes}, Edges: {g_data.num_edges}, Features: {g_data.x.shape}\")\n    return g_data\n\n\n\n# Train Step\ndef train(model, optimizer, loss_fn, data, mask):\n    model.train()\n    optimizer.zero_grad()\n    preds = model(data)[mask].squeeze()\n    loss = loss_fn(preds, data.y[mask])\n    loss.backward()\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n    optimizer.step()\n    return loss.item()\n\n# Test Step\ndef test(model, loss_fn, data, mask, y_scaler):\n    model.eval()\n    with torch.no_grad():\n        preds = model(data)[mask].squeeze().cpu().numpy()\n        actuals = data.y[mask].cpu().numpy()\n\n        preds_original = y_scaler.inverse_transform(preds.reshape(-1, 1)).flatten()\n        actuals_original = y_scaler.inverse_transform(actuals.reshape(-1, 1)).flatten()\n\n        pd.DataFrame({\n            'predicted_swe': preds_original,\n            'actual_swe': actuals_original\n        }).to_csv(\"predicted_vs_actual.csv\", index=False)\n\n        mse = mean_squared_error(actuals_original, preds_original)\n        r2 = r2_score(actuals_original, preds_original)\n        mae = mean_absolute_error(actuals_original, preds_original)\n        rmse = mean_squared_error(actuals_original, preds_original, squared=False)\n\n    return mse, r2, rmse, mae\n\n# Main\ndef main():\n    g_path = \"/media/volume1/gnn_graph_with_time_all_states_2.pt\"\n    data = load_graph(g_path)\n\n    # Normalize features\n    x_scaler = MinMaxScaler()\n    data.x = torch.tensor(x_scaler.fit_transform(data.x.cpu()), dtype=torch.float).to(device)\n\n    # Normalize labels\n    y_scaler = MinMaxScaler()\n    data.y = torch.tensor(y_scaler.fit_transform(data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\n\n    # Add self-loops\n    data.edge_index, _ = add_self_loops(data.edge_index, num_nodes=data.num_nodes)\n\n    # Train/Test Split\n    indices = torch.randperm(data.num_nodes)\n    train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n    train_mask[indices[:int(0.8 * data.num_nodes)]] = True\n    test_mask = ~train_mask\n\n    print(f\"üîÅ Training on {train_mask.sum().item()}, Testing on {test_mask.sum().item()}\")\n\n    model = GraphSAGE(input_dim=data.num_features).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-4)\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=25)\n    criterion = CustomLoss(alpha=0.7)\n\n    best_r2 = -float(\"inf\")\n    wait = 0\n    patience = 200\n\n    print(\"üöÄ Starting training...\")\n    for epoch in range(1, 2001):\n        loss = train(model, optimizer, criterion, data, train_mask)\n\n        if epoch % 100 == 0 or epoch == 1:\n            mse, r2, rmse, mae = test(model, criterion, data, test_mask, y_scaler)\n            print(f\"Epoch {epoch} ‚û§ Train Loss: {loss:.4f} | R¬≤: {r2:.4f} | RMSE: {rmse:.4f} | MAE: {mae:.4f}\")\n\n            if r2 > best_r2:\n                best_r2 = r2\n                wait = 0\n            else:\n                wait += 1\n\n            if wait >= patience:\n                print(f\"üõë Early stopping at epoch {epoch}\")\n                break\n\n    print(f\"\\n‚úÖ Best R¬≤: {best_r2:.4f}\")\n\nif __name__ == \"__main__\":\n    main()\n",
  "history_output" : "Using device: cpu\n‚úÖ Loaded graph from /media/volume1/gnn_graph_with_time_all_states_2.pt\nNodes: 10623, Edges: 63610, Features: torch.Size([10623, 25])\nüîÅ Training on 8498, Testing on 2125\nüöÄ Starting training...\nEpoch 1 ‚û§ Train Loss: 0.0210 | R¬≤: -0.3007 | RMSE: 3.4407 | MAE: 2.8439\nEpoch 100 ‚û§ Train Loss: 0.0009 | R¬≤: 0.8786 | RMSE: 1.0511 | MAE: 0.5777\nEpoch 200 ‚û§ Train Loss: 0.0005 | R¬≤: 0.8817 | RMSE: 1.0377 | MAE: 0.4652\nEpoch 300 ‚û§ Train Loss: 0.0004 | R¬≤: 0.8819 | RMSE: 1.0368 | MAE: 0.3884\nEpoch 400 ‚û§ Train Loss: 0.0003 | R¬≤: 0.8541 | RMSE: 1.1522 | MAE: 0.3932\nEpoch 500 ‚û§ Train Loss: 0.0002 | R¬≤: 0.8796 | RMSE: 1.0467 | MAE: 0.3544\nEpoch 600 ‚û§ Train Loss: 0.0002 | R¬≤: 0.8777 | RMSE: 1.0550 | MAE: 0.3467\nEpoch 700 ‚û§ Train Loss: 0.0001 | R¬≤: 0.8535 | RMSE: 1.1545 | MAE: 0.3581\nEpoch 800 ‚û§ Train Loss: 0.0001 | R¬≤: 0.8186 | RMSE: 1.2848 | MAE: 0.4763\nEpoch 900 ‚û§ Train Loss: 0.0001 | R¬≤: 0.8587 | RMSE: 1.1340 | MAE: 0.3611\nEpoch 1000 ‚û§ Train Loss: 0.0001 | R¬≤: 0.8581 | RMSE: 1.1364 | MAE: 0.3672\nEpoch 1100 ‚û§ Train Loss: 0.0001 | R¬≤: 0.7863 | RMSE: 1.3947 | MAE: 0.4282\nEpoch 1200 ‚û§ Train Loss: 0.0001 | R¬≤: 0.8270 | RMSE: 1.2548 | MAE: 0.4120\nEpoch 1300 ‚û§ Train Loss: 0.0001 | R¬≤: 0.8570 | RMSE: 1.1407 | MAE: 0.3728\nEpoch 1400 ‚û§ Train Loss: 0.0001 | R¬≤: 0.8546 | RMSE: 1.1506 | MAE: 0.3580\nEpoch 1500 ‚û§ Train Loss: 0.0000 | R¬≤: 0.8686 | RMSE: 1.0935 | MAE: 0.3777\nEpoch 1600 ‚û§ Train Loss: 0.0001 | R¬≤: 0.8658 | RMSE: 1.1051 | MAE: 0.3388\nEpoch 1700 ‚û§ Train Loss: 0.0000 | R¬≤: 0.8467 | RMSE: 1.1813 | MAE: 0.3591\nEpoch 1800 ‚û§ Train Loss: 0.0000 | R¬≤: 0.8745 | RMSE: 1.0689 | MAE: 0.3507\nEpoch 1900 ‚û§ Train Loss: 0.0000 | R¬≤: 0.9021 | RMSE: 0.9442 | MAE: 0.3185\nEpoch 2000 ‚û§ Train Loss: 0.0001 | R¬≤: 0.8336 | RMSE: 1.2306 | MAE: 0.3775\n‚úÖ Best R¬≤: 0.9021\n",
  "history_begin_time" : 1746109074192,
  "history_end_time" : 1746116296078,
  "history_notes" : null,
  "history_process" : "kevt5u",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "AM6LEerwgCQ7",
  "history_input" : "import numpy as np\nimport torch\nimport warnings\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport pandas as pd\nfrom torch_geometric.nn import SAGEConv, BatchNorm\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch_geometric.utils import add_self_loops\n\nwarnings.filterwarnings(\"ignore\")\n\n# Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Loss Function\nclass CustomLoss(nn.Module):\n    def __init__(self, alpha=0.7):\n        super(CustomLoss, self).__init__()\n        self.mse = nn.MSELoss()\n        self.smooth_l1 = nn.SmoothL1Loss(beta=1.0)\n        self.alpha = alpha\n\n    def forward(self, predictions, targets):\n        return self.alpha * self.mse(predictions, targets) + (1 - self.alpha) * self.smooth_l1(predictions, targets)\n\n# GraphSAGE Model\nclass GraphSAGE(nn.Module):\n    def __init__(self, input_dim, hidden_dim=256, output_dim=1, dropout=0.2):\n        super(GraphSAGE, self).__init__()\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"max\")\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\n        self.conv5 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\n\n        self.bn1 = BatchNorm(hidden_dim)\n        self.bn2 = BatchNorm(hidden_dim)\n        self.bn3 = BatchNorm(hidden_dim)\n        self.bn4 = BatchNorm(hidden_dim)\n        self.bn5 = BatchNorm(hidden_dim)\n\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\n        x = self.dropout(x)\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\n        x = self.dropout(x)\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\n        x = self.dropout(x)\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\n        x = self.dropout(x)\n        x = F.leaky_relu(self.bn5(self.conv5(x, edge_index)))\n        x = F.leaky_relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Load Graph\ndef load_graph(path):\n    from torch_geometric.data import Data\n    from torch.serialization import add_safe_globals\n    add_safe_globals([Data])  \n\n    with open(path, 'rb') as f:\n        g_data = torch.load(f, weights_only=False)  \n    print(f\"‚úÖ Loaded graph from {path}\")\n    print(f\"Nodes: {g_data.num_nodes}, Edges: {g_data.num_edges}, Features: {g_data.x.shape}\")\n    return g_data\n\n\n\n# Train Step\ndef train(model, optimizer, loss_fn, data, mask):\n    model.train()\n    optimizer.zero_grad()\n    preds = model(data)[mask].squeeze()\n    loss = loss_fn(preds, data.y[mask])\n    loss.backward()\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n    optimizer.step()\n    return loss.item()\n\n# Test Step\ndef test(model, loss_fn, data, mask, y_scaler):\n    model.eval()\n    with torch.no_grad():\n        preds = model(data)[mask].squeeze().cpu().numpy()\n        actuals = data.y[mask].cpu().numpy()\n\n        preds_original = y_scaler.inverse_transform(preds.reshape(-1, 1)).flatten()\n        actuals_original = y_scaler.inverse_transform(actuals.reshape(-1, 1)).flatten()\n\n        pd.DataFrame({\n            'predicted_swe': preds_original,\n            'actual_swe': actuals_original\n        }).to_csv(\"predicted_vs_actual.csv\", index=False)\n\n        mse = mean_squared_error(actuals_original, preds_original)\n        r2 = r2_score(actuals_original, preds_original)\n        mae = mean_absolute_error(actuals_original, preds_original)\n        rmse = mean_squared_error(actuals_original, preds_original, squared=False)\n\n    return mse, r2, rmse, mae\n\n# Main\ndef main():\n    g_path = \"/media/volume1/gnn_graph_with_time_all_states_2.pt\"\n    data = load_graph(g_path)\n\n    # Normalize features\n    x_scaler = MinMaxScaler()\n    data.x = torch.tensor(x_scaler.fit_transform(data.x.cpu()), dtype=torch.float).to(device)\n\n    # Normalize labels\n    y_scaler = MinMaxScaler()\n    data.y = torch.tensor(y_scaler.fit_transform(data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\n\n    # Add self-loops\n    data.edge_index, _ = add_self_loops(data.edge_index, num_nodes=data.num_nodes)\n\n    # Train/Test Split\n    indices = torch.randperm(data.num_nodes)\n    train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n    train_mask[indices[:int(0.8 * data.num_nodes)]] = True\n    test_mask = ~train_mask\n\n    print(f\"üîÅ Training on {train_mask.sum().item()}, Testing on {test_mask.sum().item()}\")\n\n    model = GraphSAGE(input_dim=data.num_features).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-4)\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=25)\n    criterion = CustomLoss(alpha=0.7)\n\n    best_r2 = -float(\"inf\")\n    wait = 0\n    patience = 200\n\n    print(\"üöÄ Starting training...\")\n    for epoch in range(1, 2001):\n        loss = train(model, optimizer, criterion, data, train_mask)\n\n        if epoch % 100 == 0 or epoch == 1:\n            mse, r2, rmse, mae = test(model, criterion, data, test_mask, y_scaler)\n            print(f\"Epoch {epoch} ‚û§ Train Loss: {loss:.4f} | R¬≤: {r2:.4f} | RMSE: {rmse:.4f} | MAE: {mae:.4f}\")\n\n            if r2 > best_r2:\n                best_r2 = r2\n                wait = 0\n            else:\n                wait += 1\n\n            if wait >= patience:\n                print(f\"üõë Early stopping at epoch {epoch}\")\n                break\n\n    print(f\"\\n‚úÖ Best R¬≤: {best_r2:.4f}\")\n\nif __name__ == \"__main__\":\n    main()\n",
  "history_output" : "Using device: cpu\n‚úÖ Loaded graph from /media/volume1/gnn_graph_with_time_all_states_2.pt\nNodes: 19885, Edges: 119232, Features: torch.Size([19885, 26])\nüîÅ Training on 15908, Testing on 3977\nüöÄ Starting training...\nEpoch 1 ‚û§ Train Loss: 0.0322 | R¬≤: -0.5353 | RMSE: 4.6865 | MAE: 4.0909\nEpoch 100 ‚û§ Train Loss: 0.0009 | R¬≤: 0.7840 | RMSE: 1.7578 | MAE: 0.7679\nEpoch 200 ‚û§ Train Loss: 0.0005 | R¬≤: 0.8257 | RMSE: 1.5791 | MAE: 0.6015\nEpoch 300 ‚û§ Train Loss: 0.0003 | R¬≤: 0.8024 | RMSE: 1.6814 | MAE: 0.5885\nEpoch 400 ‚û§ Train Loss: 0.0002 | R¬≤: 0.8004 | RMSE: 1.6896 | MAE: 0.5730\nEpoch 500 ‚û§ Train Loss: 0.0002 | R¬≤: 0.7984 | RMSE: 1.6981 | MAE: 0.5534\nEpoch 600 ‚û§ Train Loss: 0.0002 | R¬≤: 0.8003 | RMSE: 1.6902 | MAE: 0.5365\nEpoch 700 ‚û§ Train Loss: 0.0002 | R¬≤: 0.8502 | RMSE: 1.4640 | MAE: 0.4594\nEpoch 800 ‚û§ Train Loss: 0.0001 | R¬≤: 0.8674 | RMSE: 1.3775 | MAE: 0.4295\nEpoch 900 ‚û§ Train Loss: 0.0001 | R¬≤: 0.8757 | RMSE: 1.3332 | MAE: 0.4103\nEpoch 1000 ‚û§ Train Loss: 0.0001 | R¬≤: 0.8653 | RMSE: 1.3880 | MAE: 0.4299\nEpoch 1100 ‚û§ Train Loss: 0.0001 | R¬≤: 0.8617 | RMSE: 1.4066 | MAE: 0.4538\nEpoch 1200 ‚û§ Train Loss: 0.0001 | R¬≤: 0.8437 | RMSE: 1.4955 | MAE: 0.4508\nEpoch 1300 ‚û§ Train Loss: 0.0001 | R¬≤: 0.8490 | RMSE: 1.4699 | MAE: 0.4832\nEpoch 1400 ‚û§ Train Loss: 0.0001 | R¬≤: 0.8559 | RMSE: 1.4356 | MAE: 0.4349\nEpoch 1500 ‚û§ Train Loss: 0.0000 | R¬≤: 0.8485 | RMSE: 1.4721 | MAE: 0.4619\nEpoch 1600 ‚û§ Train Loss: 0.0000 | R¬≤: 0.8687 | RMSE: 1.3705 | MAE: 0.4595\nEpoch 1700 ‚û§ Train Loss: 0.0001 | R¬≤: 0.8696 | RMSE: 1.3659 | MAE: 0.4185\nEpoch 1800 ‚û§ Train Loss: 0.0004 | R¬≤: 0.7456 | RMSE: 1.9076 | MAE: 0.6223\nEpoch 1900 ‚û§ Train Loss: 0.0002 | R¬≤: 0.7205 | RMSE: 1.9996 | MAE: 0.6099\nEpoch 2000 ‚û§ Train Loss: 0.0002 | R¬≤: 0.7062 | RMSE: 2.0502 | MAE: 0.6137\n‚úÖ Best R¬≤: 0.8757\n",
  "history_begin_time" : 1746108028569,
  "history_end_time" : 1746118002068,
  "history_notes" : null,
  "history_process" : "kevt5u",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "OW8rAxRBKdJM",
  "history_input" : "import numpy as np\nimport torch\nimport warnings\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport pandas as pd\nfrom torch_geometric.nn import SAGEConv, BatchNorm\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch_geometric.utils import add_self_loops\n\nwarnings.filterwarnings(\"ignore\")\n\n# Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Loss Function\nclass CustomLoss(nn.Module):\n    def __init__(self, alpha=0.7):\n        super(CustomLoss, self).__init__()\n        self.mse = nn.MSELoss()\n        self.smooth_l1 = nn.SmoothL1Loss(beta=1.0)\n        self.alpha = alpha\n\n    def forward(self, predictions, targets):\n        return self.alpha * self.mse(predictions, targets) + (1 - self.alpha) * self.smooth_l1(predictions, targets)\n\n# GraphSAGE Model\nclass GraphSAGE(nn.Module):\n    def __init__(self, input_dim, hidden_dim=256, output_dim=1, dropout=0.2):\n        super(GraphSAGE, self).__init__()\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"max\")\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\n        self.conv5 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\n\n        self.bn1 = BatchNorm(hidden_dim)\n        self.bn2 = BatchNorm(hidden_dim)\n        self.bn3 = BatchNorm(hidden_dim)\n        self.bn4 = BatchNorm(hidden_dim)\n        self.bn5 = BatchNorm(hidden_dim)\n\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\n        x = self.dropout(x)\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\n        x = self.dropout(x)\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\n        x = self.dropout(x)\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\n        x = self.dropout(x)\n        x = F.leaky_relu(self.bn5(self.conv5(x, edge_index)))\n        x = F.leaky_relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Load Graph\ndef load_graph(path):\n    from torch_geometric.data import Data\n    from torch.serialization import add_safe_globals\n    add_safe_globals([Data])  \n\n    with open(path, 'rb') as f:\n        g_data = torch.load(f, weights_only=False)  \n    print(f\"‚úÖ Loaded graph from {path}\")\n    print(f\"Nodes: {g_data.num_nodes}, Edges: {g_data.num_edges}, Features: {g_data.x.shape}\")\n    return g_data\n\n\n\n# Train Step\ndef train(model, optimizer, loss_fn, data, mask):\n    model.train()\n    optimizer.zero_grad()\n    preds = model(data)[mask].squeeze()\n    loss = loss_fn(preds, data.y[mask])\n    loss.backward()\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n    optimizer.step()\n    return loss.item()\n\n# Test Step\ndef test(model, loss_fn, data, mask, y_scaler):\n    model.eval()\n    with torch.no_grad():\n        preds = model(data)[mask].squeeze().cpu().numpy()\n        actuals = data.y[mask].cpu().numpy()\n\n        preds_original = y_scaler.inverse_transform(preds.reshape(-1, 1)).flatten()\n        actuals_original = y_scaler.inverse_transform(actuals.reshape(-1, 1)).flatten()\n\n        pd.DataFrame({\n            'predicted_swe': preds_original,\n            'actual_swe': actuals_original\n        }).to_csv(\"predicted_vs_actual.csv\", index=False)\n\n        mse = mean_squared_error(actuals_original, preds_original)\n        r2 = r2_score(actuals_original, preds_original)\n        mae = mean_absolute_error(actuals_original, preds_original)\n        rmse = mean_squared_error(actuals_original, preds_original, squared=False)\n\n    return mse, r2, rmse, mae\n\n# Main\ndef main():\n    g_path = \"/media/volume1/gnn_graph_with_time_all_states.pt\"\n    data = load_graph(g_path)\n\n    # Normalize features\n    x_scaler = MinMaxScaler()\n    data.x = torch.tensor(x_scaler.fit_transform(data.x.cpu()), dtype=torch.float).to(device)\n\n    # Normalize labels\n    y_scaler = MinMaxScaler()\n    data.y = torch.tensor(y_scaler.fit_transform(data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\n\n    # Add self-loops\n    data.edge_index, _ = add_self_loops(data.edge_index, num_nodes=data.num_nodes)\n\n    # Train/Test Split\n    indices = torch.randperm(data.num_nodes)\n    train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n    train_mask[indices[:int(0.8 * data.num_nodes)]] = True\n    test_mask = ~train_mask\n\n    print(f\"üîÅ Training on {train_mask.sum().item()}, Testing on {test_mask.sum().item()}\")\n\n    model = GraphSAGE(input_dim=data.num_features).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-4)\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=25)\n    criterion = CustomLoss(alpha=0.7)\n\n    best_r2 = -float(\"inf\")\n    wait = 0\n    patience = 200\n\n    print(\"üöÄ Starting training...\")\n    for epoch in range(1, 2001):\n        loss = train(model, optimizer, criterion, data, train_mask)\n\n        if epoch % 100 == 0 or epoch == 1:\n            mse, r2, rmse, mae = test(model, criterion, data, test_mask, y_scaler)\n            print(f\"Epoch {epoch} ‚û§ Train Loss: {loss:.4f} | R¬≤: {r2:.4f} | RMSE: {rmse:.4f} | MAE: {mae:.4f}\")\n\n            if r2 > best_r2:\n                best_r2 = r2\n                wait = 0\n            else:\n                wait += 1\n\n            if wait >= patience:\n                print(f\"üõë Early stopping at epoch {epoch}\")\n                break\n\n    print(f\"\\n‚úÖ Best R¬≤: {best_r2:.4f}\")\n\nif __name__ == \"__main__\":\n    main()\n",
  "history_output" : "Using device: cpu\n‚úÖ Loaded graph from /media/volume1/gnn_graph_with_time_all_states.pt\nNodes: 68557, Edges: 411292, Features: torch.Size([68557, 26])\nüîÅ Training on 54845, Testing on 13712\nüöÄ Starting training...\nEpoch 1 ‚û§ Train Loss: 0.0351 | R¬≤: -0.3160 | RMSE: 4.9176 | MAE: 2.4434\nEpoch 100 ‚û§ Train Loss: 0.0009 | R¬≤: 0.7346 | RMSE: 2.2084 | MAE: 0.9921\nEpoch 200 ‚û§ Train Loss: 0.0006 | R¬≤: 0.8031 | RMSE: 1.9019 | MAE: 0.7238\nEpoch 300 ‚û§ Train Loss: 0.0004 | R¬≤: 0.8112 | RMSE: 1.8627 | MAE: 0.6462\nEpoch 400 ‚û§ Train Loss: 0.0003 | R¬≤: 0.8091 | RMSE: 1.8727 | MAE: 0.6316\nEpoch 500 ‚û§ Train Loss: 0.0003 | R¬≤: 0.8192 | RMSE: 1.8225 | MAE: 0.5943\nEpoch 600 ‚û§ Train Loss: 0.0002 | R¬≤: 0.8327 | RMSE: 1.7532 | MAE: 0.5268\nEpoch 700 ‚û§ Train Loss: 0.0002 | R¬≤: 0.8279 | RMSE: 1.7785 | MAE: 0.5258\nEpoch 800 ‚û§ Train Loss: 0.0002 | R¬≤: 0.8309 | RMSE: 1.7629 | MAE: 0.5149\nEpoch 900 ‚û§ Train Loss: 0.0001 | R¬≤: 0.8452 | RMSE: 1.6868 | MAE: 0.4792\nEpoch 1000 ‚û§ Train Loss: 0.0001 | R¬≤: 0.8685 | RMSE: 1.5543 | MAE: 0.4288\nEpoch 1100 ‚û§ Train Loss: 0.0001 | R¬≤: 0.8654 | RMSE: 1.5726 | MAE: 0.4449\nEpoch 1200 ‚û§ Train Loss: 0.0001 | R¬≤: 0.8878 | RMSE: 1.4360 | MAE: 0.4114\nEpoch 1300 ‚û§ Train Loss: 0.0001 | R¬≤: 0.9043 | RMSE: 1.3263 | MAE: 0.3979\nEpoch 1400 ‚û§ Train Loss: 0.0001 | R¬≤: 0.9089 | RMSE: 1.2941 | MAE: 0.3819\nEpoch 1500 ‚û§ Train Loss: 0.0001 | R¬≤: 0.8768 | RMSE: 1.5045 | MAE: 0.4298\nEpoch 1600 ‚û§ Train Loss: 0.0001 | R¬≤: 0.8644 | RMSE: 1.5784 | MAE: 0.4418\nEpoch 1700 ‚û§ Train Loss: 0.0001 | R¬≤: 0.9431 | RMSE: 1.0228 | MAE: 0.3141\nEpoch 1800 ‚û§ Train Loss: 0.0003 | R¬≤: 0.8105 | RMSE: 1.8661 | MAE: 0.4996\nEpoch 1900 ‚û§ Train Loss: 0.0001 | R¬≤: 0.8493 | RMSE: 1.6640 | MAE: 0.4587\nEpoch 2000 ‚û§ Train Loss: 0.0001 | R¬≤: 0.8947 | RMSE: 1.3910 | MAE: 0.3835\n‚úÖ Best R¬≤: 0.9431\n",
  "history_begin_time" : 1746106176943,
  "history_end_time" : 1746125764967,
  "history_notes" : null,
  "history_process" : "kevt5u",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "EJxONifkKRfo",
  "history_input" : "import numpy as np\nimport torch\nimport warnings\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport pandas as pd\nfrom torch_geometric.nn import SAGEConv, BatchNorm\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch_geometric.utils import add_self_loops\n\nwarnings.filterwarnings(\"ignore\")\n\n# Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Loss Function\nclass CustomLoss(nn.Module):\n    def __init__(self, alpha=0.7):\n        super(CustomLoss, self).__init__()\n        self.mse = nn.MSELoss()\n        self.smooth_l1 = nn.SmoothL1Loss(beta=1.0)\n        self.alpha = alpha\n\n    def forward(self, predictions, targets):\n        return self.alpha * self.mse(predictions, targets) + (1 - self.alpha) * self.smooth_l1(predictions, targets)\n\n# GraphSAGE Model\nclass GraphSAGE(nn.Module):\n    def __init__(self, input_dim, hidden_dim=1024, output_dim=1, dropout=0.2):\n        super(GraphSAGE, self).__init__()\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"max\")\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\n        self.conv5 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\n\n        self.bn1 = BatchNorm(hidden_dim)\n        self.bn2 = BatchNorm(hidden_dim)\n        self.bn3 = BatchNorm(hidden_dim)\n        self.bn4 = BatchNorm(hidden_dim)\n        self.bn5 = BatchNorm(hidden_dim)\n\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\n        x = self.dropout(x)\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\n        x = self.dropout(x)\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\n        x = self.dropout(x)\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\n        x = self.dropout(x)\n        x = F.leaky_relu(self.bn5(self.conv5(x, edge_index)))\n        x = F.leaky_relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Load Graph\ndef load_graph(path):\n    from torch_geometric.data import Data\n    from torch.serialization import add_safe_globals\n    add_safe_globals([Data])  \n\n    with open(path, 'rb') as f:\n        g_data = torch.load(f, weights_only=False)  \n    print(f\"‚úÖ Loaded graph from {path}\")\n    print(f\"Nodes: {g_data.num_nodes}, Edges: {g_data.num_edges}, Features: {g_data.x.shape}\")\n    return g_data\n\n\n\n# Train Step\ndef train(model, optimizer, loss_fn, data, mask):\n    model.train()\n    optimizer.zero_grad()\n    preds = model(data)[mask].squeeze()\n    loss = loss_fn(preds, data.y[mask])\n    loss.backward()\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n    optimizer.step()\n    return loss.item()\n\n# Test Step\ndef test(model, loss_fn, data, mask, y_scaler):\n    model.eval()\n    with torch.no_grad():\n        preds = model(data)[mask].squeeze().cpu().numpy()\n        actuals = data.y[mask].cpu().numpy()\n\n        preds_original = y_scaler.inverse_transform(preds.reshape(-1, 1)).flatten()\n        actuals_original = y_scaler.inverse_transform(actuals.reshape(-1, 1)).flatten()\n\n        pd.DataFrame({\n            'predicted_swe': preds_original,\n            'actual_swe': actuals_original\n        }).to_csv(\"predicted_vs_actual.csv\", index=False)\n\n        mse = mean_squared_error(actuals_original, preds_original)\n        r2 = r2_score(actuals_original, preds_original)\n        mae = mean_absolute_error(actuals_original, preds_original)\n        rmse = mean_squared_error(actuals_original, preds_original, squared=False)\n\n    return mse, r2, rmse, mae\n\n# Main\ndef main():\n    g_path = \"/media/volume1/gnn_graph_with_time_all_states.pt\"\n    data = load_graph(g_path)\n\n    # Normalize features\n    x_scaler = MinMaxScaler()\n    data.x = torch.tensor(x_scaler.fit_transform(data.x.cpu()), dtype=torch.float).to(device)\n\n    # Normalize labels\n    y_scaler = MinMaxScaler()\n    data.y = torch.tensor(y_scaler.fit_transform(data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\n\n    # Add self-loops\n    data.edge_index, _ = add_self_loops(data.edge_index, num_nodes=data.num_nodes)\n\n    # Train/Test Split\n    indices = torch.randperm(data.num_nodes)\n    train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n    train_mask[indices[:int(0.8 * data.num_nodes)]] = True\n    test_mask = ~train_mask\n\n    print(f\"üîÅ Training on {train_mask.sum().item()}, Testing on {test_mask.sum().item()}\")\n\n    model = GraphSAGE(input_dim=data.num_features).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-4)\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=25)\n    criterion = CustomLoss(alpha=0.7)\n\n    best_r2 = -float(\"inf\")\n    wait = 0\n    patience = 200\n\n    print(\"üöÄ Starting training...\")\n    for epoch in range(1, 2001):\n        loss = train(model, optimizer, criterion, data, train_mask)\n\n        if epoch % 100 == 0 or epoch == 1:\n            mse, r2, rmse, mae = test(model, criterion, data, test_mask, y_scaler)\n            print(f\"Epoch {epoch} ‚û§ Train Loss: {loss:.4f} | R¬≤: {r2:.4f} | RMSE: {rmse:.4f} | MAE: {mae:.4f}\")\n\n            if r2 > best_r2:\n                best_r2 = r2\n                wait = 0\n            else:\n                wait += 1\n\n            if wait >= patience:\n                print(f\"üõë Early stopping at epoch {epoch}\")\n                break\n\n    print(f\"\\n‚úÖ Best R¬≤: {best_r2:.4f}\")\n\nif __name__ == \"__main__\":\n    main()\n",
  "history_output" : "Using device: cpu\n‚úÖ Loaded graph from /media/volume1/gnn_graph_with_time_all_states.pt\nNodes: 68557, Edges: 411292, Features: torch.Size([68557, 26])\nüîÅ Training on 54845, Testing on 13712\nüöÄ Starting training...\nEpoch 1 ‚û§ Train Loss: 0.0182 | R¬≤: 0.1504 | RMSE: 3.9718 | MAE: 2.2063\nEpoch 100 ‚û§ Train Loss: 0.0007 | R¬≤: 0.6995 | RMSE: 2.3621 | MAE: 0.9251\nEpoch 200 ‚û§ Train Loss: 0.0004 | R¬≤: 0.8298 | RMSE: 1.7779 | MAE: 0.6791\nEpoch 300 ‚û§ Train Loss: 0.0003 | R¬≤: 0.8662 | RMSE: 1.5758 | MAE: 0.5858\nEpoch 400 ‚û§ Train Loss: 0.0002 | R¬≤: 0.8786 | RMSE: 1.5015 | MAE: 0.5379\nEpoch 500 ‚û§ Train Loss: 0.0002 | R¬≤: 0.8636 | RMSE: 1.5912 | MAE: 0.5398\nEpoch 600 ‚û§ Train Loss: 0.0002 | R¬≤: 0.8956 | RMSE: 1.3924 | MAE: 0.4719\nEpoch 700 ‚û§ Train Loss: 0.0002 | R¬≤: 0.8811 | RMSE: 1.4857 | MAE: 0.4903\nEpoch 800 ‚û§ Train Loss: 0.0001 | R¬≤: 0.8896 | RMSE: 1.4319 | MAE: 0.4833\nEpoch 900 ‚û§ Train Loss: 0.0001 | R¬≤: 0.8696 | RMSE: 1.5561 | MAE: 0.4737\nEpoch 1000 ‚û§ Train Loss: 0.0002 | R¬≤: 0.8776 | RMSE: 1.5074 | MAE: 0.6193\nEpoch 1100 ‚û§ Train Loss: 0.0001 | R¬≤: 0.8949 | RMSE: 1.3967 | MAE: 0.4158\nEpoch 1200 ‚û§ Train Loss: 0.0001 | R¬≤: 0.8688 | RMSE: 1.5606 | MAE: 0.4782\nEpoch 1300 ‚û§ Train Loss: 0.0001 | R¬≤: 0.8852 | RMSE: 1.4597 | MAE: 0.4436\nEpoch 1400 ‚û§ Train Loss: 0.0006 | R¬≤: 0.3774 | RMSE: 3.4001 | MAE: 1.7911\nEpoch 1500 ‚û§ Train Loss: 0.0001 | R¬≤: 0.5857 | RMSE: 2.7734 | MAE: 0.7985\nEpoch 1600 ‚û§ Train Loss: 0.0001 | R¬≤: 0.8528 | RMSE: 1.6530 | MAE: 0.4805\nEpoch 1700 ‚û§ Train Loss: 0.0001 | R¬≤: 0.8191 | RMSE: 1.8328 | MAE: 0.5388\nEpoch 1800 ‚û§ Train Loss: 0.0001 | R¬≤: 0.8716 | RMSE: 1.5440 | MAE: 0.5065\nEpoch 1900 ‚û§ Train Loss: 0.0001 | R¬≤: 0.7554 | RMSE: 2.1312 | MAE: 0.6363\nEpoch 2000 ‚û§ Train Loss: 0.0001 | R¬≤: 0.8261 | RMSE: 1.7966 | MAE: 0.4999\n‚úÖ Best R¬≤: 0.8956\n",
  "history_begin_time" : 1746105482969,
  "history_end_time" : 1746158347984,
  "history_notes" : null,
  "history_process" : "kevt5u",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "GliH0rAvKEcT",
  "history_input" : "import numpy as np\nimport torch\nimport warnings\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport pandas as pd\nfrom torch_geometric.nn import SAGEConv, BatchNorm\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch_geometric.utils import add_self_loops\n\nwarnings.filterwarnings(\"ignore\")\n\n# Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Loss Function\nclass CustomLoss(nn.Module):\n    def __init__(self, alpha=0.7):\n        super(CustomLoss, self).__init__()\n        self.mse = nn.MSELoss()\n        self.smooth_l1 = nn.SmoothL1Loss(beta=1.0)\n        self.alpha = alpha\n\n    def forward(self, predictions, targets):\n        return self.alpha * self.mse(predictions, targets) + (1 - self.alpha) * self.smooth_l1(predictions, targets)\n\n# GraphSAGE Model\nclass GraphSAGE(nn.Module):\n    def __init__(self, input_dim, hidden_dim=1024, output_dim=1, dropout=0.2):\n        super(GraphSAGE, self).__init__()\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"max\")\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\n        self.conv5 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\n\n        self.bn1 = BatchNorm(hidden_dim)\n        self.bn2 = BatchNorm(hidden_dim)\n        self.bn3 = BatchNorm(hidden_dim)\n        self.bn4 = BatchNorm(hidden_dim)\n        self.bn5 = BatchNorm(hidden_dim)\n\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\n        x = self.dropout(x)\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\n        x = self.dropout(x)\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\n        x = self.dropout(x)\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\n        x = self.dropout(x)\n        x = F.leaky_relu(self.bn5(self.conv5(x, edge_index)))\n        x = F.leaky_relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Load Graph\ndef load_graph(path):\n    import torch.serialization\n    torch.serialization._load = torch.load  \n    g_data = torch.load(path, weights_only=False)  \n    print(f\"‚úÖ Loaded graph from {path}\")\n    print(f\"Nodes: {g_data.num_nodes}, Edges: {g_data.num_edges}, Features: {g_data.x.shape}\")\n    return g_data\n\n\n# Train Step\ndef train(model, optimizer, loss_fn, data, mask):\n    model.train()\n    optimizer.zero_grad()\n    preds = model(data)[mask].squeeze()\n    loss = loss_fn(preds, data.y[mask])\n    loss.backward()\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n    optimizer.step()\n    return loss.item()\n\n# Test Step\ndef test(model, loss_fn, data, mask, y_scaler):\n    model.eval()\n    with torch.no_grad():\n        preds = model(data)[mask].squeeze().cpu().numpy()\n        actuals = data.y[mask].cpu().numpy()\n\n        preds_original = y_scaler.inverse_transform(preds.reshape(-1, 1)).flatten()\n        actuals_original = y_scaler.inverse_transform(actuals.reshape(-1, 1)).flatten()\n\n        pd.DataFrame({\n            'predicted_swe': preds_original,\n            'actual_swe': actuals_original\n        }).to_csv(\"predicted_vs_actual.csv\", index=False)\n\n        mse = mean_squared_error(actuals_original, preds_original)\n        r2 = r2_score(actuals_original, preds_original)\n        mae = mean_absolute_error(actuals_original, preds_original)\n        rmse = mean_squared_error(actuals_original, preds_original, squared=False)\n\n    return mse, r2, rmse, mae\n\n# Main\ndef main():\n    g_path = \"/media/volume1/gnn_graph_with_time_all_states.pt\"\n    data = load_graph(g_path)\n\n    # Normalize features\n    x_scaler = MinMaxScaler()\n    data.x = torch.tensor(x_scaler.fit_transform(data.x.cpu()), dtype=torch.float).to(device)\n\n    # Normalize labels\n    y_scaler = MinMaxScaler()\n    data.y = torch.tensor(y_scaler.fit_transform(data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\n\n    # Add self-loops\n    data.edge_index, _ = add_self_loops(data.edge_index, num_nodes=data.num_nodes)\n\n    # Train/Test Split\n    indices = torch.randperm(data.num_nodes)\n    train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n    train_mask[indices[:int(0.8 * data.num_nodes)]] = True\n    test_mask = ~train_mask\n\n    print(f\"üîÅ Training on {train_mask.sum().item()}, Testing on {test_mask.sum().item()}\")\n\n    model = GraphSAGE(input_dim=data.num_features).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-4)\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=25)\n    criterion = CustomLoss(alpha=0.7)\n\n    best_r2 = -float(\"inf\")\n    wait = 0\n    patience = 200\n\n    print(\"üöÄ Starting training...\")\n    for epoch in range(1, 2001):\n        loss = train(model, optimizer, criterion, data, train_mask)\n\n        if epoch % 100 == 0 or epoch == 1:\n            mse, r2, rmse, mae = test(model, criterion, data, test_mask, y_scaler)\n            print(f\"Epoch {epoch} ‚û§ Train Loss: {loss:.4f} | R¬≤: {r2:.4f} | RMSE: {rmse:.4f} | MAE: {mae:.4f}\")\n\n            if r2 > best_r2:\n                best_r2 = r2\n                wait = 0\n            else:\n                wait += 1\n\n            if wait >= patience:\n                print(f\"üõë Early stopping at epoch {epoch}\")\n                break\n\n    print(f\"\\n‚úÖ Best R¬≤: {best_r2:.4f}\")\n\nif __name__ == \"__main__\":\n    main()\n",
  "history_output" : "Using device: cpu\nTraceback (most recent call last):\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/torch/serialization.py\", line 851, in _check_seekable\n    f.seek(f.tell())\n    ^^^^^^\nAttributeError: 'torch._C.PyTorchFileReader' object has no attribute 'seek'\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"/media/volume1/gw-workspace/GliH0rAvKEcT/model_creation_gnn.py\", line 163, in <module>\n    main()\n  File \"/media/volume1/gw-workspace/GliH0rAvKEcT/model_creation_gnn.py\", line 112, in main\n    data = load_graph(g_path)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/media/volume1/gw-workspace/GliH0rAvKEcT/model_creation_gnn.py\", line 70, in load_graph\n    g_data = torch.load(path, weights_only=False)  \n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/torch/serialization.py\", line 1471, in load\n    return _load(\n           ^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/torch/serialization.py\", line 1425, in load\n    with _open_file_like(f, \"rb\") as opened_file:\n         ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/torch/serialization.py\", line 756, in _open_file_like\n    return _open_buffer_reader(name_or_buffer)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/torch/serialization.py\", line 741, in __init__\n    _check_seekable(buffer)\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/torch/serialization.py\", line 854, in _check_seekable\n    raise_err_msg([\"seek\", \"tell\"], e)\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/torch/serialization.py\", line 847, in raise_err_msg\n    raise type(e)(msg)\nAttributeError: 'torch._C.PyTorchFileReader' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead.\n",
  "history_begin_time" : 1746104289860,
  "history_end_time" : 1746104294917,
  "history_notes" : null,
  "history_process" : "kevt5u",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "xaiw2BYZszkw",
  "history_input" : "import numpy as np\nimport torch\nimport warnings\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport pandas as pd\nfrom torch_geometric.nn import SAGEConv, BatchNorm\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch_geometric.utils import add_self_loops\n\nwarnings.filterwarnings(\"ignore\")\n\n# Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Loss Function\nclass CustomLoss(nn.Module):\n    def __init__(self, alpha=0.7):\n        super(CustomLoss, self).__init__()\n        self.mse = nn.MSELoss()\n        self.smooth_l1 = nn.SmoothL1Loss(beta=1.0)\n        self.alpha = alpha\n\n    def forward(self, predictions, targets):\n        return self.alpha * self.mse(predictions, targets) + (1 - self.alpha) * self.smooth_l1(predictions, targets)\n\n# GraphSAGE Model\nclass GraphSAGE(nn.Module):\n    def __init__(self, input_dim, hidden_dim=1024, output_dim=1, dropout=0.2):\n        super(GraphSAGE, self).__init__()\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"max\")\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\n        self.conv5 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\n\n        self.bn1 = BatchNorm(hidden_dim)\n        self.bn2 = BatchNorm(hidden_dim)\n        self.bn3 = BatchNorm(hidden_dim)\n        self.bn4 = BatchNorm(hidden_dim)\n        self.bn5 = BatchNorm(hidden_dim)\n\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\n        x = self.dropout(x)\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\n        x = self.dropout(x)\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\n        x = self.dropout(x)\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\n        x = self.dropout(x)\n        x = F.leaky_relu(self.bn5(self.conv5(x, edge_index)))\n        x = F.leaky_relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Load Graph\ndef load_graph(path):\n    g_data = torch.load(path)\n    print(f\"‚úÖ Loaded graph from {path}\")\n    print(f\"Nodes: {g_data.num_nodes}, Edges: {g_data.num_edges}, Features: {g_data.x.shape}\")\n    return g_data\n\n# Train Step\ndef train(model, optimizer, loss_fn, data, mask):\n    model.train()\n    optimizer.zero_grad()\n    preds = model(data)[mask].squeeze()\n    loss = loss_fn(preds, data.y[mask])\n    loss.backward()\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n    optimizer.step()\n    return loss.item()\n\n# Test Step\ndef test(model, loss_fn, data, mask, y_scaler):\n    model.eval()\n    with torch.no_grad():\n        preds = model(data)[mask].squeeze().cpu().numpy()\n        actuals = data.y[mask].cpu().numpy()\n\n        preds_original = y_scaler.inverse_transform(preds.reshape(-1, 1)).flatten()\n        actuals_original = y_scaler.inverse_transform(actuals.reshape(-1, 1)).flatten()\n\n        pd.DataFrame({\n            'predicted_swe': preds_original,\n            'actual_swe': actuals_original\n        }).to_csv(\"predicted_vs_actual.csv\", index=False)\n\n        mse = mean_squared_error(actuals_original, preds_original)\n        r2 = r2_score(actuals_original, preds_original)\n        mae = mean_absolute_error(actuals_original, preds_original)\n        rmse = mean_squared_error(actuals_original, preds_original, squared=False)\n\n    return mse, r2, rmse, mae\n\n# Main\ndef main():\n    g_path = \"/media/volume1/gnn_graph_with_time_all_states.pt\"\n    data = load_graph(g_path)\n\n    # Normalize features\n    x_scaler = MinMaxScaler()\n    data.x = torch.tensor(x_scaler.fit_transform(data.x.cpu()), dtype=torch.float).to(device)\n\n    # Normalize labels\n    y_scaler = MinMaxScaler()\n    data.y = torch.tensor(y_scaler.fit_transform(data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\n\n    # Add self-loops\n    data.edge_index, _ = add_self_loops(data.edge_index, num_nodes=data.num_nodes)\n\n    # Train/Test Split\n    indices = torch.randperm(data.num_nodes)\n    train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n    train_mask[indices[:int(0.8 * data.num_nodes)]] = True\n    test_mask = ~train_mask\n\n    print(f\"üîÅ Training on {train_mask.sum().item()}, Testing on {test_mask.sum().item()}\")\n\n    model = GraphSAGE(input_dim=data.num_features).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-4)\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=25)\n    criterion = CustomLoss(alpha=0.7)\n\n    best_r2 = -float(\"inf\")\n    wait = 0\n    patience = 200\n\n    print(\"üöÄ Starting training...\")\n    for epoch in range(1, 2001):\n        loss = train(model, optimizer, criterion, data, train_mask)\n\n        if epoch % 100 == 0 or epoch == 1:\n            mse, r2, rmse, mae = test(model, criterion, data, test_mask, y_scaler)\n            print(f\"Epoch {epoch} ‚û§ Train Loss: {loss:.4f} | R¬≤: {r2:.4f} | RMSE: {rmse:.4f} | MAE: {mae:.4f}\")\n\n            if r2 > best_r2:\n                best_r2 = r2\n                wait = 0\n            else:\n                wait += 1\n\n            if wait >= patience:\n                print(f\"üõë Early stopping at epoch {epoch}\")\n                break\n\n    print(f\"\\n‚úÖ Best R¬≤: {best_r2:.4f}\")\n\nif __name__ == \"__main__\":\n    main()\n",
  "history_output" : "Using device: cpu\nTraceback (most recent call last):\n  File \"/media/volume1/gw-workspace/xaiw2BYZszkw/model_creation_gnn.py\", line 160, in <module>\n    main()\n  File \"/media/volume1/gw-workspace/xaiw2BYZszkw/model_creation_gnn.py\", line 109, in main\n    data = load_graph(g_path)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/media/volume1/gw-workspace/xaiw2BYZszkw/model_creation_gnn.py\", line 68, in load_graph\n    g_data = torch.load(path)\n             ^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/torch/serialization.py\", line 1470, in load\n    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None\n_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, \u001B[1mdo those steps only if you trust the source of the checkpoint\u001B[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL torch_geometric.data.data.DataEdgeAttr was not an allowed global by default. Please use `torch.serialization.add_safe_globals([DataEdgeAttr])` or the `torch.serialization.safe_globals([DataEdgeAttr])` context manager to allowlist this global if you trust this class/function.\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
  "history_begin_time" : 1746104218687,
  "history_end_time" : 1746104224121,
  "history_notes" : null,
  "history_process" : "kevt5u",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "LOpXQGLAWgoZ",
  "history_input" : "import numpy as np\nimport torch\nimport warnings\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch_geometric.nn import SAGEConv, BatchNorm\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch_geometric.utils import add_self_loops\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Check Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Custom Loss Function\nclass CustomLoss(nn.Module):\n    def __init__(self, alpha=0.7):\n        super(CustomLoss, self).__init__()\n        self.mse = nn.MSELoss()\n        self.smooth_l1 = nn.SmoothL1Loss(beta=1.0)\n        self.alpha = alpha\n\n    def forward(self, predictions, targets):\n        return self.alpha * self.mse(predictions, targets) + (1 - self.alpha) * self.smooth_l1(predictions, targets)\n\n# Define GraphSAGE model with max aggregation\nclass GraphSAGE(nn.Module):\n    def __init__(self, input_dim, hidden_dim=1024, output_dim=1, dropout=0.2):\n        super(GraphSAGE, self).__init__()\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"max\")\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\n        self.conv5 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\n\n        self.bn1 = BatchNorm(hidden_dim)\n        self.bn2 = BatchNorm(hidden_dim)\n        self.bn3 = BatchNorm(hidden_dim)\n        self.bn4 = BatchNorm(hidden_dim)\n        self.bn5 = BatchNorm(hidden_dim)\n\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\n        x = self.dropout(x)\n\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\n        x = self.dropout(x)\n\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\n        x = self.dropout(x)\n\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\n        x = self.dropout(x)\n\n        x = F.leaky_relu(self.bn5(self.conv5(x, edge_index)))\n\n        x = F.leaky_relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Load and verify graph\ndef load_and_verify_graph(g_path):\n    print(\"Loading graph dataset\")\n    try:\n        g_data = torch.load(g_path)\n        print(f\"Graph successfully loaded from {g_path}\")\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\n        print(f\"Number of Edges: {g_data.num_edges}\")\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\n\n        if hasattr(g_data, \"y\") and g_data.y is not None:\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\n        else:\n            print(f\"Warning: Target variable `y` is missing or None!\")\n\n        return g_data\n    except Exception as e:\n        print(f\"Error loading graph: {e}\")\n        exit()\n\n# Training function\ndef train(model, optimizer, loss_fn, graph, train_mask):\n    model.train()\n    optimizer.zero_grad()\n    predictions = model(graph)[train_mask].squeeze()\n    loss = loss_fn(predictions, graph.y[train_mask])\n\n    loss.backward()\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n    optimizer.step()\n    return loss.item()\n\n# Testing function\ndef test(model, loss_fn, graph, test_mask):\n    model.eval()\n    with torch.no_grad():\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\n        actuals = graph.y[test_mask].cpu().numpy()\n\n        mse = mean_squared_error(actuals, predictions)\n        r2 = r2_score(actuals, predictions)\n        mae = mean_absolute_error(actuals, predictions)\n        rmse = mean_squared_error(actuals, predictions, squared=False)\n\n    return mse, r2, rmse, mae\n\n# Main function\ndef main():\n    g_path = \"/media/volume1/gnn_graph_final_all_data.pt\"  \n    g_data = load_and_verify_graph(g_path)\n\n    # Normalize node features using MinMaxScaler\n    scaler_x = MinMaxScaler()\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\n\n    # Normalize target variable using MinMaxScaler\n    y_scaler = MinMaxScaler()\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\n\n    # Add self-loops to improve message passing\n    g_data.edge_index, _ = add_self_loops(g_data.edge_index, num_nodes=g_data.num_nodes)\n\n    # Training split\n    indices = torch.randperm(g_data.num_nodes)\n    train_size = int(0.8 * g_data.num_nodes)\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\n    train_mask[indices[:train_size]] = True  \n    test_mask = ~train_mask  \n\n    print(f\"Training on {train_size} nodes & Testing on {g_data.num_nodes - train_size} nodes\")\n\n    # Define model, optimizer, and loss function\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=1024, output_dim=1, dropout=0.2).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-4)\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=25)\n    criterion = CustomLoss(alpha=0.7)\n\n    best_r2 = -float(\"inf\")\n    best_epoch = 0\n    patience = 200\n    wait = 0\n\n    print(\"Starting training for up to 2000 epochs with early stopping...\")\n\n    for epoch in range(1, 2001):  \n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\n\n        if epoch % 100 == 0 or epoch == 5:  \n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\n            print(f\"Epoch {epoch}: Train Loss={train_loss:.4f} | R¬≤={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\n\n            if r2 > best_r2:\n                best_r2 = r2\n                best_epoch = epoch\n                wait = 0\n            else:\n                wait += 1\n\n            if wait >= patience:\n                print(f\"Early stopping at epoch {epoch}\")\n                break\n\n    print(f\"Best R¬≤ score: {best_r2:.4f} at epoch {best_epoch}\")\n\nif __name__ == \"__main__\":\n    main()\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph_final_all_data.pt\nNumber of Nodes: 7029\nNumber of Edges: 3387\nNode Feature Shape: torch.Size([7029, 11])\nEdge Index Shape: torch.Size([2, 3387])\nTarget Variable Shape: torch.Size([7029])\nTraining on 5623 nodes & Testing on 1406 nodes\nStarting training for up to 2000 epochs with early stopping...\nEpoch 5: Train Loss=0.0778 | R¬≤=-0.5929 | RMSE=0.5285 | MAE=0.4483\nEpoch 100: Train Loss=0.0203 | R¬≤=0.8519 | RMSE=0.1612 | MAE=0.0852\nEpoch 200: Train Loss=0.0168 | R¬≤=0.8571 | RMSE=0.1583 | MAE=0.0790\nEpoch 300: Train Loss=0.0149 | R¬≤=0.8552 | RMSE=0.1593 | MAE=0.0751\nEpoch 400: Train Loss=0.0135 | R¬≤=0.8507 | RMSE=0.1618 | MAE=0.0758\nEpoch 500: Train Loss=0.0114 | R¬≤=0.8431 | RMSE=0.1659 | MAE=0.0805\nEpoch 600: Train Loss=0.0104 | R¬≤=0.8379 | RMSE=0.1686 | MAE=0.0746\nEpoch 700: Train Loss=0.0098 | R¬≤=0.8461 | RMSE=0.1643 | MAE=0.0736\nEpoch 800: Train Loss=0.0082 | R¬≤=0.8429 | RMSE=0.1659 | MAE=0.0725\nEpoch 900: Train Loss=0.0077 | R¬≤=0.8357 | RMSE=0.1697 | MAE=0.0760\nEpoch 1000: Train Loss=0.0072 | R¬≤=0.8411 | RMSE=0.1669 | MAE=0.0716\nEpoch 1100: Train Loss=0.0066 | R¬≤=0.8465 | RMSE=0.1641 | MAE=0.0730\nEpoch 1200: Train Loss=0.0065 | R¬≤=0.8431 | RMSE=0.1659 | MAE=0.0751\nEpoch 1300: Train Loss=0.0064 | R¬≤=0.8422 | RMSE=0.1663 | MAE=0.0713\nEpoch 1400: Train Loss=0.0057 | R¬≤=0.8381 | RMSE=0.1685 | MAE=0.0719\nEpoch 1500: Train Loss=0.0062 | R¬≤=0.8461 | RMSE=0.1642 | MAE=0.0709\nEpoch 1600: Train Loss=0.0059 | R¬≤=0.8365 | RMSE=0.1693 | MAE=0.0754\nEpoch 1700: Train Loss=0.0059 | R¬≤=0.8424 | RMSE=0.1662 | MAE=0.0707\nEpoch 1800: Train Loss=0.0056 | R¬≤=0.8401 | RMSE=0.1675 | MAE=0.0721\nEpoch 1900: Train Loss=0.0061 | R¬≤=0.8374 | RMSE=0.1689 | MAE=0.0736\nEpoch 2000: Train Loss=0.0054 | R¬≤=0.8413 | RMSE=0.1668 | MAE=0.0719\nBest R¬≤ score: 0.8571 at epoch 200\n",
  "history_begin_time" : 1742236667387,
  "history_end_time" : 1742241362127,
  "history_notes" : "all_combination",
  "history_process" : "kevt5u",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "7jBVvLqimXk6",
  "history_input" : "import numpy as np\nimport torch\nimport warnings\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch_geometric.nn import SAGEConv, BatchNorm\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Check Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Load and verify graph\ndef load_and_verify_graph(g_path):\n    print(\"Loading graph dataset\")\n    try:\n        g_data = torch.load(g_path)\n        print(f\"Graph successfully loaded from {g_path}\")\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\n        print(f\"Number of Edges: {g_data.num_edges}\")\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\n\n        if hasattr(g_data, \"y\") and g_data.y is not None:\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\n        else:\n            print(f\"Warning: Target variable `y` is missing or None!\")\n\n        return g_data\n    except Exception as e:\n        print(f\"Error loading graph: {e}\")\n        exit()\n\n# Define GraphSAGE model with deeper layers & mean aggregation\nclass GraphSAGE(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\n        super(GraphSAGE, self).__init__()\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\n        self.conv5 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")  # Added extra layer\n\n        self.bn1 = BatchNorm(hidden_dim)\n        self.bn2 = BatchNorm(hidden_dim)\n        self.bn3 = BatchNorm(hidden_dim)\n        self.bn4 = BatchNorm(hidden_dim)\n        self.bn5 = BatchNorm(hidden_dim)\n\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\n        x = self.dropout(x)\n\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\n        x = self.dropout(x)\n\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\n        x = self.dropout(x)\n\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\n        x = self.dropout(x)\n\n        x = F.leaky_relu(self.bn5(self.conv5(x, edge_index)))\n\n        x = F.leaky_relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Training function\ndef train(model, optimizer, loss_fn, graph, train_mask):\n    model.train()\n    optimizer.zero_grad()\n    predictions = model(graph)[train_mask].squeeze()\n    loss = loss_fn(predictions, graph.y[train_mask])\n\n    loss.backward()\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\n    optimizer.step()\n    return loss.item()\n\n# Testing function\ndef test(model, loss_fn, graph, test_mask):\n    model.eval()\n    with torch.no_grad():\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\n        actuals = graph.y[test_mask].cpu().numpy()\n\n        mse = mean_squared_error(actuals, predictions)\n        r2 = r2_score(actuals, predictions)\n        mae = mean_absolute_error(actuals, predictions)\n        rmse = mean_squared_error(actuals, predictions, squared=False)\n\n    return mse, r2, rmse, mae\n\n# Main function\ndef main():\n    g_path = \"/media/volume1/gnn_graph_final_all_data.pt\"  \n    g_data = load_and_verify_graph(g_path)\n\n    # Normalize node features\n    scaler_x = StandardScaler()\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\n\n    # Normalize target variable\n    y_scaler = StandardScaler()\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\n\n    # Training split\n    indices= torch.randperm(g_data.num_nodes)\n    train_size=int(0.8*g_data.num_nodes)\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\n    train_mask[indices[:train_size]] = True  \n    test_mask = ~train_mask  \n\n    print(f\"Training on {train_size} nodes & Testing on {g_data.num_nodes - train_size} nodes\")\n\n    # Define model, optimizer, and loss function\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=512, output_dim=1, dropout=0.2).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-3)\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=50)\n    criterion = nn.SmoothL1Loss(beta=1.0)\n\n    best_r2 = -float(\"inf\")\n\n    print(\"Starting training for 1000 epochs...\")\n\n    for epoch in range(1, 1001):  \n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\n\n        if epoch % 100 == 0 or epoch == 5:  \n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\n            print(f\"Epoch {epoch}: Train Loss={train_loss:.4f} | R¬≤={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\n\n            if r2 > best_r2:\n                best_r2 = r2\n\n    print(f\"Best R¬≤ score: {best_r2:.4f}\")\n\nif __name__ == \"__main__\":\n    main()\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph_final_all_data.pt\nNumber of Nodes: 7029\nNumber of Edges: 3387\nNode Feature Shape: torch.Size([7029, 11])\nEdge Index Shape: torch.Size([2, 3387])\nTarget Variable Shape: torch.Size([7029])\nTraining on 5623 nodes & Testing on 1406 nodes\nStarting training for 1000 epochs...\nEpoch 5: Train Loss=0.3118 | R¬≤=0.0842 | RMSE=0.9574 | MAE=0.9008\nEpoch 100: Train Loss=0.0745 | R¬≤=0.8206 | RMSE=0.4237 | MAE=0.2150\nEpoch 200: Train Loss=0.0618 | R¬≤=0.8265 | RMSE=0.4167 | MAE=0.2026\nEpoch 300: Train Loss=0.0575 | R¬≤=0.8278 | RMSE=0.4151 | MAE=0.1979\nEpoch 400: Train Loss=0.0517 | R¬≤=0.8305 | RMSE=0.4119 | MAE=0.1924\nEpoch 500: Train Loss=0.0488 | R¬≤=0.8317 | RMSE=0.4104 | MAE=0.1819\nEpoch 600: Train Loss=0.0458 | R¬≤=0.8349 | RMSE=0.4065 | MAE=0.1791\nEpoch 700: Train Loss=0.0433 | R¬≤=0.8334 | RMSE=0.4083 | MAE=0.1793\nEpoch 800: Train Loss=0.0405 | R¬≤=0.8354 | RMSE=0.4058 | MAE=0.1812\nEpoch 900: Train Loss=0.0375 | R¬≤=0.8390 | RMSE=0.4014 | MAE=0.1735\nEpoch 1000: Train Loss=0.0351 | R¬≤=0.8369 | RMSE=0.4040 | MAE=0.1703\nBest R¬≤ score: 0.8390\n",
  "history_begin_time" : 1742236452188,
  "history_end_time" : 1742237734763,
  "history_notes" : "drop_out = 0.2",
  "history_process" : "kevt5u",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "IArA250fWOnk",
  "history_input" : "import numpy as np\nimport torch\nimport warnings\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch_geometric.nn import SAGEConv, BatchNorm\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Check Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Load and verify graph\ndef load_and_verify_graph(g_path):\n    print(\"Loading graph dataset\")\n    try:\n        g_data = torch.load(g_path)\n        print(f\"Graph successfully loaded from {g_path}\")\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\n        print(f\"Number of Edges: {g_data.num_edges}\")\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\n\n        if hasattr(g_data, \"y\") and g_data.y is not None:\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\n        else:\n            print(f\"Warning: Target variable `y` is missing or None!\")\n\n        return g_data\n    except Exception as e:\n        print(f\"Error loading graph: {e}\")\n        exit()\n\n# Define GraphSAGE model with deeper layers & mean aggregation\nclass GraphSAGE(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\n        super(GraphSAGE, self).__init__()\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\n        self.conv5 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")  # Added extra layer\n\n        self.bn1 = BatchNorm(hidden_dim)\n        self.bn2 = BatchNorm(hidden_dim)\n        self.bn3 = BatchNorm(hidden_dim)\n        self.bn4 = BatchNorm(hidden_dim)\n        self.bn5 = BatchNorm(hidden_dim)\n\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\n        x = self.dropout(x)\n\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\n        x = self.dropout(x)\n\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\n        x = self.dropout(x)\n\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\n        x = self.dropout(x)\n\n        x = F.leaky_relu(self.bn5(self.conv5(x, edge_index)))\n\n        x = F.leaky_relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Training function\ndef train(model, optimizer, loss_fn, graph, train_mask):\n    model.train()\n    optimizer.zero_grad()\n    predictions = model(graph)[train_mask].squeeze()\n    loss = loss_fn(predictions, graph.y[train_mask])\n\n    loss.backward()\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\n    optimizer.step()\n    return loss.item()\n\n# Testing function\ndef test(model, loss_fn, graph, test_mask):\n    model.eval()\n    with torch.no_grad():\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\n        actuals = graph.y[test_mask].cpu().numpy()\n\n        mse = mean_squared_error(actuals, predictions)\n        r2 = r2_score(actuals, predictions)\n        mae = mean_absolute_error(actuals, predictions)\n        rmse = mean_squared_error(actuals, predictions, squared=False)\n\n    return mse, r2, rmse, mae\n\n# Main function\ndef main():\n    g_path = \"/media/volume1/gnn_graph_final_all_data.pt\"  \n    g_data = load_and_verify_graph(g_path)\n\n    # Normalize node features\n    scaler_x = StandardScaler()\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\n\n    # Normalize target variable\n    y_scaler = StandardScaler()\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\n\n    # Training split\n    indices= torch.randperm(g_data.num_nodes)\n    train_size=int(0.8*g_data.num_nodes)\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\n    train_mask[indices[:train_size]] = True  \n    test_mask = ~train_mask  \n\n    print(f\"Training on {train_size} nodes & Testing on {g_data.num_nodes - train_size} nodes\")\n\n    # Define model, optimizer, and loss function\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=1024, output_dim=1, dropout=0.4).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-3)\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=50)\n    criterion = nn.SmoothL1Loss(beta=1.0)\n\n    best_r2 = -float(\"inf\")\n\n    print(\"Starting training for 1000 epochs...\")\n\n    for epoch in range(1, 1001):  \n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\n\n        if epoch % 100 == 0 or epoch == 5:  \n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\n            print(f\"Epoch {epoch}: Train Loss={train_loss:.4f} | R¬≤={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\n\n            if r2 > best_r2:\n                best_r2 = r2\n\n    print(f\"Best R¬≤ score: {best_r2:.4f}\")\n\nif __name__ == \"__main__\":\n    main()\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph_final_all_data.pt\nNumber of Nodes: 7029\nNumber of Edges: 3387\nNode Feature Shape: torch.Size([7029, 11])\nEdge Index Shape: torch.Size([2, 3387])\nTarget Variable Shape: torch.Size([7029])\nTraining on 5623 nodes & Testing on 1406 nodes\nStarting training for 1000 epochs...\nEpoch 5: Train Loss=0.3000 | R¬≤=0.1737 | RMSE=0.9016 | MAE=0.8348\nEpoch 100: Train Loss=0.0745 | R¬≤=0.7796 | RMSE=0.4656 | MAE=0.2484\nEpoch 200: Train Loss=0.0651 | R¬≤=0.8050 | RMSE=0.4380 | MAE=0.2311\nEpoch 300: Train Loss=0.0603 | R¬≤=0.8243 | RMSE=0.4158 | MAE=0.2058\nEpoch 400: Train Loss=0.0556 | R¬≤=0.8214 | RMSE=0.4192 | MAE=0.1906\nEpoch 500: Train Loss=0.0510 | R¬≤=0.8189 | RMSE=0.4221 | MAE=0.1929\nEpoch 600: Train Loss=0.0502 | R¬≤=0.8149 | RMSE=0.4268 | MAE=0.1911\nEpoch 700: Train Loss=0.0464 | R¬≤=0.8120 | RMSE=0.4301 | MAE=0.1846\nEpoch 800: Train Loss=0.0461 | R¬≤=0.8150 | RMSE=0.4266 | MAE=0.1819\nEpoch 900: Train Loss=0.0436 | R¬≤=0.8126 | RMSE=0.4294 | MAE=0.1826\nEpoch 1000: Train Loss=0.0409 | R¬≤=0.8081 | RMSE=0.4345 | MAE=0.1834\nBest R¬≤ score: 0.8243\n",
  "history_begin_time" : 1742236384362,
  "history_end_time" : 1742239375166,
  "history_notes" : "h_dim = 1024",
  "history_process" : "kevt5u",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "Iv8tFqGWjjc0",
  "history_input" : "import numpy as np\nimport torch\nimport warnings\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch_geometric.nn import SAGEConv, BatchNorm\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Check Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Load and verify graph\ndef load_and_verify_graph(g_path):\n    print(\"Loading graph dataset\")\n    try:\n        g_data = torch.load(g_path)\n        print(f\"Graph successfully loaded from {g_path}\")\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\n        print(f\"Number of Edges: {g_data.num_edges}\")\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\n\n        if hasattr(g_data, \"y\") and g_data.y is not None:\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\n        else:\n            print(f\"Warning: Target variable `y` is missing or None!\")\n\n        return g_data\n    except Exception as e:\n        print(f\"Error loading graph: {e}\")\n        exit()\n\n# Define GraphSAGE model with deeper layers & mean aggregation\nclass GraphSAGE(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\n        super(GraphSAGE, self).__init__()\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\n        self.conv5 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")  # Added extra layer\n\n        self.bn1 = BatchNorm(hidden_dim)\n        self.bn2 = BatchNorm(hidden_dim)\n        self.bn3 = BatchNorm(hidden_dim)\n        self.bn4 = BatchNorm(hidden_dim)\n        self.bn5 = BatchNorm(hidden_dim)\n\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\n        x = self.dropout(x)\n\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\n        x = self.dropout(x)\n\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\n        x = self.dropout(x)\n\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\n        x = self.dropout(x)\n\n        x = F.leaky_relu(self.bn5(self.conv5(x, edge_index)))\n\n        x = F.leaky_relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Training function\ndef train(model, optimizer, loss_fn, graph, train_mask):\n    model.train()\n    optimizer.zero_grad()\n    predictions = model(graph)[train_mask].squeeze()\n    loss = loss_fn(predictions, graph.y[train_mask])\n\n    loss.backward()\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\n    optimizer.step()\n    return loss.item()\n\n# Testing function\ndef test(model, loss_fn, graph, test_mask):\n    model.eval()\n    with torch.no_grad():\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\n        actuals = graph.y[test_mask].cpu().numpy()\n\n        mse = mean_squared_error(actuals, predictions)\n        r2 = r2_score(actuals, predictions)\n        mae = mean_absolute_error(actuals, predictions)\n        rmse = mean_squared_error(actuals, predictions, squared=False)\n\n    return mse, r2, rmse, mae\n\n# Main function\ndef main():\n    g_path = \"/media/volume1/gnn_graph_final_all_data.pt\"  \n    g_data = load_and_verify_graph(g_path)\n\n    # Normalize node features\n    scaler_x = StandardScaler()\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\n\n    # Normalize target variable\n    y_scaler = StandardScaler()\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\n\n    # Training split\n    indices= torch.randperm(g_data.num_nodes)\n    train_size=int(0.8*g_data.num_nodes)\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\n    train_mask[indices[:train_size]] = True  \n    test_mask = ~train_mask  \n\n    print(f\"Training on {train_size} nodes & Testing on {g_data.num_nodes - train_size} nodes\")\n\n    # Define model, optimizer, and loss function\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=512, output_dim=1, dropout=0.4).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-3)\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=50)\n    criterion = nn.SmoothL1Loss(beta=1.0)\n\n    best_r2 = -float(\"inf\")\n\n    print(\"Starting training for 1000 epochs...\")\n\n    for epoch in range(1, 2001):  \n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\n\n        if epoch % 100 == 0 or epoch == 5:  \n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\n            print(f\"Epoch {epoch}: Train Loss={train_loss:.4f} | R¬≤={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\n\n            if r2 > best_r2:\n                best_r2 = r2\n\n    print(f\"Best R¬≤ score: {best_r2:.4f}\")\n\nif __name__ == \"__main__\":\n    main()\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph_final_all_data.pt\nNumber of Nodes: 7029\nNumber of Edges: 3387\nNode Feature Shape: torch.Size([7029, 11])\nEdge Index Shape: torch.Size([2, 3387])\nTarget Variable Shape: torch.Size([7029])\nTraining on 5623 nodes & Testing on 1406 nodes\nStarting training for 1000 epochs...\nEpoch 5: Train Loss=0.4159 | R¬≤=0.0596 | RMSE=0.9694 | MAE=0.9096\nEpoch 100: Train Loss=0.0919 | R¬≤=0.7824 | RMSE=0.4663 | MAE=0.2505\nEpoch 200: Train Loss=0.0788 | R¬≤=0.8014 | RMSE=0.4454 | MAE=0.2299\nEpoch 300: Train Loss=0.0714 | R¬≤=0.8215 | RMSE=0.4224 | MAE=0.2129\nEpoch 400: Train Loss=0.0678 | R¬≤=0.8359 | RMSE=0.4049 | MAE=0.1980\nEpoch 500: Train Loss=0.0627 | R¬≤=0.8414 | RMSE=0.3981 | MAE=0.1889\nEpoch 600: Train Loss=0.0608 | R¬≤=0.8461 | RMSE=0.3921 | MAE=0.1776\nEpoch 700: Train Loss=0.0567 | R¬≤=0.8512 | RMSE=0.3856 | MAE=0.1704\nEpoch 800: Train Loss=0.0555 | R¬≤=0.8549 | RMSE=0.3808 | MAE=0.1728\nEpoch 900: Train Loss=0.0538 | R¬≤=0.8578 | RMSE=0.3769 | MAE=0.1623\nEpoch 1000: Train Loss=0.0503 | R¬≤=0.8580 | RMSE=0.3766 | MAE=0.1614\nEpoch 1100: Train Loss=0.0490 | R¬≤=0.8572 | RMSE=0.3778 | MAE=0.1747\nEpoch 1200: Train Loss=0.0475 | R¬≤=0.8598 | RMSE=0.3743 | MAE=0.1605\nEpoch 1300: Train Loss=0.0467 | R¬≤=0.8609 | RMSE=0.3728 | MAE=0.1635\nEpoch 1400: Train Loss=0.0472 | R¬≤=0.8654 | RMSE=0.3667 | MAE=0.1630\nEpoch 1500: Train Loss=0.0456 | R¬≤=0.8518 | RMSE=0.3849 | MAE=0.1636\nEpoch 1600: Train Loss=0.0465 | R¬≤=0.8623 | RMSE=0.3710 | MAE=0.1628\nEpoch 1700: Train Loss=0.0440 | R¬≤=0.8617 | RMSE=0.3717 | MAE=0.1580\nEpoch 1800: Train Loss=0.0422 | R¬≤=0.8608 | RMSE=0.3729 | MAE=0.1595\nEpoch 1900: Train Loss=0.0409 | R¬≤=0.8546 | RMSE=0.3811 | MAE=0.1665\nEpoch 2000: Train Loss=0.0396 | R¬≤=0.8545 | RMSE=0.3812 | MAE=0.1625\nBest R¬≤ score: 0.8654\n",
  "history_begin_time" : 1742233858544,
  "history_end_time" : 1742234711028,
  "history_notes" : "R¬≤ score: 0.8654, 2000 eppochs",
  "history_process" : "kevt5u",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "SyDaNVhsDa6v",
  "history_input" : "import numpy as np\nimport torch\nimport warnings\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch_geometric.nn import SAGEConv, BatchNorm\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Check Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Load and verify graph\ndef load_and_verify_graph(g_path):\n    print(\"Loading graph dataset\")\n    try:\n        g_data = torch.load(g_path)\n        print(f\"Graph successfully loaded from {g_path}\")\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\n        print(f\"Number of Edges: {g_data.num_edges}\")\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\n\n        if hasattr(g_data, \"y\") and g_data.y is not None:\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\n        else:\n            print(f\"Warning: Target variable `y` is missing or None!\")\n\n        return g_data\n    except Exception as e:\n        print(f\"Error loading graph: {e}\")\n        exit()\n\n# Define GraphSAGE model with deeper layers & mean aggregation\nclass GraphSAGE(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\n        super(GraphSAGE, self).__init__()\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\n        self.conv5 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")  # Added extra layer\n\n        self.bn1 = BatchNorm(hidden_dim)\n        self.bn2 = BatchNorm(hidden_dim)\n        self.bn3 = BatchNorm(hidden_dim)\n        self.bn4 = BatchNorm(hidden_dim)\n        self.bn5 = BatchNorm(hidden_dim)\n\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\n        x = self.dropout(x)\n\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\n        x = self.dropout(x)\n\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\n        x = self.dropout(x)\n\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\n        x = self.dropout(x)\n\n        x = F.leaky_relu(self.bn5(self.conv5(x, edge_index)))\n\n        x = F.leaky_relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Training function\ndef train(model, optimizer, loss_fn, graph, train_mask):\n    model.train()\n    optimizer.zero_grad()\n    predictions = model(graph)[train_mask].squeeze()\n    loss = loss_fn(predictions, graph.y[train_mask])\n\n    loss.backward()\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\n    optimizer.step()\n    return loss.item()\n\n# Testing function\ndef test(model, loss_fn, graph, test_mask):\n    model.eval()\n    with torch.no_grad():\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\n        actuals = graph.y[test_mask].cpu().numpy()\n\n        mse = mean_squared_error(actuals, predictions)\n        r2 = r2_score(actuals, predictions)\n        mae = mean_absolute_error(actuals, predictions)\n        rmse = mean_squared_error(actuals, predictions, squared=False)\n\n    return mse, r2, rmse, mae\n\n# Main function\ndef main():\n    g_path = \"/media/volume1/gnn_graph_final_all_data.pt\"  \n    g_data = load_and_verify_graph(g_path)\n\n    # Normalize node features\n    scaler_x = StandardScaler()\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\n\n    # Normalize target variable\n    y_scaler = StandardScaler()\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\n\n    # Training split\n    indices= torch.randperm(g_data.num_nodes)\n    train_size=int(0.8*g_data.num_nodes)\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\n    train_mask[indices[:train_size]] = True  \n    test_mask = ~train_mask  \n\n    print(f\"Training on {train_size} nodes & Testing on {g_data.num_nodes - train_size} nodes\")\n\n    # Define model, optimizer, and loss function\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=512, output_dim=1, dropout=0.4).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-3)\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=50)\n    criterion = nn.SmoothL1Loss(beta=1.0)\n\n    best_r2 = -float(\"inf\")\n\n    print(\"Starting training for 1000 epochs...\")\n\n    for epoch in range(1, 1001):  \n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\n\n        if epoch % 100 == 0 or epoch == 5:  \n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\n            print(f\"Epoch {epoch}: Train Loss={train_loss:.4f} | R¬≤={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\n\n            if r2 > best_r2:\n                best_r2 = r2\n\n    print(f\"Best R¬≤ score: {best_r2:.4f}\")\n\nif __name__ == \"__main__\":\n    main()\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph_final_all_data.pt\nNumber of Nodes: 7029\nNumber of Edges: 3387\nNode Feature Shape: torch.Size([7029, 11])\nEdge Index Shape: torch.Size([2, 3387])\nTarget Variable Shape: torch.Size([7029])\nTraining on 5623 nodes & Testing on 1406 nodes\nStarting training for 1000 epochs...\nEpoch 5: Train Loss=0.4312 | R¬≤=0.0467 | RMSE=0.9848 | MAE=0.9280\nEpoch 100: Train Loss=0.0923 | R¬≤=0.7907 | RMSE=0.4615 | MAE=0.2608\nEpoch 200: Train Loss=0.0772 | R¬≤=0.8079 | RMSE=0.4421 | MAE=0.2313\nEpoch 300: Train Loss=0.0706 | R¬≤=0.8273 | RMSE=0.4192 | MAE=0.2121\nEpoch 400: Train Loss=0.0661 | R¬≤=0.8415 | RMSE=0.4015 | MAE=0.1943\nEpoch 500: Train Loss=0.0611 | R¬≤=0.8527 | RMSE=0.3871 | MAE=0.1762\nEpoch 600: Train Loss=0.0615 | R¬≤=0.8503 | RMSE=0.3902 | MAE=0.1759\nEpoch 700: Train Loss=0.0578 | R¬≤=0.8515 | RMSE=0.3887 | MAE=0.1698\nEpoch 800: Train Loss=0.0568 | R¬≤=0.8557 | RMSE=0.3832 | MAE=0.1711\nEpoch 900: Train Loss=0.0541 | R¬≤=0.8578 | RMSE=0.3804 | MAE=0.1608\nEpoch 1000: Train Loss=0.0531 | R¬≤=0.8573 | RMSE=0.3810 | MAE=0.1626\nBest R¬≤ score: 0.8578\n",
  "history_begin_time" : 1741226700358,
  "history_end_time" : 1741227143826,
  "history_notes" : "ALL data: Number of Nodes: 7029 Number of Edges: 3387 R2 = 0.85",
  "history_process" : "kevt5u",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "s7fhoFbvQ547",
  "history_input" : "import numpy as np\nimport torch\nimport warnings\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch_geometric.nn import SAGEConv, BatchNorm\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Check Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Load and verify graph\ndef load_and_verify_graph(g_path):\n    print(\"Loading graph dataset\")\n    try:\n        g_data = torch.load(g_path)\n        print(f\"Graph successfully loaded from {g_path}\")\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\n        print(f\"Number of Edges: {g_data.num_edges}\")\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\n\n        if hasattr(g_data, \"y\") and g_data.y is not None:\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\n        else:\n            print(f\"Warning: Target variable `y` is missing or None!\")\n\n        return g_data\n    except Exception as e:\n        print(f\"Error loading graph: {e}\")\n        exit()\n\n# Define GraphSAGE model with deeper layers & mean aggregation\nclass GraphSAGE(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\n        super(GraphSAGE, self).__init__()\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\n        self.conv5 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")  # Added extra layer\n\n        self.bn1 = BatchNorm(hidden_dim)\n        self.bn2 = BatchNorm(hidden_dim)\n        self.bn3 = BatchNorm(hidden_dim)\n        self.bn4 = BatchNorm(hidden_dim)\n        self.bn5 = BatchNorm(hidden_dim)\n\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\n        x = self.dropout(x)\n\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\n        x = self.dropout(x)\n\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\n        x = self.dropout(x)\n\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\n        x = self.dropout(x)\n\n        x = F.leaky_relu(self.bn5(self.conv5(x, edge_index)))\n\n        x = F.leaky_relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Training function\ndef train(model, optimizer, loss_fn, graph, train_mask):\n    model.train()\n    optimizer.zero_grad()\n    predictions = model(graph)[train_mask].squeeze()\n    loss = loss_fn(predictions, graph.y[train_mask])\n\n    loss.backward()\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\n    optimizer.step()\n    return loss.item()\n\n# Testing function\ndef test(model, loss_fn, graph, test_mask):\n    model.eval()\n    with torch.no_grad():\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\n        actuals = graph.y[test_mask].cpu().numpy()\n\n        mse = mean_squared_error(actuals, predictions)\n        r2 = r2_score(actuals, predictions)\n        mae = mean_absolute_error(actuals, predictions)\n        rmse = mean_squared_error(actuals, predictions, squared=False)\n\n    return mse, r2, rmse, mae\n\n# Main function\ndef main():\n    g_path = \"/media/volume1/gnn_graph_final_2_2.pt\"  \n    g_data = load_and_verify_graph(g_path)\n\n    # Normalize node features\n    scaler_x = StandardScaler()\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\n\n    # Normalize target variable\n    y_scaler = StandardScaler()\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\n\n    # Training split\n    indices= torch.randperm(g_data.num_nodes)\n    train_size=int(0.8*g_data.num_nodes)\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\n    train_mask[indices[:train_size]] = True  \n    test_mask = ~train_mask  \n\n    print(f\"Training on {train_size} nodes & Testing on {g_data.num_nodes - train_size} nodes\")\n\n    # Define model, optimizer, and loss function\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=512, output_dim=1, dropout=0.4).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-3)\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=50)\n    criterion = nn.SmoothL1Loss(beta=1.0)\n\n    best_r2 = -float(\"inf\")\n\n    print(\"Starting training for 1000 epochs...\")\n\n    for epoch in range(1, 1001):  \n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\n\n        if epoch % 100 == 0 or epoch == 5:  \n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\n            print(f\"Epoch {epoch}: Train Loss={train_loss:.4f} | R¬≤={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\n\n            if r2 > best_r2:\n                best_r2 = r2\n\n    print(f\"Best R¬≤ score: {best_r2:.4f}\")\n\nif __name__ == \"__main__\":\n    main()\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph_final_2_2.pt\nNumber of Nodes: 2064\nNumber of Edges: 1010\nNode Feature Shape: torch.Size([2064, 11])\nEdge Index Shape: torch.Size([2, 1010])\nTarget Variable Shape: torch.Size([2064])\nTraining on 1651 nodes & Testing on 413 nodes\nStarting training for 1000 epochs...\nEpoch 5: Train Loss=0.4123 | R¬≤=0.0508 | RMSE=0.9758 | MAE=0.8913\nEpoch 100: Train Loss=0.0820 | R¬≤=0.7576 | RMSE=0.4931 | MAE=0.2622\nEpoch 200: Train Loss=0.0689 | R¬≤=0.7877 | RMSE=0.4615 | MAE=0.2233\nEpoch 300: Train Loss=0.0593 | R¬≤=0.8119 | RMSE=0.4344 | MAE=0.2123\nEpoch 400: Train Loss=0.0546 | R¬≤=0.8292 | RMSE=0.4139 | MAE=0.1987\nEpoch 500: Train Loss=0.0491 | R¬≤=0.8419 | RMSE=0.3983 | MAE=0.1815\nEpoch 600: Train Loss=0.0452 | R¬≤=0.8486 | RMSE=0.3898 | MAE=0.1713\nEpoch 700: Train Loss=0.0457 | R¬≤=0.8520 | RMSE=0.3853 | MAE=0.1570\nEpoch 800: Train Loss=0.0426 | R¬≤=0.8519 | RMSE=0.3854 | MAE=0.1499\nEpoch 900: Train Loss=0.0367 | R¬≤=0.8607 | RMSE=0.3739 | MAE=0.1449\nEpoch 1000: Train Loss=0.0375 | R¬≤=0.8641 | RMSE=0.3693 | MAE=0.1470\nBest R¬≤ score: 0.8641\n",
  "history_begin_time" : 1741225352609,
  "history_end_time" : 1741225485710,
  "history_notes" : "0.86",
  "history_process" : "kevt5u",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "dhSqV9SSxsRW",
  "history_input" : "import numpy as np\nimport torch\nimport warnings\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch_geometric.nn import SAGEConv, BatchNorm\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Check Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Load and verify graph\ndef load_and_verify_graph(g_path):\n    print(\"Loading graph dataset\")\n    try:\n        g_data = torch.load(g_path)\n        print(f\"Graph successfully loaded from {g_path}\")\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\n        print(f\"Number of Edges: {g_data.num_edges}\")\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\n\n        if hasattr(g_data, \"y\") and g_data.y is not None:\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\n        else:\n            print(f\"Warning: Target variable `y` is missing or None!\")\n\n        return g_data\n    except Exception as e:\n        print(f\"Error loading graph: {e}\")\n        exit()\n\n# Define GraphSAGE model with deeper layers & mean aggregation\nclass GraphSAGE(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\n        super(GraphSAGE, self).__init__()\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\n        self.conv5 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")  # Added extra layer\n\n        self.bn1 = BatchNorm(hidden_dim)\n        self.bn2 = BatchNorm(hidden_dim)\n        self.bn3 = BatchNorm(hidden_dim)\n        self.bn4 = BatchNorm(hidden_dim)\n        self.bn5 = BatchNorm(hidden_dim)\n\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\n        x = self.dropout(x)\n\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\n        x = self.dropout(x)\n\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\n        x = self.dropout(x)\n\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\n        x = self.dropout(x)\n\n        x = F.leaky_relu(self.bn5(self.conv5(x, edge_index)))\n\n        x = F.leaky_relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Training function\ndef train(model, optimizer, loss_fn, graph, train_mask):\n    model.train()\n    optimizer.zero_grad()\n    predictions = model(graph)[train_mask].squeeze()\n    loss = loss_fn(predictions, graph.y[train_mask])\n\n    loss.backward()\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\n    optimizer.step()\n    return loss.item()\n\n# Testing function\ndef test(model, loss_fn, graph, test_mask):\n    model.eval()\n    with torch.no_grad():\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\n        actuals = graph.y[test_mask].cpu().numpy()\n\n        mse = mean_squared_error(actuals, predictions)\n        r2 = r2_score(actuals, predictions)\n        mae = mean_absolute_error(actuals, predictions)\n        rmse = mean_squared_error(actuals, predictions, squared=False)\n\n    return mse, r2, rmse, mae\n\n# Main function\ndef main():\n    g_path = \"/media/volume1/gnn_graph_final_1_1.pt\"  \n    g_data = load_and_verify_graph(g_path)\n\n    # Normalize node features\n    scaler_x = StandardScaler()\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\n\n    # Normalize target variable\n    y_scaler = StandardScaler()\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\n\n    # Training split\n    indices= torch.randperm(g_data.num_nodes)\n    train_size=int(0.8*g_data.num_nodes)\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\n    train_mask[indices[:train_size]] = True  \n    test_mask = ~train_mask  \n\n    print(f\"Training on {train_size} nodes & Testing on {g_data.num_nodes - train_size} nodes\")\n\n    # Define model, optimizer, and loss function\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=512, output_dim=1, dropout=0.4).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-3)\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=50)\n    criterion = nn.SmoothL1Loss(beta=1.0)\n\n    best_r2 = -float(\"inf\")\n\n    print(\"Starting training for 1000 epochs...\")\n\n    for epoch in range(1, 1001):  \n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\n\n        if epoch % 100 == 0 or epoch == 5:  \n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\n            print(f\"Epoch {epoch}: Train Loss={train_loss:.4f} | R¬≤={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\n\n            if r2 > best_r2:\n                best_r2 = r2\n\n    print(f\"Best R¬≤ score: {best_r2:.4f}\")\n\nif __name__ == \"__main__\":\n    main()\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph_final_1_1.pt\nNumber of Nodes: 703\nNumber of Edges: 348\nNode Feature Shape: torch.Size([703, 11])\nEdge Index Shape: torch.Size([2, 348])\nTarget Variable Shape: torch.Size([703])\nTraining on 562 nodes & Testing on 141 nodes\nStarting training for 1000 epochs...\nEpoch 5: Train Loss=0.4274 | R¬≤=0.0223 | RMSE=0.9822 | MAE=0.8960\nEpoch 100: Train Loss=0.1258 | R¬≤=0.7038 | RMSE=0.5406 | MAE=0.3150\nEpoch 200: Train Loss=0.0941 | R¬≤=0.7204 | RMSE=0.5253 | MAE=0.2845\nEpoch 300: Train Loss=0.0726 | R¬≤=0.7357 | RMSE=0.5107 | MAE=0.2629\nEpoch 400: Train Loss=0.0690 | R¬≤=0.7444 | RMSE=0.5022 | MAE=0.2452\nEpoch 500: Train Loss=0.0658 | R¬≤=0.7563 | RMSE=0.4904 | MAE=0.2338\nEpoch 600: Train Loss=0.0595 | R¬≤=0.7501 | RMSE=0.4966 | MAE=0.2360\nEpoch 700: Train Loss=0.0545 | R¬≤=0.7589 | RMSE=0.4878 | MAE=0.2238\nEpoch 800: Train Loss=0.0532 | R¬≤=0.7587 | RMSE=0.4880 | MAE=0.2234\nEpoch 900: Train Loss=0.0474 | R¬≤=0.7589 | RMSE=0.4878 | MAE=0.2209\nEpoch 1000: Train Loss=0.0442 | R¬≤=0.7600 | RMSE=0.4866 | MAE=0.2164\nBest R¬≤ score: 0.7600\n",
  "history_begin_time" : 1741224962226,
  "history_end_time" : 1741225024799,
  "history_notes" : null,
  "history_process" : "kevt5u",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "67wyyttqv0o",
  "history_input" : "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os\nimport warnings\nfrom datetime import datetime\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import GATConv\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch_geometric.utils import is_undirected, to_undirected, contains_self_loops, degree\n\n# Suppress Warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Check Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"‚úÖ Using device: {device}\")\n\n# Step 1: Define the GAT Model\nclass GAT(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, heads=4, dropout=0.5):  # Reduced heads for memory efficiency\n        super(GAT, self).__init__()\n        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, concat=True, dropout=dropout)\n        self.conv2 = GATConv(hidden_dim * heads, hidden_dim, heads=heads, concat=True, dropout=dropout)\n        self.conv3 = GATConv(hidden_dim * heads, hidden_dim, heads=1, concat=False, dropout=dropout)\n        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = self.conv1(x, edge_index)\n        x = F.elu(x)\n        x = self.dropout(x)\n\n        x = self.conv2(x, edge_index)\n        x = F.elu(x)\n        x = self.dropout(x)\n\n        x = self.conv3(x, edge_index)\n        x = F.elu(x)\n\n        x = self.fc(x)  # Output layer\n        return x\n\n# Step 2: Load Graph Data\ndef load_data(graph_file):\n    print(\"üìå Loading graph data...\")\n    try:\n        graph_data = torch.load(graph_file)\n        graph_data = graph_data.to(device)\n\n        # Ensure labels are properly formatted\n        graph_data.y = torch.tensor(StandardScaler().fit_transform(graph_data.y.view(-1, 1)), dtype=torch.float).to(device)\n        graph_data.y = graph_data.y.view(-1)  # Flatten\n\n        print(f\"‚úÖ Graph Loaded: Nodes={graph_data.x.shape[0]}, Features={graph_data.num_features}, Edges={graph_data.edge_index.shape[1]}\")\n        return graph_data\n\n    except Exception as e:\n        print(f\"‚ùå Error loading graph: {e}\")\n        exit()\n\n# Step 3: Validate and Clean Graph\ndef validate_and_clean_graph(graph_data):\n    print(\"üîç Validating Graph Structure...\")\n    \n    # Ensure graph is undirected\n    if not is_undirected(graph_data.edge_index):\n        graph_data.edge_index = to_undirected(graph_data.edge_index)\n    \n    print(\"‚úÖ Graph is undirected.\")\n\n    # Remove isolated nodes\n    degrees = degree(graph_data.edge_index[0], num_nodes=graph_data.num_nodes)\n    non_isolated_nodes = (degrees > 0).nonzero(as_tuple=True)[0]\n\n    graph_data.x = graph_data.x[non_isolated_nodes]\n    graph_data.y = graph_data.y[non_isolated_nodes]\n\n    node_map = {old_idx: new_idx for new_idx, old_idx in enumerate(non_isolated_nodes.tolist())}\n    new_edges = [[node_map[i], node_map[j]] for i, j in graph_data.edge_index.t().tolist() if i in node_map and j in node_map]\n    graph_data.edge_index = torch.tensor(new_edges, dtype=torch.long).t().contiguous()\n    \n    print(f\"‚úÖ Cleaned graph: {graph_data.x.shape[0]} nodes, {graph_data.edge_index.shape[1]} edges\")\n    return graph_data\n\n# Step 4: Training & Testing Functions\ndef train(model, optimizer, criterion, graph_data, train_mask):\n    model.train()\n    optimizer.zero_grad()\n    output = model(graph_data)[train_mask]\n    loss = criterion(output.squeeze(), graph_data.y[train_mask])\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n\ndef test(model, criterion, graph_data, test_mask):\n    model.eval()\n    with torch.no_grad():\n        predictions = model(graph_data)[test_mask].squeeze().cpu().numpy()\n        actuals = graph_data.y[test_mask].cpu().numpy()\n\n        mse = mean_squared_error(actuals, predictions)\n        r2 = r2_score(actuals, predictions)\n        mae = mean_absolute_error(actuals, predictions)\n        rmse = mean_squared_error(actuals, predictions, squared=False)\n\n    return mse, r2, rmse, mae\n\n# Step 5: Main Function\ndef main():\n    graph_file = \"/media/volume1/gnn_graph_final_1_1.pt\"\n    log_dir = \"/media/volume1/swe/data/training_logs\"\n    os.makedirs(log_dir, exist_ok=True)\n    log_file = os.path.join(log_dir, f\"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\")\n\n    def log(message):\n        print(message)\n        with open(log_file, \"a\") as f:\n            f.write(message + \"\\n\")\n\n    graph_data = load_data(graph_file)\n    graph_data = validate_and_clean_graph(graph_data)\n\n    train_ratio = 0.8\n    num_train_samples = int(train_ratio * graph_data.x.shape[0])\n    train_mask = torch.zeros(graph_data.x.shape[0], dtype=torch.bool)\n    train_mask[:num_train_samples] = True\n    test_mask = ~train_mask\n\n    log(\"‚úÖ Model initialized. Training started...\")\n\n    model = GAT(input_dim=graph_data.num_features, hidden_dim=64, output_dim=1, heads=4, dropout=0.5).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.MSELoss()\n\n    best_loss = float('inf')\n    model_save_path = \"/media/volume1/swe/data/best_gat_model_1_1.pt\"\n\n    for epoch in range(1, 201):\n        train_loss = train(model, optimizer, criterion, graph_data, train_mask)\n        test_loss, r2, rmse, mae = test(model, criterion, graph_data, test_mask)\n\n        if test_loss < best_loss:\n            best_loss = test_loss\n            torch.save(model.state_dict(), model_save_path)\n\n        if epoch % 20 == 0:\n            log(f\"Epoch {epoch:03d}, Train Loss: {train_loss:.4f}, R¬≤: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n\n    log(f\"‚úÖ Best model saved at: {model_save_path}\")\n\nif __name__ == \"__main__\":\n    main()",
  "history_output" : "‚úÖ Using device: cpu\nüìå Loading graph data...\n‚úÖ Graph Loaded: Nodes=703, Features=11, Edges=348\nüîç Validating Graph Structure...\n‚úÖ Graph is undirected.\n‚úÖ Cleaned graph: 292 nodes, 348 edges\n‚úÖ Model initialized. Training started...\nEpoch 020, Train Loss: 0.5951, R¬≤: 0.1816, RMSE: 0.8731, MAE: 0.7543\nEpoch 040, Train Loss: 0.5404, R¬≤: 0.1865, RMSE: 0.8705, MAE: 0.7200\nEpoch 060, Train Loss: 0.5503, R¬≤: 0.1734, RMSE: 0.8775, MAE: 0.7481\nEpoch 080, Train Loss: 0.5567, R¬≤: 0.1901, RMSE: 0.8685, MAE: 0.7269\nEpoch 100, Train Loss: 0.5492, R¬≤: 0.1814, RMSE: 0.8732, MAE: 0.7340\nEpoch 120, Train Loss: 0.5665, R¬≤: 0.1870, RMSE: 0.8702, MAE: 0.7286\nEpoch 140, Train Loss: 0.5302, R¬≤: 0.1862, RMSE: 0.8706, MAE: 0.7368\nEpoch 160, Train Loss: 0.5369, R¬≤: 0.1865, RMSE: 0.8705, MAE: 0.7366\nEpoch 180, Train Loss: 0.5297, R¬≤: 0.2043, RMSE: 0.8609, MAE: 0.7212\nEpoch 200, Train Loss: 0.5258, R¬≤: 0.2230, RMSE: 0.8507, MAE: 0.7166\n‚úÖ Best model saved at: /media/volume1/swe/data/best_gat_model_1_1.pt\n",
  "history_begin_time" : 1741212720547,
  "history_end_time" : 1741212802268,
  "history_notes" : null,
  "history_process" : "kevt5u",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "zFrvj8KP2QnR",
  "history_input" : "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os\nimport warnings\nfrom datetime import datetime\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import GATConv\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch_geometric.utils import is_undirected, to_undirected, contains_self_loops, degree\n\n# Suppress Warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Check Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"‚úÖ Using device: {device}\")\n\n# Step 1: Define the GAT Model\nclass GAT(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, heads=4, dropout=0.5):  # Reduced heads for memory efficiency\n        super(GAT, self).__init__()\n        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, concat=True, dropout=dropout)\n        self.conv2 = GATConv(hidden_dim * heads, hidden_dim, heads=heads, concat=True, dropout=dropout)\n        self.conv3 = GATConv(hidden_dim * heads, hidden_dim, heads=1, concat=False, dropout=dropout)\n        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = self.conv1(x, edge_index)\n        x = F.elu(x)\n        x = self.dropout(x)\n\n        x = self.conv2(x, edge_index)\n        x = F.elu(x)\n        x = self.dropout(x)\n\n        x = self.conv3(x, edge_index)\n        x = F.elu(x)\n\n        x = self.fc(x)  # Output layer\n        return x\n\n# Step 2: Load Graph Data\ndef load_data(graph_file):\n    print(\"üìå Loading graph data...\")\n    try:\n        graph_data = torch.load(graph_file)\n        graph_data = graph_data.to(device)\n\n        # Ensure labels are properly formatted\n        graph_data.y = torch.tensor(StandardScaler().fit_transform(graph_data.y.view(-1, 1)), dtype=torch.float).to(device)\n        graph_data.y = graph_data.y.view(-1)  # Flatten\n\n        print(f\"‚úÖ Graph Loaded: Nodes={graph_data.x.shape[0]}, Features={graph_data.num_features}, Edges={graph_data.edge_index.shape[1]}\")\n        return graph_data\n\n    except Exception as e:\n        print(f\"‚ùå Error loading graph: {e}\")\n        exit()\n\n# Step 3: Validate and Clean Graph\ndef validate_and_clean_graph(graph_data):\n    print(\"üîç Validating Graph Structure...\")\n    \n    # Ensure graph is undirected\n    if not is_undirected(graph_data.edge_index):\n        graph_data.edge_index = to_undirected(graph_data.edge_index)\n    \n    print(\"‚úÖ Graph is undirected.\")\n\n    # Remove isolated nodes\n    degrees = degree(graph_data.edge_index[0], num_nodes=graph_data.num_nodes)\n    non_isolated_nodes = (degrees > 0).nonzero(as_tuple=True)[0]\n\n    graph_data.x = graph_data.x[non_isolated_nodes]\n    graph_data.y = graph_data.y[non_isolated_nodes]\n\n    node_map = {old_idx: new_idx for new_idx, old_idx in enumerate(non_isolated_nodes.tolist())}\n    new_edges = [[node_map[i], node_map[j]] for i, j in graph_data.edge_index.t().tolist() if i in node_map and j in node_map]\n    graph_data.edge_index = torch.tensor(new_edges, dtype=torch.long).t().contiguous()\n    \n    print(f\"‚úÖ Cleaned graph: {graph_data.x.shape[0]} nodes, {graph_data.edge_index.shape[1]} edges\")\n    return graph_data\n\n# Step 4: Training & Testing Functions\ndef train(model, optimizer, criterion, graph_data, train_mask):\n    model.train()\n    optimizer.zero_grad()\n    output = model(graph_data)[train_mask]\n    loss = criterion(output.squeeze(), graph_data.y[train_mask])\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n\ndef test(model, criterion, graph_data, test_mask):\n    model.eval()\n    with torch.no_grad():\n        predictions = model(graph_data)[test_mask].squeeze().cpu().numpy()\n        actuals = graph_data.y[test_mask].cpu().numpy()\n\n        mse = mean_squared_error(actuals, predictions)\n        r2 = r2_score(actuals, predictions)\n        mae = mean_absolute_error(actuals, predictions)\n        rmse = mean_squared_error(actuals, predictions, squared=False)\n\n    return mse, r2, rmse, mae\n\n# Step 5: Main Function\ndef main():\n    graph_file = \"/media/volume1/gnn_graph_final_1_1.pt\"\n    log_dir = \"/media/volume1/swe/data/training_logs\"\n    os.makedirs(log_dir, exist_ok=True)\n    log_file = os.path.join(log_dir, f\"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\")\n\n    def log(message):\n        print(message)\n        with open(log_file, \"a\") as f:\n            f.write(message + \"\\n\")\n\n    graph_data = load_data(graph_file)\n    graph_data = validate_and_clean_graph(graph_data)\n\n    train_ratio = 0.8\n    num_train_samples = int(train_ratio * graph_data.x.shape[0])\n    train_mask = torch.zeros(graph_data.x.shape[0], dtype=torch.bool)\n    train_mask[:num_train_samples] = True\n    test_mask = ~train_mask\n\n    log(\"‚úÖ Model initialized. Training started...\")\n\n    model = GAT(input_dim=graph_data.num_features, hidden_dim=64, output_dim=1, heads=4, dropout=0.5).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.MSELoss()\n\n    best_loss = float('inf')\n    model_save_path = \"/media/volume1/swe/data/best_gat_model_1_1.pt\"\n\n    for epoch in range(1, 201):\n        train_loss = train(model, optimizer, criterion, graph_data, train_mask)\n        test_loss, r2, rmse, mae = test(model, criterion, graph_data, test_mask)\n\n        if test_loss < best_loss:\n            best_loss = test_loss\n            torch.save(model.state_dict(), model_save_path)\n\n        if epoch % 20 == 0:\n            log(f\"Epoch {epoch:03d}, Train Loss: {train_loss:.4f}, R¬≤: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n\n    log(f\"‚úÖ Best model saved at: {model_save_path}\")\n\nif __name__ == \"__main__\":\n    main()",
  "history_output" : "‚úÖ Using device: cpu\nüìå Loading graph data...\n‚úÖ Graph Loaded: Nodes=703, Features=11, Edges=348\nüîç Validating Graph Structure...\n‚úÖ Graph is undirected.\n‚úÖ Cleaned graph: 292 nodes, 348 edges\n‚úÖ Model initialized. Training started...\nEpoch 020, Train Loss: 0.5886, R¬≤: 0.1905, RMSE: 0.8683, MAE: 0.7519\nEpoch 040, Train Loss: 0.5575, R¬≤: 0.1646, RMSE: 0.8821, MAE: 0.7449\nEpoch 060, Train Loss: 0.5015, R¬≤: 0.1923, RMSE: 0.8674, MAE: 0.7370\nEpoch 080, Train Loss: 0.5954, R¬≤: 0.1723, RMSE: 0.8781, MAE: 0.7536\nEpoch 100, Train Loss: 0.5065, R¬≤: 0.1939, RMSE: 0.8665, MAE: 0.7144\nEpoch 120, Train Loss: 0.4927, R¬≤: 0.1866, RMSE: 0.8705, MAE: 0.7312\nEpoch 140, Train Loss: 0.5493, R¬≤: 0.1844, RMSE: 0.8716, MAE: 0.7342\nEpoch 160, Train Loss: 0.5242, R¬≤: 0.1980, RMSE: 0.8643, MAE: 0.7345\nEpoch 180, Train Loss: 0.5670, R¬≤: 0.1686, RMSE: 0.8800, MAE: 0.7353\nEpoch 200, Train Loss: 0.4954, R¬≤: 0.1959, RMSE: 0.8654, MAE: 0.7308\n‚úÖ Best model saved at: /media/volume1/swe/data/best_gat_model_1_1.pt\n",
  "history_begin_time" : 1741031422970,
  "history_end_time" : 1741031430522,
  "history_notes" : null,
  "history_process" : "kevt5u",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "Un4c7Vv111hk",
  "history_input" : "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os\nimport warnings\nfrom datetime import datetime\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import GATConv\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch_geometric.utils import is_undirected, to_undirected, contains_self_loops, degree\n\n# Suppress Warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Check Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"‚úÖ Using device: {device}\")\n\n# Step 1: Define the GAT Model\nclass GAT(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, heads=4, dropout=0.5):  # Reduced heads for memory efficiency\n        super(GAT, self).__init__()\n        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, concat=True, dropout=dropout)\n        self.conv2 = GATConv(hidden_dim * heads, hidden_dim, heads=heads, concat=True, dropout=dropout)\n        self.conv3 = GATConv(hidden_dim * heads, hidden_dim, heads=1, concat=False, dropout=dropout)\n        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = self.conv1(x, edge_index)\n        x = F.elu(x)\n        x = self.dropout(x)\n\n        x = self.conv2(x, edge_index)\n        x = F.elu(x)\n        x = self.dropout(x)\n\n        x = self.conv3(x, edge_index)\n        x = F.elu(x)\n\n        x = self.fc(x)  # Output layer\n        return x\n\n# Step 2: Load Graph Data\ndef load_data(graph_file):\n    print(\"üìå Loading graph data...\")\n    try:\n        graph_data = torch.load(graph_file)\n        graph_data = graph_data.to(device)\n\n        # Ensure labels are properly formatted\n        graph_data.y = torch.tensor(StandardScaler().fit_transform(graph_data.y.view(-1, 1)), dtype=torch.float).to(device)\n        graph_data.y = graph_data.y.view(-1)  # Flatten\n\n        print(f\"‚úÖ Graph Loaded: Nodes={graph_data.x.shape[0]}, Features={graph_data.num_features}, Edges={graph_data.edge_index.shape[1]}\")\n        return graph_data\n\n    except Exception as e:\n        print(f\"‚ùå Error loading graph: {e}\")\n        exit()\n\n# Step 3: Validate and Clean Graph\ndef validate_and_clean_graph(graph_data):\n    print(\"üîç Validating Graph Structure...\")\n    \n    # Ensure graph is undirected\n    if not is_undirected(graph_data.edge_index):\n        graph_data.edge_index = to_undirected(graph_data.edge_index)\n    \n    print(\"‚úÖ Graph is undirected.\")\n\n    # Remove isolated nodes\n    degrees = degree(graph_data.edge_index[0], num_nodes=graph_data.num_nodes)\n    non_isolated_nodes = (degrees > 0).nonzero(as_tuple=True)[0]\n\n    graph_data.x = graph_data.x[non_isolated_nodes]\n    graph_data.y = graph_data.y[non_isolated_nodes]\n\n    node_map = {old_idx: new_idx for new_idx, old_idx in enumerate(non_isolated_nodes.tolist())}\n    new_edges = [[node_map[i], node_map[j]] for i, j in graph_data.edge_index.t().tolist() if i in node_map and j in node_map]\n    graph_data.edge_index = torch.tensor(new_edges, dtype=torch.long).t().contiguous()\n    \n    print(f\"‚úÖ Cleaned graph: {graph_data.x.shape[0]} nodes, {graph_data.edge_index.shape[1]} edges\")\n    return graph_data\n\n# Step 4: Training & Testing Functions\ndef train(model, optimizer, criterion, graph_data, train_mask):\n    model.train()\n    optimizer.zero_grad()\n    output = model(graph_data)[train_mask]\n    loss = criterion(output.squeeze(), graph_data.y[train_mask])\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n\ndef test(model, criterion, graph_data, test_mask):\n    model.eval()\n    with torch.no_grad():\n        predictions = model(graph_data)[test_mask].squeeze().cpu().numpy()\n        actuals = graph_data.y[test_mask].cpu().numpy()\n\n        mse = mean_squared_error(actuals, predictions)\n        r2 = r2_score(actuals, predictions)\n        mae = mean_absolute_error(actuals, predictions)\n        rmse = mean_squared_error(actuals, predictions, squared=False)\n\n    return mse, r2, rmse, mae\n\n# Step 5: Main Function\ndef main():\n    graph_file = \"/media/volume1/gat_spatial_training_data_sampled_radius_final_01.pt\"\n    log_dir = \"/media/volume1/swe/data/training_logs\"\n    os.makedirs(log_dir, exist_ok=True)\n    log_file = os.path.join(log_dir, f\"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\")\n\n    def log(message):\n        print(message)\n        with open(log_file, \"a\") as f:\n            f.write(message + \"\\n\")\n\n    graph_data = load_data(graph_file)\n    graph_data = validate_and_clean_graph(graph_data)\n\n    train_ratio = 0.8\n    num_train_samples = int(train_ratio * graph_data.x.shape[0])\n    train_mask = torch.zeros(graph_data.x.shape[0], dtype=torch.bool)\n    train_mask[:num_train_samples] = True\n    test_mask = ~train_mask\n\n    log(\"‚úÖ Model initialized. Training started...\")\n\n    model = GAT(input_dim=graph_data.num_features, hidden_dim=64, output_dim=1, heads=4, dropout=0.5).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.MSELoss()\n\n    best_loss = float('inf')\n    model_save_path = \"/media/volume1/swe/data/best_gat_model_0_1.pt\"\n\n    for epoch in range(1, 201):\n        train_loss = train(model, optimizer, criterion, graph_data, train_mask)\n        test_loss, r2, rmse, mae = test(model, criterion, graph_data, test_mask)\n\n        if test_loss < best_loss:\n            best_loss = test_loss\n            torch.save(model.state_dict(), model_save_path)\n\n        if epoch % 20 == 0:\n            log(f\"Epoch {epoch:03d}, Train Loss: {train_loss:.4f}, R¬≤: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n\n    log(f\"‚úÖ Best model saved at: {model_save_path}\")\n\nif __name__ == \"__main__\":\n    main()",
  "history_output" : "‚úÖ Using device: cpu\nüìå Loading graph data...\n‚úÖ Graph Loaded: Nodes=304, Features=13, Edges=9120\nüîç Validating Graph Structure...\n‚úÖ Graph is undirected.\n‚úÖ Cleaned graph: 304 nodes, 10750 edges\n‚úÖ Model initialized. Training started...\nEpoch 020, Train Loss: 0.7842, R¬≤: 0.3262, RMSE: 0.8202, MAE: 0.7718\nEpoch 040, Train Loss: 0.7480, R¬≤: 0.3549, RMSE: 0.8026, MAE: 0.7573\nEpoch 060, Train Loss: 0.7385, R¬≤: 0.2936, RMSE: 0.8398, MAE: 0.7997\nEpoch 080, Train Loss: 0.7531, R¬≤: 0.3015, RMSE: 0.8351, MAE: 0.8051\nEpoch 100, Train Loss: 0.7470, R¬≤: 0.2226, RMSE: 0.8811, MAE: 0.8601\nEpoch 120, Train Loss: 0.7243, R¬≤: 0.2060, RMSE: 0.8904, MAE: 0.8712\nEpoch 140, Train Loss: 0.7386, R¬≤: 0.2147, RMSE: 0.8855, MAE: 0.8647\nEpoch 160, Train Loss: 0.7263, R¬≤: 0.2320, RMSE: 0.8757, MAE: 0.8481\nEpoch 180, Train Loss: 0.7281, R¬≤: 0.2151, RMSE: 0.8853, MAE: 0.8634\nEpoch 200, Train Loss: 0.7383, R¬≤: 0.2027, RMSE: 0.8922, MAE: 0.8631\n‚úÖ Best model saved at: /media/volume1/swe/data/best_gat_model_0_1.pt\n",
  "history_begin_time" : 1741030113883,
  "history_end_time" : 1741030126840,
  "history_notes" : null,
  "history_process" : "kevt5u",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "GKR8CjhtS1Gr",
  "history_input" : "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os\nimport warnings\nfrom datetime import datetime\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import GATConv\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch_geometric.utils import is_undirected, to_undirected, contains_self_loops, degree\n\n# Suppress Warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Check Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"‚úÖ Using device: {device}\")\n\n# Step 1: Define the GAT Model\nclass GAT(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, heads=4, dropout=0.5):  # Reduced heads for memory efficiency\n        super(GAT, self).__init__()\n        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, concat=True, dropout=dropout)\n        self.conv2 = GATConv(hidden_dim * heads, hidden_dim, heads=heads, concat=True, dropout=dropout)\n        self.conv3 = GATConv(hidden_dim * heads, hidden_dim, heads=1, concat=False, dropout=dropout)\n        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = self.conv1(x, edge_index)\n        x = F.elu(x)\n        x = self.dropout(x)\n\n        x = self.conv2(x, edge_index)\n        x = F.elu(x)\n        x = self.dropout(x)\n\n        x = self.conv3(x, edge_index)\n        x = F.elu(x)\n\n        x = self.fc(x)  # Output layer\n        return x\n\n# Step 2: Load Graph Data\ndef load_data(graph_file):\n    print(\"üìå Loading graph data...\")\n    try:\n        graph_data = torch.load(graph_file)\n        graph_data = graph_data.to(device)\n\n        # Ensure labels are properly formatted\n        graph_data.y = torch.tensor(StandardScaler().fit_transform(graph_data.y.view(-1, 1)), dtype=torch.float).to(device)\n        graph_data.y = graph_data.y.view(-1)  # Flatten\n\n        print(f\"‚úÖ Graph Loaded: Nodes={graph_data.x.shape[0]}, Features={graph_data.num_features}, Edges={graph_data.edge_index.shape[1]}\")\n        return graph_data\n\n    except Exception as e:\n        print(f\"‚ùå Error loading graph: {e}\")\n        exit()\n\n# Step 3: Validate and Clean Graph\ndef validate_and_clean_graph(graph_data):\n    print(\"üîç Validating Graph Structure...\")\n    \n    # Ensure graph is undirected\n    if not is_undirected(graph_data.edge_index):\n        graph_data.edge_index = to_undirected(graph_data.edge_index)\n    \n    print(\"‚úÖ Graph is undirected.\")\n\n    # Remove isolated nodes\n    degrees = degree(graph_data.edge_index[0], num_nodes=graph_data.num_nodes)\n    non_isolated_nodes = (degrees > 0).nonzero(as_tuple=True)[0]\n\n    graph_data.x = graph_data.x[non_isolated_nodes]\n    graph_data.y = graph_data.y[non_isolated_nodes]\n\n    node_map = {old_idx: new_idx for new_idx, old_idx in enumerate(non_isolated_nodes.tolist())}\n    new_edges = [[node_map[i], node_map[j]] for i, j in graph_data.edge_index.t().tolist() if i in node_map and j in node_map]\n    graph_data.edge_index = torch.tensor(new_edges, dtype=torch.long).t().contiguous()\n    \n    print(f\"‚úÖ Cleaned graph: {graph_data.x.shape[0]} nodes, {graph_data.edge_index.shape[1]} edges\")\n    return graph_data\n\n# Step 4: Training & Testing Functions\ndef train(model, optimizer, criterion, graph_data, train_mask):\n    model.train()\n    optimizer.zero_grad()\n    output = model(graph_data)[train_mask]\n    loss = criterion(output.squeeze(), graph_data.y[train_mask])\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n\ndef test(model, criterion, graph_data, test_mask):\n    model.eval()\n    with torch.no_grad():\n        predictions = model(graph_data)[test_mask].squeeze().cpu().numpy()\n        actuals = graph_data.y[test_mask].cpu().numpy()\n\n        mse = mean_squared_error(actuals, predictions)\n        r2 = r2_score(actuals, predictions)\n        mae = mean_absolute_error(actuals, predictions)\n        rmse = mean_squared_error(actuals, predictions, squared=False)\n\n    return mse, r2, rmse, mae\n\n# Step 5: Main Function\ndef main():\n    graph_file = \"/media/volume1/gat_spatial_training_data_sampled_radius_final_01.pt\"\n    log_dir = \"/media/volume1/swe/data/training_logs\"\n    os.makedirs(log_dir, exist_ok=True)\n    log_file = os.path.join(log_dir, f\"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\")\n\n    def log(message):\n        print(message)\n        with open(log_file, \"a\") as f:\n            f.write(message + \"\\n\")\n\n    graph_data = load_data(graph_file)\n    graph_data = validate_and_clean_graph(graph_data)\n\n    train_ratio = 0.8\n    num_train_samples = int(train_ratio * graph_data.x.shape[0])\n    train_mask = torch.zeros(graph_data.x.shape[0], dtype=torch.bool)\n    train_mask[:num_train_samples] = True\n    test_mask = ~train_mask\n\n    log(\"‚úÖ Model initialized. Training started...\")\n\n    model = GAT(input_dim=graph_data.num_features, hidden_dim=64, output_dim=1, heads=4, dropout=0.5).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.MSELoss()\n\n    best_loss = float('inf')\n    model_save_path = \"/media/volume1/swe/data/best_gat_model_0_1.pt\"\n\n    for epoch in range(1, 101):\n        train_loss = train(model, optimizer, criterion, graph_data, train_mask)\n        test_loss, r2, rmse, mae = test(model, criterion, graph_data, test_mask)\n\n        if test_loss < best_loss:\n            best_loss = test_loss\n            torch.save(model.state_dict(), model_save_path)\n\n        if epoch % 20 == 0:\n            log(f\"Epoch {epoch:03d}, Train Loss: {train_loss:.4f}, R¬≤: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n\n    log(f\"‚úÖ Best model saved at: {model_save_path}\")\n\nif __name__ == \"__main__\":\n    main()",
  "history_output" : "‚úÖ Using device: cpu\nüìå Loading graph data...\n‚úÖ Graph Loaded: Nodes=304, Features=13, Edges=9120\nüîç Validating Graph Structure...\n‚úÖ Graph is undirected.\n‚úÖ Cleaned graph: 304 nodes, 10750 edges\n‚úÖ Model initialized. Training started...\nEpoch 020, Train Loss: 0.7902, R¬≤: 0.3359, RMSE: 0.8143, MAE: 0.7627\nEpoch 040, Train Loss: 0.7382, R¬≤: 0.2657, RMSE: 0.8563, MAE: 0.8259\nEpoch 060, Train Loss: 0.7502, R¬≤: 0.2885, RMSE: 0.8429, MAE: 0.8025\nEpoch 080, Train Loss: 0.7494, R¬≤: 0.2716, RMSE: 0.8528, MAE: 0.8263\nEpoch 100, Train Loss: 0.7419, R¬≤: 0.2000, RMSE: 0.8938, MAE: 0.8728\n‚úÖ Best model saved at: /media/volume1/swe/data/best_gat_model_0_1.pt\n",
  "history_begin_time" : 1741030042460,
  "history_end_time" : 1741030051444,
  "history_notes" : null,
  "history_process" : "kevt5u",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "tTENCxhvDo7F",
  "history_input" : "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os\nimport warnings\nfrom datetime import datetime\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import GATConv\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch_geometric.utils import is_undirected, to_undirected, contains_self_loops, degree\n\n# Suppress Warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Check Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"‚úÖ Using device: {device}\")\n\n# Step 1: Define the GAT Model\nclass GAT(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, heads=4, dropout=0.5):  # Reduced heads for memory efficiency\n        super(GAT, self).__init__()\n        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, concat=True, dropout=dropout)\n        self.conv2 = GATConv(hidden_dim * heads, hidden_dim, heads=heads, concat=True, dropout=dropout)\n        self.conv3 = GATConv(hidden_dim * heads, hidden_dim, heads=1, concat=False, dropout=dropout)\n        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = self.conv1(x, edge_index)\n        x = F.elu(x)\n        x = self.dropout(x)\n\n        x = self.conv2(x, edge_index)\n        x = F.elu(x)\n        x = self.dropout(x)\n\n        x = self.conv3(x, edge_index)\n        x = F.elu(x)\n\n        x = self.fc(x)  # Output layer\n        return x\n\n# Step 2: Load Graph Data\ndef load_data(graph_file):\n    print(\"üìå Loading graph data...\")\n    try:\n        graph_data = torch.load(graph_file)\n        graph_data = graph_data.to(device)\n\n        # Ensure labels are properly formatted\n        graph_data.y = torch.tensor(StandardScaler().fit_transform(graph_data.y.view(-1, 1)), dtype=torch.float).to(device)\n        graph_data.y = graph_data.y.view(-1)  # Flatten\n\n        print(f\"‚úÖ Graph Loaded: Nodes={graph_data.x.shape[0]}, Features={graph_data.num_features}, Edges={graph_data.edge_index.shape[1]}\")\n        return graph_data\n\n    except Exception as e:\n        print(f\"‚ùå Error loading graph: {e}\")\n        exit()\n\n# Step 3: Validate and Clean Graph\ndef validate_and_clean_graph(graph_data):\n    print(\"üîç Validating Graph Structure...\")\n    \n    # Ensure graph is undirected\n    if not is_undirected(graph_data.edge_index):\n        graph_data.edge_index = to_undirected(graph_data.edge_index)\n    \n    print(\"‚úÖ Graph is undirected.\")\n\n    # Remove isolated nodes\n    degrees = degree(graph_data.edge_index[0], num_nodes=graph_data.num_nodes)\n    non_isolated_nodes = (degrees > 0).nonzero(as_tuple=True)[0]\n\n    graph_data.x = graph_data.x[non_isolated_nodes]\n    graph_data.y = graph_data.y[non_isolated_nodes]\n\n    node_map = {old_idx: new_idx for new_idx, old_idx in enumerate(non_isolated_nodes.tolist())}\n    new_edges = [[node_map[i], node_map[j]] for i, j in graph_data.edge_index.t().tolist() if i in node_map and j in node_map]\n    graph_data.edge_index = torch.tensor(new_edges, dtype=torch.long).t().contiguous()\n    \n    print(f\"‚úÖ Cleaned graph: {graph_data.x.shape[0]} nodes, {graph_data.edge_index.shape[1]} edges\")\n    return graph_data\n\n# Step 4: Training & Testing Functions\ndef train(model, optimizer, criterion, graph_data, train_mask):\n    model.train()\n    optimizer.zero_grad()\n    output = model(graph_data)[train_mask]\n    loss = criterion(output.squeeze(), graph_data.y[train_mask])\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n\ndef test(model, criterion, graph_data, test_mask):\n    model.eval()\n    with torch.no_grad():\n        predictions = model(graph_data)[test_mask].squeeze().cpu().numpy()\n        actuals = graph_data.y[test_mask].cpu().numpy()\n\n        mse = mean_squared_error(actuals, predictions)\n        r2 = r2_score(actuals, predictions)\n        mae = mean_absolute_error(actuals, predictions)\n        rmse = mean_squared_error(actuals, predictions, squared=False)\n\n    return mse, r2, rmse, mae\n\n# Step 5: Main Function\ndef main():\n    graph_file = \"/media/volume1/gat_spatial_training_data_sampled_k20_g02.pt\"\n    log_dir = \"/media/volume1/swe/data/training_logs\"\n    os.makedirs(log_dir, exist_ok=True)\n    log_file = os.path.join(log_dir, f\"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\")\n\n    def log(message):\n        print(message)\n        with open(log_file, \"a\") as f:\n            f.write(message + \"\\n\")\n\n    graph_data = load_data(graph_file)\n    graph_data = validate_and_clean_graph(graph_data)\n\n    train_ratio = 0.8\n    num_train_samples = int(train_ratio * graph_data.x.shape[0])\n    train_mask = torch.zeros(graph_data.x.shape[0], dtype=torch.bool)\n    train_mask[:num_train_samples] = True\n    test_mask = ~train_mask\n\n    log(\"‚úÖ Model initialized. Training started...\")\n\n    model = GAT(input_dim=graph_data.num_features, hidden_dim=64, output_dim=1, heads=4, dropout=0.5).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.MSELoss()\n\n    best_loss = float('inf')\n    model_save_path = \"/media/volume1/swe/data/best_gat_model_4.pt\"\n\n    for epoch in range(1, 101):\n        train_loss = train(model, optimizer, criterion, graph_data, train_mask)\n        test_loss, r2, rmse, mae = test(model, criterion, graph_data, test_mask)\n\n        if test_loss < best_loss:\n            best_loss = test_loss\n            torch.save(model.state_dict(), model_save_path)\n\n        if epoch % 20 == 0:\n            log(f\"Epoch {epoch:03d}, Train Loss: {train_loss:.4f}, R¬≤: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n\n    log(f\"‚úÖ Best model saved at: {model_save_path}\")\n\nif __name__ == \"__main__\":\n    main()",
  "history_output" : "‚úÖ Using device: cpu\nüìå Loading graph data...\n‚úÖ Graph Loaded: Nodes=163, Features=13, Edges=3260\nüîç Validating Graph Structure...\n‚úÖ Graph is undirected.\n‚úÖ Cleaned graph: 163 nodes, 3776 edges\n‚úÖ Model initialized. Training started...\nEpoch 020, Train Loss: 0.7797, R¬≤: 0.0400, RMSE: 0.9825, MAE: 0.9321\nEpoch 040, Train Loss: 0.7823, R¬≤: -0.0369, RMSE: 1.0211, MAE: 0.9489\nEpoch 060, Train Loss: 0.7537, R¬≤: 0.1214, RMSE: 0.9399, MAE: 0.8995\nEpoch 080, Train Loss: 0.7557, R¬≤: 0.1203, RMSE: 0.9405, MAE: 0.9152\nEpoch 100, Train Loss: 0.7563, R¬≤: 0.1020, RMSE: 0.9502, MAE: 0.9199\n‚úÖ Best model saved at: /media/volume1/swe/data/best_gat_model_4.pt\n",
  "history_begin_time" : 1741027358711,
  "history_end_time" : 1741027365824,
  "history_notes" : null,
  "history_process" : "kevt5u",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "8Pl8QOW49NHD",
  "history_input" : "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os\nimport warnings\nfrom datetime import datetime\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import GATConv\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch_geometric.utils import is_undirected, to_undirected, contains_self_loops, degree\n\n# Suppress Warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Check Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"‚úÖ Using device: {device}\")\n\n# Step 1: Define the GAT Model\nclass GAT(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, heads=4, dropout=0.5):  # Reduced heads for memory efficiency\n        super(GAT, self).__init__()\n        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, concat=True, dropout=dropout)\n        self.conv2 = GATConv(hidden_dim * heads, hidden_dim, heads=heads, concat=True, dropout=dropout)\n        self.conv3 = GATConv(hidden_dim * heads, hidden_dim, heads=1, concat=False, dropout=dropout)\n        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = self.conv1(x, edge_index)\n        x = F.elu(x)\n        x = self.dropout(x)\n\n        x = self.conv2(x, edge_index)\n        x = F.elu(x)\n        x = self.dropout(x)\n\n        x = self.conv3(x, edge_index)\n        x = F.elu(x)\n\n        x = self.fc(x)  # Output layer\n        return x\n\n# Step 2: Load Graph Data\ndef load_data(graph_file):\n    print(\"üìå Loading graph data...\")\n    try:\n        graph_data = torch.load(graph_file)\n        graph_data = graph_data.to(device)\n\n        # Ensure labels are properly formatted\n        graph_data.y = torch.tensor(StandardScaler().fit_transform(graph_data.y.view(-1, 1)), dtype=torch.float).to(device)\n        graph_data.y = graph_data.y.view(-1)  # Flatten\n\n        print(f\"‚úÖ Graph Loaded: Nodes={graph_data.x.shape[0]}, Features={graph_data.num_features}, Edges={graph_data.edge_index.shape[1]}\")\n        return graph_data\n\n    except Exception as e:\n        print(f\"‚ùå Error loading graph: {e}\")\n        exit()\n\n# Step 3: Validate and Clean Graph\ndef validate_and_clean_graph(graph_data):\n    print(\"üîç Validating Graph Structure...\")\n    \n    # Ensure graph is undirected\n    if not is_undirected(graph_data.edge_index):\n        graph_data.edge_index = to_undirected(graph_data.edge_index)\n    \n    print(\"‚úÖ Graph is undirected.\")\n\n    # Remove isolated nodes\n    degrees = degree(graph_data.edge_index[0], num_nodes=graph_data.num_nodes)\n    non_isolated_nodes = (degrees > 0).nonzero(as_tuple=True)[0]\n\n    graph_data.x = graph_data.x[non_isolated_nodes]\n    graph_data.y = graph_data.y[non_isolated_nodes]\n\n    node_map = {old_idx: new_idx for new_idx, old_idx in enumerate(non_isolated_nodes.tolist())}\n    new_edges = [[node_map[i], node_map[j]] for i, j in graph_data.edge_index.t().tolist() if i in node_map and j in node_map]\n    graph_data.edge_index = torch.tensor(new_edges, dtype=torch.long).t().contiguous()\n    \n    print(f\"‚úÖ Cleaned graph: {graph_data.x.shape[0]} nodes, {graph_data.edge_index.shape[1]} edges\")\n    return graph_data\n\n# Step 4: Training & Testing Functions\ndef train(model, optimizer, criterion, graph_data, train_mask):\n    model.train()\n    optimizer.zero_grad()\n    output = model(graph_data)[train_mask]\n    loss = criterion(output.squeeze(), graph_data.y[train_mask])\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n\ndef test(model, criterion, graph_data, test_mask):\n    model.eval()\n    with torch.no_grad():\n        predictions = model(graph_data)[test_mask].squeeze().cpu().numpy()\n        actuals = graph_data.y[test_mask].cpu().numpy()\n\n        mse = mean_squared_error(actuals, predictions)\n        r2 = r2_score(actuals, predictions)\n        mae = mean_absolute_error(actuals, predictions)\n        rmse = mean_squared_error(actuals, predictions, squared=False)\n\n    return mse, r2, rmse, mae\n\n# Step 5: Main Function\ndef main():\n    graph_file = \"/media/volume1/gat_spatial_training_data_sampled_k20_g02.pt\"\n    log_dir = \"/media/volume1/swe/data/training_logs\"\n    os.makedirs(log_dir, exist_ok=True)\n    log_file = os.path.join(log_dir, f\"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\")\n\n    def log(message):\n        print(message)\n        with open(log_file, \"a\") as f:\n            f.write(message + \"\\n\")\n\n    graph_data = load_data(graph_file)\n    graph_data = validate_and_clean_graph(graph_data)\n\n    train_ratio = 0.8\n    num_train_samples = int(train_ratio * graph_data.x.shape[0])\n    train_mask = torch.zeros(graph_data.x.shape[0], dtype=torch.bool)\n    train_mask[:num_train_samples] = True\n    test_mask = ~train_mask\n\n    log(\"‚úÖ Model initialized. Training started...\")\n\n    model = GAT(input_dim=graph_data.num_features, hidden_dim=64, output_dim=1, heads=4, dropout=0.5).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.MSELoss()\n\n    best_loss = float('inf')\n    model_save_path = \"/media/volume1/swe/data/best_gat_model_3.pt\"\n\n    for epoch in range(1, 201):\n        train_loss = train(model, optimizer, criterion, graph_data, train_mask)\n        test_loss, r2, rmse, mae = test(model, criterion, graph_data, test_mask)\n\n        if test_loss < best_loss:\n            best_loss = test_loss\n            torch.save(model.state_dict(), model_save_path)\n\n        if epoch % 20 == 0:\n            log(f\"Epoch {epoch:03d}, Train Loss: {train_loss:.4f}, R¬≤: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n\n    log(f\"‚úÖ Best model saved at: {model_save_path}\")\n\nif __name__ == \"__main__\":\n    main()",
  "history_output" : "‚úÖ Using device: cpu\nüìå Loading graph data...\n‚úÖ Graph Loaded: Nodes=163, Features=13, Edges=3260\nüîç Validating Graph Structure...\n‚úÖ Graph is undirected.\n‚úÖ Cleaned graph: 163 nodes, 3776 edges\n‚úÖ Model initialized. Training started...\nEpoch 020, Train Loss: 0.7882, R¬≤: 0.0029, RMSE: 1.0013, MAE: 0.9473\nEpoch 040, Train Loss: 0.7645, R¬≤: 0.0880, RMSE: 0.9576, MAE: 0.9126\nEpoch 060, Train Loss: 0.8002, R¬≤: 0.0366, RMSE: 0.9842, MAE: 0.9257\nEpoch 080, Train Loss: 0.7547, R¬≤: 0.1091, RMSE: 0.9464, MAE: 0.9080\nEpoch 100, Train Loss: 0.7179, R¬≤: 0.0927, RMSE: 0.9551, MAE: 0.9019\nEpoch 120, Train Loss: 0.7278, R¬≤: 0.1396, RMSE: 0.9301, MAE: 0.9141\nEpoch 140, Train Loss: 0.7460, R¬≤: 0.1079, RMSE: 0.9471, MAE: 0.9077\nEpoch 160, Train Loss: 0.7276, R¬≤: 0.1287, RMSE: 0.9360, MAE: 0.9109\nEpoch 180, Train Loss: 0.7603, R¬≤: 0.0920, RMSE: 0.9555, MAE: 0.9427\nEpoch 200, Train Loss: 0.7097, R¬≤: 0.1288, RMSE: 0.9359, MAE: 0.9142\n‚úÖ Best model saved at: /media/volume1/swe/data/best_gat_model_3.pt\n",
  "history_begin_time" : 1741027337737,
  "history_end_time" : 1741027346746,
  "history_notes" : null,
  "history_process" : "kevt5u",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "uQTCNWdhUzwI",
  "history_input" : "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os\nimport warnings\nfrom datetime import datetime\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import GATConv\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch_geometric.utils import is_undirected, to_undirected, contains_self_loops, degree\n\n# Suppress Warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Check Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"‚úÖ Using device: {device}\")\n\n# Step 1: Define the GAT Model\nclass GAT(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, heads=4, dropout=0.5):  # Reduced heads for memory efficiency\n        super(GAT, self).__init__()\n        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, concat=True, dropout=dropout)\n        self.conv2 = GATConv(hidden_dim * heads, hidden_dim, heads=heads, concat=True, dropout=dropout)\n        self.conv3 = GATConv(hidden_dim * heads, hidden_dim, heads=1, concat=False, dropout=dropout)\n        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = self.conv1(x, edge_index)\n        x = F.elu(x)\n        x = self.dropout(x)\n\n        x = self.conv2(x, edge_index)\n        x = F.elu(x)\n        x = self.dropout(x)\n\n        x = self.conv3(x, edge_index)\n        x = F.elu(x)\n\n        x = self.fc(x)  # Output layer\n        return x\n\n# Step 2: Load Graph Data\ndef load_data(graph_file):\n    print(\"üìå Loading graph data...\")\n    try:\n        graph_data = torch.load(graph_file)\n        graph_data = graph_data.to(device)\n\n        # Ensure labels are properly formatted\n        graph_data.y = torch.tensor(StandardScaler().fit_transform(graph_data.y.view(-1, 1)), dtype=torch.float).to(device)\n        graph_data.y = graph_data.y.view(-1)  # Flatten\n\n        print(f\"‚úÖ Graph Loaded: Nodes={graph_data.x.shape[0]}, Features={graph_data.num_features}, Edges={graph_data.edge_index.shape[1]}\")\n        return graph_data\n\n    except Exception as e:\n        print(f\"‚ùå Error loading graph: {e}\")\n        exit()\n\n# Step 3: Validate and Clean Graph\ndef validate_and_clean_graph(graph_data):\n    print(\"üîç Validating Graph Structure...\")\n    \n    # Ensure graph is undirected\n    if not is_undirected(graph_data.edge_index):\n        graph_data.edge_index = to_undirected(graph_data.edge_index)\n    \n    print(\"‚úÖ Graph is undirected.\")\n\n    # Remove isolated nodes\n    degrees = degree(graph_data.edge_index[0], num_nodes=graph_data.num_nodes)\n    non_isolated_nodes = (degrees > 0).nonzero(as_tuple=True)[0]\n\n    graph_data.x = graph_data.x[non_isolated_nodes]\n    graph_data.y = graph_data.y[non_isolated_nodes]\n\n    node_map = {old_idx: new_idx for new_idx, old_idx in enumerate(non_isolated_nodes.tolist())}\n    new_edges = [[node_map[i], node_map[j]] for i, j in graph_data.edge_index.t().tolist() if i in node_map and j in node_map]\n    graph_data.edge_index = torch.tensor(new_edges, dtype=torch.long).t().contiguous()\n    \n    print(f\"‚úÖ Cleaned graph: {graph_data.x.shape[0]} nodes, {graph_data.edge_index.shape[1]} edges\")\n    return graph_data\n\n# Step 4: Training & Testing Functions\ndef train(model, optimizer, criterion, graph_data, train_mask):\n    model.train()\n    optimizer.zero_grad()\n    output = model(graph_data)[train_mask]\n    loss = criterion(output.squeeze(), graph_data.y[train_mask])\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n\ndef test(model, criterion, graph_data, test_mask):\n    model.eval()\n    with torch.no_grad():\n        predictions = model(graph_data)[test_mask].squeeze().cpu().numpy()\n        actuals = graph_data.y[test_mask].cpu().numpy()\n\n        mse = mean_squared_error(actuals, predictions)\n        r2 = r2_score(actuals, predictions)\n        mae = mean_absolute_error(actuals, predictions)\n        rmse = mean_squared_error(actuals, predictions, squared=False)\n\n    return mse, r2, rmse, mae\n\n# Step 5: Main Function\ndef main():\n    graph_file = \"/media/volume1/gat_spatial_training_data_sampled_k20_g02.pt\"\n    log_dir = \"/media/volume1/swe/data/training_logs\"\n    os.makedirs(log_dir, exist_ok=True)\n    log_file = os.path.join(log_dir, f\"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\")\n\n    def log(message):\n        print(message)\n        with open(log_file, \"a\") as f:\n            f.write(message + \"\\n\")\n\n    graph_data = load_data(graph_file)\n    graph_data = validate_and_clean_graph(graph_data)\n\n    train_ratio = 0.8\n    num_train_samples = int(train_ratio * graph_data.x.shape[0])\n    train_mask = torch.zeros(graph_data.x.shape[0], dtype=torch.bool)\n    train_mask[:num_train_samples] = True\n    test_mask = ~train_mask\n\n    log(\"‚úÖ Model initialized. Training started...\")\n\n    model = GAT(input_dim=graph_data.num_features, hidden_dim=64, output_dim=1, heads=4, dropout=0.5).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.MSELoss()\n\n    best_loss = float('inf')\n    model_save_path = \"/media/volume1/swe/data/best_gat_model_3.pt\"\n\n    for epoch in range(1, 201):\n        train_loss = train(model, optimizer, criterion, graph_data, train_mask)\n        test_loss, r2, rmse, mae = test(model, criterion, graph_data, test_mask)\n\n        if test_loss < best_loss:\n            best_loss = test_loss\n            torch.save(model.state_dict(), model_save_path)\n\n        if epoch % 20 == 0:\n            log(f\"Epoch {epoch:03d}, Train Loss: {train_loss:.4f}, R¬≤: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n\n    log(f\"‚úÖ Best model saved at: {model_save_path}\")\n\nif __name__ == \"__main__\":\n    main()",
  "history_output" : "‚úÖ Using device: cpu\nüìå Loading graph data...\n‚úÖ Graph Loaded: Nodes=163, Features=13, Edges=3260\nüîç Validating Graph Structure...\n‚úÖ Graph is undirected.\n‚úÖ Cleaned graph: 163 nodes, 3776 edges\n‚úÖ Model initialized. Training started...\nEpoch 020, Train Loss: 0.8036, R¬≤: 0.0614, RMSE: 0.9715, MAE: 0.9186\nEpoch 040, Train Loss: 0.7419, R¬≤: 0.0503, RMSE: 0.9772, MAE: 0.9299\nEpoch 060, Train Loss: 0.7276, R¬≤: 0.0937, RMSE: 0.9546, MAE: 0.9150\nEpoch 080, Train Loss: 0.7570, R¬≤: 0.0869, RMSE: 0.9582, MAE: 0.9374\nEpoch 100, Train Loss: 0.7179, R¬≤: 0.1404, RMSE: 0.9297, MAE: 0.9130\nEpoch 120, Train Loss: 0.7517, R¬≤: 0.1547, RMSE: 0.9219, MAE: 0.9140\nEpoch 140, Train Loss: 0.7341, R¬≤: 0.0795, RMSE: 0.9620, MAE: 0.9546\nEpoch 160, Train Loss: 0.7177, R¬≤: 0.1163, RMSE: 0.9426, MAE: 0.9315\nEpoch 180, Train Loss: 0.7244, R¬≤: 0.0809, RMSE: 0.9613, MAE: 0.9491\nEpoch 200, Train Loss: 0.7167, R¬≤: 0.0064, RMSE: 0.9995, MAE: 0.9849\n‚úÖ Best model saved at: /media/volume1/swe/data/best_gat_model_3.pt\n",
  "history_begin_time" : 1741027308899,
  "history_end_time" : 1741027318190,
  "history_notes" : null,
  "history_process" : "kevt5u",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "K05Zuw6ve6Fm",
  "history_input" : "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os\nimport warnings\nfrom datetime import datetime\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import GATConv\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch_geometric.utils import is_undirected, to_undirected, contains_self_loops, degree\n\n# Suppress Warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Check Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"‚úÖ Using device: {device}\")\n\n# Step 1: Define the GAT Model\nclass GAT(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, heads=4, dropout=0.5):  # Reduced heads for memory efficiency\n        super(GAT, self).__init__()\n        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, concat=True, dropout=dropout)\n        self.conv2 = GATConv(hidden_dim * heads, hidden_dim, heads=heads, concat=True, dropout=dropout)\n        self.conv3 = GATConv(hidden_dim * heads, hidden_dim, heads=1, concat=False, dropout=dropout)\n        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = self.conv1(x, edge_index)\n        x = F.elu(x)\n        x = self.dropout(x)\n\n        x = self.conv2(x, edge_index)\n        x = F.elu(x)\n        x = self.dropout(x)\n\n        x = self.conv3(x, edge_index)\n        x = F.elu(x)\n\n        x = self.fc(x)  # Output layer\n        return x\n\n# Step 2: Load Graph Data\ndef load_data(graph_file):\n    print(\"üìå Loading graph data...\")\n    try:\n        graph_data = torch.load(graph_file)\n        graph_data = graph_data.to(device)\n\n        # Ensure labels are properly formatted\n        graph_data.y = torch.tensor(StandardScaler().fit_transform(graph_data.y.view(-1, 1)), dtype=torch.float).to(device)\n        graph_data.y = graph_data.y.view(-1)  # Flatten\n\n        print(f\"‚úÖ Graph Loaded: Nodes={graph_data.x.shape[0]}, Features={graph_data.num_features}, Edges={graph_data.edge_index.shape[1]}\")\n        return graph_data\n\n    except Exception as e:\n        print(f\"‚ùå Error loading graph: {e}\")\n        exit()\n\n# Step 3: Validate and Clean Graph\ndef validate_and_clean_graph(graph_data):\n    print(\"üîç Validating Graph Structure...\")\n    \n    # Ensure graph is undirected\n    if not is_undirected(graph_data.edge_index):\n        graph_data.edge_index = to_undirected(graph_data.edge_index)\n    \n    print(\"‚úÖ Graph is undirected.\")\n\n    # Remove isolated nodes\n    degrees = degree(graph_data.edge_index[0], num_nodes=graph_data.num_nodes)\n    non_isolated_nodes = (degrees > 0).nonzero(as_tuple=True)[0]\n\n    graph_data.x = graph_data.x[non_isolated_nodes]\n    graph_data.y = graph_data.y[non_isolated_nodes]\n\n    node_map = {old_idx: new_idx for new_idx, old_idx in enumerate(non_isolated_nodes.tolist())}\n    new_edges = [[node_map[i], node_map[j]] for i, j in graph_data.edge_index.t().tolist() if i in node_map and j in node_map]\n    graph_data.edge_index = torch.tensor(new_edges, dtype=torch.long).t().contiguous()\n    \n    print(f\"‚úÖ Cleaned graph: {graph_data.x.shape[0]} nodes, {graph_data.edge_index.shape[1]} edges\")\n    return graph_data\n\n# Step 4: Training & Testing Functions\ndef train(model, optimizer, criterion, graph_data, train_mask):\n    model.train()\n    optimizer.zero_grad()\n    output = model(graph_data)[train_mask]\n    loss = criterion(output.squeeze(), graph_data.y[train_mask])\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n\ndef test(model, criterion, graph_data, test_mask):\n    model.eval()\n    with torch.no_grad():\n        predictions = model(graph_data)[test_mask].squeeze().cpu().numpy()\n        actuals = graph_data.y[test_mask].cpu().numpy()\n\n        mse = mean_squared_error(actuals, predictions)\n        r2 = r2_score(actuals, predictions)\n        mae = mean_absolute_error(actuals, predictions)\n        rmse = mean_squared_error(actuals, predictions, squared=False)\n\n    return mse, r2, rmse, mae\n\n# Step 5: Main Function\ndef main():\n    graph_file = \"/media/volume1/gat_spatial_training_data_sampled_k20_g02.pt\"\n    log_dir = \"/media/volume1/swe/data/training_logs\"\n    os.makedirs(log_dir, exist_ok=True)\n    log_file = os.path.join(log_dir, f\"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\")\n\n    def log(message):\n        print(message)\n        with open(log_file, \"a\") as f:\n            f.write(message + \"\\n\")\n\n    graph_data = load_data(graph_file)\n    graph_data = validate_and_clean_graph(graph_data)\n\n    train_ratio = 0.8\n    num_train_samples = int(train_ratio * graph_data.x.shape[0])\n    train_mask = torch.zeros(graph_data.x.shape[0], dtype=torch.bool)\n    train_mask[:num_train_samples] = True\n    test_mask = ~train_mask\n\n    log(\"‚úÖ Model initialized. Training started...\")\n\n    model = GAT(input_dim=graph_data.num_features, hidden_dim=64, output_dim=1, heads=4, dropout=0.5).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.MSELoss()\n\n    best_loss = float('inf')\n    model_save_path = \"/media/volume1/swe/data/best_gat_model_3.pt\"\n\n    for epoch in range(1, 200):\n        train_loss = train(model, optimizer, criterion, graph_data, train_mask)\n        test_loss, r2, rmse, mae = test(model, criterion, graph_data, test_mask)\n\n        if test_loss < best_loss:\n            best_loss = test_loss\n            torch.save(model.state_dict(), model_save_path)\n\n        if epoch % 20 == 0:\n            log(f\"Epoch {epoch:03d}, Train Loss: {train_loss:.4f}, R¬≤: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n\n    log(f\"‚úÖ Best model saved at: {model_save_path}\")\n\nif __name__ == \"__main__\":\n    main()",
  "history_output" : "‚úÖ Using device: cpu\nüìå Loading graph data...\n‚úÖ Graph Loaded: Nodes=163, Features=13, Edges=3260\nüîç Validating Graph Structure...\n‚úÖ Graph is undirected.\n‚úÖ Cleaned graph: 163 nodes, 3776 edges\n‚úÖ Model initialized. Training started...\nEpoch 020, Train Loss: 0.8046, R¬≤: 0.0615, RMSE: 0.9714, MAE: 0.9269\nEpoch 040, Train Loss: 0.7841, R¬≤: -0.0074, RMSE: 1.0064, MAE: 0.9379\nEpoch 060, Train Loss: 0.7428, R¬≤: 0.0605, RMSE: 0.9719, MAE: 0.9292\nEpoch 080, Train Loss: 0.7626, R¬≤: 0.0574, RMSE: 0.9735, MAE: 0.9348\nEpoch 100, Train Loss: 0.7352, R¬≤: 0.1493, RMSE: 0.9249, MAE: 0.8976\nEpoch 120, Train Loss: 0.7394, R¬≤: 0.1063, RMSE: 0.9479, MAE: 0.9343\nEpoch 140, Train Loss: 0.7491, R¬≤: 0.1045, RMSE: 0.9489, MAE: 0.9424\nEpoch 160, Train Loss: 0.7482, R¬≤: 0.0665, RMSE: 0.9688, MAE: 0.9472\nEpoch 180, Train Loss: 0.7402, R¬≤: 0.0855, RMSE: 0.9589, MAE: 0.9524\n‚úÖ Best model saved at: /media/volume1/swe/data/best_gat_model_3.pt\n",
  "history_begin_time" : 1741027280911,
  "history_end_time" : 1741027289988,
  "history_notes" : null,
  "history_process" : "kevt5u",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "GbAUvMBoNIrA",
  "history_input" : "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os\nimport warnings\nfrom datetime import datetime\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import GATConv\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch_geometric.utils import is_undirected, to_undirected, contains_self_loops, degree\n\n# Suppress Warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Check Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"‚úÖ Using device: {device}\")\n\n# Step 1: Define the GAT Model\nclass GAT(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, heads=4, dropout=0.5):  # Reduced heads for memory efficiency\n        super(GAT, self).__init__()\n        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, concat=True, dropout=dropout)\n        self.conv2 = GATConv(hidden_dim * heads, hidden_dim, heads=heads, concat=True, dropout=dropout)\n        self.conv3 = GATConv(hidden_dim * heads, hidden_dim, heads=1, concat=False, dropout=dropout)\n        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = self.conv1(x, edge_index)\n        x = F.elu(x)\n        x = self.dropout(x)\n\n        x = self.conv2(x, edge_index)\n        x = F.elu(x)\n        x = self.dropout(x)\n\n        x = self.conv3(x, edge_index)\n        x = F.elu(x)\n\n        x = self.fc(x)  # Output layer\n        return x\n\n# Step 2: Load Graph Data\ndef load_data(graph_file):\n    print(\"üìå Loading graph data...\")\n    try:\n        graph_data = torch.load(graph_file)\n        graph_data = graph_data.to(device)\n\n        # Ensure labels are properly formatted\n        graph_data.y = torch.tensor(StandardScaler().fit_transform(graph_data.y.view(-1, 1)), dtype=torch.float).to(device)\n        graph_data.y = graph_data.y.view(-1)  # Flatten\n\n        print(f\"‚úÖ Graph Loaded: Nodes={graph_data.x.shape[0]}, Features={graph_data.num_features}, Edges={graph_data.edge_index.shape[1]}\")\n        return graph_data\n\n    except Exception as e:\n        print(f\"‚ùå Error loading graph: {e}\")\n        exit()\n\n# Step 3: Validate and Clean Graph\ndef validate_and_clean_graph(graph_data):\n    print(\"üîç Validating Graph Structure...\")\n    \n    # Ensure graph is undirected\n    if not is_undirected(graph_data.edge_index):\n        graph_data.edge_index = to_undirected(graph_data.edge_index)\n    \n    print(\"‚úÖ Graph is undirected.\")\n\n    # Remove isolated nodes\n    degrees = degree(graph_data.edge_index[0], num_nodes=graph_data.num_nodes)\n    non_isolated_nodes = (degrees > 0).nonzero(as_tuple=True)[0]\n\n    graph_data.x = graph_data.x[non_isolated_nodes]\n    graph_data.y = graph_data.y[non_isolated_nodes]\n\n    node_map = {old_idx: new_idx for new_idx, old_idx in enumerate(non_isolated_nodes.tolist())}\n    new_edges = [[node_map[i], node_map[j]] for i, j in graph_data.edge_index.t().tolist() if i in node_map and j in node_map]\n    graph_data.edge_index = torch.tensor(new_edges, dtype=torch.long).t().contiguous()\n    \n    print(f\"‚úÖ Cleaned graph: {graph_data.x.shape[0]} nodes, {graph_data.edge_index.shape[1]} edges\")\n    return graph_data\n\n# Step 4: Training & Testing Functions\ndef train(model, optimizer, criterion, graph_data, train_mask):\n    model.train()\n    optimizer.zero_grad()\n    output = model(graph_data)[train_mask]\n    loss = criterion(output.squeeze(), graph_data.y[train_mask])\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n\ndef test(model, criterion, graph_data, test_mask):\n    model.eval()\n    with torch.no_grad():\n        predictions = model(graph_data)[test_mask].squeeze().cpu().numpy()\n        actuals = graph_data.y[test_mask].cpu().numpy()\n\n        mse = mean_squared_error(actuals, predictions)\n        r2 = r2_score(actuals, predictions)\n        mae = mean_absolute_error(actuals, predictions)\n        rmse = mean_squared_error(actuals, predictions, squared=False)\n\n    return mse, r2, rmse, mae\n\n# Step 5: Main Function\ndef main():\n    graph_file = \"/media/volume1/gat_spatial_training_data_sampled_k20_g02.pt\"\n    log_dir = \"/media/volume1/swe/data/training_logs\"\n    os.makedirs(log_dir, exist_ok=True)\n    log_file = os.path.join(log_dir, f\"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\")\n\n    def log(message):\n        print(message)\n        with open(log_file, \"a\") as f:\n            f.write(message + \"\\n\")\n\n    graph_data = load_data(graph_file)\n    graph_data = validate_and_clean_graph(graph_data)\n\n    train_ratio = 0.8\n    num_train_samples = int(train_ratio * graph_data.x.shape[0])\n    train_mask = torch.zeros(graph_data.x.shape[0], dtype=torch.bool)\n    train_mask[:num_train_samples] = True\n    test_mask = ~train_mask\n\n    log(\"‚úÖ Model initialized. Training started...\")\n\n    model = GAT(input_dim=graph_data.num_features, hidden_dim=64, output_dim=1, heads=4, dropout=0.5).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.MSELoss()\n\n    best_loss = float('inf')\n    model_save_path = \"/media/volume1/swe/data/best_gat_model_2.pt\"\n\n    for epoch in range(1, 1000):\n        train_loss = train(model, optimizer, criterion, graph_data, train_mask)\n        test_loss, r2, rmse, mae = test(model, criterion, graph_data, test_mask)\n\n        if test_loss < best_loss:\n            best_loss = test_loss\n            torch.save(model.state_dict(), model_save_path)\n\n        if epoch % 100 == 0:\n            log(f\"Epoch {epoch:03d}, Train Loss: {train_loss:.4f}, R¬≤: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n\n    log(f\"‚úÖ Best model saved at: {model_save_path}\")\n\nif __name__ == \"__main__\":\n    main()",
  "history_output" : "‚úÖ Using device: cpu\nüìå Loading graph data...\n‚úÖ Graph Loaded: Nodes=163, Features=13, Edges=3260\nüîç Validating Graph Structure...\n‚úÖ Graph is undirected.\n‚úÖ Cleaned graph: 163 nodes, 3776 edges\n‚úÖ Model initialized. Training started...\nEpoch 100, Train Loss: 0.7314, R¬≤: 0.1252, RMSE: 0.9379, MAE: 0.9279\nEpoch 200, Train Loss: 0.7211, R¬≤: 0.0782, RMSE: 0.9627, MAE: 0.9497\nEpoch 300, Train Loss: 0.7263, R¬≤: 0.1045, RMSE: 0.9489, MAE: 0.9450\nEpoch 400, Train Loss: 0.7326, R¬≤: 0.1025, RMSE: 0.9499, MAE: 0.9402\nEpoch 500, Train Loss: 0.7313, R¬≤: 0.0799, RMSE: 0.9618, MAE: 0.9593\nEpoch 600, Train Loss: 0.6738, R¬≤: 0.0990, RMSE: 0.9518, MAE: 0.9463\nEpoch 700, Train Loss: 0.6475, R¬≤: 0.0850, RMSE: 0.9591, MAE: 0.9532\nEpoch 800, Train Loss: 0.6473, R¬≤: 0.0114, RMSE: 0.9970, MAE: 0.9729\nEpoch 900, Train Loss: 0.5921, R¬≤: 0.0746, RMSE: 0.9646, MAE: 0.9532\n‚úÖ Best model saved at: /media/volume1/swe/data/best_gat_model_2.pt\n",
  "history_begin_time" : 1741027209065,
  "history_end_time" : 1741027236357,
  "history_notes" : null,
  "history_process" : "kevt5u",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "tKcXkTEwXVzs",
  "history_input" : "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os\nimport warnings\nfrom datetime import datetime\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import GATConv\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch_geometric.utils import is_undirected, to_undirected, contains_self_loops, degree\n\n# Suppress Warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Check Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"‚úÖ Using device: {device}\")\n\n# Step 1: Define the GAT Model\nclass GAT(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, heads=32, dropout=0.2):\n        super(GAT, self).__init__()\n        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, concat=True, dropout=dropout)\n        self.conv2 = GATConv(hidden_dim * heads, hidden_dim, heads=heads, concat=True, dropout=dropout)\n        self.conv3 = GATConv(hidden_dim * heads, hidden_dim, heads=1, concat=False, dropout=dropout)\n        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = F.silu(self.conv1(x, edge_index))\n        x = self.dropout(x)\n        x = F.silu(self.conv2(x, edge_index))\n        x = self.dropout(x)\n        x = F.silu(self.conv3(x, edge_index))\n        x = self.fc(x)\n        return x\n\n# Step 2: Load Graph Data\ndef load_data(graph_file):\n    print(\"üìå Loading graph data...\")\n    try:\n        graph_data = torch.load(graph_file)\n        graph_data = graph_data.to(device)\n\n        # Normalize features\n        scaler_x = StandardScaler()\n        graph_data.x = torch.tensor(scaler_x.fit_transform(graph_data.x.cpu().numpy()), dtype=torch.float).to(device)\n\n        # Normalize labels\n        scaler_y = StandardScaler()\n        graph_data.y = torch.tensor(scaler_y.fit_transform(graph_data.y.view(-1, 1)), dtype=torch.float).to(device)\n        graph_data.y = graph_data.y.view(-1)  # Flatten\n\n        print(f\"‚úÖ Graph Loaded: Nodes={graph_data.x.shape[0]}, Features={graph_data.num_features}, Edges={graph_data.edge_index.shape[1]}\")\n        return graph_data\n\n    except Exception as e:\n        print(f\"‚ùå Error loading graph: {e}\")\n        exit()\n\n# Step 3: Validate and Clean Graph\ndef validate_and_clean_graph(graph_data):\n    print(\"üîç Validating Graph Structure...\")\n    \n    if not is_undirected(graph_data.edge_index):\n        graph_data.edge_index = to_undirected(graph_data.edge_index)\n    \n    print(\"‚úÖ Graph is undirected.\")\n\n    degrees = degree(graph_data.edge_index[0], num_nodes=graph_data.num_nodes)\n    non_isolated_nodes = (degrees > 0).nonzero(as_tuple=True)[0]\n\n    graph_data.x = graph_data.x[non_isolated_nodes]\n    graph_data.y = graph_data.y[non_isolated_nodes]\n\n    node_map = {old_idx: new_idx for new_idx, old_idx in enumerate(non_isolated_nodes.tolist())}\n    new_edges = [[node_map[i], node_map[j]] for i, j in graph_data.edge_index.t().tolist() if i in node_map and j in node_map]\n    graph_data.edge_index = torch.tensor(new_edges, dtype=torch.long).t().contiguous()\n    \n    print(f\"‚úÖ Cleaned graph: {graph_data.x.shape[0]} nodes, {graph_data.edge_index.shape[1]} edges\")\n    return graph_data\n\n# Step 4: Training & Testing Functions\ndef train(model, optimizer, criterion, graph_data, train_mask):\n    model.train()\n    optimizer.zero_grad()\n    output = model(graph_data)[train_mask]\n    loss = criterion(output.squeeze(), graph_data.y[train_mask])\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n\ndef test(model, criterion, graph_data, test_mask):\n    model.eval()\n    with torch.no_grad():\n        predictions = model(graph_data)[test_mask].squeeze().cpu().numpy()\n        actuals = graph_data.y[test_mask].cpu().numpy()\n\n        mse = mean_squared_error(actuals, predictions)\n        r2 = r2_score(actuals, predictions)\n        mae = mean_absolute_error(actuals, predictions)\n        rmse = mean_squared_error(actuals, predictions, squared=False)\n\n    return mse, r2, rmse, mae\n\n# Step 5: Main Function\ndef main():\n    graph_file = \"/media/volume1/gat_spatial_training_data_sampled_k20_g02.pt\"\n    log_dir = \"/media/volume1/swe/data/training_logs\"\n    os.makedirs(log_dir, exist_ok=True)\n    log_file = os.path.join(log_dir, f\"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\")\n\n    def log(message):\n        print(message)\n        with open(log_file, \"a\") as f:\n            f.write(message + \"\\n\")\n\n    graph_data = load_data(graph_file)\n    graph_data = validate_and_clean_graph(graph_data)\n\n    train_ratio = 0.8\n    num_train_samples = int(train_ratio * graph_data.x.shape[0])\n    indices = torch.randperm(graph_data.x.shape[0])\n    train_mask = torch.zeros(graph_data.x.shape[0], dtype=torch.bool)\n    train_mask[indices[:num_train_samples]] = True\n    test_mask = ~train_mask\n\n    log(\"‚úÖ Model initialized. Training started...\")\n\n    model = GAT(input_dim=graph_data.num_features, hidden_dim=256, output_dim=1, heads=16, dropout=0.2).to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0003, weight_decay=1e-4)\n    criterion = torch.nn.SmoothL1Loss(beta=1.0)  \n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=50, verbose=True)\n\n    for epoch in range(1, 2001):\n        train_loss = train(model, optimizer, criterion, graph_data, train_mask)\n        test_loss, r2, rmse, mae = test(model, criterion, graph_data, test_mask)\n        scheduler.step(test_loss)\n\n        if epoch % 100 == 0:\n            log(f\"Epoch {epoch:03d}, Train Loss: {train_loss:.4f}, R¬≤: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n\nif __name__ == \"__main__\":\n    main()",
  "history_output" : "‚úÖ Using device: cpu\nüìå Loading graph data...\n‚úÖ Graph Loaded: Nodes=163, Features=13, Edges=3260\nüîç Validating Graph Structure...\n‚úÖ Graph is undirected.\n‚úÖ Cleaned graph: 163 nodes, 3776 edges\n‚úÖ Model initialized. Training started...\nEpoch 100, Train Loss: 0.3185, R¬≤: -0.0025, RMSE: 1.0003, MAE: 0.8077\nEpoch 200, Train Loss: 0.3074, R¬≤: -0.0177, RMSE: 1.0079, MAE: 0.7905\nEpoch 300, Train Loss: 0.2901, R¬≤: -0.0289, RMSE: 1.0134, MAE: 0.7951\nEpoch 400, Train Loss: 0.2921, R¬≤: -0.0313, RMSE: 1.0146, MAE: 0.7954\n",
  "history_begin_time" : 1741027009025,
  "history_end_time" : 1741027129438,
  "history_notes" : null,
  "history_process" : "kevt5u",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "kYhoVEbdXsID",
  "history_input" : "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os\nimport warnings\nfrom datetime import datetime\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import GATConv\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch_geometric.utils import is_undirected, to_undirected, contains_self_loops, degree\n\n# Suppress Warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Check Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"‚úÖ Using device: {device}\")\n\n# Step 1: Define the GAT Model\nclass GAT(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, heads=32, dropout=0.2):\n        super(GAT, self).__init__()\n        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, concat=True, dropout=dropout)\n        self.conv2 = GATConv(hidden_dim * heads, hidden_dim, heads=heads, concat=True, dropout=dropout)\n        self.conv3 = GATConv(hidden_dim * heads, hidden_dim, heads=1, concat=False, dropout=dropout)\n        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = F.silu(self.conv1(x, edge_index))\n        x = self.dropout(x)\n        x = F.silu(self.conv2(x, edge_index))\n        x = self.dropout(x)\n        x = F.silu(self.conv3(x, edge_index))\n        x = self.fc(x)\n        return x\n\n# Step 2: Load Graph Data\ndef load_data(graph_file):\n    print(\"üìå Loading graph data...\")\n    try:\n        graph_data = torch.load(graph_file)\n        graph_data = graph_data.to(device)\n\n        # Normalize features\n        scaler_x = StandardScaler()\n        graph_data.x = torch.tensor(scaler_x.fit_transform(graph_data.x.cpu().numpy()), dtype=torch.float).to(device)\n\n        # Normalize labels\n        scaler_y = StandardScaler()\n        graph_data.y = torch.tensor(scaler_y.fit_transform(graph_data.y.view(-1, 1)), dtype=torch.float).to(device)\n        graph_data.y = graph_data.y.view(-1)  # Flatten\n\n        print(f\"‚úÖ Graph Loaded: Nodes={graph_data.x.shape[0]}, Features={graph_data.num_features}, Edges={graph_data.edge_index.shape[1]}\")\n        return graph_data\n\n    except Exception as e:\n        print(f\"‚ùå Error loading graph: {e}\")\n        exit()\n\n# Step 3: Validate and Clean Graph\ndef validate_and_clean_graph(graph_data):\n    print(\"üîç Validating Graph Structure...\")\n    \n    if not is_undirected(graph_data.edge_index):\n        graph_data.edge_index = to_undirected(graph_data.edge_index)\n    \n    print(\"‚úÖ Graph is undirected.\")\n\n    degrees = degree(graph_data.edge_index[0], num_nodes=graph_data.num_nodes)\n    non_isolated_nodes = (degrees > 0).nonzero(as_tuple=True)[0]\n\n    graph_data.x = graph_data.x[non_isolated_nodes]\n    graph_data.y = graph_data.y[non_isolated_nodes]\n\n    node_map = {old_idx: new_idx for new_idx, old_idx in enumerate(non_isolated_nodes.tolist())}\n    new_edges = [[node_map[i], node_map[j]] for i, j in graph_data.edge_index.t().tolist() if i in node_map and j in node_map]\n    graph_data.edge_index = torch.tensor(new_edges, dtype=torch.long).t().contiguous()\n    \n    print(f\"‚úÖ Cleaned graph: {graph_data.x.shape[0]} nodes, {graph_data.edge_index.shape[1]} edges\")\n    return graph_data\n\n# Step 4: Training & Testing Functions\ndef train(model, optimizer, criterion, graph_data, train_mask):\n    model.train()\n    optimizer.zero_grad()\n    output = model(graph_data)[train_mask]\n    loss = criterion(output.squeeze(), graph_data.y[train_mask])\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n\ndef test(model, criterion, graph_data, test_mask):\n    model.eval()\n    with torch.no_grad():\n        predictions = model(graph_data)[test_mask].squeeze().cpu().numpy()\n        actuals = graph_data.y[test_mask].cpu().numpy()\n\n        mse = mean_squared_error(actuals, predictions)\n        r2 = r2_score(actuals, predictions)\n        mae = mean_absolute_error(actuals, predictions)\n        rmse = mean_squared_error(actuals, predictions, squared=False)\n\n    return mse, r2, rmse, mae\n\n# Step 5: Main Function\ndef main():\n    graph_file = \"/media/volume1/gat_spatial_training_data_sampled_knn_10km.pt\"\n    log_dir = \"/media/volume1/swe/data/training_logs\"\n    os.makedirs(log_dir, exist_ok=True)\n    log_file = os.path.join(log_dir, f\"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\")\n\n    def log(message):\n        print(message)\n        with open(log_file, \"a\") as f:\n            f.write(message + \"\\n\")\n\n    graph_data = load_data(graph_file)\n    graph_data = validate_and_clean_graph(graph_data)\n\n    train_ratio = 0.8\n    num_train_samples = int(train_ratio * graph_data.x.shape[0])\n    indices = torch.randperm(graph_data.x.shape[0])\n    train_mask = torch.zeros(graph_data.x.shape[0], dtype=torch.bool)\n    train_mask[indices[:num_train_samples]] = True\n    test_mask = ~train_mask\n\n    log(\"‚úÖ Model initialized. Training started...\")\n\n    model = GAT(input_dim=graph_data.num_features, hidden_dim=256, output_dim=1, heads=16, dropout=0.2).to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0003, weight_decay=1e-4)\n    criterion = torch.nn.SmoothL1Loss(beta=1.0)  \n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=50, verbose=True)\n\n    for epoch in range(1, 2001):\n        train_loss = train(model, optimizer, criterion, graph_data, train_mask)\n        test_loss, r2, rmse, mae = test(model, criterion, graph_data, test_mask)\n        scheduler.step(test_loss)\n\n        if epoch % 100 == 0:\n            log(f\"Epoch {epoch:03d}, Train Loss: {train_loss:.4f}, R¬≤: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n\nif __name__ == \"__main__\":\n    main()",
  "history_output" : "‚úÖ Using device: cpu\nüìå Loading graph data...\n‚úÖ Graph Loaded: Nodes=247, Features=13, Edges=1235\nüîç Validating Graph Structure...\n‚úÖ Graph is undirected.\n‚úÖ Cleaned graph: 247 nodes, 1504 edges\n‚úÖ Model initialized. Training started...\nEpoch 100, Train Loss: 0.3008, R¬≤: 0.3618, RMSE: 0.7977, MAE: 0.6009\nEpoch 200, Train Loss: 0.2815, R¬≤: 0.3358, RMSE: 0.8138, MAE: 0.6070\nEpoch 300, Train Loss: 0.2815, R¬≤: 0.3341, RMSE: 0.8148, MAE: 0.6109\nEpoch 400, Train Loss: 0.2793, R¬≤: 0.3355, RMSE: 0.8140, MAE: 0.6078\nEpoch 500, Train Loss: 0.2902, R¬≤: 0.3334, RMSE: 0.8153, MAE: 0.6069\nEpoch 600, Train Loss: 0.2824, R¬≤: 0.3328, RMSE: 0.8156, MAE: 0.6072\nEpoch 700, Train Loss: 0.2870, R¬≤: 0.3328, RMSE: 0.8157, MAE: 0.6071\nEpoch 800, Train Loss: 0.2816, R¬≤: 0.3328, RMSE: 0.8157, MAE: 0.6072\nEpoch 900, Train Loss: 0.2823, R¬≤: 0.3328, RMSE: 0.8156, MAE: 0.6072\nEpoch 1000, Train Loss: 0.2809, R¬≤: 0.3328, RMSE: 0.8157, MAE: 0.6072\nEpoch 1100, Train Loss: 0.2781, R¬≤: 0.3328, RMSE: 0.8156, MAE: 0.6071\nEpoch 1200, Train Loss: 0.2904, R¬≤: 0.3329, RMSE: 0.8156, MAE: 0.6071\nEpoch 1300, Train Loss: 0.2792, R¬≤: 0.3329, RMSE: 0.8156, MAE: 0.6072\nEpoch 1400, Train Loss: 0.2809, R¬≤: 0.3329, RMSE: 0.8156, MAE: 0.6071\nEpoch 1500, Train Loss: 0.2878, R¬≤: 0.3329, RMSE: 0.8156, MAE: 0.6071\nEpoch 1600, Train Loss: 0.2777, R¬≤: 0.3328, RMSE: 0.8156, MAE: 0.6071\nEpoch 1700, Train Loss: 0.2793, R¬≤: 0.3328, RMSE: 0.8156, MAE: 0.6071\nEpoch 1800, Train Loss: 0.2811, R¬≤: 0.3328, RMSE: 0.8156, MAE: 0.6071\nEpoch 1900, Train Loss: 0.2876, R¬≤: 0.3329, RMSE: 0.8156, MAE: 0.6070\nEpoch 2000, Train Loss: 0.2916, R¬≤: 0.3329, RMSE: 0.8156, MAE: 0.6071\n",
  "history_begin_time" : 1741025525186,
  "history_end_time" : 1741025940563,
  "history_notes" : null,
  "history_process" : "kevt5u",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "GyNyCDsnT7gz",
  "history_input" : "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os\nimport warnings\nfrom datetime import datetime\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import GATConv\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch_geometric.utils import is_undirected, to_undirected, contains_self_loops, degree\n\n# Suppress Warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Check Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"‚úÖ Using device: {device}\")\n\n# Step 1: Define the GAT Model\nclass GAT(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, heads=16, dropout=0.2):\n        super(GAT, self).__init__()\n        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, concat=True, dropout=dropout)\n        self.conv2 = GATConv(hidden_dim * heads, hidden_dim, heads=heads, concat=True, dropout=dropout)\n        self.conv3 = GATConv(hidden_dim * heads, hidden_dim, heads=1, concat=False, dropout=dropout)\n        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = F.leaky_relu(self.conv1(x, edge_index))\n        x = self.dropout(x)\n        x = F.leaky_relu(self.conv2(x, edge_index))\n        x = self.dropout(x)\n        x = F.leaky_relu(self.conv3(x, edge_index))\n        x = self.fc(x)\n        return x\n\n# Step 2: Load Graph Data\ndef load_data(graph_file):\n    print(\"üìå Loading graph data...\")\n    try:\n        graph_data = torch.load(graph_file)\n        graph_data = graph_data.to(device)\n\n        # Normalize features\n        scaler_x = StandardScaler()\n        graph_data.x = torch.tensor(scaler_x.fit_transform(graph_data.x.cpu().numpy()), dtype=torch.float).to(device)\n\n        # Normalize labels\n        scaler_y = StandardScaler()\n        graph_data.y = torch.tensor(scaler_y.fit_transform(graph_data.y.view(-1, 1)), dtype=torch.float).to(device)\n        graph_data.y = graph_data.y.view(-1)  # Flatten\n\n        print(f\"‚úÖ Graph Loaded: Nodes={graph_data.x.shape[0]}, Features={graph_data.num_features}, Edges={graph_data.edge_index.shape[1]}\")\n        return graph_data\n\n    except Exception as e:\n        print(f\"‚ùå Error loading graph: {e}\")\n        exit()\n\n# Step 3: Validate and Clean Graph\ndef validate_and_clean_graph(graph_data):\n    print(\"üîç Validating Graph Structure...\")\n    \n    if not is_undirected(graph_data.edge_index):\n        graph_data.edge_index = to_undirected(graph_data.edge_index)\n    \n    print(\"‚úÖ Graph is undirected.\")\n\n    degrees = degree(graph_data.edge_index[0], num_nodes=graph_data.num_nodes)\n    non_isolated_nodes = (degrees > 0).nonzero(as_tuple=True)[0]\n\n    graph_data.x = graph_data.x[non_isolated_nodes]\n    graph_data.y = graph_data.y[non_isolated_nodes]\n\n    node_map = {old_idx: new_idx for new_idx, old_idx in enumerate(non_isolated_nodes.tolist())}\n    new_edges = [[node_map[i], node_map[j]] for i, j in graph_data.edge_index.t().tolist() if i in node_map and j in node_map]\n    graph_data.edge_index = torch.tensor(new_edges, dtype=torch.long).t().contiguous()\n    \n    print(f\"‚úÖ Cleaned graph: {graph_data.x.shape[0]} nodes, {graph_data.edge_index.shape[1]} edges\")\n    return graph_data\n\n# Step 4: Training & Testing Functions\ndef train(model, optimizer, criterion, graph_data, train_mask):\n    model.train()\n    optimizer.zero_grad()\n    output = model(graph_data)[train_mask]\n    loss = criterion(output.squeeze(), graph_data.y[train_mask])\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n\ndef test(model, criterion, graph_data, test_mask):\n    model.eval()\n    with torch.no_grad():\n        predictions = model(graph_data)[test_mask].squeeze().cpu().numpy()\n        actuals = graph_data.y[test_mask].cpu().numpy()\n\n        mse = mean_squared_error(actuals, predictions)\n        r2 = r2_score(actuals, predictions)\n        mae = mean_absolute_error(actuals, predictions)\n        rmse = mean_squared_error(actuals, predictions, squared=False)\n\n    return mse, r2, rmse, mae\n\n# Step 5: Main Function\ndef main():\n    graph_file = \"/media/volume1/gat_spatial_training_data_sampled_knn_10km.pt\"\n    log_dir = \"/media/volume1/swe/data/training_logs\"\n    os.makedirs(log_dir, exist_ok=True)\n    log_file = os.path.join(log_dir, f\"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\")\n\n    def log(message):\n        print(message)\n        with open(log_file, \"a\") as f:\n            f.write(message + \"\\n\")\n\n    graph_data = load_data(graph_file)\n    graph_data = validate_and_clean_graph(graph_data)\n\n    train_ratio = 0.8\n    num_train_samples = int(train_ratio * graph_data.x.shape[0])\n    indices = torch.randperm(graph_data.x.shape[0])\n    train_mask = torch.zeros(graph_data.x.shape[0], dtype=torch.bool)\n    train_mask[indices[:num_train_samples]] = True\n    test_mask = ~train_mask\n\n    log(\"‚úÖ Model initialized. Training started...\")\n\n    model = GAT(input_dim=graph_data.num_features, hidden_dim=128, output_dim=1, heads=16, dropout=0.2).to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0003, weight_decay=1e-4)\n    criterion = torch.nn.MSELoss()\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=100, verbose=True)\n\n    for epoch in range(1, 2001):\n        train_loss = train(model, optimizer, criterion, graph_data, train_mask)\n        test_loss, r2, rmse, mae = test(model, criterion, graph_data, test_mask)\n        scheduler.step(test_loss)\n\n        if epoch % 100 == 0:\n            log(f\"Epoch {epoch:03d}, Train Loss: {train_loss:.4f}, R¬≤: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n\nif __name__ == \"__main__\":\n    main()",
  "history_output" : "‚úÖ Using device: cpu\nüìå Loading graph data...\n‚úÖ Graph Loaded: Nodes=247, Features=13, Edges=1235\nüîç Validating Graph Structure...\n‚úÖ Graph is undirected.\n‚úÖ Cleaned graph: 247 nodes, 1504 edges\n‚úÖ Model initialized. Training started...\nEpoch 100, Train Loss: 0.5760, R¬≤: 0.0296, RMSE: 0.9576, MAE: 0.7641\nEpoch 200, Train Loss: 0.5076, R¬≤: -0.0320, RMSE: 0.9875, MAE: 0.7902\nEpoch 300, Train Loss: 0.5242, R¬≤: -0.0544, RMSE: 0.9982, MAE: 0.7855\nEpoch 400, Train Loss: 0.5034, R¬≤: -0.0702, RMSE: 1.0057, MAE: 0.7785\nEpoch 500, Train Loss: 0.4878, R¬≤: -0.0765, RMSE: 1.0086, MAE: 0.7878\nEpoch 600, Train Loss: 0.4580, R¬≤: -0.0730, RMSE: 1.0069, MAE: 0.7731\nEpoch 700, Train Loss: 0.5041, R¬≤: -0.0677, RMSE: 1.0044, MAE: 0.7715\nEpoch 800, Train Loss: 0.4863, R¬≤: -0.0758, RMSE: 1.0083, MAE: 0.7736\nEpoch 900, Train Loss: 0.5045, R¬≤: -0.0790, RMSE: 1.0098, MAE: 0.7737\nEpoch 1000, Train Loss: 0.4763, R¬≤: -0.0786, RMSE: 1.0096, MAE: 0.7751\nEpoch 1100, Train Loss: 0.4767, R¬≤: -0.0786, RMSE: 1.0096, MAE: 0.7742\nEpoch 1200, Train Loss: 0.4743, R¬≤: -0.0785, RMSE: 1.0095, MAE: 0.7745\nEpoch 1300, Train Loss: 0.4729, R¬≤: -0.0783, RMSE: 1.0094, MAE: 0.7743\nEpoch 1400, Train Loss: 0.4612, R¬≤: -0.0785, RMSE: 1.0095, MAE: 0.7744\nEpoch 1500, Train Loss: 0.4941, R¬≤: -0.0785, RMSE: 1.0095, MAE: 0.7744\nEpoch 1600, Train Loss: 0.4726, R¬≤: -0.0784, RMSE: 1.0095, MAE: 0.7744\nEpoch 1700, Train Loss: 0.4619, R¬≤: -0.0783, RMSE: 1.0094, MAE: 0.7744\nEpoch 1800, Train Loss: 0.4804, R¬≤: -0.0783, RMSE: 1.0094, MAE: 0.7744\nEpoch 1900, Train Loss: 0.5102, R¬≤: -0.0783, RMSE: 1.0094, MAE: 0.7743\nEpoch 2000, Train Loss: 0.4894, R¬≤: -0.0783, RMSE: 1.0094, MAE: 0.7743\n",
  "history_begin_time" : 1741024979838,
  "history_end_time" : 1741025134725,
  "history_notes" : null,
  "history_process" : "kevt5u",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "ZWMgrR3BaOTX",
  "history_input" : "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os\nimport warnings\nfrom datetime import datetime\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import GATConv\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch_geometric.utils import is_undirected, to_undirected, contains_self_loops, degree\n\n# Suppress Warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Check Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"‚úÖ Using device: {device}\")\n\n# Step 1: Define the GAT Model\nclass GAT(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, heads=8, dropout=0.5):  # Increased heads for better learning\n        super(GAT, self).__init__()\n        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, concat=True, dropout=dropout)\n        self.conv2 = GATConv(hidden_dim * heads, hidden_dim, heads=heads, concat=True, dropout=dropout)\n        self.conv3 = GATConv(hidden_dim * heads, hidden_dim, heads=1, concat=False, dropout=dropout)\n        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = self.conv1(x, edge_index)\n        x = F.elu(x)\n        x = self.dropout(x)\n\n        x = self.conv2(x, edge_index)\n        x = F.elu(x)\n        x = self.dropout(x)\n\n        x = self.conv3(x, edge_index)\n        x = F.elu(x)\n\n        x = self.fc(x)  # Output layer\n        return x\n\n# Step 2: Load Graph Data\ndef load_data(graph_file):\n    print(\"üìå Loading graph data...\")\n    try:\n        graph_data = torch.load(graph_file)\n        graph_data = graph_data.to(device)\n\n        # Normalize features\n        scaler_x = StandardScaler()\n        graph_data.x = torch.tensor(scaler_x.fit_transform(graph_data.x.cpu().numpy()), dtype=torch.float).to(device)\n\n        # Normalize labels\n        scaler_y = StandardScaler()\n        graph_data.y = torch.tensor(scaler_y.fit_transform(graph_data.y.view(-1, 1)), dtype=torch.float).to(device)\n        graph_data.y = graph_data.y.view(-1)  # Flatten\n\n        print(f\"‚úÖ Graph Loaded: Nodes={graph_data.x.shape[0]}, Features={graph_data.num_features}, Edges={graph_data.edge_index.shape[1]}\")\n        return graph_data\n\n    except Exception as e:\n        print(f\"‚ùå Error loading graph: {e}\")\n        exit()\n\n# Step 3: Validate and Clean Graph\ndef validate_and_clean_graph(graph_data):\n    print(\"üîç Validating Graph Structure...\")\n    \n    # Ensure graph is undirected\n    if not is_undirected(graph_data.edge_index):\n        graph_data.edge_index = to_undirected(graph_data.edge_index)\n    \n    print(\"‚úÖ Graph is undirected.\")\n\n    # Remove isolated nodes\n    degrees = degree(graph_data.edge_index[0], num_nodes=graph_data.num_nodes)\n    non_isolated_nodes = (degrees > 0).nonzero(as_tuple=True)[0]\n\n    graph_data.x = graph_data.x[non_isolated_nodes]\n    graph_data.y = graph_data.y[non_isolated_nodes]\n\n    node_map = {old_idx: new_idx for new_idx, old_idx in enumerate(non_isolated_nodes.tolist())}\n    new_edges = [[node_map[i], node_map[j]] for i, j in graph_data.edge_index.t().tolist() if i in node_map and j in node_map]\n    graph_data.edge_index = torch.tensor(new_edges, dtype=torch.long).t().contiguous()\n    \n    print(f\"‚úÖ Cleaned graph: {graph_data.x.shape[0]} nodes, {graph_data.edge_index.shape[1]} edges\")\n    return graph_data\n\n# Step 4: Training & Testing Functions\ndef train(model, optimizer, criterion, graph_data, train_mask):\n    model.train()\n    optimizer.zero_grad()\n    output = model(graph_data)[train_mask]\n    loss = criterion(output.squeeze(), graph_data.y[train_mask])\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n\ndef test(model, criterion, graph_data, test_mask):\n    model.eval()\n    with torch.no_grad():\n        predictions = model(graph_data)[test_mask].squeeze().cpu().numpy()\n        actuals = graph_data.y[test_mask].cpu().numpy()\n\n        mse = mean_squared_error(actuals, predictions)\n        r2 = r2_score(actuals, predictions)\n        mae = mean_absolute_error(actuals, predictions)\n        rmse = mean_squared_error(actuals, predictions, squared=False)\n\n    return mse, r2, rmse, mae\n\n# Step 5: Main Function\ndef main():\n    graph_file = \"/media/volume1/gat_spatial_training_data_sampled_knn_10km.pt\"\n    log_dir = \"/media/volume1/swe/data/training_logs\"\n    os.makedirs(log_dir, exist_ok=True)\n    log_file = os.path.join(log_dir, f\"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\")\n\n    def log(message):\n        print(message)\n        with open(log_file, \"a\") as f:\n            f.write(message + \"\\n\")\n\n    graph_data = load_data(graph_file)\n    graph_data = validate_and_clean_graph(graph_data)\n\n    train_ratio = 0.8\n    num_train_samples = int(train_ratio * graph_data.x.shape[0])\n    train_mask = torch.zeros(graph_data.x.shape[0], dtype=torch.bool)\n    train_mask[:num_train_samples] = True\n    test_mask = ~train_mask\n\n    log(\"‚úÖ Model initialized. Training started...\")\n\n    model = GAT(input_dim=graph_data.num_features, hidden_dim=128, output_dim=1, heads=8, dropout=0.5).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=5e-4)\n    criterion = torch.nn.MSELoss()\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=50, verbose=True)\n\n    for epoch in range(1, 2001):\n        train_loss = train(model, optimizer, criterion, graph_data, train_mask)\n        test_loss, r2, rmse, mae = test(model, criterion, graph_data, test_mask)\n        scheduler.step(test_loss)\n\n        if epoch % 100 == 0:\n            log(f\"Epoch {epoch:03d}, Train Loss: {train_loss:.4f}, R¬≤: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n\nif __name__ == \"__main__\":\n    main()",
  "history_output" : "‚úÖ Using device: cpu\nüìå Loading graph data...\n‚úÖ Graph Loaded: Nodes=247, Features=13, Edges=1235\nüîç Validating Graph Structure...\n‚úÖ Graph is undirected.\n‚úÖ Cleaned graph: 247 nodes, 1504 edges\n‚úÖ Model initialized. Training started...\nEpoch 100, Train Loss: 0.6680, R¬≤: 0.2014, RMSE: 0.8920, MAE: 0.8187\nEpoch 200, Train Loss: 0.6662, R¬≤: 0.0745, RMSE: 0.9603, MAE: 0.8645\nEpoch 300, Train Loss: 0.6787, R¬≤: 0.0356, RMSE: 0.9803, MAE: 0.8823\nEpoch 400, Train Loss: 0.6360, R¬≤: 0.0519, RMSE: 0.9720, MAE: 0.8698\nEpoch 500, Train Loss: 0.6681, R¬≤: 0.0463, RMSE: 0.9748, MAE: 0.8751\nEpoch 600, Train Loss: 0.6705, R¬≤: 0.0410, RMSE: 0.9776, MAE: 0.8775\nEpoch 700, Train Loss: 0.6863, R¬≤: 0.0416, RMSE: 0.9772, MAE: 0.8770\nEpoch 800, Train Loss: 0.6474, R¬≤: 0.0415, RMSE: 0.9773, MAE: 0.8770\nEpoch 900, Train Loss: 0.6615, R¬≤: 0.0415, RMSE: 0.9773, MAE: 0.8770\nEpoch 1000, Train Loss: 0.6713, R¬≤: 0.0415, RMSE: 0.9773, MAE: 0.8770\nEpoch 1100, Train Loss: 0.6453, R¬≤: 0.0415, RMSE: 0.9773, MAE: 0.8769\nEpoch 1200, Train Loss: 0.6911, R¬≤: 0.0415, RMSE: 0.9773, MAE: 0.8769\nEpoch 1300, Train Loss: 0.6714, R¬≤: 0.0416, RMSE: 0.9773, MAE: 0.8769\nEpoch 1400, Train Loss: 0.6445, R¬≤: 0.0416, RMSE: 0.9773, MAE: 0.8769\nEpoch 1500, Train Loss: 0.6523, R¬≤: 0.0417, RMSE: 0.9772, MAE: 0.8768\nEpoch 1600, Train Loss: 0.6442, R¬≤: 0.0417, RMSE: 0.9772, MAE: 0.8768\nEpoch 1700, Train Loss: 0.6574, R¬≤: 0.0418, RMSE: 0.9771, MAE: 0.8767\nEpoch 1800, Train Loss: 0.6746, R¬≤: 0.0419, RMSE: 0.9771, MAE: 0.8766\nEpoch 1900, Train Loss: 0.6700, R¬≤: 0.0418, RMSE: 0.9771, MAE: 0.8766\nEpoch 2000, Train Loss: 0.6682, R¬≤: 0.0419, RMSE: 0.9771, MAE: 0.8766\n",
  "history_begin_time" : 1741024635727,
  "history_end_time" : 1741024712512,
  "history_notes" : null,
  "history_process" : "kevt5u",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "n9LBOeAFSJwp",
  "history_input" : "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os\nimport warnings\nfrom datetime import datetime\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import GATConv\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch_geometric.utils import is_undirected, to_undirected, contains_self_loops, degree\n\n# Suppress Warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Check Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"‚úÖ Using device: {device}\")\n\n# Step 1: Define the GAT Model\nclass GAT(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, heads=4, dropout=0.5):  # Reduced heads for memory efficiency\n        super(GAT, self).__init__()\n        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, concat=True, dropout=dropout)\n        self.conv2 = GATConv(hidden_dim * heads, hidden_dim, heads=heads, concat=True, dropout=dropout)\n        self.conv3 = GATConv(hidden_dim * heads, hidden_dim, heads=1, concat=False, dropout=dropout)\n        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = self.conv1(x, edge_index)\n        x = F.elu(x)\n        x = self.dropout(x)\n\n        x = self.conv2(x, edge_index)\n        x = F.elu(x)\n        x = self.dropout(x)\n\n        x = self.conv3(x, edge_index)\n        x = F.elu(x)\n\n        x = self.fc(x)  # Output layer\n        return x\n\n# Step 2: Load Graph Data\ndef load_data(graph_file):\n    print(\"üìå Loading graph data...\")\n    try:\n        graph_data = torch.load(graph_file)\n        graph_data = graph_data.to(device)\n\n        # Ensure labels are properly formatted\n        graph_data.y = torch.tensor(StandardScaler().fit_transform(graph_data.y.view(-1, 1)), dtype=torch.float).to(device)\n        graph_data.y = graph_data.y.view(-1)  # Flatten\n\n        print(f\"‚úÖ Graph Loaded: Nodes={graph_data.x.shape[0]}, Features={graph_data.num_features}, Edges={graph_data.edge_index.shape[1]}\")\n        return graph_data\n\n    except Exception as e:\n        print(f\"‚ùå Error loading graph: {e}\")\n        exit()\n\n# Step 3: Validate and Clean Graph\ndef validate_and_clean_graph(graph_data):\n    print(\"üîç Validating Graph Structure...\")\n    \n    # Ensure graph is undirected\n    if not is_undirected(graph_data.edge_index):\n        graph_data.edge_index = to_undirected(graph_data.edge_index)\n    \n    print(\"‚úÖ Graph is undirected.\")\n\n    # Remove isolated nodes\n    degrees = degree(graph_data.edge_index[0], num_nodes=graph_data.num_nodes)\n    non_isolated_nodes = (degrees > 0).nonzero(as_tuple=True)[0]\n\n    graph_data.x = graph_data.x[non_isolated_nodes]\n    graph_data.y = graph_data.y[non_isolated_nodes]\n\n    node_map = {old_idx: new_idx for new_idx, old_idx in enumerate(non_isolated_nodes.tolist())}\n    new_edges = [[node_map[i], node_map[j]] for i, j in graph_data.edge_index.t().tolist() if i in node_map and j in node_map]\n    graph_data.edge_index = torch.tensor(new_edges, dtype=torch.long).t().contiguous()\n    \n    print(f\"‚úÖ Cleaned graph: {graph_data.x.shape[0]} nodes, {graph_data.edge_index.shape[1]} edges\")\n    return graph_data\n\n# Step 4: Training & Testing Functions\ndef train(model, optimizer, criterion, graph_data, train_mask):\n    model.train()\n    optimizer.zero_grad()\n    output = model(graph_data)[train_mask]\n    loss = criterion(output.squeeze(), graph_data.y[train_mask])\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n\ndef test(model, criterion, graph_data, test_mask):\n    model.eval()\n    with torch.no_grad():\n        predictions = model(graph_data)[test_mask].squeeze().cpu().numpy()\n        actuals = graph_data.y[test_mask].cpu().numpy()\n\n        mse = mean_squared_error(actuals, predictions)\n        r2 = r2_score(actuals, predictions)\n        mae = mean_absolute_error(actuals, predictions)\n        rmse = mean_squared_error(actuals, predictions, squared=False)\n\n    return mse, r2, rmse, mae\n\n# Step 5: Main Function\ndef main():\n    graph_file = \"/media/volume1/gat_spatial_training_data_sampled_knn_10km.pt\"\n    log_dir = \"/media/volume1/swe/data/training_logs\"\n    os.makedirs(log_dir, exist_ok=True)\n    log_file = os.path.join(log_dir, f\"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\")\n\n    def log(message):\n        print(message)\n        with open(log_file, \"a\") as f:\n            f.write(message + \"\\n\")\n\n    graph_data = load_data(graph_file)\n    graph_data = validate_and_clean_graph(graph_data)\n\n    train_ratio = 0.8\n    num_train_samples = int(train_ratio * graph_data.x.shape[0])\n    train_mask = torch.zeros(graph_data.x.shape[0], dtype=torch.bool)\n    train_mask[:num_train_samples] = True\n    test_mask = ~train_mask\n\n    log(\"‚úÖ Model initialized. Training started...\")\n\n    model = GAT(input_dim=graph_data.num_features, hidden_dim=64, output_dim=1, heads=4, dropout=0.5).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.MSELoss()\n\n    best_loss = float('inf')\n    model_save_path = \"/media/volume1/swe/data/best_gat_model.pt\"\n\n    for epoch in range(1, 501):\n        train_loss = train(model, optimizer, criterion, graph_data, train_mask)\n        test_loss, r2, rmse, mae = test(model, criterion, graph_data, test_mask)\n\n        if test_loss < best_loss:\n            best_loss = test_loss\n            torch.save(model.state_dict(), model_save_path)\n\n        if epoch % 100 == 0:\n            log(f\"Epoch {epoch:03d}, Train Loss: {train_loss:.4f}, R¬≤: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n\n    log(f\"‚úÖ Best model saved at: {model_save_path}\")\n\nif __name__ == \"__main__\":\n    main()",
  "history_output" : "‚úÖ Using device: cpu\nüìå Loading graph data...\n‚úÖ Graph Loaded: Nodes=247, Features=13, Edges=1235\nüîç Validating Graph Structure...\n‚úÖ Graph is undirected.\n‚úÖ Cleaned graph: 247 nodes, 1504 edges\n‚úÖ Model initialized. Training started...\nEpoch 100, Train Loss: 0.6833, R¬≤: 0.2091, RMSE: 0.8877, MAE: 0.8295\nEpoch 200, Train Loss: 0.6695, R¬≤: 0.1040, RMSE: 0.9449, MAE: 0.8595\nEpoch 300, Train Loss: 0.6353, R¬≤: 0.1777, RMSE: 0.9052, MAE: 0.8664\nEpoch 400, Train Loss: 0.6514, R¬≤: 0.1328, RMSE: 0.9296, MAE: 0.8670\nEpoch 500, Train Loss: 0.6611, R¬≤: 0.0446, RMSE: 0.9757, MAE: 0.9294\n‚úÖ Best model saved at: /media/volume1/swe/data/best_gat_model.pt\n",
  "history_begin_time" : 1741024462195,
  "history_end_time" : 1741024475933,
  "history_notes" : null,
  "history_process" : "kevt5u",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "vQCfy9Q5cv6n",
  "history_input" : "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os\nimport warnings\nfrom datetime import datetime\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import GATConv\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch_geometric.utils import is_undirected, to_undirected, contains_self_loops, degree\n\n# Suppress Warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Check Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"‚úÖ Using device: {device}\")\n\n# Step 1: Define the GAT Model\nclass GAT(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, heads=4, dropout=0.5):  # Reduced heads for memory efficiency\n        super(GAT, self).__init__()\n        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, concat=True, dropout=dropout)\n        self.conv2 = GATConv(hidden_dim * heads, hidden_dim, heads=heads, concat=True, dropout=dropout)\n        self.conv3 = GATConv(hidden_dim * heads, hidden_dim, heads=1, concat=False, dropout=dropout)\n        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = self.conv1(x, edge_index)\n        x = F.elu(x)\n        x = self.dropout(x)\n\n        x = self.conv2(x, edge_index)\n        x = F.elu(x)\n        x = self.dropout(x)\n\n        x = self.conv3(x, edge_index)\n        x = F.elu(x)\n\n        x = self.fc(x)  # Output layer\n        return x\n\n# Step 2: Load Graph Data\ndef load_data(graph_file):\n    print(\"üìå Loading graph data...\")\n    try:\n        graph_data = torch.load(graph_file)\n        graph_data = graph_data.to(device)\n\n        # Ensure labels are properly formatted\n        graph_data.y = torch.tensor(StandardScaler().fit_transform(graph_data.y.view(-1, 1)), dtype=torch.float).to(device)\n        graph_data.y = graph_data.y.view(-1)  # Flatten\n\n        print(f\"‚úÖ Graph Loaded: Nodes={graph_data.x.shape[0]}, Features={graph_data.num_features}, Edges={graph_data.edge_index.shape[1]}\")\n        return graph_data\n\n    except Exception as e:\n        print(f\"‚ùå Error loading graph: {e}\")\n        exit()\n\n# Step 3: Validate and Clean Graph\ndef validate_and_clean_graph(graph_data):\n    print(\"üîç Validating Graph Structure...\")\n    \n    # Ensure graph is undirected\n    if not is_undirected(graph_data.edge_index):\n        graph_data.edge_index = to_undirected(graph_data.edge_index)\n    \n    print(\"‚úÖ Graph is undirected.\")\n\n    # Remove isolated nodes\n    degrees = degree(graph_data.edge_index[0], num_nodes=graph_data.num_nodes)\n    non_isolated_nodes = (degrees > 0).nonzero(as_tuple=True)[0]\n\n    graph_data.x = graph_data.x[non_isolated_nodes]\n    graph_data.y = graph_data.y[non_isolated_nodes]\n\n    node_map = {old_idx: new_idx for new_idx, old_idx in enumerate(non_isolated_nodes.tolist())}\n    new_edges = [[node_map[i], node_map[j]] for i, j in graph_data.edge_index.t().tolist() if i in node_map and j in node_map]\n    graph_data.edge_index = torch.tensor(new_edges, dtype=torch.long).t().contiguous()\n    \n    print(f\"‚úÖ Cleaned graph: {graph_data.x.shape[0]} nodes, {graph_data.edge_index.shape[1]} edges\")\n    return graph_data\n\n# Step 4: Training & Testing Functions\ndef train(model, optimizer, criterion, graph_data, train_mask):\n    model.train()\n    optimizer.zero_grad()\n    output = model(graph_data)[train_mask]\n    loss = criterion(output.squeeze(), graph_data.y[train_mask])\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n\ndef test(model, criterion, graph_data, test_mask):\n    model.eval()\n    with torch.no_grad():\n        predictions = model(graph_data)[test_mask].squeeze().cpu().numpy()\n        actuals = graph_data.y[test_mask].cpu().numpy()\n\n        mse = mean_squared_error(actuals, predictions)\n        r2 = r2_score(actuals, predictions)\n        mae = mean_absolute_error(actuals, predictions)\n        rmse = mean_squared_error(actuals, predictions, squared=False)\n\n    return mse, r2, rmse, mae\n\n# Step 5: Main Function\ndef main():\n    graph_file = \"/media/volume1/gat_spatial_training_data_sampled_knn_10km.pt\"\n    log_dir = \"/media/volume1/swe/data/training_logs\"\n    os.makedirs(log_dir, exist_ok=True)\n    log_file = os.path.join(log_dir, f\"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\")\n\n    def log(message):\n        print(message)\n        with open(log_file, \"a\") as f:\n            f.write(message + \"\\n\")\n\n    graph_data = load_data(graph_file)\n    graph_data = validate_and_clean_graph(graph_data)\n\n    train_ratio = 0.8\n    num_train_samples = int(train_ratio * graph_data.x.shape[0])\n    train_mask = torch.zeros(graph_data.x.shape[0], dtype=torch.bool)\n    train_mask[:num_train_samples] = True\n    test_mask = ~train_mask\n\n    log(\"‚úÖ Model initialized. Training started...\")\n\n    model = GAT(input_dim=graph_data.num_features, hidden_dim=64, output_dim=1, heads=4, dropout=0.5).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.MSELoss()\n\n    best_loss = float('inf')\n    model_save_path = \"/media/volume1/swe/data/best_gat_model.pt\"\n\n    for epoch in range(1, 500):\n        train_loss = train(model, optimizer, criterion, graph_data, train_mask)\n        test_loss, r2, rmse, mae = test(model, criterion, graph_data, test_mask)\n\n        if test_loss < best_loss:\n            best_loss = test_loss\n            torch.save(model.state_dict(), model_save_path)\n\n        if epoch % 100 == 0:\n            log(f\"Epoch {epoch:03d}, Train Loss: {train_loss:.4f}, R¬≤: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n\n    log(f\"‚úÖ Best model saved at: {model_save_path}\")\n\nif __name__ == \"__main__\":\n    main()",
  "history_output" : "‚úÖ Using device: cpu\nüìå Loading graph data...\n‚úÖ Graph Loaded: Nodes=247, Features=13, Edges=1235\nüîç Validating Graph Structure...\n‚úÖ Graph is undirected.\n‚úÖ Cleaned graph: 247 nodes, 1504 edges\n‚úÖ Model initialized. Training started...\nEpoch 100, Train Loss: 0.6734, R¬≤: 0.2440, RMSE: 0.8680, MAE: 0.8162\nEpoch 200, Train Loss: 0.6468, R¬≤: 0.1791, RMSE: 0.9044, MAE: 0.8716\nEpoch 300, Train Loss: 0.6465, R¬≤: 0.1045, RMSE: 0.9446, MAE: 0.8896\nEpoch 400, Train Loss: 0.6591, R¬≤: 0.0736, RMSE: 0.9608, MAE: 0.9014\n‚úÖ Best model saved at: /media/volume1/swe/data/best_gat_model.pt\n",
  "history_begin_time" : 1741024423320,
  "history_end_time" : 1741024436452,
  "history_notes" : null,
  "history_process" : "kevt5u",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "00JIY9nYdhF7",
  "history_input" : "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os\nimport warnings\nfrom datetime import datetime\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import GATConv\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch_geometric.utils import is_undirected, to_undirected, contains_self_loops, degree\n\n# Suppress Warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Check Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"‚úÖ Using device: {device}\")\n\n# Step 1: Define the GAT Model\nclass GAT(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, heads=4, dropout=0.5):  # Reduced heads for memory efficiency\n        super(GAT, self).__init__()\n        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, concat=True, dropout=dropout)\n        self.conv2 = GATConv(hidden_dim * heads, hidden_dim, heads=heads, concat=True, dropout=dropout)\n        self.conv3 = GATConv(hidden_dim * heads, hidden_dim, heads=1, concat=False, dropout=dropout)\n        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = self.conv1(x, edge_index)\n        x = F.elu(x)\n        x = self.dropout(x)\n\n        x = self.conv2(x, edge_index)\n        x = F.elu(x)\n        x = self.dropout(x)\n\n        x = self.conv3(x, edge_index)\n        x = F.elu(x)\n\n        x = self.fc(x)  # Output layer\n        return x\n\n# Step 2: Load Graph Data\ndef load_data(graph_file):\n    print(\"üìå Loading graph data...\")\n    try:\n        graph_data = torch.load(graph_file)\n        graph_data = graph_data.to(device)\n\n        # Ensure labels are properly formatted\n        graph_data.y = torch.tensor(StandardScaler().fit_transform(graph_data.y.view(-1, 1)), dtype=torch.float).to(device)\n        graph_data.y = graph_data.y.view(-1)  # Flatten\n\n        print(f\"‚úÖ Graph Loaded: Nodes={graph_data.x.shape[0]}, Features={graph_data.num_features}, Edges={graph_data.edge_index.shape[1]}\")\n        return graph_data\n\n    except Exception as e:\n        print(f\"‚ùå Error loading graph: {e}\")\n        exit()\n\n# Step 3: Validate and Clean Graph\ndef validate_and_clean_graph(graph_data):\n    print(\"üîç Validating Graph Structure...\")\n    \n    # Ensure graph is undirected\n    if not is_undirected(graph_data.edge_index):\n        graph_data.edge_index = to_undirected(graph_data.edge_index)\n    \n    print(\"‚úÖ Graph is undirected.\")\n\n    # Remove isolated nodes\n    degrees = degree(graph_data.edge_index[0], num_nodes=graph_data.num_nodes)\n    non_isolated_nodes = (degrees > 0).nonzero(as_tuple=True)[0]\n\n    graph_data.x = graph_data.x[non_isolated_nodes]\n    graph_data.y = graph_data.y[non_isolated_nodes]\n\n    node_map = {old_idx: new_idx for new_idx, old_idx in enumerate(non_isolated_nodes.tolist())}\n    new_edges = [[node_map[i], node_map[j]] for i, j in graph_data.edge_index.t().tolist() if i in node_map and j in node_map]\n    graph_data.edge_index = torch.tensor(new_edges, dtype=torch.long).t().contiguous()\n    \n    print(f\"‚úÖ Cleaned graph: {graph_data.x.shape[0]} nodes, {graph_data.edge_index.shape[1]} edges\")\n    return graph_data\n\n# Step 4: Training & Testing Functions\ndef train(model, optimizer, criterion, graph_data, train_mask):\n    model.train()\n    optimizer.zero_grad()\n    output = model(graph_data)[train_mask]\n    loss = criterion(output.squeeze(), graph_data.y[train_mask])\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n\ndef test(model, criterion, graph_data, test_mask):\n    model.eval()\n    with torch.no_grad():\n        predictions = model(graph_data)[test_mask].squeeze().cpu().numpy()\n        actuals = graph_data.y[test_mask].cpu().numpy()\n\n        mse = mean_squared_error(actuals, predictions)\n        r2 = r2_score(actuals, predictions)\n        mae = mean_absolute_error(actuals, predictions)\n        rmse = mean_squared_error(actuals, predictions, squared=False)\n\n    return mse, r2, rmse, mae\n\n# Step 5: Main Function\ndef main():\n    graph_file = \"/media/volume1/gat_spatial_training_data_sampled_knn_10km.pt\"\n    log_dir = \"/media/volume1/swe/data/training_logs\"\n    os.makedirs(log_dir, exist_ok=True)\n    log_file = os.path.join(log_dir, f\"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\")\n\n    def log(message):\n        print(message)\n        with open(log_file, \"a\") as f:\n            f.write(message + \"\\n\")\n\n    graph_data = load_data(graph_file)\n    graph_data = validate_and_clean_graph(graph_data)\n\n    train_ratio = 0.8\n    num_train_samples = int(train_ratio * graph_data.x.shape[0])\n    train_mask = torch.zeros(graph_data.x.shape[0], dtype=torch.bool)\n    train_mask[:num_train_samples] = True\n    test_mask = ~train_mask\n\n    log(\"‚úÖ Model initialized. Training started...\")\n\n    model = GAT(input_dim=graph_data.num_features, hidden_dim=64, output_dim=1, heads=4, dropout=0.5).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.MSELoss()\n\n    best_loss = float('inf')\n    model_save_path = \"/media/volume1/swe/data/best_gat_model.pt\"\n\n    for epoch in range(1, 500):\n        train_loss = train(model, optimizer, criterion, graph_data, train_mask)\n        test_loss, r2, rmse, mae = test(model, criterion, graph_data, test_mask)\n\n        if test_loss < best_loss:\n            best_loss = test_loss\n            torch.save(model.state_dict(), model_save_path)\n\n        if epoch % 10 == 0:\n            log(f\"Epoch {epoch:03d}, Train Loss: {train_loss:.4f}, R¬≤: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n\n    log(f\"‚úÖ Best model saved at: {model_save_path}\")\n\nif __name__ == \"__main__\":\n    main()",
  "history_output" : "‚úÖ Using device: cpu\nüìå Loading graph data...\n‚úÖ Graph Loaded: Nodes=247, Features=13, Edges=1235\nüîç Validating Graph Structure...\n‚úÖ Graph is undirected.\n‚úÖ Cleaned graph: 247 nodes, 1504 edges\n‚úÖ Model initialized. Training started...\nEpoch 010, Train Loss: 0.7461, R¬≤: 0.2185, RMSE: 0.8825, MAE: 0.8318\nEpoch 020, Train Loss: 0.7400, R¬≤: 0.2461, RMSE: 0.8667, MAE: 0.7248\nEpoch 030, Train Loss: 0.7219, R¬≤: 0.2441, RMSE: 0.8679, MAE: 0.7868\nEpoch 040, Train Loss: 0.6966, R¬≤: 0.2555, RMSE: 0.8613, MAE: 0.7716\nEpoch 050, Train Loss: 0.7170, R¬≤: 0.2634, RMSE: 0.8568, MAE: 0.7737\nEpoch 060, Train Loss: 0.6711, R¬≤: 0.2601, RMSE: 0.8587, MAE: 0.7922\nEpoch 070, Train Loss: 0.7229, R¬≤: 0.2393, RMSE: 0.8706, MAE: 0.7975\nEpoch 080, Train Loss: 0.7174, R¬≤: 0.2500, RMSE: 0.8645, MAE: 0.7938\nEpoch 090, Train Loss: 0.6916, R¬≤: 0.2489, RMSE: 0.8651, MAE: 0.7876\n",
  "history_begin_time" : 1741024380620,
  "history_end_time" : 1741024386526,
  "history_notes" : null,
  "history_process" : "kevt5u",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "H8JBExoeMSX3",
  "history_input" : "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os\nimport warnings\nfrom datetime import datetime\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import GATConv\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch_geometric.utils import is_undirected, to_undirected, contains_self_loops, degree\n\n# Suppress Warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Check Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"‚úÖ Using device: {device}\")\n\n# Step 1: Define the GAT Model\nclass GAT(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, heads=4, dropout=0.5):  # Reduced heads for memory efficiency\n        super(GAT, self).__init__()\n        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, concat=True, dropout=dropout)\n        self.conv2 = GATConv(hidden_dim * heads, hidden_dim, heads=heads, concat=True, dropout=dropout)\n        self.conv3 = GATConv(hidden_dim * heads, hidden_dim, heads=1, concat=False, dropout=dropout)\n        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = self.conv1(x, edge_index)\n        x = F.elu(x)\n        x = self.dropout(x)\n\n        x = self.conv2(x, edge_index)\n        x = F.elu(x)\n        x = self.dropout(x)\n\n        x = self.conv3(x, edge_index)\n        x = F.elu(x)\n\n        x = self.fc(x)  # Output layer\n        return x\n\n# Step 2: Load Graph Data\ndef load_data(graph_file):\n    print(\"üìå Loading graph data...\")\n    try:\n        graph_data = torch.load(graph_file)\n        graph_data = graph_data.to(device)\n\n        # Ensure labels are properly formatted\n        graph_data.y = torch.tensor(StandardScaler().fit_transform(graph_data.y.view(-1, 1)), dtype=torch.float).to(device)\n        graph_data.y = graph_data.y.view(-1)  # Flatten\n\n        print(f\"‚úÖ Graph Loaded: Nodes={graph_data.x.shape[0]}, Features={graph_data.num_features}, Edges={graph_data.edge_index.shape[1]}\")\n        return graph_data\n\n    except Exception as e:\n        print(f\"‚ùå Error loading graph: {e}\")\n        exit()\n\n# Step 3: Validate and Clean Graph\ndef validate_and_clean_graph(graph_data):\n    print(\"üîç Validating Graph Structure...\")\n    \n    # Ensure graph is undirected\n    if not is_undirected(graph_data.edge_index):\n        graph_data.edge_index = to_undirected(graph_data.edge_index)\n    \n    print(\"‚úÖ Graph is undirected.\")\n\n    # Remove isolated nodes\n    degrees = degree(graph_data.edge_index[0], num_nodes=graph_data.num_nodes)\n    non_isolated_nodes = (degrees > 0).nonzero(as_tuple=True)[0]\n\n    graph_data.x = graph_data.x[non_isolated_nodes]\n    graph_data.y = graph_data.y[non_isolated_nodes]\n\n    node_map = {old_idx: new_idx for new_idx, old_idx in enumerate(non_isolated_nodes.tolist())}\n    new_edges = [[node_map[i], node_map[j]] for i, j in graph_data.edge_index.t().tolist() if i in node_map and j in node_map]\n    graph_data.edge_index = torch.tensor(new_edges, dtype=torch.long).t().contiguous()\n    \n    print(f\"‚úÖ Cleaned graph: {graph_data.x.shape[0]} nodes, {graph_data.edge_index.shape[1]} edges\")\n    return graph_data\n\n# Step 4: Training & Testing Functions\ndef train(model, optimizer, criterion, graph_data, train_mask):\n    model.train()\n    optimizer.zero_grad()\n    output = model(graph_data)[train_mask]\n    loss = criterion(output.squeeze(), graph_data.y[train_mask])\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n\ndef test(model, criterion, graph_data, test_mask):\n    model.eval()\n    with torch.no_grad():\n        predictions = model(graph_data)[test_mask].squeeze().cpu().numpy()\n        actuals = graph_data.y[test_mask].cpu().numpy()\n\n        mse = mean_squared_error(actuals, predictions)\n        r2 = r2_score(actuals, predictions)\n        mae = mean_absolute_error(actuals, predictions)\n        rmse = mean_squared_error(actuals, predictions, squared=False)\n\n    return mse, r2, rmse, mae\n\n# Step 5: Main Function\ndef main():\n    graph_file = \"/media/volume1/gat_spatial_training_data_sampled_knn_10km.pt\"\n    log_dir = \"/media/volume1/swe/data/training_logs\"\n    os.makedirs(log_dir, exist_ok=True)\n    log_file = os.path.join(log_dir, f\"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\")\n\n    def log(message):\n        print(message)\n        with open(log_file, \"a\") as f:\n            f.write(message + \"\\n\")\n\n    graph_data = load_data(graph_file)\n    graph_data = validate_and_clean_graph(graph_data)\n\n    train_ratio = 0.8\n    num_train_samples = int(train_ratio * graph_data.x.shape[0])\n    train_mask = torch.zeros(graph_data.x.shape[0], dtype=torch.bool)\n    train_mask[:num_train_samples] = True\n    test_mask = ~train_mask\n\n    log(\"‚úÖ Model initialized. Training started...\")\n\n    model = GAT(input_dim=graph_data.num_features, hidden_dim=64, output_dim=1, heads=4, dropout=0.5).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.MSELoss()\n\n    best_loss = float('inf')\n    model_save_path = \"/media/volume1/swe/data/best_gat_model.pt\"\n\n    for epoch in range(1, 100):\n        train_loss = train(model, optimizer, criterion, graph_data, train_mask)\n        test_loss, r2, rmse, mae = test(model, criterion, graph_data, test_mask)\n\n        if test_loss < best_loss:\n            best_loss = test_loss\n            torch.save(model.state_dict(), model_save_path)\n\n        if epoch % 10 == 0:\n            log(f\"Epoch {epoch:03d}, Train Loss: {train_loss:.4f}, R¬≤: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n\n    log(f\"‚úÖ Best model saved at: {model_save_path}\")\n\nif __name__ == \"__main__\":\n    main()",
  "history_output" : "‚úÖ Using device: cpu\nüìå Loading graph data...\n‚úÖ Graph Loaded: Nodes=247, Features=13, Edges=1235\nüîç Validating Graph Structure...\n‚úÖ Graph is undirected.\n‚úÖ Cleaned graph: 247 nodes, 1504 edges\n‚úÖ Model initialized. Training started...\nEpoch 010, Train Loss: 0.7837, R¬≤: 0.2046, RMSE: 0.8902, MAE: 0.8262\nEpoch 020, Train Loss: 0.7361, R¬≤: 0.2358, RMSE: 0.8726, MAE: 0.7721\nEpoch 030, Train Loss: 0.7365, R¬≤: 0.2604, RMSE: 0.8585, MAE: 0.7772\nEpoch 040, Train Loss: 0.7183, R¬≤: 0.2569, RMSE: 0.8605, MAE: 0.7957\nEpoch 050, Train Loss: 0.7461, R¬≤: 0.2563, RMSE: 0.8608, MAE: 0.7986\nEpoch 060, Train Loss: 0.7142, R¬≤: 0.2553, RMSE: 0.8614, MAE: 0.8016\nEpoch 070, Train Loss: 0.7131, R¬≤: 0.2449, RMSE: 0.8674, MAE: 0.8225\nEpoch 080, Train Loss: 0.6978, R¬≤: 0.2451, RMSE: 0.8673, MAE: 0.8242\nEpoch 090, Train Loss: 0.7160, R¬≤: 0.2391, RMSE: 0.8708, MAE: 0.8137\n‚úÖ Best model saved at: /media/volume1/swe/data/best_gat_model.pt\n",
  "history_begin_time" : 1741024356706,
  "history_end_time" : 1741024363272,
  "history_notes" : null,
  "history_process" : "kevt5u",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "8gFqrz8Vmw0r",
  "history_input" : "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os\nimport warnings\nfrom datetime import datetime\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import GATConv\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch_geometric.utils import is_undirected, to_undirected, contains_self_loops, degree\n\n# Suppress Warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Check Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"‚úÖ Using device: {device}\")\n\n# Step 1: Define the GAT Model\nclass GAT(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, heads=4, dropout=0.5):  # Reduced heads for memory efficiency\n        super(GAT, self).__init__()\n        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, concat=True, dropout=dropout)\n        self.conv2 = GATConv(hidden_dim * heads, hidden_dim, heads=heads, concat=True, dropout=dropout)\n        self.conv3 = GATConv(hidden_dim * heads, hidden_dim, heads=1, concat=False, dropout=dropout)\n        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = self.conv1(x, edge_index)\n        x = F.elu(x)\n        x = self.dropout(x)\n\n        x = self.conv2(x, edge_index)\n        x = F.elu(x)\n        x = self.dropout(x)\n\n        x = self.conv3(x, edge_index)\n        x = F.elu(x)\n\n        x = self.fc(x)  # Output layer\n        return x\n\n# Step 2: Load Graph Data\ndef load_data(graph_file):\n    print(\"üìå Loading graph data...\")\n    try:\n        graph_data = torch.load(graph_file)\n        graph_data = graph_data.to(device)\n\n        # Ensure labels are properly formatted\n        graph_data.y = torch.tensor(StandardScaler().fit_transform(graph_data.y.view(-1, 1)), dtype=torch.float).to(device)\n        graph_data.y = graph_data.y.view(-1)  # Flatten\n\n        print(f\"‚úÖ Graph Loaded: Nodes={graph_data.x.shape[0]}, Features={graph_data.num_features}, Edges={graph_data.edge_index.shape[1]}\")\n        return graph_data\n\n    except Exception as e:\n        print(f\"‚ùå Error loading graph: {e}\")\n        exit()\n\n# Step 3: Validate and Clean Graph\ndef validate_and_clean_graph(graph_data):\n    print(\"üîç Validating Graph Structure...\")\n    \n    # Ensure graph is undirected\n    if not is_undirected(graph_data.edge_index):\n        graph_data.edge_index = to_undirected(graph_data.edge_index)\n    \n    print(\"‚úÖ Graph is undirected.\")\n\n    # Remove isolated nodes\n    degrees = degree(graph_data.edge_index[0], num_nodes=graph_data.num_nodes)\n    non_isolated_nodes = (degrees > 0).nonzero(as_tuple=True)[0]\n\n    graph_data.x = graph_data.x[non_isolated_nodes]\n    graph_data.y = graph_data.y[non_isolated_nodes]\n\n    node_map = {old_idx: new_idx for new_idx, old_idx in enumerate(non_isolated_nodes.tolist())}\n    new_edges = [[node_map[i], node_map[j]] for i, j in graph_data.edge_index.t().tolist() if i in node_map and j in node_map]\n    graph_data.edge_index = torch.tensor(new_edges, dtype=torch.long).t().contiguous()\n    \n    print(f\"‚úÖ Cleaned graph: {graph_data.x.shape[0]} nodes, {graph_data.edge_index.shape[1]} edges\")\n    return graph_data\n\n# Step 4: Training & Testing Functions\ndef train(model, optimizer, criterion, graph_data, train_mask):\n    model.train()\n    optimizer.zero_grad()\n    output = model(graph_data)[train_mask]\n    loss = criterion(output.squeeze(), graph_data.y[train_mask])\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n\ndef test(model, criterion, graph_data, test_mask):\n    model.eval()\n    with torch.no_grad():\n        predictions = model(graph_data)[test_mask].squeeze().cpu().numpy()\n        actuals = graph_data.y[test_mask].cpu().numpy()\n\n        mse = mean_squared_error(actuals, predictions)\n        r2 = r2_score(actuals, predictions)\n        mae = mean_absolute_error(actuals, predictions)\n        rmse = mean_squared_error(actuals, predictions, squared=False)\n\n    return mse, r2, rmse, mae\n\n# Step 5: Main Function\ndef main():\n    graph_file = \"/media/volume1/gat_spatial_training_data_filtered.pt\"\n    log_dir = \"/media/volume1/swe/data/training_logs\"\n    os.makedirs(log_dir, exist_ok=True)\n    log_file = os.path.join(log_dir, f\"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\")\n\n    def log(message):\n        print(message)\n        with open(log_file, \"a\") as f:\n            f.write(message + \"\\n\")\n\n    graph_data = load_data(graph_file)\n    graph_data = validate_and_clean_graph(graph_data)\n\n    train_ratio = 0.8\n    num_train_samples = int(train_ratio * graph_data.x.shape[0])\n    train_mask = torch.zeros(graph_data.x.shape[0], dtype=torch.bool)\n    train_mask[:num_train_samples] = True\n    test_mask = ~train_mask\n\n    log(\"‚úÖ Model initialized. Training started...\")\n\n    model = GAT(input_dim=graph_data.num_features, hidden_dim=64, output_dim=1, heads=4, dropout=0.5).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.MSELoss()\n\n    for epoch in range(1, 1001):\n        train_loss = train(model, optimizer, criterion, graph_data, train_mask)\n        test_loss, r2, rmse, mae = test(model, criterion, graph_data, test_mask)\n\n        if epoch % 100 == 0:  # Log only every 100 epochs\n            log(f\"Epoch {epoch:03d}, Train Loss: {train_loss:.4f}, R¬≤: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
  "history_output" : "‚úÖ Using device: cpu\nüìå Loading graph data...\n‚úÖ Graph Loaded: Nodes=50000, Features=13, Edges=7064598\nüîç Validating Graph Structure...\n‚úÖ Graph is undirected.\n‚úÖ Cleaned graph: 49999 nodes, 7064598 edges\n‚úÖ Model initialized. Training started...\nEpoch 100, Train Loss: 0.6645, R¬≤: 0.3448, RMSE: 0.8096, MAE: 0.6733\nEpoch 200, Train Loss: 0.6389, R¬≤: 0.3714, RMSE: 0.7930, MAE: 0.6500\nEpoch 300, Train Loss: 0.6091, R¬≤: 0.4000, RMSE: 0.7748, MAE: 0.6380\nEpoch 400, Train Loss: 0.5897, R¬≤: 0.4135, RMSE: 0.7660, MAE: 0.6239\nEpoch 500, Train Loss: 0.5730, R¬≤: 0.4222, RMSE: 0.7602, MAE: 0.6027\nEpoch 600, Train Loss: 0.5714, R¬≤: 0.4283, RMSE: 0.7562, MAE: 0.6077\nEpoch 700, Train Loss: 0.5643, R¬≤: 0.4268, RMSE: 0.7572, MAE: 0.6126\nEpoch 800, Train Loss: 0.5572, R¬≤: 0.4310, RMSE: 0.7544, MAE: 0.6239\nEpoch 900, Train Loss: 0.5466, R¬≤: 0.4425, RMSE: 0.7468, MAE: 0.6043\nEpoch 1000, Train Loss: 0.5413, R¬≤: 0.4426, RMSE: 0.7467, MAE: 0.6121\n",
  "history_begin_time" : 1740671158839,
  "history_end_time" : 1740694188608,
  "history_notes" : "Epoch 1000, Train Loss: 0.5413, R¬≤: 0.4426, RMSE: 0.7467, MAE: 0.6121",
  "history_process" : "kevt5u",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "xFsJOWR8oplm",
  "history_input" : "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os\nimport warnings\nfrom datetime import datetime\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import GATConv\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch_geometric.utils import is_undirected, to_undirected, contains_self_loops, degree\n\n# Suppress Warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Check Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"‚úÖ Using device: {device}\")\n\n# Step 1: Define the GAT Model\nclass GAT(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, heads=4, dropout=0.5):  # Reduced heads for memory efficiency\n        super(GAT, self).__init__()\n        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, concat=True, dropout=dropout)\n        self.conv2 = GATConv(hidden_dim * heads, hidden_dim, heads=heads, concat=True, dropout=dropout)\n        self.conv3 = GATConv(hidden_dim * heads, hidden_dim, heads=1, concat=False, dropout=dropout)\n        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = self.conv1(x, edge_index)\n        x = F.elu(x)\n        x = self.dropout(x)\n\n        x = self.conv2(x, edge_index)\n        x = F.elu(x)\n        x = self.dropout(x)\n\n        x = self.conv3(x, edge_index)\n        x = F.elu(x)\n\n        x = self.fc(x)  # Output layer\n        return x\n\n# Step 2: Load Graph Data\ndef load_data(graph_file):\n    print(\"üìå Loading graph data...\")\n    try:\n        graph_data = torch.load(graph_file)\n        graph_data = graph_data.to(device)\n\n        # Ensure labels are properly formatted\n        graph_data.y = torch.tensor(StandardScaler().fit_transform(graph_data.y.view(-1, 1)), dtype=torch.float).to(device)\n        graph_data.y = graph_data.y.view(-1)  # Flatten\n\n        print(f\"‚úÖ Graph Loaded: Nodes={graph_data.x.shape[0]}, Features={graph_data.num_features}, Edges={graph_data.edge_index.shape[1]}\")\n        return graph_data\n\n    except Exception as e:\n        print(f\"‚ùå Error loading graph: {e}\")\n        exit()\n\n# Step 3: Validate and Clean Graph\ndef validate_and_clean_graph(graph_data):\n    print(\"üîç Validating Graph Structure...\")\n    \n    # Ensure graph is undirected\n    if not is_undirected(graph_data.edge_index):\n        graph_data.edge_index = to_undirected(graph_data.edge_index)\n    \n    print(\"‚úÖ Graph is undirected.\")\n\n    # Remove isolated nodes\n    degrees = degree(graph_data.edge_index[0], num_nodes=graph_data.num_nodes)\n    non_isolated_nodes = (degrees > 0).nonzero(as_tuple=True)[0]\n\n    graph_data.x = graph_data.x[non_isolated_nodes]\n    graph_data.y = graph_data.y[non_isolated_nodes]\n\n    node_map = {old_idx: new_idx for new_idx, old_idx in enumerate(non_isolated_nodes.tolist())}\n    new_edges = [[node_map[i], node_map[j]] for i, j in graph_data.edge_index.t().tolist() if i in node_map and j in node_map]\n    graph_data.edge_index = torch.tensor(new_edges, dtype=torch.long).t().contiguous()\n    \n    print(f\"‚úÖ Cleaned graph: {graph_data.x.shape[0]} nodes, {graph_data.edge_index.shape[1]} edges\")\n    return graph_data\n\n# Step 4: Training & Testing Functions\ndef train(model, optimizer, criterion, graph_data, train_mask):\n    model.train()\n    optimizer.zero_grad()\n    output = model(graph_data)[train_mask]\n    loss = criterion(output.squeeze(), graph_data.y[train_mask])\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n\ndef test(model, criterion, graph_data, test_mask):\n    model.eval()\n    with torch.no_grad():\n        predictions = model(graph_data)[test_mask].squeeze().cpu().numpy()\n        actuals = graph_data.y[test_mask].cpu().numpy()\n\n        mse = mean_squared_error(actuals, predictions)\n        r2 = r2_score(actuals, predictions)\n        mae = mean_absolute_error(actuals, predictions)\n        rmse = mean_squared_error(actuals, predictions, squared=False)\n\n    return mse, r2, rmse, mae\n\n# Step 5: Main Function\ndef main():\n    graph_file = \"/media/volume1/gat_spatial_training_data_filtered.pt\"\n    log_dir = \"/media/volume1/swe/data/training_logs\"\n    os.makedirs(log_dir, exist_ok=True)\n    log_file = os.path.join(log_dir, f\"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\")\n\n    def log(message):\n        print(message)\n        with open(log_file, \"a\") as f:\n            f.write(message + \"\\n\")\n\n    graph_data = load_data(graph_file)\n    graph_data = validate_and_clean_graph(graph_data)\n\n    train_ratio = 0.8\n    num_train_samples = int(train_ratio * graph_data.x.shape[0])\n    train_mask = torch.zeros(graph_data.x.shape[0], dtype=torch.bool)\n    train_mask[:num_train_samples] = True\n    test_mask = ~train_mask\n\n    log(\"‚úÖ Model initialized. Training started...\")\n\n    model = GAT(input_dim=graph_data.num_features, hidden_dim=64, output_dim=1, heads=4, dropout=0.5).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.MSELoss()\n\n    for epoch in range(1, 500 + 1):\n        train_loss = train(model, optimizer, criterion, graph_data, train_mask)\n        test_loss, r2, rmse, mae = test(model, criterion, graph_data, test_mask)\n        log(f\"Epoch {epoch:03d}, Train Loss: {train_loss:.4f}, R¬≤: {r2:.4f}, RMSE: {rmse:.4f}\")\n\nif __name__ == \"__main__\":\n    main()\n",
  "history_output" : "‚úÖ Using device: cpu\nüìå Loading graph data...\n‚úÖ Graph Loaded: Nodes=50000, Features=13, Edges=7064598\nüîç Validating Graph Structure...\n‚úÖ Graph is undirected.\n‚úÖ Cleaned graph: 49999 nodes, 7064598 edges\n‚úÖ Model initialized. Training started...\nEpoch 001, Train Loss: 1.0995, R¬≤: 0.1697, RMSE: 0.9114\nEpoch 002, Train Loss: 0.8337, R¬≤: 0.1674, RMSE: 0.9126\nEpoch 003, Train Loss: 0.8395, R¬≤: 0.1384, RMSE: 0.9284\nEpoch 004, Train Loss: 0.8711, R¬≤: 0.1712, RMSE: 0.9105\nEpoch 005, Train Loss: 0.8328, R¬≤: 0.2175, RMSE: 0.8847\nEpoch 006, Train Loss: 0.7858, R¬≤: 0.2395, RMSE: 0.8722\nEpoch 007, Train Loss: 0.7621, R¬≤: 0.2344, RMSE: 0.8751\nEpoch 008, Train Loss: 0.7670, R¬≤: 0.2207, RMSE: 0.8829\nEpoch 009, Train Loss: 0.7784, R¬≤: 0.2161, RMSE: 0.8855\nEpoch 010, Train Loss: 0.7829, R¬≤: 0.2250, RMSE: 0.8805\nEpoch 011, Train Loss: 0.7754, R¬≤: 0.2412, RMSE: 0.8712\nEpoch 012, Train Loss: 0.7586, R¬≤: 0.2565, RMSE: 0.8624\nEpoch 013, Train Loss: 0.7457, R¬≤: 0.2652, RMSE: 0.8573\nEpoch 014, Train Loss: 0.7394, R¬≤: 0.2666, RMSE: 0.8565\nEpoch 015, Train Loss: 0.7387, R¬≤: 0.2640, RMSE: 0.8580\nEpoch 016, Train Loss: 0.7423, R¬≤: 0.2628, RMSE: 0.8588\nEpoch 017, Train Loss: 0.7437, R¬≤: 0.2657, RMSE: 0.8571\nEpoch 018, Train Loss: 0.7436, R¬≤: 0.2722, RMSE: 0.8533\nEpoch 019, Train Loss: 0.7349, R¬≤: 0.2790, RMSE: 0.8492\nEpoch 020, Train Loss: 0.7266, R¬≤: 0.2833, RMSE: 0.8467\nEpoch 021, Train Loss: 0.7231, R¬≤: 0.2843, RMSE: 0.8461\nEpoch 022, Train Loss: 0.7222, R¬≤: 0.2831, RMSE: 0.8468\nEpoch 023, Train Loss: 0.7215, R¬≤: 0.2821, RMSE: 0.8474\nEpoch 024, Train Loss: 0.7232, R¬≤: 0.2829, RMSE: 0.8469\nEpoch 025, Train Loss: 0.7220, R¬≤: 0.2860, RMSE: 0.8451\nEpoch 026, Train Loss: 0.7179, R¬≤: 0.2900, RMSE: 0.8427\nEpoch 027, Train Loss: 0.7132, R¬≤: 0.2936, RMSE: 0.8406\nEpoch 028, Train Loss: 0.7132, R¬≤: 0.2956, RMSE: 0.8394\nEpoch 029, Train Loss: 0.7102, R¬≤: 0.2967, RMSE: 0.8388\nEpoch 030, Train Loss: 0.7099, R¬≤: 0.2979, RMSE: 0.8380\nEpoch 031, Train Loss: 0.7076, R¬≤: 0.3002, RMSE: 0.8367\nEpoch 032, Train Loss: 0.7072, R¬≤: 0.3031, RMSE: 0.8349\nEpoch 033, Train Loss: 0.7052, R¬≤: 0.3056, RMSE: 0.8334\nEpoch 034, Train Loss: 0.7022, R¬≤: 0.3068, RMSE: 0.8327\nEpoch 035, Train Loss: 0.7008, R¬≤: 0.3070, RMSE: 0.8326\nEpoch 036, Train Loss: 0.7003, R¬≤: 0.3072, RMSE: 0.8325\nEpoch 037, Train Loss: 0.6983, R¬≤: 0.3083, RMSE: 0.8318\nEpoch 038, Train Loss: 0.6984, R¬≤: 0.3102, RMSE: 0.8307\nEpoch 039, Train Loss: 0.6964, R¬≤: 0.3120, RMSE: 0.8296\nEpoch 040, Train Loss: 0.6941, R¬≤: 0.3130, RMSE: 0.8290\nEpoch 041, Train Loss: 0.6937, R¬≤: 0.3133, RMSE: 0.8288\nEpoch 042, Train Loss: 0.6924, R¬≤: 0.3136, RMSE: 0.8286\nEpoch 043, Train Loss: 0.6926, R¬≤: 0.3146, RMSE: 0.8280\nEpoch 044, Train Loss: 0.6923, R¬≤: 0.3158, RMSE: 0.8273\nEpoch 045, Train Loss: 0.6907, R¬≤: 0.3169, RMSE: 0.8266\nEpoch 046, Train Loss: 0.6885, R¬≤: 0.3176, RMSE: 0.8262\nEpoch 047, Train Loss: 0.6903, R¬≤: 0.3184, RMSE: 0.8257\nEpoch 048, Train Loss: 0.6902, R¬≤: 0.3196, RMSE: 0.8250\nEpoch 049, Train Loss: 0.6879, R¬≤: 0.3210, RMSE: 0.8241\nEpoch 050, Train Loss: 0.6871, R¬≤: 0.3220, RMSE: 0.8235\nEpoch 051, Train Loss: 0.6878, R¬≤: 0.3226, RMSE: 0.8232\nEpoch 052, Train Loss: 0.6844, R¬≤: 0.3231, RMSE: 0.8229\nEpoch 053, Train Loss: 0.6849, R¬≤: 0.3237, RMSE: 0.8225\nEpoch 054, Train Loss: 0.6854, R¬≤: 0.3243, RMSE: 0.8222\nEpoch 055, Train Loss: 0.6815, R¬≤: 0.3246, RMSE: 0.8220\nEpoch 056, Train Loss: 0.6813, R¬≤: 0.3250, RMSE: 0.8217\nEpoch 057, Train Loss: 0.6824, R¬≤: 0.3257, RMSE: 0.8213\nEpoch 058, Train Loss: 0.6810, R¬≤: 0.3266, RMSE: 0.8207\nEpoch 059, Train Loss: 0.6814, R¬≤: 0.3275, RMSE: 0.8202\nEpoch 060, Train Loss: 0.6819, R¬≤: 0.3281, RMSE: 0.8198\nEpoch 061, Train Loss: 0.6806, R¬≤: 0.3288, RMSE: 0.8194\nEpoch 062, Train Loss: 0.6810, R¬≤: 0.3295, RMSE: 0.8190\nEpoch 063, Train Loss: 0.6796, R¬≤: 0.3300, RMSE: 0.8187\nEpoch 064, Train Loss: 0.6773, R¬≤: 0.3303, RMSE: 0.8185\nEpoch 065, Train Loss: 0.6784, R¬≤: 0.3308, RMSE: 0.8182\nEpoch 066, Train Loss: 0.6783, R¬≤: 0.3314, RMSE: 0.8178\nEpoch 067, Train Loss: 0.6746, R¬≤: 0.3320, RMSE: 0.8174\nEpoch 068, Train Loss: 0.6762, R¬≤: 0.3326, RMSE: 0.8171\nEpoch 069, Train Loss: 0.6757, R¬≤: 0.3331, RMSE: 0.8167\nEpoch 070, Train Loss: 0.6766, R¬≤: 0.3336, RMSE: 0.8164\nEpoch 071, Train Loss: 0.6753, R¬≤: 0.3340, RMSE: 0.8162\nEpoch 072, Train Loss: 0.6755, R¬≤: 0.3344, RMSE: 0.8160\nEpoch 073, Train Loss: 0.6728, R¬≤: 0.3349, RMSE: 0.8156\nEpoch 074, Train Loss: 0.6726, R¬≤: 0.3356, RMSE: 0.8152\nEpoch 075, Train Loss: 0.6725, R¬≤: 0.3363, RMSE: 0.8148\nEpoch 076, Train Loss: 0.6735, R¬≤: 0.3369, RMSE: 0.8145\nEpoch 077, Train Loss: 0.6706, R¬≤: 0.3375, RMSE: 0.8141\nEpoch 078, Train Loss: 0.6725, R¬≤: 0.3380, RMSE: 0.8138\nEpoch 079, Train Loss: 0.6733, R¬≤: 0.3385, RMSE: 0.8135\nEpoch 080, Train Loss: 0.6697, R¬≤: 0.3390, RMSE: 0.8132\nEpoch 081, Train Loss: 0.6690, R¬≤: 0.3395, RMSE: 0.8128\nEpoch 082, Train Loss: 0.6707, R¬≤: 0.3401, RMSE: 0.8125\nEpoch 083, Train Loss: 0.6692, R¬≤: 0.3406, RMSE: 0.8122\nEpoch 084, Train Loss: 0.6667, R¬≤: 0.3410, RMSE: 0.8119\nEpoch 085, Train Loss: 0.6678, R¬≤: 0.3415, RMSE: 0.8116\nEpoch 086, Train Loss: 0.6694, R¬≤: 0.3420, RMSE: 0.8113\nEpoch 087, Train Loss: 0.6675, R¬≤: 0.3425, RMSE: 0.8110\nEpoch 088, Train Loss: 0.6659, R¬≤: 0.3431, RMSE: 0.8106\nEpoch 089, Train Loss: 0.6657, R¬≤: 0.3436, RMSE: 0.8103\nEpoch 090, Train Loss: 0.6646, R¬≤: 0.3441, RMSE: 0.8100\nEpoch 091, Train Loss: 0.6651, R¬≤: 0.3446, RMSE: 0.8097\nEpoch 092, Train Loss: 0.6657, R¬≤: 0.3450, RMSE: 0.8095\nEpoch 093, Train Loss: 0.6657, R¬≤: 0.3453, RMSE: 0.8093\nEpoch 094, Train Loss: 0.6640, R¬≤: 0.3457, RMSE: 0.8090\nEpoch 095, Train Loss: 0.6651, R¬≤: 0.3461, RMSE: 0.8087\nEpoch 096, Train Loss: 0.6652, R¬≤: 0.3467, RMSE: 0.8084\nEpoch 097, Train Loss: 0.6659, R¬≤: 0.3471, RMSE: 0.8082\nEpoch 098, Train Loss: 0.6608, R¬≤: 0.3474, RMSE: 0.8080\nEpoch 099, Train Loss: 0.6643, R¬≤: 0.3477, RMSE: 0.8078\nEpoch 100, Train Loss: 0.6636, R¬≤: 0.3481, RMSE: 0.8076\nEpoch 101, Train Loss: 0.6624, R¬≤: 0.3483, RMSE: 0.8074\nEpoch 102, Train Loss: 0.6613, R¬≤: 0.3486, RMSE: 0.8073\nEpoch 103, Train Loss: 0.6626, R¬≤: 0.3488, RMSE: 0.8071\nEpoch 104, Train Loss: 0.6600, R¬≤: 0.3493, RMSE: 0.8068\nEpoch 105, Train Loss: 0.6592, R¬≤: 0.3497, RMSE: 0.8065\nEpoch 106, Train Loss: 0.6607, R¬≤: 0.3501, RMSE: 0.8063\nEpoch 107, Train Loss: 0.6582, R¬≤: 0.3504, RMSE: 0.8061\nEpoch 108, Train Loss: 0.6620, R¬≤: 0.3507, RMSE: 0.8059\nEpoch 109, Train Loss: 0.6596, R¬≤: 0.3508, RMSE: 0.8059\nEpoch 110, Train Loss: 0.6626, R¬≤: 0.3509, RMSE: 0.8058\nEpoch 111, Train Loss: 0.6575, R¬≤: 0.3512, RMSE: 0.8056\nEpoch 112, Train Loss: 0.6607, R¬≤: 0.3515, RMSE: 0.8054\nEpoch 113, Train Loss: 0.6579, R¬≤: 0.3520, RMSE: 0.8051\nEpoch 114, Train Loss: 0.6603, R¬≤: 0.3524, RMSE: 0.8049\nEpoch 115, Train Loss: 0.6584, R¬≤: 0.3527, RMSE: 0.8047\nEpoch 116, Train Loss: 0.6599, R¬≤: 0.3529, RMSE: 0.8045\nEpoch 117, Train Loss: 0.6589, R¬≤: 0.3530, RMSE: 0.8045\nEpoch 118, Train Loss: 0.6608, R¬≤: 0.3531, RMSE: 0.8044\nEpoch 119, Train Loss: 0.6580, R¬≤: 0.3534, RMSE: 0.8043\nEpoch 120, Train Loss: 0.6584, R¬≤: 0.3538, RMSE: 0.8040\nEpoch 121, Train Loss: 0.6558, R¬≤: 0.3542, RMSE: 0.8037\nEpoch 122, Train Loss: 0.6559, R¬≤: 0.3546, RMSE: 0.8035\nEpoch 123, Train Loss: 0.6559, R¬≤: 0.3549, RMSE: 0.8033\nEpoch 124, Train Loss: 0.6549, R¬≤: 0.3550, RMSE: 0.8032\nEpoch 125, Train Loss: 0.6568, R¬≤: 0.3549, RMSE: 0.8033\nEpoch 126, Train Loss: 0.6569, R¬≤: 0.3552, RMSE: 0.8031\nEpoch 127, Train Loss: 0.6565, R¬≤: 0.3557, RMSE: 0.8028\nEpoch 128, Train Loss: 0.6566, R¬≤: 0.3558, RMSE: 0.8028\nEpoch 129, Train Loss: 0.6530, R¬≤: 0.3559, RMSE: 0.8027\nEpoch 130, Train Loss: 0.6560, R¬≤: 0.3560, RMSE: 0.8026\nEpoch 131, Train Loss: 0.6531, R¬≤: 0.3563, RMSE: 0.8024\nEpoch 132, Train Loss: 0.6584, R¬≤: 0.3567, RMSE: 0.8022\nEpoch 133, Train Loss: 0.6518, R¬≤: 0.3572, RMSE: 0.8019\nEpoch 134, Train Loss: 0.6545, R¬≤: 0.3576, RMSE: 0.8016\nEpoch 135, Train Loss: 0.6524, R¬≤: 0.3578, RMSE: 0.8015\nEpoch 136, Train Loss: 0.6539, R¬≤: 0.3577, RMSE: 0.8015\nEpoch 137, Train Loss: 0.6540, R¬≤: 0.3580, RMSE: 0.8014\nEpoch 138, Train Loss: 0.6536, R¬≤: 0.3585, RMSE: 0.8011\nEpoch 139, Train Loss: 0.6544, R¬≤: 0.3589, RMSE: 0.8008\nEpoch 140, Train Loss: 0.6537, R¬≤: 0.3591, RMSE: 0.8007\nEpoch 141, Train Loss: 0.6545, R¬≤: 0.3592, RMSE: 0.8006\nEpoch 142, Train Loss: 0.6493, R¬≤: 0.3592, RMSE: 0.8006\nEpoch 143, Train Loss: 0.6512, R¬≤: 0.3593, RMSE: 0.8006\nEpoch 144, Train Loss: 0.6500, R¬≤: 0.3595, RMSE: 0.8005\nEpoch 145, Train Loss: 0.6506, R¬≤: 0.3599, RMSE: 0.8002\nEpoch 146, Train Loss: 0.6529, R¬≤: 0.3601, RMSE: 0.8001\nEpoch 147, Train Loss: 0.6494, R¬≤: 0.3608, RMSE: 0.7996\nEpoch 148, Train Loss: 0.6483, R¬≤: 0.3613, RMSE: 0.7993\nEpoch 149, Train Loss: 0.6477, R¬≤: 0.3614, RMSE: 0.7992\nEpoch 150, Train Loss: 0.6502, R¬≤: 0.3611, RMSE: 0.7994\nEpoch 151, Train Loss: 0.6490, R¬≤: 0.3605, RMSE: 0.7998\nEpoch 152, Train Loss: 0.6502, R¬≤: 0.3606, RMSE: 0.7998\nEpoch 153, Train Loss: 0.6476, R¬≤: 0.3611, RMSE: 0.7994\nEpoch 154, Train Loss: 0.6492, R¬≤: 0.3617, RMSE: 0.7991\nEpoch 155, Train Loss: 0.6480, R¬≤: 0.3623, RMSE: 0.7987\nEpoch 156, Train Loss: 0.6461, R¬≤: 0.3626, RMSE: 0.7985\nEpoch 157, Train Loss: 0.6490, R¬≤: 0.3630, RMSE: 0.7983\nEpoch 158, Train Loss: 0.6488, R¬≤: 0.3629, RMSE: 0.7983\nEpoch 159, Train Loss: 0.6499, R¬≤: 0.3628, RMSE: 0.7984\nEpoch 160, Train Loss: 0.6471, R¬≤: 0.3629, RMSE: 0.7983\nEpoch 161, Train Loss: 0.6489, R¬≤: 0.3634, RMSE: 0.7980\nEpoch 162, Train Loss: 0.6473, R¬≤: 0.3636, RMSE: 0.7979\nEpoch 163, Train Loss: 0.6475, R¬≤: 0.3637, RMSE: 0.7978\nEpoch 164, Train Loss: 0.6488, R¬≤: 0.3639, RMSE: 0.7977\nEpoch 165, Train Loss: 0.6428, R¬≤: 0.3641, RMSE: 0.7976\nEpoch 166, Train Loss: 0.6454, R¬≤: 0.3644, RMSE: 0.7974\nEpoch 167, Train Loss: 0.6415, R¬≤: 0.3649, RMSE: 0.7970\nEpoch 168, Train Loss: 0.6458, R¬≤: 0.3655, RMSE: 0.7967\nEpoch 169, Train Loss: 0.6454, R¬≤: 0.3656, RMSE: 0.7966\nEpoch 170, Train Loss: 0.6464, R¬≤: 0.3655, RMSE: 0.7967\nEpoch 171, Train Loss: 0.6439, R¬≤: 0.3654, RMSE: 0.7967\nEpoch 172, Train Loss: 0.6446, R¬≤: 0.3654, RMSE: 0.7968\nEpoch 173, Train Loss: 0.6443, R¬≤: 0.3657, RMSE: 0.7965\nEpoch 174, Train Loss: 0.6399, R¬≤: 0.3663, RMSE: 0.7962\nEpoch 175, Train Loss: 0.6435, R¬≤: 0.3667, RMSE: 0.7960\nEpoch 176, Train Loss: 0.6440, R¬≤: 0.3667, RMSE: 0.7959\nEpoch 177, Train Loss: 0.6430, R¬≤: 0.3669, RMSE: 0.7958\nEpoch 178, Train Loss: 0.6448, R¬≤: 0.3671, RMSE: 0.7957\nEpoch 179, Train Loss: 0.6398, R¬≤: 0.3674, RMSE: 0.7955\nEpoch 180, Train Loss: 0.6398, R¬≤: 0.3675, RMSE: 0.7954\nEpoch 181, Train Loss: 0.6406, R¬≤: 0.3675, RMSE: 0.7954\nEpoch 182, Train Loss: 0.6422, R¬≤: 0.3673, RMSE: 0.7955\nEpoch 183, Train Loss: 0.6397, R¬≤: 0.3676, RMSE: 0.7954\nEpoch 184, Train Loss: 0.6411, R¬≤: 0.3682, RMSE: 0.7950\nEpoch 185, Train Loss: 0.6412, R¬≤: 0.3688, RMSE: 0.7946\nEpoch 186, Train Loss: 0.6423, R¬≤: 0.3692, RMSE: 0.7943\nEpoch 187, Train Loss: 0.6394, R¬≤: 0.3694, RMSE: 0.7942\nEpoch 188, Train Loss: 0.6409, R¬≤: 0.3693, RMSE: 0.7943\nEpoch 189, Train Loss: 0.6433, R¬≤: 0.3692, RMSE: 0.7943\nEpoch 190, Train Loss: 0.6383, R¬≤: 0.3692, RMSE: 0.7944\nEpoch 191, Train Loss: 0.6363, R¬≤: 0.3692, RMSE: 0.7944\nEpoch 192, Train Loss: 0.6408, R¬≤: 0.3694, RMSE: 0.7942\nEpoch 193, Train Loss: 0.6382, R¬≤: 0.3698, RMSE: 0.7940\nEpoch 194, Train Loss: 0.6380, R¬≤: 0.3703, RMSE: 0.7937\nEpoch 195, Train Loss: 0.6355, R¬≤: 0.3703, RMSE: 0.7936\nEpoch 196, Train Loss: 0.6393, R¬≤: 0.3704, RMSE: 0.7936\nEpoch 197, Train Loss: 0.6378, R¬≤: 0.3709, RMSE: 0.7933\nEpoch 198, Train Loss: 0.6361, R¬≤: 0.3713, RMSE: 0.7931\nEpoch 199, Train Loss: 0.6393, R¬≤: 0.3707, RMSE: 0.7934\nEpoch 200, Train Loss: 0.6379, R¬≤: 0.3701, RMSE: 0.7938\nEpoch 201, Train Loss: 0.6336, R¬≤: 0.3699, RMSE: 0.7939\nEpoch 202, Train Loss: 0.6347, R¬≤: 0.3694, RMSE: 0.7942\nEpoch 203, Train Loss: 0.6362, R¬≤: 0.3699, RMSE: 0.7939\nEpoch 204, Train Loss: 0.6348, R¬≤: 0.3713, RMSE: 0.7930\nEpoch 205, Train Loss: 0.6348, R¬≤: 0.3717, RMSE: 0.7928\nEpoch 206, Train Loss: 0.6349, R¬≤: 0.3713, RMSE: 0.7930\nEpoch 207, Train Loss: 0.6309, R¬≤: 0.3700, RMSE: 0.7938\nEpoch 208, Train Loss: 0.6351, R¬≤: 0.3704, RMSE: 0.7936\nEpoch 209, Train Loss: 0.6347, R¬≤: 0.3698, RMSE: 0.7940\nEpoch 210, Train Loss: 0.6368, R¬≤: 0.3693, RMSE: 0.7943\nEpoch 211, Train Loss: 0.6365, R¬≤: 0.3697, RMSE: 0.7941\nEpoch 212, Train Loss: 0.6342, R¬≤: 0.3711, RMSE: 0.7932\nEpoch 213, Train Loss: 0.6347, R¬≤: 0.3713, RMSE: 0.7930\nEpoch 214, Train Loss: 0.6315, R¬≤: 0.3709, RMSE: 0.7933\nEpoch 215, Train Loss: 0.6283, R¬≤: 0.3708, RMSE: 0.7934\nEpoch 216, Train Loss: 0.6310, R¬≤: 0.3706, RMSE: 0.7935\nEpoch 217, Train Loss: 0.6332, R¬≤: 0.3705, RMSE: 0.7935\nEpoch 218, Train Loss: 0.6301, R¬≤: 0.3714, RMSE: 0.7929\nEpoch 219, Train Loss: 0.6316, R¬≤: 0.3718, RMSE: 0.7927\nEpoch 220, Train Loss: 0.6296, R¬≤: 0.3720, RMSE: 0.7926\nEpoch 221, Train Loss: 0.6309, R¬≤: 0.3715, RMSE: 0.7929\nEpoch 222, Train Loss: 0.6293, R¬≤: 0.3724, RMSE: 0.7924\nEpoch 223, Train Loss: 0.6313, R¬≤: 0.3718, RMSE: 0.7927\nEpoch 224, Train Loss: 0.6264, R¬≤: 0.3704, RMSE: 0.7936\nEpoch 225, Train Loss: 0.6289, R¬≤: 0.3696, RMSE: 0.7941\nEpoch 226, Train Loss: 0.6301, R¬≤: 0.3706, RMSE: 0.7935\nEpoch 227, Train Loss: 0.6303, R¬≤: 0.3706, RMSE: 0.7935\nEpoch 228, Train Loss: 0.6279, R¬≤: 0.3696, RMSE: 0.7941\nEpoch 229, Train Loss: 0.6249, R¬≤: 0.3718, RMSE: 0.7927\nEpoch 230, Train Loss: 0.6261, R¬≤: 0.3737, RMSE: 0.7915\nEpoch 231, Train Loss: 0.6268, R¬≤: 0.3755, RMSE: 0.7904\nEpoch 232, Train Loss: 0.6283, R¬≤: 0.3738, RMSE: 0.7915\nEpoch 233, Train Loss: 0.6280, R¬≤: 0.3735, RMSE: 0.7917\nEpoch 234, Train Loss: 0.6279, R¬≤: 0.3712, RMSE: 0.7931\nEpoch 235, Train Loss: 0.6253, R¬≤: 0.3697, RMSE: 0.7941\nEpoch 236, Train Loss: 0.6298, R¬≤: 0.3698, RMSE: 0.7939\nEpoch 237, Train Loss: 0.6240, R¬≤: 0.3724, RMSE: 0.7923\nEpoch 238, Train Loss: 0.6264, R¬≤: 0.3731, RMSE: 0.7919\nEpoch 239, Train Loss: 0.6253, R¬≤: 0.3729, RMSE: 0.7920\nEpoch 240, Train Loss: 0.6237, R¬≤: 0.3725, RMSE: 0.7923\nEpoch 241, Train Loss: 0.6256, R¬≤: 0.3726, RMSE: 0.7922\nEpoch 242, Train Loss: 0.6224, R¬≤: 0.3728, RMSE: 0.7921\nEpoch 243, Train Loss: 0.6246, R¬≤: 0.3733, RMSE: 0.7918\nEpoch 244, Train Loss: 0.6267, R¬≤: 0.3738, RMSE: 0.7915\nEpoch 245, Train Loss: 0.6238, R¬≤: 0.3738, RMSE: 0.7914\nEpoch 246, Train Loss: 0.6204, R¬≤: 0.3731, RMSE: 0.7919\nEpoch 247, Train Loss: 0.6222, R¬≤: 0.3735, RMSE: 0.7917\nEpoch 248, Train Loss: 0.6250, R¬≤: 0.3731, RMSE: 0.7919\nEpoch 249, Train Loss: 0.6173, R¬≤: 0.3713, RMSE: 0.7930\nEpoch 250, Train Loss: 0.6226, R¬≤: 0.3710, RMSE: 0.7932\nEpoch 251, Train Loss: 0.6245, R¬≤: 0.3733, RMSE: 0.7918\nEpoch 252, Train Loss: 0.6212, R¬≤: 0.3752, RMSE: 0.7906\nEpoch 253, Train Loss: 0.6252, R¬≤: 0.3748, RMSE: 0.7908\nEpoch 254, Train Loss: 0.6188, R¬≤: 0.3740, RMSE: 0.7913\nEpoch 255, Train Loss: 0.6215, R¬≤: 0.3729, RMSE: 0.7920\nEpoch 256, Train Loss: 0.6201, R¬≤: 0.3728, RMSE: 0.7921\nEpoch 257, Train Loss: 0.6246, R¬≤: 0.3722, RMSE: 0.7925\nEpoch 258, Train Loss: 0.6235, R¬≤: 0.3721, RMSE: 0.7925\nEpoch 259, Train Loss: 0.6205, R¬≤: 0.3740, RMSE: 0.7913\nEpoch 260, Train Loss: 0.6213, R¬≤: 0.3750, RMSE: 0.7907\nEpoch 261, Train Loss: 0.6239, R¬≤: 0.3743, RMSE: 0.7911\nEpoch 262, Train Loss: 0.6168, R¬≤: 0.3759, RMSE: 0.7901\nEpoch 263, Train Loss: 0.6217, R¬≤: 0.3753, RMSE: 0.7905\nEpoch 264, Train Loss: 0.6203, R¬≤: 0.3749, RMSE: 0.7907\nEpoch 265, Train Loss: 0.6180, R¬≤: 0.3750, RMSE: 0.7907\nEpoch 266, Train Loss: 0.6184, R¬≤: 0.3745, RMSE: 0.7910\nEpoch 267, Train Loss: 0.6210, R¬≤: 0.3745, RMSE: 0.7910\nEpoch 268, Train Loss: 0.6185, R¬≤: 0.3738, RMSE: 0.7915\nEpoch 269, Train Loss: 0.6162, R¬≤: 0.3732, RMSE: 0.7918\nEpoch 270, Train Loss: 0.6183, R¬≤: 0.3729, RMSE: 0.7920\nEpoch 271, Train Loss: 0.6219, R¬≤: 0.3736, RMSE: 0.7916\nEpoch 272, Train Loss: 0.6138, R¬≤: 0.3743, RMSE: 0.7911\nEpoch 273, Train Loss: 0.6129, R¬≤: 0.3755, RMSE: 0.7904\nEpoch 274, Train Loss: 0.6168, R¬≤: 0.3750, RMSE: 0.7907\nEpoch 275, Train Loss: 0.6152, R¬≤: 0.3747, RMSE: 0.7909\nEpoch 276, Train Loss: 0.6124, R¬≤: 0.3736, RMSE: 0.7916\nEpoch 277, Train Loss: 0.6164, R¬≤: 0.3737, RMSE: 0.7915\nEpoch 278, Train Loss: 0.6148, R¬≤: 0.3772, RMSE: 0.7893\nEpoch 279, Train Loss: 0.6129, R¬≤: 0.3784, RMSE: 0.7885\nEpoch 280, Train Loss: 0.6176, R¬≤: 0.3765, RMSE: 0.7898\nEpoch 281, Train Loss: 0.6194, R¬≤: 0.3739, RMSE: 0.7914\nEpoch 282, Train Loss: 0.6158, R¬≤: 0.3734, RMSE: 0.7917\nEpoch 283, Train Loss: 0.6165, R¬≤: 0.3728, RMSE: 0.7921\nEpoch 284, Train Loss: 0.6098, R¬≤: 0.3742, RMSE: 0.7912\nEpoch 285, Train Loss: 0.6161, R¬≤: 0.3750, RMSE: 0.7907\nEpoch 286, Train Loss: 0.6129, R¬≤: 0.3768, RMSE: 0.7895\nEpoch 287, Train Loss: 0.6131, R¬≤: 0.3748, RMSE: 0.7908\nEpoch 288, Train Loss: 0.6075, R¬≤: 0.3749, RMSE: 0.7908\nEpoch 289, Train Loss: 0.6113, R¬≤: 0.3770, RMSE: 0.7894\nEpoch 290, Train Loss: 0.6101, R¬≤: 0.3778, RMSE: 0.7889\nEpoch 291, Train Loss: 0.6130, R¬≤: 0.3783, RMSE: 0.7886\nEpoch 292, Train Loss: 0.6094, R¬≤: 0.3747, RMSE: 0.7909\nEpoch 293, Train Loss: 0.6127, R¬≤: 0.3704, RMSE: 0.7936\nEpoch 294, Train Loss: 0.6159, R¬≤: 0.3726, RMSE: 0.7922\nEpoch 295, Train Loss: 0.6081, R¬≤: 0.3753, RMSE: 0.7905\nEpoch 296, Train Loss: 0.6084, R¬≤: 0.3761, RMSE: 0.7900\nEpoch 297, Train Loss: 0.6089, R¬≤: 0.3772, RMSE: 0.7893\nEpoch 298, Train Loss: 0.6109, R¬≤: 0.3791, RMSE: 0.7881\nEpoch 299, Train Loss: 0.6114, R¬≤: 0.3780, RMSE: 0.7888\nEpoch 300, Train Loss: 0.6172, R¬≤: 0.3778, RMSE: 0.7889\nEpoch 301, Train Loss: 0.6087, R¬≤: 0.3783, RMSE: 0.7886\nEpoch 302, Train Loss: 0.6089, R¬≤: 0.3797, RMSE: 0.7877\nEpoch 303, Train Loss: 0.6082, R¬≤: 0.3792, RMSE: 0.7881\nEpoch 304, Train Loss: 0.6089, R¬≤: 0.3782, RMSE: 0.7886\nEpoch 305, Train Loss: 0.6046, R¬≤: 0.3786, RMSE: 0.7884\nEpoch 306, Train Loss: 0.6097, R¬≤: 0.3783, RMSE: 0.7886\nEpoch 307, Train Loss: 0.6106, R¬≤: 0.3767, RMSE: 0.7896\nEpoch 308, Train Loss: 0.6096, R¬≤: 0.3781, RMSE: 0.7887\nEpoch 309, Train Loss: 0.6084, R¬≤: 0.3791, RMSE: 0.7881\nEpoch 310, Train Loss: 0.6097, R¬≤: 0.3820, RMSE: 0.7862\nEpoch 311, Train Loss: 0.6085, R¬≤: 0.3842, RMSE: 0.7848\nEpoch 312, Train Loss: 0.6034, R¬≤: 0.3818, RMSE: 0.7864\nEpoch 313, Train Loss: 0.6046, R¬≤: 0.3812, RMSE: 0.7868\nEpoch 314, Train Loss: 0.6091, R¬≤: 0.3804, RMSE: 0.7873\nEpoch 315, Train Loss: 0.6058, R¬≤: 0.3797, RMSE: 0.7877\nEpoch 316, Train Loss: 0.6089, R¬≤: 0.3819, RMSE: 0.7863\nEpoch 317, Train Loss: 0.6081, R¬≤: 0.3842, RMSE: 0.7849\nEpoch 318, Train Loss: 0.6084, R¬≤: 0.3843, RMSE: 0.7848\nEpoch 319, Train Loss: 0.6046, R¬≤: 0.3832, RMSE: 0.7855\nEpoch 320, Train Loss: 0.6061, R¬≤: 0.3839, RMSE: 0.7850\nEpoch 321, Train Loss: 0.5999, R¬≤: 0.3811, RMSE: 0.7868\nEpoch 322, Train Loss: 0.6053, R¬≤: 0.3863, RMSE: 0.7835\nEpoch 323, Train Loss: 0.6036, R¬≤: 0.3851, RMSE: 0.7843\nEpoch 324, Train Loss: 0.5992, R¬≤: 0.3836, RMSE: 0.7852\nEpoch 325, Train Loss: 0.6034, R¬≤: 0.3833, RMSE: 0.7854\nEpoch 326, Train Loss: 0.6040, R¬≤: 0.3805, RMSE: 0.7872\nEpoch 327, Train Loss: 0.6034, R¬≤: 0.3815, RMSE: 0.7866\nEpoch 328, Train Loss: 0.6038, R¬≤: 0.3843, RMSE: 0.7848\nEpoch 329, Train Loss: 0.5994, R¬≤: 0.3849, RMSE: 0.7844\nEpoch 330, Train Loss: 0.6074, R¬≤: 0.3859, RMSE: 0.7838\nEpoch 331, Train Loss: 0.5967, R¬≤: 0.3832, RMSE: 0.7855\nEpoch 332, Train Loss: 0.6014, R¬≤: 0.3818, RMSE: 0.7864\nEpoch 333, Train Loss: 0.6052, R¬≤: 0.3858, RMSE: 0.7839\nEpoch 334, Train Loss: 0.6004, R¬≤: 0.3844, RMSE: 0.7847\nEpoch 335, Train Loss: 0.6060, R¬≤: 0.3857, RMSE: 0.7839\nEpoch 336, Train Loss: 0.6005, R¬≤: 0.3854, RMSE: 0.7841\nEpoch 337, Train Loss: 0.6042, R¬≤: 0.3849, RMSE: 0.7844\nEpoch 338, Train Loss: 0.6020, R¬≤: 0.3854, RMSE: 0.7841\nEpoch 339, Train Loss: 0.5999, R¬≤: 0.3849, RMSE: 0.7844\nEpoch 340, Train Loss: 0.6021, R¬≤: 0.3849, RMSE: 0.7844\nEpoch 341, Train Loss: 0.6016, R¬≤: 0.3863, RMSE: 0.7835\nEpoch 342, Train Loss: 0.5965, R¬≤: 0.3861, RMSE: 0.7836\nEpoch 343, Train Loss: 0.5987, R¬≤: 0.3863, RMSE: 0.7835\nEpoch 344, Train Loss: 0.5999, R¬≤: 0.3872, RMSE: 0.7829\nEpoch 345, Train Loss: 0.5960, R¬≤: 0.3873, RMSE: 0.7829\nEpoch 346, Train Loss: 0.5964, R¬≤: 0.3869, RMSE: 0.7831\nEpoch 347, Train Loss: 0.6000, R¬≤: 0.3856, RMSE: 0.7839\nEpoch 348, Train Loss: 0.6012, R¬≤: 0.3867, RMSE: 0.7833\nEpoch 349, Train Loss: 0.5970, R¬≤: 0.3870, RMSE: 0.7831\nEpoch 350, Train Loss: 0.5972, R¬≤: 0.3882, RMSE: 0.7823\nEpoch 351, Train Loss: 0.6010, R¬≤: 0.3882, RMSE: 0.7823\nEpoch 352, Train Loss: 0.5923, R¬≤: 0.3915, RMSE: 0.7802\nEpoch 353, Train Loss: 0.5965, R¬≤: 0.3872, RMSE: 0.7829\nEpoch 354, Train Loss: 0.6004, R¬≤: 0.3844, RMSE: 0.7847\nEpoch 355, Train Loss: 0.5989, R¬≤: 0.3836, RMSE: 0.7852\nEpoch 356, Train Loss: 0.5994, R¬≤: 0.3846, RMSE: 0.7846\nEpoch 357, Train Loss: 0.6020, R¬≤: 0.3921, RMSE: 0.7798\nEpoch 358, Train Loss: 0.5921, R¬≤: 0.3928, RMSE: 0.7793\nEpoch 359, Train Loss: 0.6002, R¬≤: 0.3896, RMSE: 0.7814\nEpoch 360, Train Loss: 0.5932, R¬≤: 0.3874, RMSE: 0.7828\nEpoch 361, Train Loss: 0.5934, R¬≤: 0.3863, RMSE: 0.7835\nEpoch 362, Train Loss: 0.5933, R¬≤: 0.3864, RMSE: 0.7835\nEpoch 363, Train Loss: 0.5979, R¬≤: 0.3890, RMSE: 0.7818\nEpoch 364, Train Loss: 0.5992, R¬≤: 0.3876, RMSE: 0.7827\nEpoch 365, Train Loss: 0.5930, R¬≤: 0.3895, RMSE: 0.7815\nEpoch 366, Train Loss: 0.5975, R¬≤: 0.3913, RMSE: 0.7803\nEpoch 367, Train Loss: 0.5892, R¬≤: 0.3918, RMSE: 0.7800\nEpoch 368, Train Loss: 0.5987, R¬≤: 0.3915, RMSE: 0.7802\nEpoch 369, Train Loss: 0.5893, R¬≤: 0.3921, RMSE: 0.7798\nEpoch 370, Train Loss: 0.5962, R¬≤: 0.3914, RMSE: 0.7802\nEpoch 371, Train Loss: 0.5952, R¬≤: 0.3921, RMSE: 0.7798\nEpoch 372, Train Loss: 0.5877, R¬≤: 0.3907, RMSE: 0.7807\nEpoch 373, Train Loss: 0.5951, R¬≤: 0.3879, RMSE: 0.7825\nEpoch 374, Train Loss: 0.5916, R¬≤: 0.3932, RMSE: 0.7791\nEpoch 375, Train Loss: 0.5947, R¬≤: 0.3954, RMSE: 0.7777\nEpoch 376, Train Loss: 0.5950, R¬≤: 0.3904, RMSE: 0.7809\nEpoch 377, Train Loss: 0.5976, R¬≤: 0.3890, RMSE: 0.7818\nEpoch 378, Train Loss: 0.5947, R¬≤: 0.3837, RMSE: 0.7851\nEpoch 379, Train Loss: 0.5955, R¬≤: 0.3926, RMSE: 0.7795\nEpoch 380, Train Loss: 0.5930, R¬≤: 0.3961, RMSE: 0.7772\nEpoch 381, Train Loss: 0.5995, R¬≤: 0.3909, RMSE: 0.7806\nEpoch 382, Train Loss: 0.5951, R¬≤: 0.3928, RMSE: 0.7793\nEpoch 383, Train Loss: 0.5961, R¬≤: 0.3914, RMSE: 0.7802\nEpoch 384, Train Loss: 0.5883, R¬≤: 0.3859, RMSE: 0.7838\nEpoch 385, Train Loss: 0.5907, R¬≤: 0.3923, RMSE: 0.7796\nEpoch 386, Train Loss: 0.5916, R¬≤: 0.3961, RMSE: 0.7772\nEpoch 387, Train Loss: 0.5869, R¬≤: 0.3927, RMSE: 0.7794\nEpoch 388, Train Loss: 0.5914, R¬≤: 0.3960, RMSE: 0.7773\nEpoch 389, Train Loss: 0.5897, R¬≤: 0.3950, RMSE: 0.7779\nEpoch 390, Train Loss: 0.5875, R¬≤: 0.3933, RMSE: 0.7790\nEpoch 391, Train Loss: 0.5916, R¬≤: 0.3962, RMSE: 0.7772\nEpoch 392, Train Loss: 0.5941, R¬≤: 0.3967, RMSE: 0.7769\nEpoch 393, Train Loss: 0.5951, R¬≤: 0.3962, RMSE: 0.7772\nEpoch 394, Train Loss: 0.5853, R¬≤: 0.3965, RMSE: 0.7770\nEpoch 395, Train Loss: 0.5909, R¬≤: 0.3958, RMSE: 0.7775\nEpoch 396, Train Loss: 0.5930, R¬≤: 0.3967, RMSE: 0.7768\nEpoch 397, Train Loss: 0.5911, R¬≤: 0.3979, RMSE: 0.7761\nEpoch 398, Train Loss: 0.5895, R¬≤: 0.3990, RMSE: 0.7754\nEpoch 399, Train Loss: 0.5871, R¬≤: 0.3989, RMSE: 0.7754\nEpoch 400, Train Loss: 0.5898, R¬≤: 0.3984, RMSE: 0.7757\nEpoch 401, Train Loss: 0.5885, R¬≤: 0.3951, RMSE: 0.7779\nEpoch 402, Train Loss: 0.5878, R¬≤: 0.3967, RMSE: 0.7769\nEpoch 403, Train Loss: 0.5896, R¬≤: 0.3986, RMSE: 0.7757\nEpoch 404, Train Loss: 0.5826, R¬≤: 0.3984, RMSE: 0.7758\nEpoch 405, Train Loss: 0.5878, R¬≤: 0.4003, RMSE: 0.7745\nEpoch 406, Train Loss: 0.5897, R¬≤: 0.3969, RMSE: 0.7767\nEpoch 407, Train Loss: 0.5902, R¬≤: 0.3975, RMSE: 0.7763\nEpoch 408, Train Loss: 0.5843, R¬≤: 0.3975, RMSE: 0.7763\nEpoch 409, Train Loss: 0.5939, R¬≤: 0.3950, RMSE: 0.7779\nEpoch 410, Train Loss: 0.5888, R¬≤: 0.3988, RMSE: 0.7755\nEpoch 411, Train Loss: 0.5856, R¬≤: 0.3979, RMSE: 0.7761\nEpoch 412, Train Loss: 0.5823, R¬≤: 0.3935, RMSE: 0.7789\nEpoch 413, Train Loss: 0.5908, R¬≤: 0.3979, RMSE: 0.7761\nEpoch 414, Train Loss: 0.5908, R¬≤: 0.3992, RMSE: 0.7752\nEpoch 415, Train Loss: 0.5858, R¬≤: 0.4007, RMSE: 0.7743\nEpoch 416, Train Loss: 0.5878, R¬≤: 0.4006, RMSE: 0.7743\nEpoch 417, Train Loss: 0.5895, R¬≤: 0.3970, RMSE: 0.7767\nEpoch 418, Train Loss: 0.5868, R¬≤: 0.3955, RMSE: 0.7776\nEpoch 419, Train Loss: 0.5887, R¬≤: 0.3939, RMSE: 0.7786\nEpoch 420, Train Loss: 0.5942, R¬≤: 0.3920, RMSE: 0.7798\nEpoch 421, Train Loss: 0.5916, R¬≤: 0.4004, RMSE: 0.7745\nEpoch 422, Train Loss: 0.5872, R¬≤: 0.4036, RMSE: 0.7724\nEpoch 423, Train Loss: 0.5925, R¬≤: 0.3971, RMSE: 0.7766\nEpoch 424, Train Loss: 0.5957, R¬≤: 0.4029, RMSE: 0.7729\nEpoch 425, Train Loss: 0.5904, R¬≤: 0.3968, RMSE: 0.7768\nEpoch 426, Train Loss: 0.5876, R¬≤: 0.3914, RMSE: 0.7803\nEpoch 427, Train Loss: 0.5913, R¬≤: 0.3960, RMSE: 0.7773\nEpoch 428, Train Loss: 0.5897, R¬≤: 0.4044, RMSE: 0.7719\nEpoch 429, Train Loss: 0.5854, R¬≤: 0.4042, RMSE: 0.7720\nEpoch 430, Train Loss: 0.5882, R¬≤: 0.4072, RMSE: 0.7700\nEpoch 431, Train Loss: 0.5854, R¬≤: 0.4006, RMSE: 0.7744\nEpoch 432, Train Loss: 0.5820, R¬≤: 0.3959, RMSE: 0.7774\nEpoch 433, Train Loss: 0.5858, R¬≤: 0.3963, RMSE: 0.7771\nEpoch 434, Train Loss: 0.5884, R¬≤: 0.4013, RMSE: 0.7739\nEpoch 435, Train Loss: 0.5854, R¬≤: 0.4023, RMSE: 0.7732\nEpoch 436, Train Loss: 0.5828, R¬≤: 0.4067, RMSE: 0.7704\nEpoch 437, Train Loss: 0.5841, R¬≤: 0.4041, RMSE: 0.7721\nEpoch 438, Train Loss: 0.5801, R¬≤: 0.4010, RMSE: 0.7741\nEpoch 439, Train Loss: 0.5861, R¬≤: 0.3997, RMSE: 0.7749\nEpoch 440, Train Loss: 0.5801, R¬≤: 0.3999, RMSE: 0.7748\nEpoch 441, Train Loss: 0.5814, R¬≤: 0.4033, RMSE: 0.7726\nEpoch 442, Train Loss: 0.5802, R¬≤: 0.4051, RMSE: 0.7714\nEpoch 443, Train Loss: 0.5791, R¬≤: 0.4044, RMSE: 0.7719\nEpoch 444, Train Loss: 0.5852, R¬≤: 0.4031, RMSE: 0.7727\nEpoch 445, Train Loss: 0.5819, R¬≤: 0.4031, RMSE: 0.7727\nEpoch 446, Train Loss: 0.5809, R¬≤: 0.4044, RMSE: 0.7719\nEpoch 447, Train Loss: 0.5814, R¬≤: 0.4056, RMSE: 0.7711\nEpoch 448, Train Loss: 0.5813, R¬≤: 0.4049, RMSE: 0.7716\nEpoch 449, Train Loss: 0.5837, R¬≤: 0.4040, RMSE: 0.7721\nEpoch 450, Train Loss: 0.5796, R¬≤: 0.4013, RMSE: 0.7739\nEpoch 451, Train Loss: 0.5834, R¬≤: 0.4022, RMSE: 0.7733\nEpoch 452, Train Loss: 0.5819, R¬≤: 0.4052, RMSE: 0.7714\nEpoch 453, Train Loss: 0.5810, R¬≤: 0.4070, RMSE: 0.7702\nEpoch 454, Train Loss: 0.5836, R¬≤: 0.4061, RMSE: 0.7708\nEpoch 455, Train Loss: 0.5777, R¬≤: 0.4036, RMSE: 0.7724\nEpoch 456, Train Loss: 0.5770, R¬≤: 0.3991, RMSE: 0.7753\nEpoch 457, Train Loss: 0.5806, R¬≤: 0.3982, RMSE: 0.7759\nEpoch 458, Train Loss: 0.5835, R¬≤: 0.4032, RMSE: 0.7727\nEpoch 459, Train Loss: 0.5812, R¬≤: 0.4067, RMSE: 0.7704\nEpoch 460, Train Loss: 0.5802, R¬≤: 0.4056, RMSE: 0.7711\nEpoch 461, Train Loss: 0.5847, R¬≤: 0.4059, RMSE: 0.7709\nEpoch 462, Train Loss: 0.5800, R¬≤: 0.4054, RMSE: 0.7712\nEpoch 463, Train Loss: 0.5853, R¬≤: 0.4051, RMSE: 0.7714\nEpoch 464, Train Loss: 0.5752, R¬≤: 0.4048, RMSE: 0.7716\nEpoch 465, Train Loss: 0.5768, R¬≤: 0.4040, RMSE: 0.7722\nEpoch 466, Train Loss: 0.5780, R¬≤: 0.4038, RMSE: 0.7723\nEpoch 467, Train Loss: 0.5789, R¬≤: 0.4047, RMSE: 0.7717\nEpoch 468, Train Loss: 0.5840, R¬≤: 0.4046, RMSE: 0.7717\nEpoch 469, Train Loss: 0.5815, R¬≤: 0.4070, RMSE: 0.7702\nEpoch 470, Train Loss: 0.5787, R¬≤: 0.4070, RMSE: 0.7702\nEpoch 471, Train Loss: 0.5836, R¬≤: 0.4047, RMSE: 0.7716\nEpoch 472, Train Loss: 0.5865, R¬≤: 0.4014, RMSE: 0.7738\nEpoch 473, Train Loss: 0.5780, R¬≤: 0.4033, RMSE: 0.7726\nEpoch 474, Train Loss: 0.5827, R¬≤: 0.4044, RMSE: 0.7719\nEpoch 475, Train Loss: 0.5802, R¬≤: 0.4107, RMSE: 0.7678\nEpoch 476, Train Loss: 0.5795, R¬≤: 0.4117, RMSE: 0.7671\nEpoch 477, Train Loss: 0.5793, R¬≤: 0.4076, RMSE: 0.7698\nEpoch 478, Train Loss: 0.5799, R¬≤: 0.4034, RMSE: 0.7725\nEpoch 479, Train Loss: 0.5780, R¬≤: 0.4024, RMSE: 0.7732\nEpoch 480, Train Loss: 0.5777, R¬≤: 0.4043, RMSE: 0.7719\nEpoch 481, Train Loss: 0.5769, R¬≤: 0.4111, RMSE: 0.7675\nEpoch 482, Train Loss: 0.5758, R¬≤: 0.4116, RMSE: 0.7672\nEpoch 483, Train Loss: 0.5780, R¬≤: 0.4047, RMSE: 0.7717\nEpoch 484, Train Loss: 0.5802, R¬≤: 0.4060, RMSE: 0.7708\nEpoch 485, Train Loss: 0.5833, R¬≤: 0.4060, RMSE: 0.7708\nEpoch 486, Train Loss: 0.5762, R¬≤: 0.4063, RMSE: 0.7706\nEpoch 487, Train Loss: 0.5803, R¬≤: 0.4076, RMSE: 0.7698\nEpoch 488, Train Loss: 0.5835, R¬≤: 0.4054, RMSE: 0.7712\nEpoch 489, Train Loss: 0.5814, R¬≤: 0.4049, RMSE: 0.7716\nEpoch 490, Train Loss: 0.5821, R¬≤: 0.4062, RMSE: 0.7707\nEpoch 491, Train Loss: 0.5789, R¬≤: 0.4049, RMSE: 0.7716\nEpoch 492, Train Loss: 0.5785, R¬≤: 0.4099, RMSE: 0.7683\nEpoch 493, Train Loss: 0.5788, R¬≤: 0.4092, RMSE: 0.7688\nEpoch 494, Train Loss: 0.5768, R¬≤: 0.4065, RMSE: 0.7705\nEpoch 495, Train Loss: 0.5763, R¬≤: 0.4076, RMSE: 0.7698\nEpoch 496, Train Loss: 0.5810, R¬≤: 0.4074, RMSE: 0.7699\nEpoch 497, Train Loss: 0.5804, R¬≤: 0.4079, RMSE: 0.7696\nEpoch 498, Train Loss: 0.5851, R¬≤: 0.4120, RMSE: 0.7669\nEpoch 499, Train Loss: 0.5765, R¬≤: 0.4120, RMSE: 0.7669\nEpoch 500, Train Loss: 0.5732, R¬≤: 0.4104, RMSE: 0.7680\n",
  "history_begin_time" : 1740630185287,
  "history_end_time" : 1740641692092,
  "history_notes" : "GAT: Epoch 500, Train Loss: 0.5732, R¬≤: 0.4104, RMSE: 0.7680",
  "history_process" : "kevt5u",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "aPhpj5VtpnSH",
  "history_input" : "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os\nimport warnings\nfrom datetime import datetime\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import GATConv\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch_geometric.utils import is_undirected, to_undirected, contains_self_loops, degree\n\n# Suppress Warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Check Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"‚úÖ Using device: {device}\")\n\n# Step 1: Define the GAT Model\nclass GAT(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, heads=4, dropout=0.5):  # Reduced heads for memory efficiency\n        super(GAT, self).__init__()\n        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, concat=True, dropout=dropout)\n        self.conv2 = GATConv(hidden_dim * heads, hidden_dim, heads=heads, concat=True, dropout=dropout)\n        self.conv3 = GATConv(hidden_dim * heads, hidden_dim, heads=1, concat=False, dropout=dropout)\n        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = self.conv1(x, edge_index)\n        x = F.elu(x)\n        x = self.dropout(x)\n\n        x = self.conv2(x, edge_index)\n        x = F.elu(x)\n        x = self.dropout(x)\n\n        x = self.conv3(x, edge_index)\n        x = F.elu(x)\n\n        x = self.fc(x)  # Output layer\n        return x\n\n# Step 2: Load Graph Data\ndef load_data(graph_file):\n    print(\"üìå Loading graph data...\")\n    try:\n        graph_data = torch.load(graph_file)\n        graph_data = graph_data.to(device)\n\n        # Ensure labels are properly formatted\n        graph_data.y = torch.tensor(StandardScaler().fit_transform(graph_data.y.view(-1, 1)), dtype=torch.float).to(device)\n        graph_data.y = graph_data.y.view(-1)  # Flatten\n\n        print(f\"‚úÖ Graph Loaded: Nodes={graph_data.x.shape[0]}, Features={graph_data.num_features}, Edges={graph_data.edge_index.shape[1]}\")\n        return graph_data\n\n    except Exception as e:\n        print(f\"‚ùå Error loading graph: {e}\")\n        exit()\n\n# Step 3: Validate and Clean Graph\ndef validate_and_clean_graph(graph_data):\n    print(\"üîç Validating Graph Structure...\")\n    \n    # Ensure graph is undirected\n    if not is_undirected(graph_data.edge_index):\n        graph_data.edge_index = to_undirected(graph_data.edge_index)\n    \n    print(\"‚úÖ Graph is undirected.\")\n\n    # Remove isolated nodes\n    degrees = degree(graph_data.edge_index[0], num_nodes=graph_data.num_nodes)\n    non_isolated_nodes = (degrees > 0).nonzero(as_tuple=True)[0]\n\n    graph_data.x = graph_data.x[non_isolated_nodes]\n    graph_data.y = graph_data.y[non_isolated_nodes]\n\n    node_map = {old_idx: new_idx for new_idx, old_idx in enumerate(non_isolated_nodes.tolist())}\n    new_edges = [[node_map[i], node_map[j]] for i, j in graph_data.edge_index.t().tolist() if i in node_map and j in node_map]\n    graph_data.edge_index = torch.tensor(new_edges, dtype=torch.long).t().contiguous()\n    \n    print(f\"‚úÖ Cleaned graph: {graph_data.x.shape[0]} nodes, {graph_data.edge_index.shape[1]} edges\")\n    return graph_data\n\n# Step 4: Training & Testing Functions\ndef train(model, optimizer, criterion, graph_data, train_mask):\n    model.train()\n    optimizer.zero_grad()\n    output = model(graph_data)[train_mask]\n    loss = criterion(output.squeeze(), graph_data.y[train_mask])\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n\ndef test(model, criterion, graph_data, test_mask):\n    model.eval()\n    with torch.no_grad():\n        predictions = model(graph_data)[test_mask].squeeze().cpu().numpy()\n        actuals = graph_data.y[test_mask].cpu().numpy()\n\n        mse = mean_squared_error(actuals, predictions)\n        r2 = r2_score(actuals, predictions)\n        mae = mean_absolute_error(actuals, predictions)\n        rmse = mean_squared_error(actuals, predictions, squared=False)\n\n    return mse, r2, rmse, mae\n\n# Step 5: Main Function\ndef main():\n    graph_file = \"/media/volume1/gat_spatial_training_data_filtered.pt\"\n    log_dir = \"/media/volume1/swe/data/training_logs\"\n    os.makedirs(log_dir, exist_ok=True)\n    log_file = os.path.join(log_dir, f\"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\")\n\n    def log(message):\n        print(message)\n        with open(log_file, \"a\") as f:\n            f.write(message + \"\\n\")\n\n    graph_data = load_data(graph_file)\n    graph_data = validate_and_clean_graph(graph_data)\n\n    train_ratio = 0.8\n    num_train_samples = int(train_ratio * graph_data.x.shape[0])\n    train_mask = torch.zeros(graph_data.x.shape[0], dtype=torch.bool)\n    train_mask[:num_train_samples] = True\n    test_mask = ~train_mask\n\n    log(\"‚úÖ Model initialized. Training started...\")\n\n    model = GAT(input_dim=graph_data.num_features, hidden_dim=64, output_dim=1, heads=4, dropout=0.5).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.MSELoss()\n\n    for epoch in range(1, 200 + 1):\n        train_loss = train(model, optimizer, criterion, graph_data, train_mask)\n        test_loss, r2, rmse, mae = test(model, criterion, graph_data, test_mask)\n        log(f\"Epoch {epoch:03d}, Train Loss: {train_loss:.4f}, R¬≤: {r2:.4f}, RMSE: {rmse:.4f}\")\n\nif __name__ == \"__main__\":\n    main()\n",
  "history_output" : "‚úÖ Using device: cpu\nüìå Loading graph data...\n‚úÖ Graph Loaded: Nodes=50000, Features=13, Edges=7064598\nüîç Validating Graph Structure...\n‚úÖ Graph is undirected.\n‚úÖ Cleaned graph: 49999 nodes, 7064598 edges\n‚úÖ Model initialized. Training started...\nEpoch 001, Train Loss: 1.0741, R¬≤: 0.1776, RMSE: 0.9070\nEpoch 002, Train Loss: 0.8285, R¬≤: 0.1548, RMSE: 0.9195\nEpoch 003, Train Loss: 0.8568, R¬≤: 0.1472, RMSE: 0.9236\nEpoch 004, Train Loss: 0.8618, R¬≤: 0.1956, RMSE: 0.8970\nEpoch 005, Train Loss: 0.8132, R¬≤: 0.2390, RMSE: 0.8725\nEpoch 006, Train Loss: 0.7671, R¬≤: 0.2474, RMSE: 0.8677\nEpoch 007, Train Loss: 0.7555, R¬≤: 0.2331, RMSE: 0.8758\nEpoch 008, Train Loss: 0.7684, R¬≤: 0.2241, RMSE: 0.8810\nEpoch 009, Train Loss: 0.7780, R¬≤: 0.2329, RMSE: 0.8760\nEpoch 010, Train Loss: 0.7683, R¬≤: 0.2525, RMSE: 0.8647\nEpoch 011, Train Loss: 0.7521, R¬≤: 0.2701, RMSE: 0.8544\nEpoch 012, Train Loss: 0.7365, R¬≤: 0.2778, RMSE: 0.8500\nEpoch 013, Train Loss: 0.7321, R¬≤: 0.2755, RMSE: 0.8513\nEpoch 014, Train Loss: 0.7339, R¬≤: 0.2704, RMSE: 0.8543\nEpoch 015, Train Loss: 0.7428, R¬≤: 0.2695, RMSE: 0.8548\nEpoch 016, Train Loss: 0.7428, R¬≤: 0.2745, RMSE: 0.8519\nEpoch 017, Train Loss: 0.7374, R¬≤: 0.2823, RMSE: 0.8473\nEpoch 018, Train Loss: 0.7273, R¬≤: 0.2880, RMSE: 0.8440\nEpoch 019, Train Loss: 0.7213, R¬≤: 0.2891, RMSE: 0.8433\nEpoch 020, Train Loss: 0.7184, R¬≤: 0.2868, RMSE: 0.8446\nEpoch 021, Train Loss: 0.7209, R¬≤: 0.2841, RMSE: 0.8462\nEpoch 022, Train Loss: 0.7239, R¬≤: 0.2838, RMSE: 0.8464\nEpoch 023, Train Loss: 0.7214, R¬≤: 0.2865, RMSE: 0.8448\nEpoch 024, Train Loss: 0.7195, R¬≤: 0.2909, RMSE: 0.8422\nEpoch 025, Train Loss: 0.7162, R¬≤: 0.2947, RMSE: 0.8400\nEpoch 026, Train Loss: 0.7115, R¬≤: 0.2966, RMSE: 0.8388\nEpoch 027, Train Loss: 0.7099, R¬≤: 0.2970, RMSE: 0.8386\nEpoch 028, Train Loss: 0.7069, R¬≤: 0.2973, RMSE: 0.8384\nEpoch 029, Train Loss: 0.7082, R¬≤: 0.2990, RMSE: 0.8374\nEpoch 030, Train Loss: 0.7079, R¬≤: 0.3019, RMSE: 0.8357\nEpoch 031, Train Loss: 0.7049, R¬≤: 0.3050, RMSE: 0.8338\nEpoch 032, Train Loss: 0.7020, R¬≤: 0.3069, RMSE: 0.8326\nEpoch 033, Train Loss: 0.7014, R¬≤: 0.3075, RMSE: 0.8323\nEpoch 034, Train Loss: 0.6985, R¬≤: 0.3075, RMSE: 0.8323\nEpoch 035, Train Loss: 0.7004, R¬≤: 0.3080, RMSE: 0.8320\nEpoch 036, Train Loss: 0.7005, R¬≤: 0.3095, RMSE: 0.8311\nEpoch 037, Train Loss: 0.7001, R¬≤: 0.3115, RMSE: 0.8299\nEpoch 038, Train Loss: 0.6977, R¬≤: 0.3130, RMSE: 0.8290\nEpoch 039, Train Loss: 0.6958, R¬≤: 0.3136, RMSE: 0.8286\nEpoch 040, Train Loss: 0.6945, R¬≤: 0.3137, RMSE: 0.8286\nEpoch 041, Train Loss: 0.6941, R¬≤: 0.3140, RMSE: 0.8284\nEpoch 042, Train Loss: 0.6913, R¬≤: 0.3148, RMSE: 0.8279\nEpoch 043, Train Loss: 0.6928, R¬≤: 0.3158, RMSE: 0.8273\nEpoch 044, Train Loss: 0.6906, R¬≤: 0.3165, RMSE: 0.8269\nEpoch 045, Train Loss: 0.6907, R¬≤: 0.3169, RMSE: 0.8266\nEpoch 046, Train Loss: 0.6913, R¬≤: 0.3173, RMSE: 0.8264\nEpoch 047, Train Loss: 0.6888, R¬≤: 0.3183, RMSE: 0.8258\nEpoch 048, Train Loss: 0.6897, R¬≤: 0.3195, RMSE: 0.8250\nEpoch 049, Train Loss: 0.6887, R¬≤: 0.3205, RMSE: 0.8244\nEpoch 050, Train Loss: 0.6871, R¬≤: 0.3211, RMSE: 0.8241\nEpoch 051, Train Loss: 0.6867, R¬≤: 0.3216, RMSE: 0.8238\nEpoch 052, Train Loss: 0.6867, R¬≤: 0.3223, RMSE: 0.8233\nEpoch 053, Train Loss: 0.6861, R¬≤: 0.3232, RMSE: 0.8228\nEpoch 054, Train Loss: 0.6870, R¬≤: 0.3237, RMSE: 0.8225\nEpoch 055, Train Loss: 0.6870, R¬≤: 0.3239, RMSE: 0.8224\nEpoch 056, Train Loss: 0.6860, R¬≤: 0.3242, RMSE: 0.8222\nEpoch 057, Train Loss: 0.6836, R¬≤: 0.3247, RMSE: 0.8219\nEpoch 058, Train Loss: 0.6843, R¬≤: 0.3253, RMSE: 0.8215\nEpoch 059, Train Loss: 0.6838, R¬≤: 0.3258, RMSE: 0.8212\nEpoch 060, Train Loss: 0.6820, R¬≤: 0.3260, RMSE: 0.8211\nEpoch 061, Train Loss: 0.6823, R¬≤: 0.3264, RMSE: 0.8209\nEpoch 062, Train Loss: 0.6812, R¬≤: 0.3269, RMSE: 0.8206\nEpoch 063, Train Loss: 0.6819, R¬≤: 0.3273, RMSE: 0.8203\nEpoch 064, Train Loss: 0.6831, R¬≤: 0.3276, RMSE: 0.8201\nEpoch 065, Train Loss: 0.6822, R¬≤: 0.3279, RMSE: 0.8199\nEpoch 066, Train Loss: 0.6810, R¬≤: 0.3284, RMSE: 0.8196\nEpoch 067, Train Loss: 0.6800, R¬≤: 0.3289, RMSE: 0.8193\nEpoch 068, Train Loss: 0.6796, R¬≤: 0.3293, RMSE: 0.8191\nEpoch 069, Train Loss: 0.6780, R¬≤: 0.3297, RMSE: 0.8188\nEpoch 070, Train Loss: 0.6770, R¬≤: 0.3302, RMSE: 0.8185\nEpoch 071, Train Loss: 0.6787, R¬≤: 0.3307, RMSE: 0.8182\nEpoch 072, Train Loss: 0.6796, R¬≤: 0.3311, RMSE: 0.8180\nEpoch 073, Train Loss: 0.6788, R¬≤: 0.3314, RMSE: 0.8178\nEpoch 074, Train Loss: 0.6766, R¬≤: 0.3319, RMSE: 0.8175\nEpoch 075, Train Loss: 0.6768, R¬≤: 0.3322, RMSE: 0.8173\nEpoch 076, Train Loss: 0.6762, R¬≤: 0.3324, RMSE: 0.8172\nEpoch 077, Train Loss: 0.6770, R¬≤: 0.3327, RMSE: 0.8170\nEpoch 078, Train Loss: 0.6741, R¬≤: 0.3332, RMSE: 0.8167\nEpoch 079, Train Loss: 0.6766, R¬≤: 0.3337, RMSE: 0.8164\nEpoch 080, Train Loss: 0.6724, R¬≤: 0.3341, RMSE: 0.8162\nEpoch 081, Train Loss: 0.6781, R¬≤: 0.3345, RMSE: 0.8159\nEpoch 082, Train Loss: 0.6739, R¬≤: 0.3350, RMSE: 0.8156\nEpoch 083, Train Loss: 0.6734, R¬≤: 0.3354, RMSE: 0.8154\nEpoch 084, Train Loss: 0.6725, R¬≤: 0.3358, RMSE: 0.8151\nEpoch 085, Train Loss: 0.6715, R¬≤: 0.3361, RMSE: 0.8149\nEpoch 086, Train Loss: 0.6717, R¬≤: 0.3365, RMSE: 0.8147\nEpoch 087, Train Loss: 0.6711, R¬≤: 0.3369, RMSE: 0.8145\nEpoch 088, Train Loss: 0.6711, R¬≤: 0.3372, RMSE: 0.8143\nEpoch 089, Train Loss: 0.6736, R¬≤: 0.3375, RMSE: 0.8141\nEpoch 090, Train Loss: 0.6696, R¬≤: 0.3378, RMSE: 0.8139\nEpoch 091, Train Loss: 0.6723, R¬≤: 0.3381, RMSE: 0.8137\nEpoch 092, Train Loss: 0.6712, R¬≤: 0.3385, RMSE: 0.8135\nEpoch 093, Train Loss: 0.6725, R¬≤: 0.3390, RMSE: 0.8132\nEpoch 094, Train Loss: 0.6676, R¬≤: 0.3395, RMSE: 0.8128\nEpoch 095, Train Loss: 0.6663, R¬≤: 0.3401, RMSE: 0.8125\nEpoch 096, Train Loss: 0.6682, R¬≤: 0.3406, RMSE: 0.8122\nEpoch 097, Train Loss: 0.6693, R¬≤: 0.3409, RMSE: 0.8120\nEpoch 098, Train Loss: 0.6674, R¬≤: 0.3411, RMSE: 0.8118\nEpoch 099, Train Loss: 0.6683, R¬≤: 0.3414, RMSE: 0.8117\nEpoch 100, Train Loss: 0.6647, R¬≤: 0.3416, RMSE: 0.8116\nEpoch 101, Train Loss: 0.6674, R¬≤: 0.3418, RMSE: 0.8114\nEpoch 102, Train Loss: 0.6658, R¬≤: 0.3421, RMSE: 0.8113\nEpoch 103, Train Loss: 0.6665, R¬≤: 0.3423, RMSE: 0.8111\nEpoch 104, Train Loss: 0.6685, R¬≤: 0.3427, RMSE: 0.8109\nEpoch 105, Train Loss: 0.6656, R¬≤: 0.3432, RMSE: 0.8106\nEpoch 106, Train Loss: 0.6662, R¬≤: 0.3437, RMSE: 0.8102\nEpoch 107, Train Loss: 0.6657, R¬≤: 0.3442, RMSE: 0.8100\nEpoch 108, Train Loss: 0.6608, R¬≤: 0.3445, RMSE: 0.8098\nEpoch 109, Train Loss: 0.6635, R¬≤: 0.3446, RMSE: 0.8097\nEpoch 110, Train Loss: 0.6640, R¬≤: 0.3448, RMSE: 0.8096\nEpoch 111, Train Loss: 0.6652, R¬≤: 0.3451, RMSE: 0.8094\nEpoch 112, Train Loss: 0.6623, R¬≤: 0.3454, RMSE: 0.8092\nEpoch 113, Train Loss: 0.6651, R¬≤: 0.3458, RMSE: 0.8090\nEpoch 114, Train Loss: 0.6635, R¬≤: 0.3462, RMSE: 0.8087\nEpoch 115, Train Loss: 0.6627, R¬≤: 0.3465, RMSE: 0.8085\nEpoch 116, Train Loss: 0.6626, R¬≤: 0.3467, RMSE: 0.8084\nEpoch 117, Train Loss: 0.6631, R¬≤: 0.3468, RMSE: 0.8083\nEpoch 118, Train Loss: 0.6596, R¬≤: 0.3470, RMSE: 0.8082\nEpoch 119, Train Loss: 0.6616, R¬≤: 0.3474, RMSE: 0.8080\nEpoch 120, Train Loss: 0.6624, R¬≤: 0.3478, RMSE: 0.8077\nEpoch 121, Train Loss: 0.6584, R¬≤: 0.3484, RMSE: 0.8074\nEpoch 122, Train Loss: 0.6596, R¬≤: 0.3488, RMSE: 0.8071\nEpoch 123, Train Loss: 0.6583, R¬≤: 0.3490, RMSE: 0.8070\nEpoch 124, Train Loss: 0.6618, R¬≤: 0.3492, RMSE: 0.8068\nEpoch 125, Train Loss: 0.6595, R¬≤: 0.3493, RMSE: 0.8068\nEpoch 126, Train Loss: 0.6597, R¬≤: 0.3492, RMSE: 0.8068\nEpoch 127, Train Loss: 0.6581, R¬≤: 0.3494, RMSE: 0.8067\nEpoch 128, Train Loss: 0.6617, R¬≤: 0.3499, RMSE: 0.8064\nEpoch 129, Train Loss: 0.6588, R¬≤: 0.3504, RMSE: 0.8061\nEpoch 130, Train Loss: 0.6602, R¬≤: 0.3509, RMSE: 0.8058\nEpoch 131, Train Loss: 0.6579, R¬≤: 0.3512, RMSE: 0.8056\nEpoch 132, Train Loss: 0.6576, R¬≤: 0.3510, RMSE: 0.8057\nEpoch 133, Train Loss: 0.6601, R¬≤: 0.3510, RMSE: 0.8058\nEpoch 134, Train Loss: 0.6571, R¬≤: 0.3511, RMSE: 0.8057\nEpoch 135, Train Loss: 0.6559, R¬≤: 0.3514, RMSE: 0.8055\nEpoch 136, Train Loss: 0.6559, R¬≤: 0.3520, RMSE: 0.8051\nEpoch 137, Train Loss: 0.6571, R¬≤: 0.3525, RMSE: 0.8048\nEpoch 138, Train Loss: 0.6564, R¬≤: 0.3530, RMSE: 0.8045\nEpoch 139, Train Loss: 0.6539, R¬≤: 0.3534, RMSE: 0.8042\nEpoch 140, Train Loss: 0.6551, R¬≤: 0.3536, RMSE: 0.8041\nEpoch 141, Train Loss: 0.6562, R¬≤: 0.3534, RMSE: 0.8042\nEpoch 142, Train Loss: 0.6568, R¬≤: 0.3531, RMSE: 0.8044\nEpoch 143, Train Loss: 0.6562, R¬≤: 0.3530, RMSE: 0.8045\nEpoch 144, Train Loss: 0.6506, R¬≤: 0.3534, RMSE: 0.8043\nEpoch 145, Train Loss: 0.6551, R¬≤: 0.3543, RMSE: 0.8037\nEpoch 146, Train Loss: 0.6524, R¬≤: 0.3553, RMSE: 0.8031\nEpoch 147, Train Loss: 0.6559, R¬≤: 0.3555, RMSE: 0.8029\nEpoch 148, Train Loss: 0.6545, R¬≤: 0.3553, RMSE: 0.8031\nEpoch 149, Train Loss: 0.6520, R¬≤: 0.3547, RMSE: 0.8034\nEpoch 150, Train Loss: 0.6536, R¬≤: 0.3545, RMSE: 0.8036\nEpoch 151, Train Loss: 0.6533, R¬≤: 0.3547, RMSE: 0.8034\nEpoch 152, Train Loss: 0.6535, R¬≤: 0.3553, RMSE: 0.8030\nEpoch 153, Train Loss: 0.6514, R¬≤: 0.3563, RMSE: 0.8024\nEpoch 154, Train Loss: 0.6548, R¬≤: 0.3569, RMSE: 0.8021\nEpoch 155, Train Loss: 0.6521, R¬≤: 0.3571, RMSE: 0.8020\nEpoch 156, Train Loss: 0.6518, R¬≤: 0.3568, RMSE: 0.8021\nEpoch 157, Train Loss: 0.6489, R¬≤: 0.3567, RMSE: 0.8022\nEpoch 158, Train Loss: 0.6538, R¬≤: 0.3568, RMSE: 0.8021\nEpoch 159, Train Loss: 0.6487, R¬≤: 0.3576, RMSE: 0.8016\nEpoch 160, Train Loss: 0.6508, R¬≤: 0.3584, RMSE: 0.8011\nEpoch 161, Train Loss: 0.6503, R¬≤: 0.3585, RMSE: 0.8011\nEpoch 162, Train Loss: 0.6490, R¬≤: 0.3585, RMSE: 0.8010\nEpoch 163, Train Loss: 0.6486, R¬≤: 0.3586, RMSE: 0.8010\nEpoch 164, Train Loss: 0.6488, R¬≤: 0.3586, RMSE: 0.8010\nEpoch 165, Train Loss: 0.6515, R¬≤: 0.3587, RMSE: 0.8010\nEpoch 166, Train Loss: 0.6505, R¬≤: 0.3588, RMSE: 0.8009\nEpoch 167, Train Loss: 0.6455, R¬≤: 0.3594, RMSE: 0.8005\nEpoch 168, Train Loss: 0.6471, R¬≤: 0.3598, RMSE: 0.8003\nEpoch 169, Train Loss: 0.6454, R¬≤: 0.3598, RMSE: 0.8002\nEpoch 170, Train Loss: 0.6489, R¬≤: 0.3599, RMSE: 0.8002\nEpoch 171, Train Loss: 0.6520, R¬≤: 0.3602, RMSE: 0.8000\nEpoch 172, Train Loss: 0.6444, R¬≤: 0.3607, RMSE: 0.7997\nEpoch 173, Train Loss: 0.6469, R¬≤: 0.3611, RMSE: 0.7994\nEpoch 174, Train Loss: 0.6470, R¬≤: 0.3614, RMSE: 0.7993\nEpoch 175, Train Loss: 0.6476, R¬≤: 0.3614, RMSE: 0.7992\nEpoch 176, Train Loss: 0.6498, R¬≤: 0.3615, RMSE: 0.7992\nEpoch 177, Train Loss: 0.6494, R¬≤: 0.3615, RMSE: 0.7992\nEpoch 178, Train Loss: 0.6481, R¬≤: 0.3621, RMSE: 0.7988\nEpoch 179, Train Loss: 0.6452, R¬≤: 0.3625, RMSE: 0.7985\nEpoch 180, Train Loss: 0.6441, R¬≤: 0.3628, RMSE: 0.7984\nEpoch 181, Train Loss: 0.6427, R¬≤: 0.3634, RMSE: 0.7980\nEpoch 182, Train Loss: 0.6445, R¬≤: 0.3634, RMSE: 0.7980\nEpoch 183, Train Loss: 0.6440, R¬≤: 0.3632, RMSE: 0.7981\nEpoch 184, Train Loss: 0.6446, R¬≤: 0.3631, RMSE: 0.7982\nEpoch 185, Train Loss: 0.6454, R¬≤: 0.3630, RMSE: 0.7983\nEpoch 186, Train Loss: 0.6442, R¬≤: 0.3630, RMSE: 0.7982\nEpoch 187, Train Loss: 0.6456, R¬≤: 0.3640, RMSE: 0.7976\nEpoch 188, Train Loss: 0.6436, R¬≤: 0.3645, RMSE: 0.7973\nEpoch 189, Train Loss: 0.6415, R¬≤: 0.3656, RMSE: 0.7966\nEpoch 190, Train Loss: 0.6424, R¬≤: 0.3653, RMSE: 0.7968\nEpoch 191, Train Loss: 0.6414, R¬≤: 0.3649, RMSE: 0.7971\nEpoch 192, Train Loss: 0.6404, R¬≤: 0.3650, RMSE: 0.7970\nEpoch 193, Train Loss: 0.6430, R¬≤: 0.3659, RMSE: 0.7964\nEpoch 194, Train Loss: 0.6446, R¬≤: 0.3665, RMSE: 0.7961\nEpoch 195, Train Loss: 0.6415, R¬≤: 0.3664, RMSE: 0.7961\nEpoch 196, Train Loss: 0.6399, R¬≤: 0.3657, RMSE: 0.7966\nEpoch 197, Train Loss: 0.6435, R¬≤: 0.3659, RMSE: 0.7965\nEpoch 198, Train Loss: 0.6436, R¬≤: 0.3669, RMSE: 0.7958\nEpoch 199, Train Loss: 0.6390, R¬≤: 0.3677, RMSE: 0.7953\nEpoch 200, Train Loss: 0.6393, R¬≤: 0.3688, RMSE: 0.7946\n",
  "history_begin_time" : 1740445170143,
  "history_end_time" : 1740449822566,
  "history_notes" : "GAT : Epoch 200, Train Loss: 0.6393, R¬≤: 0.3688, RMSE: 0.7946",
  "history_process" : "kevt5u",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "FI5FNaLfDnPO",
  "history_input" : "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os\nimport warnings\nfrom datetime import datetime\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import GATConv\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch_geometric.utils import is_undirected, to_undirected, contains_self_loops, degree\n\n# Suppress Warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Check Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"‚úÖ Using device: {device}\")\n\n# Step 1: Define the GAT Model\nclass GAT(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, heads=4, dropout=0.5):  # Reduced heads for memory efficiency\n        super(GAT, self).__init__()\n        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, concat=True, dropout=dropout)\n        self.conv2 = GATConv(hidden_dim * heads, hidden_dim, heads=heads, concat=True, dropout=dropout)\n        self.conv3 = GATConv(hidden_dim * heads, hidden_dim, heads=1, concat=False, dropout=dropout)\n        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = self.conv1(x, edge_index)\n        x = F.elu(x)\n        x = self.dropout(x)\n\n        x = self.conv2(x, edge_index)\n        x = F.elu(x)\n        x = self.dropout(x)\n\n        x = self.conv3(x, edge_index)\n        x = F.elu(x)\n\n        x = self.fc(x)  # Output layer\n        return x\n\n# Step 2: Load Graph Data\ndef load_data(graph_file):\n    print(\"üìå Loading graph data...\")\n    try:\n        graph_data = torch.load(graph_file)\n        graph_data = graph_data.to(device)\n\n        # Ensure labels are properly formatted\n        graph_data.y = torch.tensor(StandardScaler().fit_transform(graph_data.y.view(-1, 1)), dtype=torch.float).to(device)\n        graph_data.y = graph_data.y.view(-1)  # Flatten\n\n        print(f\"‚úÖ Graph Loaded: Nodes={graph_data.x.shape[0]}, Features={graph_data.num_features}, Edges={graph_data.edge_index.shape[1]}\")\n        return graph_data\n\n    except Exception as e:\n        print(f\"‚ùå Error loading graph: {e}\")\n        exit()\n\n# Step 3: Validate and Clean Graph\ndef validate_and_clean_graph(graph_data):\n    print(\"üîç Validating Graph Structure...\")\n    \n    # Ensure graph is undirected\n    if not is_undirected(graph_data.edge_index):\n        graph_data.edge_index = to_undirected(graph_data.edge_index)\n    \n    print(\"‚úÖ Graph is undirected.\")\n\n    # Remove isolated nodes\n    degrees = degree(graph_data.edge_index[0], num_nodes=graph_data.num_nodes)\n    non_isolated_nodes = (degrees > 0).nonzero(as_tuple=True)[0]\n\n    graph_data.x = graph_data.x[non_isolated_nodes]\n    graph_data.y = graph_data.y[non_isolated_nodes]\n\n    node_map = {old_idx: new_idx for new_idx, old_idx in enumerate(non_isolated_nodes.tolist())}\n    new_edges = [[node_map[i], node_map[j]] for i, j in graph_data.edge_index.t().tolist() if i in node_map and j in node_map]\n    graph_data.edge_index = torch.tensor(new_edges, dtype=torch.long).t().contiguous()\n    \n    print(f\"‚úÖ Cleaned graph: {graph_data.x.shape[0]} nodes, {graph_data.edge_index.shape[1]} edges\")\n    return graph_data\n\n# Step 4: Training & Testing Functions\ndef train(model, optimizer, criterion, graph_data, train_mask):\n    model.train()\n    optimizer.zero_grad()\n    output = model(graph_data)[train_mask]\n    loss = criterion(output.squeeze(), graph_data.y[train_mask])\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n\ndef test(model, criterion, graph_data, test_mask):\n    model.eval()\n    with torch.no_grad():\n        predictions = model(graph_data)[test_mask].squeeze().cpu().numpy()\n        actuals = graph_data.y[test_mask].cpu().numpy()\n\n        mse = mean_squared_error(actuals, predictions)\n        r2 = r2_score(actuals, predictions)\n        mae = mean_absolute_error(actuals, predictions)\n        rmse = mean_squared_error(actuals, predictions, squared=False)\n\n    return mse, r2, rmse, mae\n\n# Step 5: Main Function\ndef main():\n    graph_file = \"/media/volume1/gat_spatial_training_data_filtered.pt\"\n    log_dir = \"/media/volume1/swe/data/training_logs\"\n    os.makedirs(log_dir, exist_ok=True)\n    log_file = os.path.join(log_dir, f\"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\")\n\n    def log(message):\n        print(message)\n        with open(log_file, \"a\") as f:\n            f.write(message + \"\\n\")\n\n    graph_data = load_data(graph_file)\n    graph_data = validate_and_clean_graph(graph_data)\n\n    train_ratio = 0.8\n    num_train_samples = int(train_ratio * graph_data.x.shape[0])\n    train_mask = torch.zeros(graph_data.x.shape[0], dtype=torch.bool)\n    train_mask[:num_train_samples] = True\n    test_mask = ~train_mask\n\n    log(\"‚úÖ Model initialized. Training started...\")\n\n    model = GAT(input_dim=graph_data.num_features, hidden_dim=64, output_dim=1, heads=4, dropout=0.5).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.MSELoss()\n\n    for epoch in range(1, 100 + 1):\n        train_loss = train(model, optimizer, criterion, graph_data, train_mask)\n        test_loss, r2, rmse, mae = test(model, criterion, graph_data, test_mask)\n        log(f\"Epoch {epoch:03d}, Train Loss: {train_loss:.4f}, R¬≤: {r2:.4f}, RMSE: {rmse:.4f}\")\n\nif __name__ == \"__main__\":\n    main()\n",
  "history_output" : "‚úÖ Using device: cpu\nüìå Loading graph data...\n‚úÖ Graph Loaded: Nodes=50000, Features=13, Edges=7064598\nüîç Validating Graph Structure...\n‚úÖ Graph is undirected.\n‚úÖ Cleaned graph: 49999 nodes, 7064598 edges\n‚úÖ Model initialized. Training started...\nEpoch 001, Train Loss: 0.8732, R¬≤: 0.2024, RMSE: 0.8932\nEpoch 002, Train Loss: 0.8008, R¬≤: 0.1975, RMSE: 0.8960\nEpoch 003, Train Loss: 0.8077, R¬≤: 0.2292, RMSE: 0.8781\nEpoch 004, Train Loss: 0.7754, R¬≤: 0.2477, RMSE: 0.8675\nEpoch 005, Train Loss: 0.7563, R¬≤: 0.2435, RMSE: 0.8699\nEpoch 006, Train Loss: 0.7613, R¬≤: 0.2420, RMSE: 0.8708\nEpoch 007, Train Loss: 0.7628, R¬≤: 0.2530, RMSE: 0.8644\nEpoch 008, Train Loss: 0.7520, R¬≤: 0.2658, RMSE: 0.8570\nEpoch 009, Train Loss: 0.7372, R¬≤: 0.2702, RMSE: 0.8544\nEpoch 010, Train Loss: 0.7336, R¬≤: 0.2685, RMSE: 0.8554\nEpoch 011, Train Loss: 0.7387, R¬≤: 0.2696, RMSE: 0.8547\nEpoch 012, Train Loss: 0.7369, R¬≤: 0.2767, RMSE: 0.8506\nEpoch 013, Train Loss: 0.7299, R¬≤: 0.2846, RMSE: 0.8459\nEpoch 014, Train Loss: 0.7203, R¬≤: 0.2881, RMSE: 0.8439\nEpoch 015, Train Loss: 0.7178, R¬≤: 0.2875, RMSE: 0.8442\nEpoch 016, Train Loss: 0.7194, R¬≤: 0.2878, RMSE: 0.8441\nEpoch 017, Train Loss: 0.7183, R¬≤: 0.2915, RMSE: 0.8419\nEpoch 018, Train Loss: 0.7163, R¬≤: 0.2966, RMSE: 0.8388\nEpoch 019, Train Loss: 0.7105, R¬≤: 0.2998, RMSE: 0.8369\nEpoch 020, Train Loss: 0.7064, R¬≤: 0.3004, RMSE: 0.8365\nEpoch 021, Train Loss: 0.7063, R¬≤: 0.3010, RMSE: 0.8362\nEpoch 022, Train Loss: 0.7056, R¬≤: 0.3031, RMSE: 0.8349\nEpoch 023, Train Loss: 0.7045, R¬≤: 0.3058, RMSE: 0.8333\nEpoch 024, Train Loss: 0.7010, R¬≤: 0.3071, RMSE: 0.8326\nEpoch 025, Train Loss: 0.6981, R¬≤: 0.3077, RMSE: 0.8322\nEpoch 026, Train Loss: 0.6994, R¬≤: 0.3096, RMSE: 0.8310\nEpoch 027, Train Loss: 0.6976, R¬≤: 0.3124, RMSE: 0.8294\nEpoch 028, Train Loss: 0.6947, R¬≤: 0.3142, RMSE: 0.8283\nEpoch 029, Train Loss: 0.6926, R¬≤: 0.3144, RMSE: 0.8281\nEpoch 030, Train Loss: 0.6925, R¬≤: 0.3151, RMSE: 0.8277\nEpoch 031, Train Loss: 0.6940, R¬≤: 0.3172, RMSE: 0.8264\nEpoch 032, Train Loss: 0.6905, R¬≤: 0.3190, RMSE: 0.8253\nEpoch 033, Train Loss: 0.6875, R¬≤: 0.3194, RMSE: 0.8251\nEpoch 034, Train Loss: 0.6886, R¬≤: 0.3197, RMSE: 0.8249\nEpoch 035, Train Loss: 0.6859, R¬≤: 0.3209, RMSE: 0.8242\nEpoch 036, Train Loss: 0.6868, R¬≤: 0.3219, RMSE: 0.8236\nEpoch 037, Train Loss: 0.6846, R¬≤: 0.3220, RMSE: 0.8236\nEpoch 038, Train Loss: 0.6861, R¬≤: 0.3224, RMSE: 0.8233\nEpoch 039, Train Loss: 0.6847, R¬≤: 0.3238, RMSE: 0.8225\nEpoch 040, Train Loss: 0.6832, R¬≤: 0.3248, RMSE: 0.8219\nEpoch 041, Train Loss: 0.6842, R¬≤: 0.3252, RMSE: 0.8216\nEpoch 042, Train Loss: 0.6851, R¬≤: 0.3260, RMSE: 0.8211\nEpoch 043, Train Loss: 0.6835, R¬≤: 0.3271, RMSE: 0.8204\nEpoch 044, Train Loss: 0.6823, R¬≤: 0.3274, RMSE: 0.8203\nEpoch 045, Train Loss: 0.6816, R¬≤: 0.3276, RMSE: 0.8201\nEpoch 046, Train Loss: 0.6838, R¬≤: 0.3285, RMSE: 0.8196\nEpoch 047, Train Loss: 0.6779, R¬≤: 0.3292, RMSE: 0.8192\nEpoch 048, Train Loss: 0.6785, R¬≤: 0.3295, RMSE: 0.8190\nEpoch 049, Train Loss: 0.6799, R¬≤: 0.3305, RMSE: 0.8184\nEpoch 050, Train Loss: 0.6769, R¬≤: 0.3313, RMSE: 0.8178\nEpoch 051, Train Loss: 0.6778, R¬≤: 0.3319, RMSE: 0.8175\nEpoch 052, Train Loss: 0.6794, R¬≤: 0.3327, RMSE: 0.8170\nEpoch 053, Train Loss: 0.6767, R¬≤: 0.3335, RMSE: 0.8165\nEpoch 054, Train Loss: 0.6749, R¬≤: 0.3339, RMSE: 0.8163\nEpoch 055, Train Loss: 0.6775, R¬≤: 0.3344, RMSE: 0.8160\nEpoch 056, Train Loss: 0.6756, R¬≤: 0.3349, RMSE: 0.8156\nEpoch 057, Train Loss: 0.6752, R¬≤: 0.3351, RMSE: 0.8156\nEpoch 058, Train Loss: 0.6776, R¬≤: 0.3356, RMSE: 0.8152\nEpoch 059, Train Loss: 0.6721, R¬≤: 0.3363, RMSE: 0.8148\nEpoch 060, Train Loss: 0.6715, R¬≤: 0.3369, RMSE: 0.8145\nEpoch 061, Train Loss: 0.6724, R¬≤: 0.3376, RMSE: 0.8140\nEpoch 062, Train Loss: 0.6722, R¬≤: 0.3383, RMSE: 0.8136\nEpoch 063, Train Loss: 0.6723, R¬≤: 0.3388, RMSE: 0.8133\nEpoch 064, Train Loss: 0.6688, R¬≤: 0.3392, RMSE: 0.8130\nEpoch 065, Train Loss: 0.6710, R¬≤: 0.3398, RMSE: 0.8126\nEpoch 066, Train Loss: 0.6680, R¬≤: 0.3403, RMSE: 0.8124\nEpoch 067, Train Loss: 0.6679, R¬≤: 0.3408, RMSE: 0.8121\nEpoch 068, Train Loss: 0.6690, R¬≤: 0.3415, RMSE: 0.8116\nEpoch 069, Train Loss: 0.6677, R¬≤: 0.3422, RMSE: 0.8112\nEpoch 070, Train Loss: 0.6684, R¬≤: 0.3428, RMSE: 0.8108\nEpoch 071, Train Loss: 0.6670, R¬≤: 0.3435, RMSE: 0.8104\nEpoch 072, Train Loss: 0.6683, R¬≤: 0.3440, RMSE: 0.8101\nEpoch 073, Train Loss: 0.6664, R¬≤: 0.3445, RMSE: 0.8098\nEpoch 074, Train Loss: 0.6648, R¬≤: 0.3449, RMSE: 0.8095\nEpoch 075, Train Loss: 0.6638, R¬≤: 0.3453, RMSE: 0.8093\nEpoch 076, Train Loss: 0.6647, R¬≤: 0.3459, RMSE: 0.8089\nEpoch 077, Train Loss: 0.6641, R¬≤: 0.3464, RMSE: 0.8086\nEpoch 078, Train Loss: 0.6649, R¬≤: 0.3469, RMSE: 0.8083\nEpoch 079, Train Loss: 0.6641, R¬≤: 0.3474, RMSE: 0.8080\nEpoch 080, Train Loss: 0.6640, R¬≤: 0.3478, RMSE: 0.8077\nEpoch 081, Train Loss: 0.6619, R¬≤: 0.3484, RMSE: 0.8074\nEpoch 082, Train Loss: 0.6621, R¬≤: 0.3488, RMSE: 0.8071\nEpoch 083, Train Loss: 0.6603, R¬≤: 0.3493, RMSE: 0.8068\nEpoch 084, Train Loss: 0.6589, R¬≤: 0.3496, RMSE: 0.8066\nEpoch 085, Train Loss: 0.6587, R¬≤: 0.3500, RMSE: 0.8064\nEpoch 086, Train Loss: 0.6608, R¬≤: 0.3505, RMSE: 0.8061\nEpoch 087, Train Loss: 0.6605, R¬≤: 0.3510, RMSE: 0.8058\nEpoch 088, Train Loss: 0.6570, R¬≤: 0.3514, RMSE: 0.8055\nEpoch 089, Train Loss: 0.6573, R¬≤: 0.3520, RMSE: 0.8051\nEpoch 090, Train Loss: 0.6586, R¬≤: 0.3523, RMSE: 0.8049\nEpoch 091, Train Loss: 0.6594, R¬≤: 0.3527, RMSE: 0.8047\nEpoch 092, Train Loss: 0.6588, R¬≤: 0.3531, RMSE: 0.8044\nEpoch 093, Train Loss: 0.6604, R¬≤: 0.3535, RMSE: 0.8042\nEpoch 094, Train Loss: 0.6581, R¬≤: 0.3539, RMSE: 0.8039\nEpoch 095, Train Loss: 0.6582, R¬≤: 0.3542, RMSE: 0.8038\nEpoch 096, Train Loss: 0.6545, R¬≤: 0.3545, RMSE: 0.8035\nEpoch 097, Train Loss: 0.6559, R¬≤: 0.3550, RMSE: 0.8033\nEpoch 098, Train Loss: 0.6559, R¬≤: 0.3554, RMSE: 0.8030\nEpoch 099, Train Loss: 0.6550, R¬≤: 0.3558, RMSE: 0.8027\nEpoch 100, Train Loss: 0.6578, R¬≤: 0.3563, RMSE: 0.8025\n",
  "history_begin_time" : 1740440151094,
  "history_end_time" : 1740442473970,
  "history_notes" : "GAT : Epoch 100, Train Loss: 0.6578, R¬≤: 0.3563, RMSE: 0.8025",
  "history_process" : "kevt5u",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "uMz6vluGUm9f",
  "history_input" : "import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch_geometric.data import Data\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\nfrom datetime import datetime\nimport os\nfrom sklearn.metrics import r2_score, mean_absolute_error\n\n# Check scikit-learn version and use appropriate RMSE function\ntry:\n    from sklearn.metrics import root_mean_squared_error\n    use_new_rmse = True\nexcept ImportError:\n    from sklearn.metrics import mean_squared_error\n    use_new_rmse = False\n\n# Step 1: Define the GCN model\nclass GCN(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\n        super(GCN, self).__init__()\n        self.conv1 = GCNConv(input_dim, hidden_dim)\n        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n        self.conv4 = GCNConv(hidden_dim, hidden_dim)\n        self.conv5 = GCNConv(hidden_dim, hidden_dim)\n        self.conv6 = GCNConv(hidden_dim, hidden_dim)  # Added Layer\n        self.conv7 = GCNConv(hidden_dim, hidden_dim)  # Added Layer\n        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(p=0.3)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = self.dropout(x)\n        x = self.conv2(x, edge_index)\n        x = F.relu(x)\n        x = self.dropout(x)\n        x = self.conv3(x, edge_index)\n        x = F.relu(x)\n        x = self.dropout(x)\n        x = self.conv4(x, edge_index)\n        x = F.relu(x)\n        x = self.conv5(x, edge_index)\n        x = F.relu(x)\n        x = self.conv6(x, edge_index)\n        x = F.relu(x)\n        x = self.conv7(x, edge_index)\n        x = F.relu(x)\n        x = self.fc(x)\n        return x\n\n\n\nif __name__ == \"__main__\":\n    log_dir = \"/media/volume1/swe/data/training_logs\"\n    os.makedirs(log_dir, exist_ok=True)\n    log_file = os.path.join(log_dir, f\"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\")\n    \n    def log(message):\n        print(message)\n        with open(log_file, \"a\") as f:\n            f.write(message + \"\\n\")\n\n    log(\"Loading graph data...\")\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    log(f\"Using device: {device}\")\n\n    torch.serialization.add_safe_globals([Data])\n    graph_data = torch.load('/media/volume1/gat_training_data_west.pt').to(device)\n    log(f\"Graph Data Loaded: Nodes={graph_data.x.shape[0]}, Features={graph_data.num_features}, Edges={graph_data.edge_index.shape[1]}\")\n    \n    train_ratio = 0.8\n    num_train_samples = int(train_ratio * graph_data.x.shape[0])\n    train_mask = torch.zeros(graph_data.x.shape[0], dtype=torch.bool)\n    train_mask[:num_train_samples] = True\n    test_mask = ~train_mask\n    \n    log(\"Initializing model and optimizer...\")\n    model = GCN(input_dim=graph_data.num_features, hidden_dim=64, output_dim=1).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n    criterion = torch.nn.MSELoss()\n    \n    def train():\n        model.train()\n        optimizer.zero_grad()\n        output = model(graph_data)[train_mask]\n        loss = criterion(output.squeeze(), graph_data.y[train_mask])\n        loss.backward()\n        optimizer.step()\n        return loss.item()\n    \n    def test():\n        model.eval()\n        with torch.no_grad():\n            predictions = model(graph_data)[test_mask].squeeze().cpu().numpy()\n            actuals = graph_data.y[test_mask].cpu().numpy()\n            loss = criterion(torch.tensor(predictions), torch.tensor(actuals)).item()\n            r2 = r2_score(actuals, predictions)\n            mae = mean_absolute_error(actuals, predictions)\n\n            if use_new_rmse:\n                rmse = root_mean_squared_error(actuals, predictions)\n            else:\n                rmse = mean_squared_error(actuals, predictions, squared=False)\n\n        return loss, r2, rmse, mae\n    \n    num_epochs = 200\n    log(\"Starting training...\")\n    for epoch in range(1, num_epochs + 1):\n        train_loss = train()\n        test_loss, r2, rmse, mae = test()\n        if epoch % 10 == 0:\n            log(f\"Epoch {epoch:03d}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, R¬≤: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n    \n    model_path = '/media/volume1/gcn_trained_model.pth'\n    torch.save(model.state_dict(), model_path)\n    log(f\"Model training completed and saved as '{model_path}'\")\n",
  "history_output" : "Loading graph data...\nUsing device: cpu\n/home/geo2021/gw-workspace/uMz6vluGUm9f/model_creation_gnn.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  graph_data = torch.load('/media/volume1/gat_training_data_west.pt').to(device)\nGraph Data Loaded: Nodes=50000, Features=13, Edges=5178142\nInitializing model and optimizer...\nStarting training...\nEpoch 010, Train Loss: 223705.4531, Test Loss: 216389.6562, R¬≤: 0.0156, RMSE: 465.1771, MAE: 393.7020\nEpoch 020, Train Loss: 215378.2344, Test Loss: 208490.7812, R¬≤: 0.0515, RMSE: 456.6079, MAE: 394.5086\nEpoch 030, Train Loss: 214557.4844, Test Loss: 211161.6562, R¬≤: 0.0394, RMSE: 459.5233, MAE: 393.3806\nEpoch 040, Train Loss: 206464.1719, Test Loss: 206735.1562, R¬≤: 0.0595, RMSE: 454.6814, MAE: 389.7029\nEpoch 050, Train Loss: 202744.6875, Test Loss: 203363.2969, R¬≤: 0.0748, RMSE: 450.9582, MAE: 391.4355\nEpoch 060, Train Loss: 201113.7500, Test Loss: 201317.3125, R¬≤: 0.0841, RMSE: 448.6840, MAE: 394.6595\nEpoch 070, Train Loss: 199600.4531, Test Loss: 199700.1406, R¬≤: 0.0915, RMSE: 446.8782, MAE: 390.5392\nEpoch 080, Train Loss: 198193.5312, Test Loss: 198236.6719, R¬≤: 0.0982, RMSE: 445.2378, MAE: 389.6939\nEpoch 090, Train Loss: 197031.0312, Test Loss: 196788.2188, R¬≤: 0.1048, RMSE: 443.6082, MAE: 388.6191\nEpoch 100, Train Loss: 195403.7188, Test Loss: 195341.6406, R¬≤: 0.1113, RMSE: 441.9747, MAE: 387.6582\nEpoch 110, Train Loss: 194318.5156, Test Loss: 194099.9375, R¬≤: 0.1170, RMSE: 440.5677, MAE: 385.0990\nEpoch 120, Train Loss: 193640.2656, Test Loss: 193155.7500, R¬≤: 0.1213, RMSE: 439.4949, MAE: 383.7064\nEpoch 130, Train Loss: 192840.3750, Test Loss: 192473.1250, R¬≤: 0.1244, RMSE: 438.7176, MAE: 383.1279\nEpoch 140, Train Loss: 192865.0781, Test Loss: 191948.4688, R¬≤: 0.1268, RMSE: 438.1192, MAE: 380.7446\nEpoch 150, Train Loss: 191816.6562, Test Loss: 191515.9531, R¬≤: 0.1287, RMSE: 437.6254, MAE: 379.9248\nEpoch 160, Train Loss: 191437.8750, Test Loss: 191043.7969, R¬≤: 0.1309, RMSE: 437.0856, MAE: 380.3759\nEpoch 170, Train Loss: 190792.4844, Test Loss: 190753.8906, R¬≤: 0.1322, RMSE: 436.7538, MAE: 378.5055\nEpoch 180, Train Loss: 189708.3125, Test Loss: 190321.8438, R¬≤: 0.1342, RMSE: 436.2589, MAE: 377.9945\nEpoch 190, Train Loss: 189449.8438, Test Loss: 189597.8438, R¬≤: 0.1375, RMSE: 435.4283, MAE: 374.7476\nEpoch 200, Train Loss: 190031.0625, Test Loss: 189183.8438, R¬≤: 0.1393, RMSE: 434.9527, MAE: 373.9339\nModel training completed and saved as '/media/volume1/gcn_trained_model.pth'\n",
  "history_begin_time" : 1739668842109,
  "history_end_time" : 1739671651864,
  "history_notes" : "GCN : Train Loss: 154888.1094, Test Loss: 152728.9062, R¬≤: 0.3912, RMSE: 390.8055, MAE: 309.5089",
  "history_process" : "kevt5u",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "ieyY6Ahf2wM3",
  "history_input" : "import numpy as np\nimport torch\nfrom torch_geometric.data import Data\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\nfrom datetime import datetime\nimport os\nfrom sklearn.metrics import r2_score, mean_absolute_error\n\n# Check scikit-learn version and use appropriate RMSE function\ntry:\n    from sklearn.metrics import root_mean_squared_error\n    use_new_rmse = True\nexcept ImportError:\n    from sklearn.metrics import mean_squared_error\n    use_new_rmse = False\n\n# Step 1: Define the GCN model\nclass GCN(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(GCN, self).__init__()\n        self.conv1 = GCNConv(input_dim, hidden_dim)\n        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n        self.conv4 = GCNConv(hidden_dim, hidden_dim)\n        self.conv5 = GCNConv(hidden_dim, hidden_dim)\n        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = self.conv2(x, edge_index)\n        x = F.relu(x)\n        x = self.conv3(x, edge_index)\n        x = F.relu(x)\n        x = self.conv4(x, edge_index)\n        x = F.relu(x)\n        x = self.conv5(x, edge_index)\n        x = F.relu(x)\n        x = self.fc(x)\n        return x\n\n\nif __name__ == \"__main__\":\n    log_dir = \"/media/volume1/swe/data/training_logs\"\n    os.makedirs(log_dir, exist_ok=True)\n    log_file = os.path.join(log_dir, f\"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\")\n    \n    def log(message):\n        print(message)\n        with open(log_file, \"a\") as f:\n            f.write(message + \"\\n\")\n\n    log(\"Loading graph data...\")\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    log(f\"Using device: {device}\")\n\n    torch.serialization.add_safe_globals([Data])\n    graph_data = torch.load('/media/volume1/gat_training_data_west.pt').to(device)\n    log(f\"Graph Data Loaded: Nodes={graph_data.x.shape[0]}, Features={graph_data.num_features}, Edges={graph_data.edge_index.shape[1]}\")\n    \n    train_ratio = 0.8\n    num_train_samples = int(train_ratio * graph_data.x.shape[0])\n    train_mask = torch.zeros(graph_data.x.shape[0], dtype=torch.bool)\n    train_mask[:num_train_samples] = True\n    test_mask = ~train_mask\n    \n    log(\"Initializing model and optimizer...\")\n    model = GCN(input_dim=graph_data.num_features, hidden_dim=64, output_dim=1).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n    criterion = torch.nn.MSELoss()\n    \n    def train():\n        model.train()\n        optimizer.zero_grad()\n        output = model(graph_data)[train_mask]\n        loss = criterion(output.squeeze(), graph_data.y[train_mask])\n        loss.backward()\n        optimizer.step()\n        return loss.item()\n    \n    def test():\n        model.eval()\n        with torch.no_grad():\n            predictions = model(graph_data)[test_mask].squeeze().cpu().numpy()\n            actuals = graph_data.y[test_mask].cpu().numpy()\n            loss = criterion(torch.tensor(predictions), torch.tensor(actuals)).item()\n            r2 = r2_score(actuals, predictions)\n            mae = mean_absolute_error(actuals, predictions)\n\n            if use_new_rmse:\n                rmse = root_mean_squared_error(actuals, predictions)\n            else:\n                rmse = mean_squared_error(actuals, predictions, squared=False)\n\n        return loss, r2, rmse, mae\n    \n    num_epochs = 200\n    log(\"Starting training...\")\n    for epoch in range(1, num_epochs + 1):\n        train_loss = train()\n        test_loss, r2, rmse, mae = test()\n        if epoch % 10 == 0:\n            log(f\"Epoch {epoch:03d}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, R¬≤: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n    \n    model_path = '/media/volume1/gcn_trained_model.pth'\n    torch.save(model.state_dict(), model_path)\n    log(f\"Model training completed and saved as '{model_path}'\")\n",
  "history_output" : "Loading graph data...\nUsing device: cpu\n/home/geo2021/gw-workspace/ieyY6Ahf2wM3/model_creation_gnn.py:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  graph_data = torch.load('/media/volume1/gat_training_data_west.pt').to(device)\nGraph Data Loaded: Nodes=50000, Features=13, Edges=5178142\nInitializing model and optimizer...\nStarting training...\nEpoch 010, Train Loss: 245368.2656, Test Loss: 222106.9375, R¬≤: -0.0104, RMSE: 471.2822, MAE: 411.3687\nEpoch 020, Train Loss: 214483.2656, Test Loss: 210309.8750, R¬≤: 0.0432, RMSE: 458.5955, MAE: 380.6506\nEpoch 030, Train Loss: 205909.6875, Test Loss: 205599.8906, R¬≤: 0.0647, RMSE: 453.4312, MAE: 395.6110\nEpoch 040, Train Loss: 202459.3438, Test Loss: 202675.5625, R¬≤: 0.0780, RMSE: 450.1950, MAE: 394.9908\nEpoch 050, Train Loss: 200578.7812, Test Loss: 200846.2500, R¬≤: 0.0863, RMSE: 448.1587, MAE: 392.5598\nEpoch 060, Train Loss: 199171.9688, Test Loss: 199071.4062, R¬≤: 0.0944, RMSE: 446.1742, MAE: 395.9838\nEpoch 070, Train Loss: 197863.2969, Test Loss: 197654.7500, R¬≤: 0.1008, RMSE: 444.5838, MAE: 393.1126\nEpoch 080, Train Loss: 196365.5156, Test Loss: 195899.3438, R¬≤: 0.1088, RMSE: 442.6052, MAE: 392.3740\nEpoch 090, Train Loss: 194685.7500, Test Loss: 194062.5625, R¬≤: 0.1172, RMSE: 440.5253, MAE: 386.7523\nEpoch 100, Train Loss: 192983.2188, Test Loss: 192311.2500, R¬≤: 0.1251, RMSE: 438.5331, MAE: 381.7842\nEpoch 110, Train Loss: 191826.0781, Test Loss: 191278.3281, R¬≤: 0.1298, RMSE: 437.3538, MAE: 378.1846\nEpoch 120, Train Loss: 191157.4219, Test Loss: 190819.1562, R¬≤: 0.1319, RMSE: 436.8286, MAE: 377.5961\nEpoch 130, Train Loss: 190527.7188, Test Loss: 190086.6406, R¬≤: 0.1352, RMSE: 435.9893, MAE: 376.5156\nEpoch 140, Train Loss: 190017.1406, Test Loss: 189559.2500, R¬≤: 0.1376, RMSE: 435.3840, MAE: 375.6106\nEpoch 150, Train Loss: 189357.8750, Test Loss: 189019.0156, R¬≤: 0.1401, RMSE: 434.7632, MAE: 375.6817\nEpoch 160, Train Loss: 188968.7812, Test Loss: 188738.9688, R¬≤: 0.1414, RMSE: 434.4410, MAE: 372.6665\nEpoch 170, Train Loss: 188452.2031, Test Loss: 188487.1875, R¬≤: 0.1425, RMSE: 434.1511, MAE: 372.0967\nEpoch 180, Train Loss: 188467.0781, Test Loss: 188186.5312, R¬≤: 0.1439, RMSE: 433.8047, MAE: 374.4490\nEpoch 190, Train Loss: 187837.5000, Test Loss: 187712.9375, R¬≤: 0.1460, RMSE: 433.2585, MAE: 373.0710\nEpoch 200, Train Loss: 187775.0000, Test Loss: 187610.3906, R¬≤: 0.1465, RMSE: 433.1401, MAE: 369.8427\nModel training completed and saved as '/media/volume1/gcn_trained_model.pth'\n",
  "history_begin_time" : 1739666266927,
  "history_end_time" : 1739668223164,
  "history_notes" : null,
  "history_process" : "kevt5u",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "icqElJO1icfJ",
  "history_input" : "import numpy as np\nimport torch\nfrom torch_geometric.data import Data\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\nfrom datetime import datetime\nimport os\nfrom sklearn.metrics import r2_score, mean_absolute_error\n\n# Check scikit-learn version and use appropriate RMSE function\ntry:\n    from sklearn.metrics import root_mean_squared_error\n    use_new_rmse = True\nexcept ImportError:\n    from sklearn.metrics import mean_squared_error\n    use_new_rmse = False\n\n# Step 1: Define the GCN model\nclass GCN(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(GCN, self).__init__()\n        self.conv1 = GCNConv(input_dim, hidden_dim)\n        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = self.conv2(x, edge_index)\n        x = F.relu(x)\n        x = self.fc(x)\n        return x\n\nif __name__ == \"__main__\":\n    log_dir = \"/media/volume1/swe/data/training_logs\"\n    os.makedirs(log_dir, exist_ok=True)\n    log_file = os.path.join(log_dir, f\"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\")\n    \n    def log(message):\n        print(message)\n        with open(log_file, \"a\") as f:\n            f.write(message + \"\\n\")\n\n    log(\"Loading graph data...\")\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    log(f\"Using device: {device}\")\n\n    torch.serialization.add_safe_globals([Data])\n    graph_data = torch.load('/media/volume1/gat_training_data_west.pt').to(device)\n    log(f\"Graph Data Loaded: Nodes={graph_data.x.shape[0]}, Features={graph_data.num_features}, Edges={graph_data.edge_index.shape[1]}\")\n    \n    train_ratio = 0.8\n    num_train_samples = int(train_ratio * graph_data.x.shape[0])\n    train_mask = torch.zeros(graph_data.x.shape[0], dtype=torch.bool)\n    train_mask[:num_train_samples] = True\n    test_mask = ~train_mask\n    \n    log(\"Initializing model and optimizer...\")\n    model = GCN(input_dim=graph_data.num_features, hidden_dim=64, output_dim=1).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n    criterion = torch.nn.MSELoss()\n    \n    def train():\n        model.train()\n        optimizer.zero_grad()\n        output = model(graph_data)[train_mask]\n        loss = criterion(output.squeeze(), graph_data.y[train_mask])\n        loss.backward()\n        optimizer.step()\n        return loss.item()\n    \n    def test():\n        model.eval()\n        with torch.no_grad():\n            predictions = model(graph_data)[test_mask].squeeze().cpu().numpy()\n            actuals = graph_data.y[test_mask].cpu().numpy()\n            loss = criterion(torch.tensor(predictions), torch.tensor(actuals)).item()\n            r2 = r2_score(actuals, predictions)\n            mae = mean_absolute_error(actuals, predictions)\n\n            if use_new_rmse:\n                rmse = root_mean_squared_error(actuals, predictions)\n            else:\n                rmse = mean_squared_error(actuals, predictions, squared=False)\n\n        return loss, r2, rmse, mae\n    \n    num_epochs = 200\n    log(\"Starting training...\")\n    for epoch in range(1, num_epochs + 1):\n        train_loss = train()\n        test_loss, r2, rmse, mae = test()\n        if epoch % 10 == 0:\n            log(f\"Epoch {epoch:03d}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, R¬≤: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n    \n    model_path = '/media/volume1/gcn_trained_model.pth'\n    torch.save(model.state_dict(), model_path)\n    log(f\"Model training completed and saved as '{model_path}'\")\n",
  "history_output" : "Loading graph data...\nUsing device: cpu\n/home/geo2021/gw-workspace/icqElJO1icfJ/model_creation_gnn.py:50: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  graph_data = torch.load('/media/volume1/gat_training_data_west.pt').to(device)\nGraph Data Loaded: Nodes=50000, Features=13, Edges=5178142\nInitializing model and optimizer...\nStarting training...\nEpoch 010, Train Loss: 318890.3438, Test Loss: 319057.9375, R¬≤: -0.4515, RMSE: 564.8522, MAE: 328.9457\nEpoch 020, Train Loss: 292066.3125, Test Loss: 288649.1875, R¬≤: -0.3131, RMSE: 537.2608, MAE: 342.4662\nEpoch 030, Train Loss: 231366.5000, Test Loss: 226596.8125, R¬≤: -0.0309, RMSE: 476.0219, MAE: 382.6992\nEpoch 040, Train Loss: 213768.7812, Test Loss: 214720.9062, R¬≤: 0.0232, RMSE: 463.3799, MAE: 418.5788\nEpoch 050, Train Loss: 205440.7656, Test Loss: 205749.6406, R¬≤: 0.0640, RMSE: 453.5963, MAE: 392.6719\nEpoch 060, Train Loss: 204039.2031, Test Loss: 204176.5625, R¬≤: 0.0711, RMSE: 451.8590, MAE: 390.8355\nEpoch 070, Train Loss: 201405.8750, Test Loss: 201486.6250, R¬≤: 0.0834, RMSE: 448.8726, MAE: 402.3347\nEpoch 080, Train Loss: 200098.2344, Test Loss: 200023.9531, R¬≤: 0.0900, RMSE: 447.2404, MAE: 398.6609\nEpoch 090, Train Loss: 199229.7188, Test Loss: 199062.2656, R¬≤: 0.0944, RMSE: 446.1639, MAE: 394.9494\nEpoch 100, Train Loss: 198481.9688, Test Loss: 198215.5469, R¬≤: 0.0983, RMSE: 445.2141, MAE: 396.3066\nEpoch 110, Train Loss: 197883.9062, Test Loss: 197630.8594, R¬≤: 0.1009, RMSE: 444.5569, MAE: 395.3348\nEpoch 120, Train Loss: 197387.6406, Test Loss: 197056.0469, R¬≤: 0.1035, RMSE: 443.9099, MAE: 394.2485\nEpoch 130, Train Loss: 196972.7500, Test Loss: 196638.2344, R¬≤: 0.1054, RMSE: 443.4391, MAE: 394.0955\nEpoch 140, Train Loss: 196617.3125, Test Loss: 196265.1719, R¬≤: 0.1071, RMSE: 443.0182, MAE: 393.6359\nEpoch 150, Train Loss: 196301.8438, Test Loss: 195945.9062, R¬≤: 0.1086, RMSE: 442.6578, MAE: 393.2332\nEpoch 160, Train Loss: 196016.6094, Test Loss: 195675.1875, R¬≤: 0.1098, RMSE: 442.3519, MAE: 392.9099\nEpoch 170, Train Loss: 195761.6406, Test Loss: 195433.0000, R¬≤: 0.1109, RMSE: 442.0781, MAE: 392.5637\nEpoch 180, Train Loss: 195529.5000, Test Loss: 195227.2969, R¬≤: 0.1119, RMSE: 441.8453, MAE: 392.2134\nEpoch 190, Train Loss: 195322.1094, Test Loss: 195045.9062, R¬≤: 0.1127, RMSE: 441.6400, MAE: 391.8600\nEpoch 200, Train Loss: 195138.0938, Test Loss: 194889.9531, R¬≤: 0.1134, RMSE: 441.4634, MAE: 391.5496\nModel training completed and saved as '/media/volume1/gcn_trained_model.pth'\n",
  "history_begin_time" : 1739665041840,
  "history_end_time" : 1739665841161,
  "history_notes" : null,
  "history_process" : "kevt5u",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "DTuV2CepnWQE",
  "history_input" : "import numpy as np\nimport torch\nfrom torch_geometric.data import Data\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\nfrom datetime import datetime\nimport os\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n\n# Step 1: Define the GCN model\nclass GCN(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(GCN, self).__init__()\n        self.conv1 = GCNConv(input_dim, hidden_dim)\n        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = self.conv2(x, edge_index)\n        x = F.relu(x)\n        x = self.fc(x)\n        return x\n\nif __name__ == \"__main__\":\n    log_dir = \"/media/volume1/swe/data/training_logs\"\n    os.makedirs(log_dir, exist_ok=True)\n    log_file = os.path.join(log_dir, f\"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\")\n    \n    def log(message):\n        print(message)\n        with open(log_file, \"a\") as f:\n            f.write(message + \"\\n\")\n\n    log(\"Loading graph data...\")\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    log(f\"Using device: {device}\")\n\n    torch.serialization.add_safe_globals([Data])\n    graph_data = torch.load('/media/volume1/gat_training_data_west.pt').to(device)\n    log(f\"Graph Data Loaded: Nodes={graph_data.x.shape[0]}, Features={graph_data.num_features}, Edges={graph_data.edge_index.shape[1]}\")\n    \n    train_ratio = 0.8\n    num_train_samples = int(train_ratio * graph_data.x.shape[0])\n    train_mask = torch.zeros(graph_data.x.shape[0], dtype=torch.bool)\n    train_mask[:num_train_samples] = True\n    test_mask = ~train_mask\n    \n    log(\"Initializing model and optimizer...\")\n    model = GCN(input_dim=graph_data.num_features, hidden_dim=64, output_dim=1).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n    criterion = torch.nn.MSELoss()\n    \n    def train():\n        model.train()\n        optimizer.zero_grad()\n        output = model(graph_data)[train_mask]\n        loss = criterion(output.squeeze(), graph_data.y[train_mask])\n        loss.backward()\n        optimizer.step()\n        return loss.item()\n    \n    def test():\n        model.eval()\n        with torch.no_grad():\n            predictions = model(graph_data)[test_mask].squeeze().cpu().numpy()\n            actuals = graph_data.y[test_mask].cpu().numpy()\n            loss = criterion(torch.tensor(predictions), torch.tensor(actuals)).item()\n            r2 = r2_score(actuals, predictions)\n            rmse = mean_squared_error(actuals, predictions, squared=False)\n            mae = mean_absolute_error(actuals, predictions)\n        return loss, r2, rmse, mae\n    \n    num_epochs = 200\n    log(\"Starting training...\")\n    for epoch in range(1, num_epochs + 1):\n        train_loss = train()\n        test_loss, r2, rmse, mae = test()\n        if epoch % 10 == 0:\n            log(f\"Epoch {epoch:03d}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, R¬≤: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n    \n    model_path = '/media/volume1/gcn_trained_model.pth'\n    torch.save(model.state_dict(), model_path)\n    log(f\"Model training completed and saved as '{model_path}'\")\n",
  "history_output" : "Loading graph data...\nUsing device: cpu\n/home/geo2021/gw-workspace/DTuV2CepnWQE/model_creation_gnn.py:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  graph_data = torch.load('/media/volume1/gat_training_data_west.pt').to(device)\nGraph Data Loaded: Nodes=10000, Features=13, Edges=28306\nInitializing model and optimizer...\nStarting training...\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\nEpoch 010, Train Loss: 318959.4062, Test Loss: 312426.2500, R¬≤: -0.4335, RMSE: 558.9510, MAE: 323.6437\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\nEpoch 020, Train Loss: 285096.3750, Test Loss: 275122.2500, R¬≤: -0.2623, RMSE: 524.5210, MAE: 333.6313\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\nEpoch 030, Train Loss: 208794.6562, Test Loss: 198734.2500, R¬≤: 0.0882, RMSE: 445.7962, MAE: 359.5768\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\nEpoch 040, Train Loss: 193405.8281, Test Loss: 187622.8594, R¬≤: 0.1391, RMSE: 433.1546, MAE: 380.6952\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\nEpoch 050, Train Loss: 180343.9062, Test Loss: 176914.4375, R¬≤: 0.1883, RMSE: 420.6120, MAE: 358.7565\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\nEpoch 060, Train Loss: 179320.1250, Test Loss: 175446.5625, R¬≤: 0.1950, RMSE: 418.8634, MAE: 354.1283\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\nEpoch 070, Train Loss: 177234.3750, Test Loss: 172876.6406, R¬≤: 0.2068, RMSE: 415.7844, MAE: 355.0541\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\nEpoch 080, Train Loss: 175840.6719, Test Loss: 172156.5312, R¬≤: 0.2101, RMSE: 414.9175, MAE: 350.2084\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\nEpoch 090, Train Loss: 175029.5000, Test Loss: 171560.3594, R¬≤: 0.2128, RMSE: 414.1985, MAE: 348.3898\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\nEpoch 100, Train Loss: 174286.0781, Test Loss: 170869.0625, R¬≤: 0.2160, RMSE: 413.3631, MAE: 348.1160\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\nEpoch 110, Train Loss: 173627.9375, Test Loss: 170522.2500, R¬≤: 0.2176, RMSE: 412.9434, MAE: 346.5309\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\nEpoch 120, Train Loss: 173088.0469, Test Loss: 170066.2969, R¬≤: 0.2197, RMSE: 412.3910, MAE: 345.6497\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\nEpoch 130, Train Loss: 172625.6250, Test Loss: 169749.5156, R¬≤: 0.2211, RMSE: 412.0067, MAE: 344.7440\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\nEpoch 140, Train Loss: 172222.1094, Test Loss: 169466.2656, R¬≤: 0.2224, RMSE: 411.6628, MAE: 343.7505\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\nEpoch 150, Train Loss: 171862.5156, Test Loss: 169192.0625, R¬≤: 0.2237, RMSE: 411.3296, MAE: 343.0844\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\nEpoch 160, Train Loss: 171537.9688, Test Loss: 168959.0625, R¬≤: 0.2248, RMSE: 411.0463, MAE: 342.4318\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\nEpoch 170, Train Loss: 171237.1875, Test Loss: 168744.1250, R¬≤: 0.2258, RMSE: 410.7848, MAE: 341.8473\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\nEpoch 180, Train Loss: 170953.1875, Test Loss: 168544.5156, R¬≤: 0.2267, RMSE: 410.5417, MAE: 341.3200\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\nEpoch 190, Train Loss: 170683.0156, Test Loss: 168325.6562, R¬≤: 0.2277, RMSE: 410.2751, MAE: 340.7393\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\nEpoch 200, Train Loss: 170424.7969, Test Loss: 168130.8906, R¬≤: 0.2286, RMSE: 410.0377, MAE: 340.3391\nModel training completed and saved as '/media/volume1/gcn_trained_model.pth'\n",
  "history_begin_time" : 1739664314250,
  "history_end_time" : 1739664327512,
  "history_notes" : null,
  "history_process" : "kevt5u",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "bAYo0S4eRNfA",
  "history_input" : "import numpy as np\nimport torch\nfrom torch_geometric.data import Data\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\nfrom datetime import datetime\nimport os\n\n# Step 1: Define the GCN model\nclass GCN(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(GCN, self).__init__()\n        self.conv1 = GCNConv(input_dim, hidden_dim)\n        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = self.conv2(x, edge_index)\n        x = F.relu(x)\n        x = self.fc(x)\n        return x\n\nif __name__ == \"__main__\":\n    log_dir = \"/media/volume1/swe/data/training_logs\"\n    os.makedirs(log_dir, exist_ok=True)\n    log_file = os.path.join(log_dir, f\"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\")\n    \n    def log(message):\n        print(message)\n        with open(log_file, \"a\") as f:\n            f.write(message + \"\\n\")\n\n    log(\"Loading graph data...\")\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    log(f\"Using device: {device}\")\n\n    torch.serialization.add_safe_globals([Data])\n    graph_data = torch.load('/media/volume1/gat_training_data_west.pt').to(device)\n    log(f\"Graph Data Loaded: Nodes={graph_data.x.shape[0]}, Features={graph_data.num_features}, Edges={graph_data.edge_index.shape[1]}\")\n    \n    train_ratio = 0.8\n    num_train_samples = int(train_ratio * graph_data.x.shape[0])\n    train_mask = torch.zeros(graph_data.x.shape[0], dtype=torch.bool)\n    train_mask[:num_train_samples] = True\n    test_mask = ~train_mask\n    \n    log(\"Initializing model and optimizer...\")\n    model = GCN(input_dim=graph_data.num_features, hidden_dim=64, output_dim=1).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n    criterion = torch.nn.MSELoss()\n    \n    def train():\n        model.train()\n        optimizer.zero_grad()\n        output = model(graph_data)[train_mask]\n        loss = criterion(output.squeeze(), graph_data.y[train_mask])\n        loss.backward()\n        optimizer.step()\n        return loss.item()\n    \n    def test():\n        model.eval()\n        with torch.no_grad():\n            predictions = model(graph_data)[test_mask].squeeze()\n            loss = criterion(predictions, graph_data.y[test_mask])\n        return loss.item()\n    \n    num_epochs = 200\n    log(\"Starting training...\")\n    for epoch in range(1, num_epochs + 1):\n        train_loss = train()\n        test_loss = test()\n        if epoch % 10 == 0:\n            log(f\"Epoch {epoch:03d}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n    \n    model_path = '/media/volume1/gcn_trained_model.pth'\n    torch.save(model.state_dict(), model_path)\n    log(f\"Model training completed and saved as '{model_path}'\")\n",
  "history_output" : "Loading graph data...\nUsing device: cpu\n/home/geo2021/gw-workspace/bAYo0S4eRNfA/model_creation_gnn.py:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  graph_data = torch.load('/media/volume1/gat_training_data_west.pt').to(device)\nGraph Data Loaded: Nodes=10000, Features=13, Edges=28306\nInitializing model and optimizer...\nStarting training...\nEpoch 010, Train Loss: 316088.6250, Test Loss: 308756.3438\nEpoch 020, Train Loss: 269476.7812, Test Loss: 257602.0625\nEpoch 030, Train Loss: 194690.7812, Test Loss: 187282.5000\nEpoch 040, Train Loss: 191150.1250, Test Loss: 183786.8125\nEpoch 050, Train Loss: 183116.5469, Test Loss: 180143.6562\nEpoch 060, Train Loss: 178706.6562, Test Loss: 174644.9844\nEpoch 070, Train Loss: 177418.0312, Test Loss: 172895.4219\nEpoch 080, Train Loss: 175978.1562, Test Loss: 172191.4531\nEpoch 090, Train Loss: 174899.2656, Test Loss: 171047.6562\nEpoch 100, Train Loss: 174108.2031, Test Loss: 170621.5312\nEpoch 110, Train Loss: 173471.9062, Test Loss: 170220.1250\nEpoch 120, Train Loss: 172931.3594, Test Loss: 169824.0781\nEpoch 130, Train Loss: 172477.0938, Test Loss: 169563.5312\nEpoch 140, Train Loss: 172109.2031, Test Loss: 169305.2656\nEpoch 150, Train Loss: 171796.5312, Test Loss: 169132.9688\nEpoch 160, Train Loss: 171527.7812, Test Loss: 168974.0625\nEpoch 170, Train Loss: 171289.1250, Test Loss: 168839.7812\nEpoch 180, Train Loss: 171063.6094, Test Loss: 168705.6250\nEpoch 190, Train Loss: 170848.0312, Test Loss: 168569.6250\nEpoch 200, Train Loss: 170636.2812, Test Loss: 168428.3594\nModel training completed and saved as '/media/volume1/gcn_trained_model.pth'\n",
  "history_begin_time" : 1739663957894,
  "history_end_time" : 1739663970230,
  "history_notes" : null,
  "history_process" : "kevt5u",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "m74iA0JGgIpL",
  "history_input" : "# Write your first Python code in Geoweaver \n\nimport numpy as np\nimport torch\nfrom torch_geometric.data import Data\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\nfrom torch_geometric.data import DataLoader\n\n# Step 1: Define the GCN model\nclass GCN(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(GCN, self).__init__()\n        self.conv1 = GCNConv(input_dim, hidden_dim)\n        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n        self.fc = torch.nn.Linear(hidden_dim, output_dim)  # Fully connected layer\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = self.conv2(x, edge_index)\n        x = F.relu(x)\n\n        # Ensure output shape matches the number of nodes\n        x = self.fc(x)  # No global pooling to keep per-node output\n        return x  # Output shape: [num_nodes, output_dim]\n\n\n\n# Step 2: Load the graph data and create a DataLoader\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Allowlist the Data class for safe loading\ntorch.serialization.add_safe_globals([Data])\n\n# Load the graph data\ngraph_data = torch.load('/media/volume1/gat_training_data_sampled.pt').to(device)\n\n\n\n\n# Split into train and test sets\ntrain_ratio = 0.8  # Use 80% of data for training\nnum_train_samples = int(train_ratio * graph_data.x.shape[0])\ntrain_mask = torch.zeros(graph_data.x.shape[0], dtype=torch.bool)\ntrain_mask[:num_train_samples] = True\ntest_mask = ~train_mask\n\n# Step 3: Define model, optimizer, and loss function\nmodel = GCN(input_dim=graph_data.num_features, hidden_dim=64, output_dim=1).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\ncriterion = torch.nn.MSELoss()\n\n# Step 4: Training loop\ndef train():\n    model.train()\n    optimizer.zero_grad()\n    output = model(graph_data)[train_mask]  # Only consider training samples\n    loss = criterion(output.squeeze(), graph_data.y[train_mask])  # Supervised loss\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n\n# Step 5: Evaluation\ndef test():\n    model.eval()\n    with torch.no_grad():\n        predictions = model(graph_data)[test_mask].squeeze()\n        loss = criterion(predictions, graph_data.y[test_mask])\n        return loss.item()\n\n# Step 6: Training the model\nnum_epochs = 200\nfor epoch in range(1, num_epochs + 1):\n    train_loss = train()\n    test_loss = test()\n    if epoch % 10 == 0:\n        print(f'Epoch {epoch:03d}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')\n\n# Save the trained model\ntorch.save(model.state_dict(), '/media/volume1/gcn_trained_model.pth')\nprint(\"Model training completed and saved as 'gcn_trained_model.pth'.\")\n",
  "history_output" : "Using device: cpu\n/home/geo2021/gw-workspace/m74iA0JGgIpL/model_creation_gnn.py:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  graph_data = torch.load('/media/volume1/gat_training_data_sampled.pt').to(device)\nEpoch 010, Train Loss: 309357.6562, Test Loss: 306240.2500\nEpoch 020, Train Loss: 251966.5156, Test Loss: 243493.2812\nEpoch 030, Train Loss: 223745.8750, Test Loss: 228933.0625\nEpoch 040, Train Loss: 213338.4688, Test Loss: 213412.0625\nEpoch 050, Train Loss: 212534.9844, Test Loss: 211981.7188\n",
  "history_begin_time" : 1739322580454,
  "history_end_time" : 1739375580900,
  "history_notes" : null,
  "history_process" : "kevt5u",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "7A7EVu9Vn58K",
  "history_input" : "# Write your first Python code in Geoweaver \n\nimport numpy as np\nimport torch\nfrom torch_geometric.data import Data\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\nfrom torch_geometric.data import DataLoader\n\n# Step 1: Define the GCN model\nclass GCN(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(GCN, self).__init__()\n        self.conv1 = GCNConv(input_dim, hidden_dim)\n        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n        self.fc = torch.nn.Linear(hidden_dim, output_dim)  # Fully connected layer\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = self.conv2(x, edge_index)\n        x = F.relu(x)\n\n        # Fix: Use global mean pooling correctly\n        x = torch.mean(x, dim=0)  # Global mean pooling over nodes (dim=0 instead of dim=1)\n        x = x.view(1, -1)  # Reshape to match `fc` input requirements\n\n        x = self.fc(x)  # Fully connected layer\n        return x\n\n\n# Step 2: Load the graph data and create a DataLoader\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Allowlist the Data class for safe loading\ntorch.serialization.add_safe_globals([Data])\n\n# Load the graph data\ngraph_data = torch.load('/media/volume1/gat_training_data_sampled.pt').to(device)\n\n\n\n\n# Split into train and test sets\ntrain_ratio = 0.8  # Use 80% of data for training\nnum_train_samples = int(train_ratio * graph_data.x.shape[0])\ntrain_mask = torch.zeros(graph_data.x.shape[0], dtype=torch.bool)\ntrain_mask[:num_train_samples] = True\ntest_mask = ~train_mask\n\n# Step 3: Define model, optimizer, and loss function\nmodel = GCN(input_dim=graph_data.num_features, hidden_dim=64, output_dim=1).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\ncriterion = torch.nn.MSELoss()\n\n# Step 4: Training loop\ndef train():\n    model.train()\n    optimizer.zero_grad()\n    output = model(graph_data)[train_mask]  # Only consider training samples\n    loss = criterion(output.squeeze(), graph_data.y[train_mask])  # Supervised loss\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n\n# Step 5: Evaluation\ndef test():\n    model.eval()\n    with torch.no_grad():\n        predictions = model(graph_data)[test_mask].squeeze()\n        loss = criterion(predictions, graph_data.y[test_mask])\n        return loss.item()\n\n# Step 6: Training the model\nnum_epochs = 200\nfor epoch in range(1, num_epochs + 1):\n    train_loss = train()\n    test_loss = test()\n    if epoch % 10 == 0:\n        print(f'Epoch {epoch:03d}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')\n\n# Save the trained model\ntorch.save(model.state_dict(), '/media/volume1/gcn_trained_model.pth')\nprint(\"Model training completed and saved as 'gcn_trained_model.pth'.\")\n",
  "history_output" : "Using device: cpu\n/home/geo2021/gw-workspace/7A7EVu9Vn58K/model_creation_gnn.py:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  graph_data = torch.load('/media/volume1/gat_training_data_sampled.pt').to(device)\nTraceback (most recent call last):\n  File \"/home/geo2021/gw-workspace/7A7EVu9Vn58K/model_creation_gnn.py\", line 81, in <module>\n    train_loss = train()\n                 ^^^^^^^\n  File \"/home/geo2021/gw-workspace/7A7EVu9Vn58K/model_creation_gnn.py\", line 64, in train\n    output = model(graph_data)[train_mask]  # Only consider training samples\n             ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nIndexError: The shape of the mask [50000] at index 0 does not match the shape of the indexed tensor [1, 1] at index 0\n",
  "history_begin_time" : 1739321874341,
  "history_end_time" : 1739322263180,
  "history_notes" : null,
  "history_process" : "kevt5u",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "Vm9rYJzXP4gk",
  "history_input" : "# Write your first Python code in Geoweaver \n\nimport numpy as np\nimport torch\nfrom torch_geometric.data import Data\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\nfrom torch_geometric.data import DataLoader\n\n# Step 1: Define the GCN model\nclass GCN(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(GCN, self).__init__()\n        self.conv1 = GCNConv(input_dim, hidden_dim)\n        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n        self.fc = torch.nn.Linear(hidden_dim, output_dim)  # Fully connected layer\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = self.conv2(x, edge_index)\n        x = F.relu(x)\n        x = torch.mean(x, dim=1)  # Global pooling (if needed)\n        x = self.fc(x)\n        return x\n\n# Step 2: Load the graph data and create a DataLoader\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Allowlist the Data class for safe loading\ntorch.serialization.add_safe_globals([Data])\n\n# Load the graph data\ngraph_data = torch.load('/media/volume1/gat_training_data_sampled.pt').to(device)\n\n\n\n\n# Split into train and test sets\ntrain_ratio = 0.8  # Use 80% of data for training\nnum_train_samples = int(train_ratio * graph_data.x.shape[0])\ntrain_mask = torch.zeros(graph_data.x.shape[0], dtype=torch.bool)\ntrain_mask[:num_train_samples] = True\ntest_mask = ~train_mask\n\n# Step 3: Define model, optimizer, and loss function\nmodel = GCN(input_dim=graph_data.num_features, hidden_dim=64, output_dim=1).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\ncriterion = torch.nn.MSELoss()\n\n# Step 4: Training loop\ndef train():\n    model.train()\n    optimizer.zero_grad()\n    output = model(graph_data)[train_mask]  # Only consider training samples\n    loss = criterion(output.squeeze(), graph_data.y[train_mask])  # Supervised loss\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n\n# Step 5: Evaluation\ndef test():\n    model.eval()\n    with torch.no_grad():\n        predictions = model(graph_data)[test_mask].squeeze()\n        loss = criterion(predictions, graph_data.y[test_mask])\n        return loss.item()\n\n# Step 6: Training the model\nnum_epochs = 200\nfor epoch in range(1, num_epochs + 1):\n    train_loss = train()\n    test_loss = test()\n    if epoch % 10 == 0:\n        print(f'Epoch {epoch:03d}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')\n\n# Save the trained model\ntorch.save(model.state_dict(), '/media/volume1/gcn_trained_model.pth')\nprint(\"Model training completed and saved as 'gcn_trained_model.pth'.\")\n",
  "history_output" : "Using device: cpu\n/home/geo2021/gw-workspace/Vm9rYJzXP4gk/model_creation_gnn.py:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  graph_data = torch.load('/media/volume1/gat_training_data_sampled.pt').to(device)\nTraceback (most recent call last):\n  File \"/home/geo2021/gw-workspace/Vm9rYJzXP4gk/model_creation_gnn.py\", line 76, in <module>\n    train_loss = train()\n                 ^^^^^^^\n  File \"/home/geo2021/gw-workspace/Vm9rYJzXP4gk/model_creation_gnn.py\", line 59, in train\n    output = model(graph_data)[train_mask]  # Only consider training samples\n             ^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/gw-workspace/Vm9rYJzXP4gk/model_creation_gnn.py\", line 25, in forward\n    x = self.fc(x)\n        ^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (1x50000 and 64x1)\n",
  "history_begin_time" : 1739317615071,
  "history_end_time" : 1739319432509,
  "history_notes" : null,
  "history_process" : "kevt5u",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "y1qsjdgycnw",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1740620047950,
  "history_notes" : null,
  "history_process" : "kevt5u",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "pf27dteslqx",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1740622683644,
  "history_notes" : null,
  "history_process" : "kevt5u",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "9cr9py6k631",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1741409321063,
  "history_notes" : null,
  "history_process" : "kevt5u",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "d415l6z06x3",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1741409322373,
  "history_notes" : null,
  "history_process" : "kevt5u",
  "host_id" : "100001",
  "indicator" : "Stopped"
}]