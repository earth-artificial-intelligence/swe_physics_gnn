[{
  "history_id" : "np0u9mxy9nd",
  "history_input" : "import numpy as np\nimport torch\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nimport pandas as pd\nimport os\nfrom datetime import datetime\n\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\n\nprint(\"Loading dataset...\")\ndata = pd.read_csv(file_path)\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\n\n# Step 1: Filter data for the western USA region\nlat_min, lat_max = 32.0, 49.0  # Covers western states\nlon_min, lon_max = -125.0, -102.0  # Covers western states\nfiltered_data = data[(data['lat'] >= lat_min) & (data['lat'] <= lat_max) &\n                     (data['lon'] >= lon_min) & (data['lon'] <= lon_max)]\nprint(f\"Filtered dataset to western USA region with {filtered_data.shape[0]} rows.\")\n\n# Step 2: Sample fewer data points\nsample_size = 50000  \nsampled_data = filtered_data.sample(n=sample_size, random_state=42).reset_index(drop=True)\nprint(f\"Sampled dataset with {sampled_data.shape[0]} rows.\")\n\n# Step 3: Normalize features\nselected_features = [\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount'\n]\n\nprint(\"Normalizing features...\")\nscaler = StandardScaler()\nsampled_data[selected_features] = scaler.fit_transform(sampled_data[selected_features])\n\noutput_file_path = 'normalized_sampled_snotel_west.csv'\nsampled_data.to_csv(output_file_path, index=False)\nprint(\"Normalized sampled dataset saved as 'normalized_sampled_snotel_west.csv'.\")\n\n# Step 4: Create node features tensor\nnode_features = torch.tensor(sampled_data[selected_features].values, dtype=torch.float)\nprint(f\"Node features tensor created with shape: {node_features.shape}\")\n\n# Step 5: Create labels tensor\nlabels = torch.tensor(sampled_data['swe_value'].values, dtype=torch.float)\nprint(f\"Labels tensor created with shape: {labels.shape}\")\n\n# Step 6: Build KDTree and construct fewer edges\ncoordinates = sampled_data[['lat', 'lon']].values\nthreshold = 0.05  # Adjusted threshold to limit edges\n\nprint(\"Building KDTree and constructing edges...\")\ntree = KDTree(coordinates)\ntedges = list(tree.query_pairs(r=threshold))\n\n# Ensure bidirectional edges\nedges = np.array(tedges)\nedges = np.vstack((edges, edges[:, ::-1]))  # Add reversed edges\nedge_index = torch.tensor(edges.T, dtype=torch.long)\nprint(f\"Edge index tensor created with shape: {edge_index.shape}\")\n\n# Step 7: Create PyTorch Geometric graph data object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# Save the PyTorch Geometric Data object\ngraph_data_output_path = '/media/volume1/gat_training_data_west.pt'\ntorch.save(graph_data, graph_data_output_path)\nprint(f\"PyTorch Geometric Data object saved as '{graph_data_output_path}'.\")\n",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 105 columns.\nFiltered dataset to western USA region with 5862359 rows.\nSampled dataset with 50000 rows.\nNormalizing features...\nNormalized sampled dataset saved as 'normalized_sampled_snotel_west.csv'.\nNode features tensor created with shape: torch.Size([50000, 13])\nLabels tensor created with shape: torch.Size([50000])\nBuilding KDTree and constructing edges...\nEdge index tensor created with shape: torch.Size([2, 5178142])\nPyTorch Geometric Data object saved as '/media/volume1/gat_training_data_west.pt'.\n",
  "history_begin_time" : 1741409322742,
  "history_end_time" : 1741409572120,
  "history_notes" : null,
  "history_process" : "ppl3n3",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "9ib3kklvchb",
  "history_input" : "import numpy as np\nimport torch\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nimport pandas as pd\nimport os\nfrom datetime import datetime\n\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\n\nprint(\"Loading dataset...\")\ndata = pd.read_csv(file_path)\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\n\n# Step 1: Filter data for the western USA region\nlat_min, lat_max = 32.0, 49.0  # Covers western states\nlon_min, lon_max = -125.0, -102.0  # Covers western states\nfiltered_data = data[(data['lat'] >= lat_min) & (data['lat'] <= lat_max) &\n                     (data['lon'] >= lon_min) & (data['lon'] <= lon_max)]\nprint(f\"Filtered dataset to western USA region with {filtered_data.shape[0]} rows.\")\n\n# Step 2: Sample fewer data points\nsample_size = 50000  \nsampled_data = filtered_data.sample(n=sample_size, random_state=42).reset_index(drop=True)\nprint(f\"Sampled dataset with {sampled_data.shape[0]} rows.\")\n\n# Step 3: Normalize features\nselected_features = [\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount'\n]\n\nprint(\"Normalizing features...\")\nscaler = StandardScaler()\nsampled_data[selected_features] = scaler.fit_transform(sampled_data[selected_features])\n\noutput_file_path = 'normalized_sampled_snotel_west.csv'\nsampled_data.to_csv(output_file_path, index=False)\nprint(\"Normalized sampled dataset saved as 'normalized_sampled_snotel_west.csv'.\")\n\n# Step 4: Create node features tensor\nnode_features = torch.tensor(sampled_data[selected_features].values, dtype=torch.float)\nprint(f\"Node features tensor created with shape: {node_features.shape}\")\n\n# Step 5: Create labels tensor\nlabels = torch.tensor(sampled_data['swe_value'].values, dtype=torch.float)\nprint(f\"Labels tensor created with shape: {labels.shape}\")\n\n# Step 6: Build KDTree and construct fewer edges\ncoordinates = sampled_data[['lat', 'lon']].values\nthreshold = 0.05  # Adjusted threshold to limit edges\n\nprint(\"Building KDTree and constructing edges...\")\ntree = KDTree(coordinates)\ntedges = list(tree.query_pairs(r=threshold))\n\n# Ensure bidirectional edges\nedges = np.array(tedges)\nedges = np.vstack((edges, edges[:, ::-1]))  # Add reversed edges\nedge_index = torch.tensor(edges.T, dtype=torch.long)\nprint(f\"Edge index tensor created with shape: {edge_index.shape}\")\n\n# Step 7: Create PyTorch Geometric graph data object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# Save the PyTorch Geometric Data object\ngraph_data_output_path = '/media/volume1/gat_training_data_west.pt'\ntorch.save(graph_data, graph_data_output_path)\nprint(f\"PyTorch Geometric Data object saved as '{graph_data_output_path}'.\")\n",
  "history_output" : "",
  "history_begin_time" : 1741409299843,
  "history_end_time" : 1741409321239,
  "history_notes" : null,
  "history_process" : "ppl3n3",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "karzov6pr3s",
  "history_input" : "import numpy as np\nimport torch\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nimport pandas as pd\nimport os\nfrom datetime import datetime\n\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\n\nprint(\"Loading dataset...\")\ndata = pd.read_csv(file_path)\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\n\n# Step 1: Filter data for the western USA region\nlat_min, lat_max = 32.0, 49.0  # Covers western states\nlon_min, lon_max = -125.0, -102.0  # Covers western states\nfiltered_data = data[(data['lat'] >= lat_min) & (data['lat'] <= lat_max) &\n                     (data['lon'] >= lon_min) & (data['lon'] <= lon_max)]\nprint(f\"Filtered dataset to western USA region with {filtered_data.shape[0]} rows.\")\n\n# Step 2: Sample fewer data points\nsample_size = 50000  \nsampled_data = filtered_data.sample(n=sample_size, random_state=42).reset_index(drop=True)\nprint(f\"Sampled dataset with {sampled_data.shape[0]} rows.\")\n\n# Step 3: Normalize features\nselected_features = [\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount'\n]\n\nprint(\"Normalizing features...\")\nscaler = StandardScaler()\nsampled_data[selected_features] = scaler.fit_transform(sampled_data[selected_features])\n\noutput_file_path = 'normalized_sampled_snotel_west.csv'\nsampled_data.to_csv(output_file_path, index=False)\nprint(\"Normalized sampled dataset saved as 'normalized_sampled_snotel_west.csv'.\")\n\n# Step 4: Create node features tensor\nnode_features = torch.tensor(sampled_data[selected_features].values, dtype=torch.float)\nprint(f\"Node features tensor created with shape: {node_features.shape}\")\n\n# Step 5: Create labels tensor\nlabels = torch.tensor(sampled_data['swe_value'].values, dtype=torch.float)\nprint(f\"Labels tensor created with shape: {labels.shape}\")\n\n# Step 6: Build KDTree and construct fewer edges\ncoordinates = sampled_data[['lat', 'lon']].values\nthreshold = 0.05  # Adjusted threshold to limit edges\n\nprint(\"Building KDTree and constructing edges...\")\ntree = KDTree(coordinates)\ntedges = list(tree.query_pairs(r=threshold))\n\n# Ensure bidirectional edges\nedges = np.array(tedges)\nedges = np.vstack((edges, edges[:, ::-1]))  # Add reversed edges\nedge_index = torch.tensor(edges.T, dtype=torch.long)\nprint(f\"Edge index tensor created with shape: {edge_index.shape}\")\n\n# Step 7: Create PyTorch Geometric graph data object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# Save the PyTorch Geometric Data object\ngraph_data_output_path = '/media/volume1/gat_training_data_west.pt'\ntorch.save(graph_data, graph_data_output_path)\nprint(f\"PyTorch Geometric Data object saved as '{graph_data_output_path}'.\")\n",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 105 columns.\nFiltered dataset to western USA region with 5862359 rows.\nSampled dataset with 50000 rows.\nNormalizing features...\nNormalized sampled dataset saved as 'normalized_sampled_snotel_west.csv'.\nNode features tensor created with shape: torch.Size([50000, 13])\nLabels tensor created with shape: torch.Size([50000])\nBuilding KDTree and constructing edges...\nEdge index tensor created with shape: torch.Size([2, 5178142])\nPyTorch Geometric Data object saved as '/media/volume1/gat_training_data_west.pt'.\n",
  "history_begin_time" : 1741212657991,
  "history_end_time" : 1741213036974,
  "history_notes" : null,
  "history_process" : "ppl3n3",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "ofvtzfze7e1",
  "history_input" : "import numpy as np\nimport torch\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nimport pandas as pd\nimport os\nfrom datetime import datetime\n\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\n\nprint(\"Loading dataset...\")\ndata = pd.read_csv(file_path)\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\n\n# Step 1: Filter data for the western USA region\nlat_min, lat_max = 32.0, 49.0  # Covers western states\nlon_min, lon_max = -125.0, -102.0  # Covers western states\nfiltered_data = data[(data['lat'] >= lat_min) & (data['lat'] <= lat_max) &\n                     (data['lon'] >= lon_min) & (data['lon'] <= lon_max)]\nprint(f\"Filtered dataset to western USA region with {filtered_data.shape[0]} rows.\")\n\n# Step 2: Sample fewer data points\nsample_size = 50000  \nsampled_data = filtered_data.sample(n=sample_size, random_state=42).reset_index(drop=True)\nprint(f\"Sampled dataset with {sampled_data.shape[0]} rows.\")\n\n# Step 3: Normalize features\nselected_features = [\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount'\n]\n\nprint(\"Normalizing features...\")\nscaler = StandardScaler()\nsampled_data[selected_features] = scaler.fit_transform(sampled_data[selected_features])\n\noutput_file_path = 'normalized_sampled_snotel_west.csv'\nsampled_data.to_csv(output_file_path, index=False)\nprint(\"Normalized sampled dataset saved as 'normalized_sampled_snotel_west.csv'.\")\n\n# Step 4: Create node features tensor\nnode_features = torch.tensor(sampled_data[selected_features].values, dtype=torch.float)\nprint(f\"Node features tensor created with shape: {node_features.shape}\")\n\n# Step 5: Create labels tensor\nlabels = torch.tensor(sampled_data['swe_value'].values, dtype=torch.float)\nprint(f\"Labels tensor created with shape: {labels.shape}\")\n\n# Step 6: Build KDTree and construct fewer edges\ncoordinates = sampled_data[['lat', 'lon']].values\nthreshold = 0.05  # Adjusted threshold to limit edges\n\nprint(\"Building KDTree and constructing edges...\")\ntree = KDTree(coordinates)\ntedges = list(tree.query_pairs(r=threshold))\n\n# Ensure bidirectional edges\nedges = np.array(tedges)\nedges = np.vstack((edges, edges[:, ::-1]))  # Add reversed edges\nedge_index = torch.tensor(edges.T, dtype=torch.long)\nprint(f\"Edge index tensor created with shape: {edge_index.shape}\")\n\n# Step 7: Create PyTorch Geometric graph data object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# Save the PyTorch Geometric Data object\ngraph_data_output_path = '/media/volume1/gat_training_data_west.pt'\ntorch.save(graph_data, graph_data_output_path)\nprint(f\"PyTorch Geometric Data object saved as '{graph_data_output_path}'.\")\n",
  "history_output" : "Cannot run program \"python\" (in directory \"/home/geo2021/gw-workspace/ofvtzfze7e1\"): error=2, No such file or directory",
  "history_begin_time" : 1740622682870,
  "history_end_time" : 1740622683432,
  "history_notes" : null,
  "history_process" : "ppl3n3",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "v6pg4xmn7lz",
  "history_input" : "import numpy as np\nimport torch\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nimport pandas as pd\nimport os\nfrom datetime import datetime\n\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\n\nprint(\"Loading dataset...\")\ndata = pd.read_csv(file_path)\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\n\n# Step 1: Filter data for the western USA region\nlat_min, lat_max = 32.0, 49.0  # Covers western states\nlon_min, lon_max = -125.0, -102.0  # Covers western states\nfiltered_data = data[(data['lat'] >= lat_min) & (data['lat'] <= lat_max) &\n                     (data['lon'] >= lon_min) & (data['lon'] <= lon_max)]\nprint(f\"Filtered dataset to western USA region with {filtered_data.shape[0]} rows.\")\n\n# Step 2: Sample fewer data points\nsample_size = 50000  \nsampled_data = filtered_data.sample(n=sample_size, random_state=42).reset_index(drop=True)\nprint(f\"Sampled dataset with {sampled_data.shape[0]} rows.\")\n\n# Step 3: Normalize features\nselected_features = [\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount'\n]\n\nprint(\"Normalizing features...\")\nscaler = StandardScaler()\nsampled_data[selected_features] = scaler.fit_transform(sampled_data[selected_features])\n\noutput_file_path = 'normalized_sampled_snotel_west.csv'\nsampled_data.to_csv(output_file_path, index=False)\nprint(\"Normalized sampled dataset saved as 'normalized_sampled_snotel_west.csv'.\")\n\n# Step 4: Create node features tensor\nnode_features = torch.tensor(sampled_data[selected_features].values, dtype=torch.float)\nprint(f\"Node features tensor created with shape: {node_features.shape}\")\n\n# Step 5: Create labels tensor\nlabels = torch.tensor(sampled_data['swe_value'].values, dtype=torch.float)\nprint(f\"Labels tensor created with shape: {labels.shape}\")\n\n# Step 6: Build KDTree and construct fewer edges\ncoordinates = sampled_data[['lat', 'lon']].values\nthreshold = 0.05  # Adjusted threshold to limit edges\n\nprint(\"Building KDTree and constructing edges...\")\ntree = KDTree(coordinates)\ntedges = list(tree.query_pairs(r=threshold))\n\n# Ensure bidirectional edges\nedges = np.array(tedges)\nedges = np.vstack((edges, edges[:, ::-1]))  # Add reversed edges\nedge_index = torch.tensor(edges.T, dtype=torch.long)\nprint(f\"Edge index tensor created with shape: {edge_index.shape}\")\n\n# Step 7: Create PyTorch Geometric graph data object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# Save the PyTorch Geometric Data object\ngraph_data_output_path = '/media/volume1/gat_training_data_west.pt'\ntorch.save(graph_data, graph_data_output_path)\nprint(f\"PyTorch Geometric Data object saved as '{graph_data_output_path}'.\")\n",
  "history_output" : "",
  "history_begin_time" : 1740620101701,
  "history_end_time" : 1740620103007,
  "history_notes" : null,
  "history_process" : "ppl3n3",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "loz52rebj9e",
  "history_input" : "import numpy as np\nimport torch\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nimport pandas as pd\nimport os\nfrom datetime import datetime\n\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\n\nprint(\"Loading dataset...\")\ndata = pd.read_csv(file_path)\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\n\n# Step 1: Filter data for the western USA region\nlat_min, lat_max = 32.0, 49.0  # Covers western states\nlon_min, lon_max = -125.0, -102.0  # Covers western states\nfiltered_data = data[(data['lat'] >= lat_min) & (data['lat'] <= lat_max) &\n                     (data['lon'] >= lon_min) & (data['lon'] <= lon_max)]\nprint(f\"Filtered dataset to western USA region with {filtered_data.shape[0]} rows.\")\n\n# Step 2: Sample fewer data points\nsample_size = 50000  \nsampled_data = filtered_data.sample(n=sample_size, random_state=42).reset_index(drop=True)\nprint(f\"Sampled dataset with {sampled_data.shape[0]} rows.\")\n\n# Step 3: Normalize features\nselected_features = [\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount'\n]\n\nprint(\"Normalizing features...\")\nscaler = StandardScaler()\nsampled_data[selected_features] = scaler.fit_transform(sampled_data[selected_features])\n\noutput_file_path = 'normalized_sampled_snotel_west.csv'\nsampled_data.to_csv(output_file_path, index=False)\nprint(\"Normalized sampled dataset saved as 'normalized_sampled_snotel_west.csv'.\")\n\n# Step 4: Create node features tensor\nnode_features = torch.tensor(sampled_data[selected_features].values, dtype=torch.float)\nprint(f\"Node features tensor created with shape: {node_features.shape}\")\n\n# Step 5: Create labels tensor\nlabels = torch.tensor(sampled_data['swe_value'].values, dtype=torch.float)\nprint(f\"Labels tensor created with shape: {labels.shape}\")\n\n# Step 6: Build KDTree and construct fewer edges\ncoordinates = sampled_data[['lat', 'lon']].values\nthreshold = 0.05  # Adjusted threshold to limit edges\n\nprint(\"Building KDTree and constructing edges...\")\ntree = KDTree(coordinates)\ntedges = list(tree.query_pairs(r=threshold))\n\n# Ensure bidirectional edges\nedges = np.array(tedges)\nedges = np.vstack((edges, edges[:, ::-1]))  # Add reversed edges\nedge_index = torch.tensor(edges.T, dtype=torch.long)\nprint(f\"Edge index tensor created with shape: {edge_index.shape}\")\n\n# Step 7: Create PyTorch Geometric graph data object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# Save the PyTorch Geometric Data object\ngraph_data_output_path = '/media/volume1/gat_training_data_west.pt'\ntorch.save(graph_data, graph_data_output_path)\nprint(f\"PyTorch Geometric Data object saved as '{graph_data_output_path}'.\")\n",
  "history_output" : "Loading dataset...\n",
  "history_begin_time" : 1740620037794,
  "history_end_time" : 1740620048063,
  "history_notes" : null,
  "history_process" : "ppl3n3",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "BUUDDQIReACT",
  "history_input" : "import numpy as np\nimport torch\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nimport pandas as pd\nimport os\nfrom datetime import datetime\n\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\n\nprint(\"Loading dataset...\")\ndata = pd.read_csv(file_path)\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\n\n# Step 1: Filter data for the western USA region\nlat_min, lat_max = 32.0, 49.0  # Covers western states\nlon_min, lon_max = -125.0, -102.0  # Covers western states\nfiltered_data = data[(data['lat'] >= lat_min) & (data['lat'] <= lat_max) &\n                     (data['lon'] >= lon_min) & (data['lon'] <= lon_max)]\nprint(f\"Filtered dataset to western USA region with {filtered_data.shape[0]} rows.\")\n\n# Step 2: Sample fewer data points\nsample_size = 50000  \nsampled_data = filtered_data.sample(n=sample_size, random_state=42).reset_index(drop=True)\nprint(f\"Sampled dataset with {sampled_data.shape[0]} rows.\")\n\n# Step 3: Normalize features\nselected_features = [\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount'\n]\n\nprint(\"Normalizing features...\")\nscaler = StandardScaler()\nsampled_data[selected_features] = scaler.fit_transform(sampled_data[selected_features])\n\noutput_file_path = 'normalized_sampled_snotel_west.csv'\nsampled_data.to_csv(output_file_path, index=False)\nprint(\"Normalized sampled dataset saved as 'normalized_sampled_snotel_west.csv'.\")\n\n# Step 4: Create node features tensor\nnode_features = torch.tensor(sampled_data[selected_features].values, dtype=torch.float)\nprint(f\"Node features tensor created with shape: {node_features.shape}\")\n\n# Step 5: Create labels tensor\nlabels = torch.tensor(sampled_data['swe_value'].values, dtype=torch.float)\nprint(f\"Labels tensor created with shape: {labels.shape}\")\n\n# Step 6: Build KDTree and construct fewer edges\ncoordinates = sampled_data[['lat', 'lon']].values\nthreshold = 0.05  # Adjusted threshold to limit edges\n\nprint(\"Building KDTree and constructing edges...\")\ntree = KDTree(coordinates)\ntedges = list(tree.query_pairs(r=threshold))\n\n# Ensure bidirectional edges\nedges = np.array(tedges)\nedges = np.vstack((edges, edges[:, ::-1]))  # Add reversed edges\nedge_index = torch.tensor(edges.T, dtype=torch.long)\nprint(f\"Edge index tensor created with shape: {edge_index.shape}\")\n\n# Step 7: Create PyTorch Geometric graph data object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# Save the PyTorch Geometric Data object\ngraph_data_output_path = '/media/volume1/gat_training_data_west.pt'\ntorch.save(graph_data, graph_data_output_path)\nprint(f\"PyTorch Geometric Data object saved as '{graph_data_output_path}'.\")\n",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 105 columns.\nFiltered dataset to western USA region with 5862359 rows.\nSampled dataset with 50000 rows.\nNormalizing features...\nNormalized sampled dataset saved as 'normalized_sampled_snotel_west.csv'.\nNode features tensor created with shape: torch.Size([50000, 13])\nLabels tensor created with shape: torch.Size([50000])\nBuilding KDTree and constructing edges...\nEdge index tensor created with shape: torch.Size([2, 5178142])\nPyTorch Geometric Data object saved as '/media/volume1/gat_training_data_west.pt'.\n",
  "history_begin_time" : 1739664692529,
  "history_end_time" : 1739664764947,
  "history_notes" : null,
  "history_process" : "ppl3n3",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "tFKSxileLaoQ",
  "history_input" : "import pandas as pd\nimport torch\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nimport numpy as np\n\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\n\nprint(\"Loading dataset...\")\ndata = pd.read_csv(file_path)\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\n\n# Step 1: Filter data for the western USA region\nlat_min, lat_max = 32.0, 49.0  # Covers western states\nlon_min, lon_max = -125.0, -102.0  # Covers western states\nfiltered_data = data[(data['lat'] >= lat_min) & (data['lat'] <= lat_max) &\n                     (data['lon'] >= lon_min) & (data['lon'] <= lon_max)]\nprint(f\"Filtered dataset to western USA region with {filtered_data.shape[0]} rows.\")\n\n# Step 2: Sample fewer data points\nsample_size = 10000  # Reduced sample size\nsampled_data = filtered_data.sample(n=sample_size, random_state=42).reset_index(drop=True)\nprint(f\"Sampled dataset with {sampled_data.shape[0]} rows.\")\n\n# Step 3: Normalize features\nselected_features = [\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount'\n]\n\nprint(\"Normalizing features...\")\nscaler = StandardScaler()\nsampled_data[selected_features] = scaler.fit_transform(sampled_data[selected_features])\n\noutput_file_path = 'normalized_sampled_snotel_west.csv'\nsampled_data.to_csv(output_file_path, index=False)\nprint(\"Normalized sampled dataset saved as 'normalized_sampled_snotel_west.csv'.\")\n\n# Step 4: Create node features tensor\nnode_features = torch.tensor(sampled_data[selected_features].values, dtype=torch.float)\nprint(f\"Node features tensor created with shape: {node_features.shape}\")\n\n# Step 5: Create labels tensor\nlabels = torch.tensor(sampled_data['swe_value'].values, dtype=torch.float)\nprint(f\"Labels tensor created with shape: {labels.shape}\")\n\n# Step 6: Build KDTree and construct fewer edges\ncoordinates = sampled_data[['lat', 'lon']].values\nthreshold = 0.005  # Adjusted threshold to limit edges\n\nprint(\"Building KDTree and constructing edges...\")\ntree = KDTree(coordinates)\ntedges = list(tree.query_pairs(r=threshold))\n\n# Ensure bidirectional edges\nedges = np.array(tedges)\nedges = np.vstack((edges, edges[:, ::-1]))  # Add reversed edges\nedge_index = torch.tensor(edges.T, dtype=torch.long)\nprint(f\"Edge index tensor created with shape: {edge_index.shape}\")\n\n# Step 7: Create PyTorch Geometric graph data object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# Save the PyTorch Geometric Data object\ngraph_data_output_path = '/media/volume1/gat_training_data_west.pt'\ntorch.save(graph_data, graph_data_output_path)\nprint(f\"PyTorch Geometric Data object saved as '{graph_data_output_path}'.\")",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 105 columns.\nFiltered dataset to western USA region with 5862359 rows.\nSampled dataset with 10000 rows.\nNormalizing features...\nNormalized sampled dataset saved as 'normalized_sampled_snotel_west.csv'.\nNode features tensor created with shape: torch.Size([10000, 13])\nLabels tensor created with shape: torch.Size([10000])\nBuilding KDTree and constructing edges...\nEdge index tensor created with shape: torch.Size([2, 28306])\nPyTorch Geometric Data object saved as '/media/volume1/gat_training_data_west.pt'.\n",
  "history_begin_time" : 1739663436922,
  "history_end_time" : 1739663503471,
  "history_notes" : null,
  "history_process" : "ppl3n3",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "vrdivIr15FfX",
  "history_input" : "import pandas as pd\nimport torch\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nimport numpy as np\n\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\n\nprint(\"Loading dataset...\")\ndata = pd.read_csv(file_path)\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\n\n# Step 1: Sample data\nsample_size = 50000  # Adjust as needed\nsampled_data = data.sample(n=sample_size, random_state=42).reset_index(drop=True)\nprint(f\"Sampled dataset with {sampled_data.shape[0]} rows.\")\n\n# Step 2: Normalize the selected features\nselected_features = [\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount'\n]\n\nprint(\"Normalizing features...\")\nscaler = StandardScaler()\nsampled_data[selected_features] = scaler.fit_transform(sampled_data[selected_features])\n\n# Save normalized sampled data\noutput_file_path = 'normalized_sampled_snotel.csv'\nsampled_data.to_csv(output_file_path, index=False)\nprint(\"Normalized sampled dataset saved as 'normalized_sampled_snotel.csv'.\")\n\n# Step 3: Create node features tensor\nnode_features = torch.tensor(sampled_data[selected_features].values, dtype=torch.float)\nprint(f\"Node features tensor created with shape: {node_features.shape}\")\n\n# Step 4: Create labels tensor\nlabels = torch.tensor(sampled_data['swe_value'].values, dtype=torch.float)\nprint(f\"Labels tensor created with shape: {labels.shape}\")\n\n# Step 5: Build KDTree and construct edges\ncoordinates = sampled_data[['lat', 'lon']].values  # Extract spatial coordinates\nthreshold = 0.01  # Adjust as needed (increase for more edges)\n\nprint(\"Building KDTree and constructing edges...\")\ntree = KDTree(coordinates)\ntedges = list(tree.query_pairs(r=threshold))  # Find neighbors within the threshold\n\n# Ensure bidirectional edges\nedges = np.array(tedges)\nedges = np.vstack((edges, edges[:, ::-1]))  # Add reversed edges\nedge_index = torch.tensor(edges.T, dtype=torch.long)\nprint(f\"Edge index tensor created with shape: {edge_index.shape}\")\n\n# Step 6: Create PyTorch Geometric graph data object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# Save the PyTorch Geometric Data object\ngraph_data_output_path = '/media/volume1/gat_training_data_sampled.pt'\ntorch.save(graph_data, graph_data_output_path)\nprint(f\"PyTorch Geometric Data object saved as '{graph_data_output_path}'.\")\n",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 105 columns.\nSampled dataset with 50000 rows.\nNormalizing features...\nNormalized sampled dataset saved as 'normalized_sampled_snotel.csv'.\nNode features tensor created with shape: torch.Size([50000, 13])\nLabels tensor created with shape: torch.Size([50000])\nBuilding KDTree and constructing edges...\nEdge index tensor created with shape: torch.Size([2, 910346])\nPyTorch Geometric Data object saved as '/media/volume1/gat_training_data_sampled.pt'.\n",
  "history_begin_time" : 1739663017746,
  "history_end_time" : 1739663108872,
  "history_notes" : null,
  "history_process" : "ppl3n3",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "MQsiCwoxqdw1",
  "history_input" : "# Write your first Python code in Geoweaver\n\nimport pandas as pd\nimport torch\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nimport numpy as np\n\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\n\nprint(\"Loading dataset...\")\ndata = pd.read_csv(file_path)\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\n\n# Step 1: Spatially aware sampling using a batched KDTree search\nsample_size = 50000  # Adjust as needed\ncoordinates = data[['lat', 'lon']].values\n\nprint(\"Building KDTree for spatial sampling...\")\ntree = KDTree(coordinates)\n\n# Start with a random point\nnp.random.seed(42)\nstart_idx = np.random.choice(len(data), 1)\nselected_indices = set(start_idx)\n\n# Precompute all neighbor lists\nneighbors_dict = {i: set(tree.query_ball_point(coordinates[i], r=0.01)) for i in range(len(data))}\n\n# Select spatially close neighbors\nwhile len(selected_indices) < sample_size:\n    current_idx = np.random.choice(list(selected_indices))\n    potential_neighbors = neighbors_dict[current_idx]\n    \n    # Only pick new samples\n    new_neighbors = potential_neighbors - selected_indices\n    if new_neighbors:\n        selected_indices.add(np.random.choice(list(new_neighbors)))\n\nselected_indices = list(selected_indices)\nsampled_data = data.iloc[selected_indices]\nprint(f\"Spatially sampled dataset with {sampled_data.shape[0]} rows.\")\n\n# Step 2: Normalize features\nselected_features = [\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount'\n]\n\nprint(\"Normalizing features...\")\nscaler = StandardScaler()\nsampled_data[selected_features] = scaler.fit_transform(sampled_data[selected_features])\n\noutput_file_path = 'optimized_spatial_sampled_snotel.csv'\nsampled_data.to_csv(output_file_path, index=False)\nprint(\"Normalized sampled dataset saved as 'optimized_spatial_sampled_snotel.csv'.\")\n\n# Step 3: Create node features tensor\nnode_features = torch.tensor(sampled_data[selected_features].values, dtype=torch.float)\nprint(f\"Node features tensor created with shape: {node_features.shape}\")\n\n# Step 4: Create labels tensor\nlabels = torch.tensor(sampled_data['swe_value'].values, dtype=torch.float)\nprint(f\"Labels tensor created with shape: {labels.shape}\")\n\n# Step 5: Build KDTree and construct edges\nprint(\"Building KDTree and constructing edges...\")\ncoordinates = sampled_data[['lat', 'lon']].values\ntree = KDTree(coordinates)\nthreshold = 0.005  # Adjust as needed\n\nbatch_edges = tree.query_pairs(r=threshold)\ntedges = list(batch_edges)\nprint(f\"Number of edges created: {len(tedges)}\")\n\n# Convert to edge index tensor\nedge_index = torch.tensor(tedges, dtype=torch.long).t().contiguous()\nprint(f\"Edge index tensor created with shape: {edge_index.shape}\")\n\n# Step 6: Create PyTorch Geometric graph data object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# Save the PyTorch Geometric Data object\ngraph_data_output_path = '/media/volume1/gat_spatial_training_data_sampled.pt'\ntorch.save(graph_data, graph_data_output_path)\nprint(f\"PyTorch Geometric Data object saved as '{graph_data_output_path}'.\")\n",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 105 columns.\nBuilding KDTree for spatial sampling...\n",
  "history_begin_time" : 1739662417106,
  "history_end_time" : 1739662805989,
  "history_notes" : null,
  "history_process" : "ppl3n3",
  "host_id" : null,
  "indicator" : "Failed"
}]