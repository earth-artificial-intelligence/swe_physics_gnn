[{
  "history_id" : "urCRTuSNy2fA",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom torch_geometric.utils import degree, to_undirected, add_self_loops\r\n\r\n# Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\nchunksize = 500000\r\n#  latitude & longitude to cover Washington, Oregon, and Idaho\r\nmin_lat, max_lat = 42.365162, 48.981824\r\nmin_lon, max_lon = -123.842534, -111.627646\r\n\r\n\r\nprint(\"Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'SWE', 'swe_value', 'relative_humidity_rmin',\r\n    'potential_evapotranspiration', 'air_temperature_tmmx',\r\n    'relative_humidity_rmax', 'mean_vapor_pressure_deficit',\r\n    'air_temperature_tmmn', 'wind_speed', 'Elevation', 'Aspect',\r\n    'Curvature', 'Northness', 'Eastness', 'fsca', 'Slope', 'SWE_1',\r\n    'air_temperature_tmmn_1', 'potential_evapotranspiration_1',\r\n    'mean_vapor_pressure_deficit_1', 'relative_humidity_rmax_1',\r\n    'relative_humidity_rmin_1', 'air_temperature_tmmx_1', 'wind_speed_1',\r\n    'fsca_1', 'SWE_2', 'air_temperature_tmmn_2',\r\n    'potential_evapotranspiration_2', 'mean_vapor_pressure_deficit_2',\r\n    'relative_humidity_rmax_2', 'relative_humidity_rmin_2',\r\n    'air_temperature_tmmx_2', 'wind_speed_2', 'fsca_2', 'SWE_3',\r\n    'air_temperature_tmmn_3', 'potential_evapotranspiration_3',\r\n    'mean_vapor_pressure_deficit_3', 'relative_humidity_rmax_3',\r\n    'relative_humidity_rmin_3', 'air_temperature_tmmx_3', 'wind_speed_3',\r\n    'fsca_3', 'SWE_4', 'air_temperature_tmmn_4',\r\n    'potential_evapotranspiration_4', 'mean_vapor_pressure_deficit_4',\r\n    'relative_humidity_rmax_4', 'relative_humidity_rmin_4',\r\n    'air_temperature_tmmx_4', 'wind_speed_4', 'fsca_4', 'SWE_5',\r\n    'air_temperature_tmmn_5', 'potential_evapotranspiration_5',\r\n    'mean_vapor_pressure_deficit_5', 'relative_humidity_rmax_5',\r\n    'relative_humidity_rmin_5', 'air_temperature_tmmx_5', 'wind_speed_5',\r\n    'fsca_5', 'SWE_6', 'air_temperature_tmmn_6',\r\n    'potential_evapotranspiration_6', 'mean_vapor_pressure_deficit_6',\r\n    'relative_humidity_rmax_6', 'relative_humidity_rmin_6',\r\n    'air_temperature_tmmx_6', 'wind_speed_6', 'fsca_6', 'SWE_7',\r\n    'air_temperature_tmmn_7', 'potential_evapotranspiration_7',\r\n    'mean_vapor_pressure_deficit_7', 'relative_humidity_rmax_7',\r\n    'relative_humidity_rmin_7', 'air_temperature_tmmx_7', 'wind_speed_7',\r\n    'fsca_7', 'water_year', 'snodas_mask'\r\n]\r\n\r\ndf_list = []\r\ntotal_rows = 0\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, parse_dates=['date'], chunksize=chunksize):\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear  \r\n    total_rows += len(chunk)\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Total rows in raw dataset: {total_rows}\")\r\nprint(f\"Rows remaining after lat/lon filtering: {data.shape[0]}\")\r\n\r\n# Check for missing values\r\nmissing_values = data[['date', 'lat', 'lon', 'swe_value']].isnull().sum()\r\nprint(f\"Missing values count per column:\\n{missing_values}\")\r\n\r\n# Remove duplicates\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\"Found {num_duplicates} duplicate rows.\")\r\ndata = data.drop_duplicates()\r\nprint(f\"Dataset size after deduplication: {data.shape[0]}\")\r\n\r\n# Merge records for the same station on the same day\r\nprint(\"Merging records for the same station on the same day...\")\r\nmerge_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\ndata = data.groupby(['lat', 'lon', 'date'], as_index=False)[merge_columns].mean()\r\ndata['dayofyear'] = pd.to_datetime(data['date']).dt.dayofyear\r\nprint(f\" Merged dataset size: {data.shape[0]}\")\r\n\r\n# Round numerical values\r\nprint(\" Rounding numerical values to 3 decimals...\")\r\nnumerical_cols = list(set(merge_columns) & set(data.columns))  \r\ndata[numerical_cols] = data[numerical_cols].round(3)\r\n\r\n# Merging nodes to reduce graph size\r\ngrid_size = 0.01  \r\ntime_bins = 12  #  Increased from 10 to 12 bins\r\n\r\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\ndata['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\nmerged_nodes = data.groupby('grid_id').agg({\r\n    'Elevation': 'mean', 'Slope': 'mean', 'SWE': 'mean', 'snow_depth': 'mean',\r\n    'air_temperature_observed_f': 'mean', 'precipitation_amount': 'mean',\r\n    'relative_humidity_rmin': 'mean', 'relative_humidity_rmax': 'mean', 'wind_speed': 'mean',\r\n    'cumulative_SWE': 'mean', 'cumulative_precipitation_amount': 'mean', 'swe_value': 'mean',\r\n    'lat': 'mean', 'lon': 'mean', 'dayofyear': 'mean'\r\n}).reset_index()\r\n\r\nprint(f\" Merged nodes: {merged_nodes.shape[0]} from {data.shape[0]} original records.\")\r\n\r\n# Normalizing features\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\n\r\nmerged_nodes[feature_columns] = merged_nodes[feature_columns].fillna(merged_nodes[feature_columns].median())\r\n\r\nscaler = StandardScaler()\r\nmerged_nodes[feature_columns] = scaler.fit_transform(merged_nodes[feature_columns].values)\r\n\r\nnode_features = torch.tensor(merged_nodes[feature_columns].values, dtype=torch.float)\r\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\n\r\nprint(f\" Normalization completed. Node feature shape: {node_features.shape}\")\r\n\r\n# Adaptive spatial threshold based on data distribution\r\ncoordinates = merged_nodes[['lat', 'lon', 'dayofyear']].values  \r\ntree = KDTree(coordinates)\r\n\r\navg_degree = max(5, min(10, int(len(merged_nodes) ** 0.5 / 10)))\r\ndistances, neighbors = tree.query(coordinates, k=avg_degree + 1)\r\n\r\nspatial_threshold = np.percentile(distances[:, 1:], 75)  #  Adaptive threshold\r\ntime_weight = 0.07  \r\n\r\ndef time_aware_distance(p1, p2):\r\n    spatial_dist = np.linalg.norm(p1[:2] - p2[:2])  \r\n    time_dist = abs(p1[2] - p2[2]) * time_weight  \r\n    return spatial_dist + time_dist\r\n\r\nprint(\"Rebuilding KDTree and constructing edges...\")\r\n\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, avg_degree + 1):\r\n        if time_aware_distance(coordinates[i], coordinates[neighbors[i][j]]) < spatial_threshold:\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T\r\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\n# Ensure bidirectional edges\r\nedge_index = to_undirected(edge_index)\r\n\r\n# Graph Pruning: Remove low-degree nodes\r\ndeg = degree(edge_index[0], num_nodes=merged_nodes.shape[0])\r\nvalid_nodes = deg > 1  \r\nfiltered_node_mask = valid_nodes.nonzero(as_tuple=True)[0]\r\n\r\n# Filter node features & labels\r\nnode_features = node_features[filtered_node_mask]\r\nlabels = labels[filtered_node_mask]\r\n\r\n# Reconstruct Edge Index after filtering\r\nnode_map = {old_idx: new_idx for new_idx, old_idx in enumerate(filtered_node_mask.tolist())}\r\nnew_edges = [(node_map[i], node_map[j]) for i, j in edge_index.T.tolist() if i in node_map and j in node_map]\r\nedge_index = torch.tensor(new_edges, dtype=torch.long).T if len(new_edges) > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\nprint(f\"Final Graph: Nodes = {len(filtered_node_mask)}, Edges = {edge_index.shape[1]}\")\r\n\r\n# Save graph\r\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\ntorch.save(graph_data, \"/media/volume1/gnn_3states.pt\")\r\n\r\nprint(\" Graph successfully saved.\")\r\n",
  "history_output" : "Loading dataset...\nTraceback (most recent call last):\n  File \"/media/volume1/gw-workspace/urCRTuSNy2fA/graph_creation.py\", line 54, in <module>\n    for chunk in pd.read_csv(file_path, usecols=useful_columns, parse_dates=['date'], chunksize=chunksize):\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 1026, in read_csv\n    return _read(filepath_or_buffer, kwds)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 620, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 1620, in __init__\n    self._engine = self._make_engine(f, self.engine)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 1898, in _make_engine\n    return mapping[engine](f, **self.options)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 140, in __init__\n    self._validate_usecols_names(usecols, self.orig_names)\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/base_parser.py\", line 979, in _validate_usecols_names\n    raise ValueError(\nValueError: Usecols do not match columns, columns expected but not found: ['snodas_mask']\n",
  "history_begin_time" : 1744050773934,
  "history_end_time" : 1744050786124,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "9W2YsMDtNHi6",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom torch_geometric.utils import degree, to_undirected, add_self_loops\r\n\r\n# Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.365162, 48.981824\r\nmin_lon, max_lon = -123.842534, -111.627646\r\n\r\nprint(\"Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\n\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, parse_dates=['date'], chunksize=chunksize):\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear  \r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Rows remaining after lat/lon filtering: {data.shape[0]}\")\r\n\r\n# Remove duplicates\r\ndata = data.drop_duplicates()\r\n\r\n# Merge records for the same station on the same day\r\nmerge_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\ndata = data.groupby(['lat', 'lon', 'date'], as_index=False)[merge_columns].mean()\r\ndata['dayofyear'] = pd.to_datetime(data['date']).dt.dayofyear\r\n\r\n# Round numerical values\r\ndata[merge_columns] = data[merge_columns].round(3)\r\n\r\n# Merge nodes (Reverting to Best Settings)\r\ngrid_size = 0.01  \r\ntime_bins = 10    \r\n\r\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\ndata['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\nmerged_nodes = data.groupby('grid_id').agg({\r\n    'Elevation': 'mean', 'Slope': 'mean', 'SWE': 'mean', 'snow_depth': 'mean',\r\n    'air_temperature_observed_f': 'mean', 'precipitation_amount': 'mean',\r\n    'relative_humidity_rmin': 'mean', 'relative_humidity_rmax': 'mean', 'wind_speed': 'mean',\r\n    'cumulative_SWE': 'mean', 'cumulative_precipitation_amount': 'mean', 'swe_value': 'mean',\r\n    'lat': 'mean', 'lon': 'mean', 'dayofyear': 'mean'\r\n}).reset_index()\r\n\r\nprint(f\"✅ Merged nodes: {merged_nodes.shape[0]} from {data.shape[0]} original records.\")\r\n\r\n# Normalizing features\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\nmerged_nodes[feature_columns] = merged_nodes[feature_columns].fillna(merged_nodes[feature_columns].median())\r\n\r\nscaler = StandardScaler()\r\nmerged_nodes[feature_columns] = scaler.fit_transform(merged_nodes[feature_columns].values)\r\n\r\nnode_features = torch.tensor(merged_nodes[feature_columns].values, dtype=torch.float)\r\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\n\r\nprint(f\"✅ Normalization completed. Node feature shape: {node_features.shape}\")\r\n\r\n# **Back to Best Graph Configuration**\r\ncoordinates = merged_nodes[['lat', 'lon', 'dayofyear']].values  \r\ntree = KDTree(coordinates)\r\n\r\navg_degree = max(5, min(10, int(len(merged_nodes) ** 0.5 / 10)))\r\nspatial_threshold = 0.2  \r\ntime_weight = 0.07  \r\n\r\ndef time_aware_distance(p1, p2):\r\n    spatial_dist = np.linalg.norm(p1[:2] - p2[:2])  \r\n    time_dist = abs(p1[2] - p2[2]) * time_weight  \r\n    return spatial_dist + time_dist\r\n\r\nprint(\"🔄 Rebuilding KDTree and constructing edges...\")\r\ndistances, neighbors = tree.query(coordinates, k=avg_degree + 1)\r\n\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, avg_degree + 1):\r\n        if time_aware_distance(coordinates[i], coordinates[neighbors[i][j]]) < spatial_threshold:\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T\r\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\n# Ensure bidirectional edges\r\nedge_index = to_undirected(edge_index)\r\n\r\n# **Reverting to Best Edge Count**\r\ndeg = degree(edge_index[0], num_nodes=merged_nodes.shape[0])\r\nnum_isolated_nodes = (deg == 0).sum().item()\r\nprint(f\"✅ Isolated Nodes: {num_isolated_nodes}\")\r\n\r\nif num_isolated_nodes > 0:\r\n    edge_index, _ = add_self_loops(edge_index, num_nodes=merged_nodes.shape[0])\r\n\r\ndeg = degree(edge_index[0], num_nodes=merged_nodes.shape[0])\r\nnum_isolated_nodes = (deg == 0).sum().item()\r\nprint(f\"✅ Final Isolated Nodes: {num_isolated_nodes}\")\r\n\r\n# Save graph\r\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\ntorch.save(graph_data, \"/media/volume1/gnn_2graph.pt\")\r\n\r\nprint(\"✅ Graph successfully saved.\")\r\n",
  "history_output" : "Loading dataset...\nRows remaining after lat/lon filtering: 1682854\n✅ Merged nodes: 20617 from 1664870 original records.\n✅ Normalization completed. Node feature shape: torch.Size([20617, 11])\n🔄 Rebuilding KDTree and constructing edges...\n✅ Isolated Nodes: 4140\n✅ Final Isolated Nodes: 0\n✅ Graph successfully saved.\n",
  "history_begin_time" : 1741742420057,
  "history_end_time" : 1741742459123,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "XwjcdOGcKXrV",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom torch_geometric.utils import degree, to_undirected, add_self_loops\r\n\r\n# Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\nchunksize = 500000\r\n#  latitude & longitude to cover Washington, Oregon, and Idaho\r\nmin_lat, max_lat = 42.365162, 48.981824\r\nmin_lon, max_lon = -123.842534, -111.627646\r\n\r\n\r\nprint(\"Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\n\r\ndf_list = []\r\ntotal_rows = 0\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, parse_dates=['date'], chunksize=chunksize):\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear  \r\n    total_rows += len(chunk)\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Total rows in raw dataset: {total_rows}\")\r\nprint(f\"Rows remaining after lat/lon filtering: {data.shape[0]}\")\r\n\r\n# Check for missing values\r\nmissing_values = data[['date', 'lat', 'lon', 'swe_value']].isnull().sum()\r\nprint(f\"Missing values count per column:\\n{missing_values}\")\r\n\r\n# Remove duplicates\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\"Found {num_duplicates} duplicate rows.\")\r\ndata = data.drop_duplicates()\r\nprint(f\"Dataset size after deduplication: {data.shape[0]}\")\r\n\r\n# Merge records for the same station on the same day\r\nprint(\"Merging records for the same station on the same day...\")\r\nmerge_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\ndata = data.groupby(['lat', 'lon', 'date'], as_index=False)[merge_columns].mean()\r\ndata['dayofyear'] = pd.to_datetime(data['date']).dt.dayofyear\r\nprint(f\" Merged dataset size: {data.shape[0]}\")\r\n\r\n# Round numerical values\r\nprint(\" Rounding numerical values to 3 decimals...\")\r\nnumerical_cols = list(set(merge_columns) & set(data.columns))  \r\ndata[numerical_cols] = data[numerical_cols].round(3)\r\n\r\n# Merging nodes to reduce graph size\r\ngrid_size = 0.01  \r\ntime_bins = 12  #  Increased from 10 to 12 bins\r\n\r\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\ndata['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\nmerged_nodes = data.groupby('grid_id').agg({\r\n    'Elevation': 'mean', 'Slope': 'mean', 'SWE': 'mean', 'snow_depth': 'mean',\r\n    'air_temperature_observed_f': 'mean', 'precipitation_amount': 'mean',\r\n    'relative_humidity_rmin': 'mean', 'relative_humidity_rmax': 'mean', 'wind_speed': 'mean',\r\n    'cumulative_SWE': 'mean', 'cumulative_precipitation_amount': 'mean', 'swe_value': 'mean',\r\n    'lat': 'mean', 'lon': 'mean', 'dayofyear': 'mean'\r\n}).reset_index()\r\n\r\nprint(f\" Merged nodes: {merged_nodes.shape[0]} from {data.shape[0]} original records.\")\r\n\r\n# Normalizing features\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\n\r\nmerged_nodes[feature_columns] = merged_nodes[feature_columns].fillna(merged_nodes[feature_columns].median())\r\n\r\nscaler = StandardScaler()\r\nmerged_nodes[feature_columns] = scaler.fit_transform(merged_nodes[feature_columns].values)\r\n\r\nnode_features = torch.tensor(merged_nodes[feature_columns].values, dtype=torch.float)\r\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\n\r\nprint(f\" Normalization completed. Node feature shape: {node_features.shape}\")\r\n\r\n# Adaptive spatial threshold based on data distribution\r\ncoordinates = merged_nodes[['lat', 'lon', 'dayofyear']].values  \r\ntree = KDTree(coordinates)\r\n\r\navg_degree = max(5, min(10, int(len(merged_nodes) ** 0.5 / 10)))\r\ndistances, neighbors = tree.query(coordinates, k=avg_degree + 1)\r\n\r\nspatial_threshold = np.percentile(distances[:, 1:], 75)  #  Adaptive threshold\r\ntime_weight = 0.07  \r\n\r\ndef time_aware_distance(p1, p2):\r\n    spatial_dist = np.linalg.norm(p1[:2] - p2[:2])  \r\n    time_dist = abs(p1[2] - p2[2]) * time_weight  \r\n    return spatial_dist + time_dist\r\n\r\nprint(\"Rebuilding KDTree and constructing edges...\")\r\n\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, avg_degree + 1):\r\n        if time_aware_distance(coordinates[i], coordinates[neighbors[i][j]]) < spatial_threshold:\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T\r\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\n# Ensure bidirectional edges\r\nedge_index = to_undirected(edge_index)\r\n\r\n# Graph Pruning: Remove low-degree nodes\r\ndeg = degree(edge_index[0], num_nodes=merged_nodes.shape[0])\r\nvalid_nodes = deg > 1  \r\nfiltered_node_mask = valid_nodes.nonzero(as_tuple=True)[0]\r\n\r\n# Filter node features & labels\r\nnode_features = node_features[filtered_node_mask]\r\nlabels = labels[filtered_node_mask]\r\n\r\n# Reconstruct Edge Index after filtering\r\nnode_map = {old_idx: new_idx for new_idx, old_idx in enumerate(filtered_node_mask.tolist())}\r\nnew_edges = [(node_map[i], node_map[j]) for i, j in edge_index.T.tolist() if i in node_map and j in node_map]\r\nedge_index = torch.tensor(new_edges, dtype=torch.long).T if len(new_edges) > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\nprint(f\"Final Graph: Nodes = {len(filtered_node_mask)}, Edges = {edge_index.shape[1]}\")\r\n\r\n# Save graph\r\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\ntorch.save(graph_data, \"/media/volume1/gnn_3states.pt\")\r\n\r\nprint(\" Graph successfully saved.\")\r\n",
  "history_output" : "Loading dataset...\nTotal rows in raw dataset: 6155644\nRows remaining after lat/lon filtering: 1682854\nMissing values count per column:\ndate         0\nlat          0\nlon          0\nswe_value    0\ndtype: int64\nFound 3048 duplicate rows.\nDataset size after deduplication: 1679806\nMerging records for the same station on the same day...\n Merged dataset size: 1664870\n Rounding numerical values to 3 decimals...\n Merged nodes: 24419 from 1664870 original records.\n Normalization completed. Node feature shape: torch.Size([24419, 11])\nRebuilding KDTree and constructing edges...\nFinal Graph: Nodes = 23579, Edges = 252634\n Graph successfully saved.\n",
  "history_begin_time" : 1741740538048,
  "history_end_time" : 1741740619396,
  "history_notes" : "3 states",
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "wmoiRkxFBrql",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom torch_geometric.utils import degree, to_undirected, add_self_loops\r\n\r\n# Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\"Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\n\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, parse_dates=['date'], chunksize=chunksize):\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear  \r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Rows remaining after lat/lon filtering: {data.shape[0]}\")\r\n\r\n# Remove duplicates\r\ndata = data.drop_duplicates()\r\n\r\n# Merge records for the same station on the same day\r\nmerge_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\ndata = data.groupby(['lat', 'lon', 'date'], as_index=False)[merge_columns].mean()\r\ndata['dayofyear'] = pd.to_datetime(data['date']).dt.dayofyear\r\n\r\n# Round numerical values\r\ndata[merge_columns] = data[merge_columns].round(3)\r\n\r\n# Merge nodes (Reverting to Best Settings)\r\ngrid_size = 0.01  \r\ntime_bins = 10    \r\n\r\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\ndata['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\nmerged_nodes = data.groupby('grid_id').agg({\r\n    'Elevation': 'mean', 'Slope': 'mean', 'SWE': 'mean', 'snow_depth': 'mean',\r\n    'air_temperature_observed_f': 'mean', 'precipitation_amount': 'mean',\r\n    'relative_humidity_rmin': 'mean', 'relative_humidity_rmax': 'mean', 'wind_speed': 'mean',\r\n    'cumulative_SWE': 'mean', 'cumulative_precipitation_amount': 'mean', 'swe_value': 'mean',\r\n    'lat': 'mean', 'lon': 'mean', 'dayofyear': 'mean'\r\n}).reset_index()\r\n\r\nprint(f\"✅ Merged nodes: {merged_nodes.shape[0]} from {data.shape[0]} original records.\")\r\n\r\n# Normalizing features\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\nmerged_nodes[feature_columns] = merged_nodes[feature_columns].fillna(merged_nodes[feature_columns].median())\r\n\r\nscaler = StandardScaler()\r\nmerged_nodes[feature_columns] = scaler.fit_transform(merged_nodes[feature_columns].values)\r\n\r\nnode_features = torch.tensor(merged_nodes[feature_columns].values, dtype=torch.float)\r\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\n\r\nprint(f\"✅ Normalization completed. Node feature shape: {node_features.shape}\")\r\n\r\n# **Back to Best Graph Configuration**\r\ncoordinates = merged_nodes[['lat', 'lon', 'dayofyear']].values  \r\ntree = KDTree(coordinates)\r\n\r\navg_degree = max(5, min(10, int(len(merged_nodes) ** 0.5 / 10)))\r\nspatial_threshold = 0.2  \r\ntime_weight = 0.07  \r\n\r\ndef time_aware_distance(p1, p2):\r\n    spatial_dist = np.linalg.norm(p1[:2] - p2[:2])  \r\n    time_dist = abs(p1[2] - p2[2]) * time_weight  \r\n    return spatial_dist + time_dist\r\n\r\nprint(\"🔄 Rebuilding KDTree and constructing edges...\")\r\ndistances, neighbors = tree.query(coordinates, k=avg_degree + 1)\r\n\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, avg_degree + 1):\r\n        if time_aware_distance(coordinates[i], coordinates[neighbors[i][j]]) < spatial_threshold:\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T\r\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\n# Ensure bidirectional edges\r\nedge_index = to_undirected(edge_index)\r\n\r\n# **Reverting to Best Edge Count**\r\ndeg = degree(edge_index[0], num_nodes=merged_nodes.shape[0])\r\nnum_isolated_nodes = (deg == 0).sum().item()\r\nprint(f\"✅ Isolated Nodes: {num_isolated_nodes}\")\r\n\r\nif num_isolated_nodes > 0:\r\n    edge_index, _ = add_self_loops(edge_index, num_nodes=merged_nodes.shape[0])\r\n\r\ndeg = degree(edge_index[0], num_nodes=merged_nodes.shape[0])\r\nnum_isolated_nodes = (deg == 0).sum().item()\r\nprint(f\"✅ Final Isolated Nodes: {num_isolated_nodes}\")\r\n\r\n# Save graph\r\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\ntorch.save(graph_data, \"/media/volume1/gnn_2graph.pt\")\r\n\r\nprint(\"✅ Graph successfully saved.\")\r\n",
  "history_output" : "Loading dataset...\nRows remaining after lat/lon filtering: 620662\n✅ Merged nodes: 7375 from 610749 original records.\n✅ Normalization completed. Node feature shape: torch.Size([7375, 11])\n🔄 Rebuilding KDTree and constructing edges...\n✅ Isolated Nodes: 1347\n✅ Final Isolated Nodes: 0\n✅ Graph successfully saved.\n",
  "history_begin_time" : 1741638292473,
  "history_end_time" : 1741638326651,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "FSMeUNgumoaV",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom torch_geometric.utils import degree, to_undirected, add_self_loops\r\n\r\n# Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\"Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\n\r\ndf_list = []\r\ntotal_rows = 0\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, parse_dates=['date'], chunksize=chunksize):\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear  \r\n    total_rows += len(chunk)\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Total rows in raw dataset: {total_rows}\")\r\nprint(f\"Rows remaining after lat/lon filtering: {data.shape[0]}\")\r\n\r\n# Check for missing values\r\nmissing_values = data[['date', 'lat', 'lon', 'swe_value']].isnull().sum()\r\nprint(f\"Missing values count per column:\\n{missing_values}\")\r\n\r\n# Remove duplicates\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\"Found {num_duplicates} duplicate rows.\")\r\ndata = data.drop_duplicates()\r\nprint(f\"Dataset size after deduplication: {data.shape[0]}\")\r\n\r\n# Merge records for the same station on the same day\r\nprint(\"🔄 Merging records for the same station on the same day...\")\r\nmerge_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\ndata = data.groupby(['lat', 'lon', 'date'], as_index=False)[merge_columns].mean()\r\ndata['dayofyear'] = pd.to_datetime(data['date']).dt.dayofyear\r\nprint(f\"✅ Merged dataset size: {data.shape[0]}\")\r\n\r\n# Round numerical values\r\nprint(\"🔄 Rounding numerical values to 3 decimals...\")\r\nnumerical_cols = list(set(merge_columns) & set(data.columns))  \r\ndata[numerical_cols] = data[numerical_cols].round(3)\r\n\r\n# Merging nodes to reduce graph size\r\ngrid_size = 0.01  \r\ntime_bins = 12  # 🔥 Increased from 10 to 12 bins\r\n\r\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\ndata['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\nmerged_nodes = data.groupby('grid_id').agg({\r\n    'Elevation': 'mean', 'Slope': 'mean', 'SWE': 'mean', 'snow_depth': 'mean',\r\n    'air_temperature_observed_f': 'mean', 'precipitation_amount': 'mean',\r\n    'relative_humidity_rmin': 'mean', 'relative_humidity_rmax': 'mean', 'wind_speed': 'mean',\r\n    'cumulative_SWE': 'mean', 'cumulative_precipitation_amount': 'mean', 'swe_value': 'mean',\r\n    'lat': 'mean', 'lon': 'mean', 'dayofyear': 'mean'\r\n}).reset_index()\r\n\r\nprint(f\"✅ Merged nodes: {merged_nodes.shape[0]} from {data.shape[0]} original records.\")\r\n\r\n# Normalizing features\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\n\r\nmerged_nodes[feature_columns] = merged_nodes[feature_columns].fillna(merged_nodes[feature_columns].median())\r\n\r\nscaler = StandardScaler()\r\nmerged_nodes[feature_columns] = scaler.fit_transform(merged_nodes[feature_columns].values)\r\n\r\nnode_features = torch.tensor(merged_nodes[feature_columns].values, dtype=torch.float)\r\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\n\r\nprint(f\"✅ Normalization completed. Node feature shape: {node_features.shape}\")\r\n\r\n# Adaptive spatial threshold based on data distribution\r\ncoordinates = merged_nodes[['lat', 'lon', 'dayofyear']].values  \r\ntree = KDTree(coordinates)\r\n\r\navg_degree = max(5, min(10, int(len(merged_nodes) ** 0.5 / 10)))\r\ndistances, neighbors = tree.query(coordinates, k=avg_degree + 1)\r\n\r\nspatial_threshold = np.percentile(distances[:, 1:], 75)  # 🔥 Adaptive threshold\r\ntime_weight = 0.07  \r\n\r\ndef time_aware_distance(p1, p2):\r\n    spatial_dist = np.linalg.norm(p1[:2] - p2[:2])  \r\n    time_dist = abs(p1[2] - p2[2]) * time_weight  \r\n    return spatial_dist + time_dist\r\n\r\nprint(\"🔄 Rebuilding KDTree and constructing edges...\")\r\n\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, avg_degree + 1):\r\n        if time_aware_distance(coordinates[i], coordinates[neighbors[i][j]]) < spatial_threshold:\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T\r\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\n# Ensure bidirectional edges\r\nedge_index = to_undirected(edge_index)\r\n\r\n# Graph Pruning: Remove low-degree nodes\r\ndeg = degree(edge_index[0], num_nodes=merged_nodes.shape[0])\r\nvalid_nodes = deg > 1  \r\nfiltered_node_mask = valid_nodes.nonzero(as_tuple=True)[0]\r\n\r\n# Filter node features & labels\r\nnode_features = node_features[filtered_node_mask]\r\nlabels = labels[filtered_node_mask]\r\n\r\n# Reconstruct Edge Index after filtering\r\nnode_map = {old_idx: new_idx for new_idx, old_idx in enumerate(filtered_node_mask.tolist())}\r\nnew_edges = [(node_map[i], node_map[j]) for i, j in edge_index.T.tolist() if i in node_map and j in node_map]\r\nedge_index = torch.tensor(new_edges, dtype=torch.long).T if len(new_edges) > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\nprint(f\"✅ Final Graph: Nodes = {len(filtered_node_mask)}, Edges = {edge_index.shape[1]}\")\r\n\r\n# Save graph\r\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\ntorch.save(graph_data, \"/media/volume1/gnn_optimized.pt\")\r\n\r\nprint(\"✅ Graph successfully saved.\")\r\n",
  "history_output" : "Loading dataset...\nTotal rows in raw dataset: 6155644\nRows remaining after lat/lon filtering: 620662\nMissing values count per column:\ndate         0\nlat          0\nlon          0\nswe_value    0\ndtype: int64\nFound 2431 duplicate rows.\nDataset size after deduplication: 618231\n🔄 Merging records for the same station on the same day...\n✅ Merged dataset size: 610749\n🔄 Rounding numerical values to 3 decimals...\n✅ Merged nodes: 8720 from 610749 original records.\n✅ Normalization completed. Node feature shape: torch.Size([8720, 11])\n🔄 Rebuilding KDTree and constructing edges...\n✅ Final Graph: Nodes = 8384, Edges = 82116\n✅ Graph successfully saved.\n",
  "history_begin_time" : 1741633659245,
  "history_end_time" : 1741633694093,
  "history_notes" : "Oregon state ",
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "RPKZMExaOdX6",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom torch_geometric.utils import degree \r\n\r\n# Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\"Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\n\r\n# Step 1: Load and filter the dataset\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, parse_dates=['date'], chunksize=chunksize):\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear  # Extract day of year\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\n# Combine filtered chunks\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Filtered dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\n# Remove duplicates\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\"Found {num_duplicates} duplicate rows.\")\r\ndata = data.drop_duplicates()\r\nprint(f\"Removed duplicates. New dataset size: {data.shape[0]} rows.\")\r\n\r\n# Merge Nodes (Spatial & Time Binning)\r\nprint(\"Merging nodes to reduce graph size...\")\r\n\r\ngrid_size = 0.015  # Increased merging (0.02 was too sparse in Graph 2)\r\ntime_bins = 10  # Reduced from 20 → 10 for less fragmentation\r\n\r\n# Assign spatial bins\r\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\n\r\n# Assign temporal bins\r\ndata['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\n\r\n# Create unique spatial-temporal ID\r\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\n# Aggregate nodes by grid_id\r\nmerged_nodes = data.groupby('grid_id').agg({\r\n    'Elevation': 'mean',\r\n    'Slope': 'mean',\r\n    'SWE': 'mean',\r\n    'snow_depth': 'mean',\r\n    'air_temperature_observed_f': 'mean',\r\n    'precipitation_amount': 'mean',\r\n    'relative_humidity_rmin': 'mean',\r\n    'relative_humidity_rmax': 'mean',\r\n    'wind_speed': 'mean',\r\n    'cumulative_SWE': 'mean',\r\n    'cumulative_precipitation_amount': 'mean',\r\n    'swe_value': 'mean',  \r\n    'lat': 'mean',\r\n    'lon': 'mean',\r\n    'dayofyear': 'mean'\r\n}).reset_index()\r\n\r\nprint(f\"Merged nodes: {merged_nodes.shape[0]} from {data.shape[0]} original records.\")\r\n\r\n# Normalize Features\r\nprint(\"Normalizing features using StandardScaler...\")\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\n\r\nscaler = StandardScaler()\r\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\r\nnode_features = torch.tensor(node_features, dtype=torch.float)\r\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\n\r\nprint(f\"Normalization completed. Node feature shape: {node_features.shape}\")\r\n\r\n# Build Graph Using Time-Aware KDTree\r\nprint(\"Building KDTree and constructing edges...\")\r\n\r\ncoordinates = merged_nodes[['lat', 'lon', 'dayofyear']].values  \r\ntree = KDTree(coordinates)\r\n\r\n# Graph Parameters\r\nk = 5  # Keeping k=5 as increasing it reduced accuracy\r\nspatial_threshold = 0.08  # Keeping it stable for better connections\r\ntime_weight = 0.02  # Balancing time influence\r\n\r\n# Define time-aware distance function\r\ndef time_aware_distance(p1, p2):\r\n    spatial_dist = np.linalg.norm(p1[:2] - p2[:2])  \r\n    time_dist = abs(p1[2] - p2[2]) * time_weight  \r\n    return spatial_dist + time_dist\r\n\r\n# Query k-NN\r\ndistances, neighbors = tree.query(coordinates, k=k+1)  \r\n\r\n# Convert to edge index format with distance filtering\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, k+1):  \r\n        if time_aware_distance(coordinates[i], coordinates[neighbors[i][j]]) < spatial_threshold:\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T\r\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\nprint(f\"Graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\r\n\r\n# Create PyTorch Geometric Graph Object\r\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n\r\n# Validate Graph Before Saving\r\nprint(\"\\nGraph Data Summary:\")\r\nprint(graph_data)\r\nprint(f\"Nodes: {graph_data.num_nodes}\")\r\nprint(f\"Edges: {graph_data.num_edges}\")\r\nprint(f\"Feature shape: {graph_data.x.shape}\")\r\nprint(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n\r\n# Compute Degree Statistics\r\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\r\nprint(\"\\nDegree Stats:\")\r\nprint(f\"- Min degree: {deg.min().item()}\")\r\nprint(f\"- Max degree: {deg.max().item()}\")\r\nprint(f\"- Mean degree: {deg.float().mean().item()}\")\r\nprint(f\"- Standard deviation: {deg.float().std().item()}\")\r\n\r\n# Count Isolated Nodes\r\nnum_isolated_nodes = (deg == 0).sum().item()\r\npercentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\r\nprint(f\"\\nIsolated nodes: {num_isolated_nodes} ({percentage_isolated:.4f}%)\")\r\n\r\n# Save the Graph\r\ngraph_data_output_path = \"/media/volume1/gnn_timeaware.pt\"\r\ntorch.save(graph_data, graph_data_output_path)\r\n\r\nprint(f\"\\nPyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\r\n",
  "history_output" : "Loading dataset...\nFiltered dataset loaded with 620662 rows and 16 columns.\nFound 2431 duplicate rows.\nRemoved duplicates. New dataset size: 618231 rows.\nMerging nodes to reduce graph size...\nMerged nodes: 6815 from 618231 original records.\nNormalizing features using StandardScaler...\nNormalization completed. Node feature shape: torch.Size([6815, 11])\nBuilding KDTree and constructing edges...\nGraph constructed with 6815 nodes and 4989 edges.\nGraph Data Summary:\nData(x=[6815, 11], edge_index=[2, 4989], y=[6815])\nNodes: 6815\nEdges: 4989\nFeature shape: torch.Size([6815, 11])\nEdge index shape: torch.Size([2, 4989])\nDegree Stats:\n- Min degree: 0.0\n- Max degree: 5.0\n- Mean degree: 0.7320616245269775\n- Standard deviation: 0.9417075514793396\nIsolated nodes: 3559 (52.2230%)\nPyTorch Geometric Graph Data saved at: /media/volume1/gnn_timeaware.pt\n",
  "history_begin_time" : 1741581949217,
  "history_end_time" : 1741581983257,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "wU01KHNFyx32",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom torch_geometric.utils import degree, to_undirected, add_self_loops\r\n\r\n# Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\"Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\n\r\ndf_list = []\r\ntotal_rows = 0\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, parse_dates=['date'], chunksize=chunksize):\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear  \r\n    total_rows +=len(chunk)\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Total rows in raw dataset: {total_rows}\")\r\nprint(f\" Rows remaining after lat/lon filtering: {data.shape[0]}\")\r\n\r\n# Remove duplicates\r\ndata = data.drop_duplicates()\r\n\r\n# Merge records for the same station on the same day\r\ndata = data.groupby(['lat', 'lon', 'date'], as_index=False).mean()\r\ndata['dayofyear'] = pd.to_datetime(data['date']).dt.dayofyear\r\n\r\n# Merging nodes to reduce graph size\r\ngrid_size = 0.01  # Spatial binning granularity\r\ntime_bins = 10    # Temporal binning granularity\r\n\r\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\ndata['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\n\r\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\n# Aggregate by grid_id\r\nmerged_nodes = data.groupby('grid_id').agg({\r\n    'Elevation': 'mean', 'Slope': 'mean', 'SWE': 'mean', 'snow_depth': 'mean',\r\n    'air_temperature_observed_f': 'mean', 'precipitation_amount': 'mean',\r\n    'relative_humidity_rmin': 'mean', 'relative_humidity_rmax': 'mean', 'wind_speed': 'mean',\r\n    'cumulative_SWE': 'mean', 'cumulative_precipitation_amount': 'mean', 'swe_value': 'mean',\r\n    'lat': 'mean', 'lon': 'mean', 'dayofyear': 'mean'\r\n}).reset_index()\r\n\r\n# Normalize node features\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\n\r\nscaler = StandardScaler()\r\nmerged_nodes[feature_columns] = scaler.fit_transform(merged_nodes[feature_columns].values)\r\n\r\n# Convert to tensor\r\nnode_features = torch.tensor(merged_nodes[feature_columns].values, dtype=torch.float)\r\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\n\r\n# Adjust graph construction parameters\r\nspatial_threshold = 0.20  # Adjusted for optimal connectivity\r\ntime_weight = 0.07       # Slightly increased\r\navg_degree = max(5, min(10, int(len(merged_nodes) ** 0.5 / 10)))\r\n\r\ndef time_aware_distance(p1, p2):\r\n    spatial_dist = np.linalg.norm(p1[:2] - p2[:2])  \r\n    time_dist = abs(p1[2] - p2[2]) * time_weight  \r\n    return spatial_dist + time_dist\r\n\r\n# Build graph\r\ncoordinates = merged_nodes[['lat', 'lon', 'dayofyear']].values  \r\ntree = KDTree(coordinates)\r\n\r\ndistances, neighbors = tree.query(coordinates, k=avg_degree+1)\r\n\r\nedge_list = []\r\nedge_weights = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, avg_degree+1):\r\n        p1, p2 = coordinates[i], coordinates[neighbors[i][j]]\r\n        time_diff = abs(p1[2] - p2[2]) / 365  # Normalize by year\r\n        spatial_dist = np.linalg.norm(p1[:2] - p2[:2])  # Euclidean dist\r\n        \r\n        # Compute edge weight\r\n        weight = 1 / (1 + 0.5 * time_diff + spatial_dist)\r\n        \r\n        edge_list.append((i, neighbors[i][j]))\r\n        edge_weights.append(weight)\r\n        \r\nedges = np.array(edge_list).T\r\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\nedge_attr = torch.tensor(edge_weights, dtype=torch.float)\r\n\r\n# Ensure bidirectional edges\r\nedge_index = to_undirected(edge_index)\r\nedge_attr = torch.cat([edge_attr, edge_attr])\r\n\r\n# Save graph\r\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\ntorch.save(graph_data, \"/media/volume1/gnn_timeaware8.pt\")\r\n\r\nprint(f\"✅ Updated Graph Stats: Nodes = {graph_data.num_nodes}, Edges = {graph_data.num_edges}\")\r\n",
  "history_output" : "Loading dataset...\nTotal rows in raw dataset: 6155644\n Rows remaining after lat/lon filtering: 620662\n✅ Updated Graph Stats: Nodes = 7375, Edges = 77262\n",
  "history_begin_time" : 1741580604072,
  "history_end_time" : 1741580638500,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "RbRTvHfcN48j",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom torch_geometric.utils import degree, to_undirected, add_self_loops\r\n\r\n# Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\"Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\n\r\ndf_list = []\r\ntotal_rows = 0\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, parse_dates=['date'], chunksize=chunksize):\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear  \r\n    total_rows +=len(chunk)\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Total rows in raw dataset: {total_rows}\")\r\nprint(f\" Rows remaining after lat/lon filtering: {data.shape[0]}\")\r\n\r\n# Check for missing values in critical columns\r\nmissing_values = data[['date', 'lat', 'lon', 'swe_value']].isnull().sum()\r\nprint(f\" Missing values count per column:\\n{missing_values}\")\r\n\r\n# Remove duplicates\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\ndata = data.drop_duplicates()\r\nprint(f\" Dataset size after deduplication: {data.shape[0]}\")\r\n\r\n# Check duplicates based only on ('lat', 'lon', 'date')\r\nlocation_time_duplicates = data.duplicated(subset=['lat', 'lon', 'date']).sum()\r\nprint(f\"🔍 Possible duplicates based on same location & date: {location_time_duplicates}\")\r\n\r\n# Check duplicates based on ('lat', 'lon') only (ignoring time)\r\nlocation_duplicates = data.duplicated(subset=['lat', 'lon']).sum()\r\nprint(f\"🔍 Possible duplicates based on same location (ignoring time): {location_duplicates}\")\r\n\r\n# Check for small variations in numerical values (rounding to 3 decimals)\r\nnumerical_cols = ['Elevation', 'Slope', 'SWE', 'snow_depth',\r\n                  'air_temperature_observed_f', 'precipitation_amount',\r\n                  'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n                  'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value']\r\n\r\n# Ensure we only use columns that exist in the dataframe\r\nexisting_numerical_cols = list(set(numerical_cols) & set(data.columns))\r\n\r\n# Apply rounding to check for floating-point duplicates\r\nrounded_data = data.copy()\r\nrounded_data[existing_numerical_cols] = rounded_data[existing_numerical_cols].round(3)\r\nnumerical_variation_duplicates = rounded_data.duplicated().sum()\r\n\r\nprint(f\"🔍 Possible duplicates with small numerical variations (rounded to 3 decimals): {numerical_variation_duplicates}\")\r\n\r\n#  Merge records with the same station on the same day (average values)\r\nprint(\"🔄 Merging records for the same station on the same day...\")\r\nmerge_columns = ['Elevation', 'Slope', 'SWE', 'snow_depth',\r\n                 'air_temperature_observed_f', 'precipitation_amount',\r\n                 'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n                 'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value']\r\n\r\ndata = data.groupby(['lat', 'lon', 'date'], as_index=False)[merge_columns].mean()\r\ndata['dayofyear'] = pd.to_datetime(data['date']).dt.dayofyear\r\nprint(f\"✅ Merged dataset size (after fixing same-location duplicates): {data.shape[0]}\")\r\nprint(f\"✅ Restored `dayofyear`. Sample values:\\n{data[['date', 'dayofyear']].head()}\")\r\n\r\n# Fix 2️⃣: Round numerical values to avoid float precision issues\r\nprint(\"🔄 Rounding numerical values to 3 decimals...\")\r\nnumerical_cols = list(set(merge_columns) & set(data.columns))  # Ensure existing columns only\r\ndata[numerical_cols] = data[numerical_cols].round(3)\r\n\r\n# Check duplicates again after merging & rounding\r\nfinal_duplicates = data.duplicated().sum()\r\nprint(f\"✅ Final duplicate count after fixes: {final_duplicates}\")\r\n\r\nprint(\"🔄 Merging nodes to reduce graph size...\")\r\n\r\ngrid_size = 0.015  # Spatial binning granularity\r\ntime_bins = 7    # Temporal binning granularity\r\n\r\n# Assign bins for lat/lon\r\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\ndata['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\n\r\n# Create unique spatial-temporal ID\r\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\n# Print unique bin counts\r\nprint(f\"✅ Unique Latitude Bins: {data['lat_bin'].nunique()}\")\r\nprint(f\"✅ Unique Longitude Bins: {data['lon_bin'].nunique()}\")\r\nprint(f\"✅ Unique Time Bins: {data['time_bin'].nunique()}\")\r\nprint(f\"✅ Unique Spatial-Temporal IDs: {data['grid_id'].nunique()}\")\r\n\r\n# Aggregate by grid_id\r\nmerged_nodes = data.groupby('grid_id').agg({\r\n    'Elevation': 'mean', 'Slope': 'mean', 'SWE': 'mean', 'snow_depth': 'mean',\r\n    'air_temperature_observed_f': 'mean', 'precipitation_amount': 'mean',\r\n    'relative_humidity_rmin': 'mean', 'relative_humidity_rmax': 'mean', 'wind_speed': 'mean',\r\n    'cumulative_SWE': 'mean', 'cumulative_precipitation_amount': 'mean', 'swe_value': 'mean',\r\n    'lat': 'mean', 'lon': 'mean', 'dayofyear': 'mean'\r\n}).reset_index()\r\n\r\n# Print merging stats\r\nprint(f\"✅ Merged nodes: {merged_nodes.shape[0]} from {data.shape[0]} original records.\")\r\n\r\nprint(\"🔄 Normalizing features using StandardScaler...\")\r\n\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\n\r\n# Ensure no missing values before normalization\r\nmerged_nodes[feature_columns] = merged_nodes[feature_columns].fillna(merged_nodes[feature_columns].median())\r\n\r\n# Print stats before normalization\r\nprint(\"📊 Feature statistics BEFORE normalization:\")\r\nprint(merged_nodes[feature_columns].describe())\r\n\r\n# Normalize node features\r\nscaler = StandardScaler()\r\nmerged_nodes[feature_columns] = scaler.fit_transform(merged_nodes[feature_columns].values)\r\n\r\n# Convert to tensor\r\nnode_features = torch.tensor(merged_nodes[feature_columns].values, dtype=torch.float)\r\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\n\r\n# Print stats after normalization\r\nprint(\"📊 Feature statistics AFTER normalization:\")\r\nprint(pd.DataFrame(node_features.numpy(), columns=feature_columns).describe())\r\n\r\nprint(f\"✅ Normalization completed. Node feature shape: {node_features.shape}\")\r\n\r\n# Increase spatial threshold slightly to reduce isolated nodes naturally\r\nspatial_threshold = 0.25  # Increased from 0.15\r\ntime_weight = 0.07       # Slightly increased\r\navg_degree = max(5, min(10, int(len(merged_nodes) ** 0.5 / 10)))\r\n\r\ndef time_aware_distance(p1, p2):\r\n    spatial_dist = np.linalg.norm(p1[:2] - p2[:2])  \r\n    time_dist = abs(p1[2] - p2[2]) * time_weight  \r\n    return spatial_dist + time_dist\r\n\r\nprint(\"🔄 Rebuilding KDTree and constructing edges...\")\r\n\r\ncoordinates = merged_nodes[['lat', 'lon', 'dayofyear']].values  \r\ntree = KDTree(coordinates)\r\n\r\ndistances, neighbors = tree.query(coordinates, k=avg_degree+1)\r\n\r\nedge_list = []\r\nedge_weights = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, avg_degree+1):\r\n        p1, p2 = coordinates[i], coordinates[neighbors[i][j]]\r\n        time_diff = abs(p1[2] - p2[2]) / 365  # Normalize by year\r\n        spatial_dist = np.linalg.norm(p1[:2] - p2[:2])  # Euclidean dist\r\n        \r\n        #  Compute edge weight\r\n        weight = 1 / (1 + time_diff + spatial_dist)\r\n        \r\n        edge_list.append((i, neighbors[i][j]))\r\n        edge_weights.append(weight)\r\nedges = np.array(edge_list).T\r\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\nedge_attr = torch.tensor(edge_weights, dtype=torch.float)\r\n# Ensure bidirectional edges\r\nedge_index = to_undirected(edge_index)\r\nedge_attr = torch.cat([edge_attr, edge_attr])\r\n# Recheck isolated nodes\r\ndeg = degree(edge_index[0], num_nodes=merged_nodes.shape[0])\r\nnum_isolated_nodes = (deg == 0).sum().item()\r\n\r\nprint(f\"✅ Isolated Nodes after increasing spatial threshold: {num_isolated_nodes} ({(num_isolated_nodes / merged_nodes.shape[0]) * 100:.2f}%)\")\r\n\r\n# Only add self-loops if isolated nodes remain\r\nif num_isolated_nodes > 0:\r\n    print(\"🔄 Adding self-loops to remaining isolated nodes...\")\r\n    edge_index, _ = add_self_loops(edge_index, num_nodes=merged_nodes.shape[0])\r\n\r\n# Final isolated node check\r\ndeg = degree(edge_index[0], num_nodes=merged_nodes.shape[0])\r\nnum_isolated_nodes = (deg == 0).sum().item()\r\nprint(f\"✅ Final Isolated Nodes: {num_isolated_nodes} ({(num_isolated_nodes / merged_nodes.shape[0]) * 100:.2f}%)\")\r\n\r\n\r\n# Save graph\r\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\ntorch.save(graph_data, \"/media/volume1/gnn_timeaware7.pt\")\r\n\r\nprint(\"✅ Graph successfully saved.\")\r\n\r\ng_path = \"/media/volume1/gnn_timeaware7.pt\"\r\ng_data = torch.load(g_path)\r\nprint(f\"📌 Updated Graph Stats: Nodes = {g_data.num_nodes}, Edges = {g_data.num_edges}\")\r\n\r\n",
  "history_output" : "Loading dataset...\n",
  "history_begin_time" : 1741579573108,
  "history_end_time" : 1741579599854,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "dJtkScDqJefE",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom torch_geometric.utils import degree, to_undirected, add_self_loops\r\n\r\n# Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\"Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\n\r\ndf_list = []\r\ntotal_rows = 0\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, parse_dates=['date'], chunksize=chunksize):\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear  \r\n    total_rows +=len(chunk)\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Total rows in raw dataset: {total_rows}\")\r\nprint(f\" Rows remaining after lat/lon filtering: {data.shape[0]}\")\r\n\r\n# Check for missing values in critical columns\r\nmissing_values = data[['date', 'lat', 'lon', 'swe_value']].isnull().sum()\r\nprint(f\" Missing values count per column:\\n{missing_values}\")\r\n\r\n# Remove duplicates\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\ndata = data.drop_duplicates()\r\nprint(f\" Dataset size after deduplication: {data.shape[0]}\")\r\n\r\n# Check duplicates based only on ('lat', 'lon', 'date')\r\nlocation_time_duplicates = data.duplicated(subset=['lat', 'lon', 'date']).sum()\r\nprint(f\"🔍 Possible duplicates based on same location & date: {location_time_duplicates}\")\r\n\r\n# Check duplicates based on ('lat', 'lon') only (ignoring time)\r\nlocation_duplicates = data.duplicated(subset=['lat', 'lon']).sum()\r\nprint(f\"🔍 Possible duplicates based on same location (ignoring time): {location_duplicates}\")\r\n\r\n# Check for small variations in numerical values (rounding to 3 decimals)\r\nnumerical_cols = ['Elevation', 'Slope', 'SWE', 'snow_depth',\r\n                  'air_temperature_observed_f', 'precipitation_amount',\r\n                  'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n                  'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value']\r\n\r\n# Ensure we only use columns that exist in the dataframe\r\nexisting_numerical_cols = list(set(numerical_cols) & set(data.columns))\r\n\r\n# Apply rounding to check for floating-point duplicates\r\nrounded_data = data.copy()\r\nrounded_data[existing_numerical_cols] = rounded_data[existing_numerical_cols].round(3)\r\nnumerical_variation_duplicates = rounded_data.duplicated().sum()\r\n\r\nprint(f\"🔍 Possible duplicates with small numerical variations (rounded to 3 decimals): {numerical_variation_duplicates}\")\r\n\r\n#  Merge records with the same station on the same day (average values)\r\nprint(\"🔄 Merging records for the same station on the same day...\")\r\nmerge_columns = ['Elevation', 'Slope', 'SWE', 'snow_depth',\r\n                 'air_temperature_observed_f', 'precipitation_amount',\r\n                 'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n                 'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value']\r\n\r\ndata = data.groupby(['lat', 'lon', 'date'], as_index=False)[merge_columns].mean()\r\ndata['dayofyear'] = pd.to_datetime(data['date']).dt.dayofyear\r\nprint(f\"✅ Merged dataset size (after fixing same-location duplicates): {data.shape[0]}\")\r\nprint(f\"✅ Restored `dayofyear`. Sample values:\\n{data[['date', 'dayofyear']].head()}\")\r\n\r\n# Fix 2️⃣: Round numerical values to avoid float precision issues\r\nprint(\"🔄 Rounding numerical values to 3 decimals...\")\r\nnumerical_cols = list(set(merge_columns) & set(data.columns))  # Ensure existing columns only\r\ndata[numerical_cols] = data[numerical_cols].round(3)\r\n\r\n# Check duplicates again after merging & rounding\r\nfinal_duplicates = data.duplicated().sum()\r\nprint(f\"✅ Final duplicate count after fixes: {final_duplicates}\")\r\n\r\nprint(\"🔄 Merging nodes to reduce graph size...\")\r\n\r\ngrid_size = 0.015  # Spatial binning granularity\r\ntime_bins = 7    # Temporal binning granularity\r\n\r\n# Assign bins for lat/lon\r\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\ndata['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\n\r\n# Create unique spatial-temporal ID\r\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\n# Print unique bin counts\r\nprint(f\"✅ Unique Latitude Bins: {data['lat_bin'].nunique()}\")\r\nprint(f\"✅ Unique Longitude Bins: {data['lon_bin'].nunique()}\")\r\nprint(f\"✅ Unique Time Bins: {data['time_bin'].nunique()}\")\r\nprint(f\"✅ Unique Spatial-Temporal IDs: {data['grid_id'].nunique()}\")\r\n\r\n# Aggregate by grid_id\r\nmerged_nodes = data.groupby('grid_id').agg({\r\n    'Elevation': 'mean', 'Slope': 'mean', 'SWE': 'mean', 'snow_depth': 'mean',\r\n    'air_temperature_observed_f': 'mean', 'precipitation_amount': 'mean',\r\n    'relative_humidity_rmin': 'mean', 'relative_humidity_rmax': 'mean', 'wind_speed': 'mean',\r\n    'cumulative_SWE': 'mean', 'cumulative_precipitation_amount': 'mean', 'swe_value': 'mean',\r\n    'lat': 'mean', 'lon': 'mean', 'dayofyear': 'mean'\r\n}).reset_index()\r\n\r\n# Print merging stats\r\nprint(f\"✅ Merged nodes: {merged_nodes.shape[0]} from {data.shape[0]} original records.\")\r\n\r\nprint(\"🔄 Normalizing features using StandardScaler...\")\r\n\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\n\r\n# Ensure no missing values before normalization\r\nmerged_nodes[feature_columns] = merged_nodes[feature_columns].fillna(merged_nodes[feature_columns].median())\r\n\r\n# Print stats before normalization\r\nprint(\"📊 Feature statistics BEFORE normalization:\")\r\nprint(merged_nodes[feature_columns].describe())\r\n\r\n# Normalize node features\r\nscaler = StandardScaler()\r\nmerged_nodes[feature_columns] = scaler.fit_transform(merged_nodes[feature_columns].values)\r\n\r\n# Convert to tensor\r\nnode_features = torch.tensor(merged_nodes[feature_columns].values, dtype=torch.float)\r\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\n\r\n# Print stats after normalization\r\nprint(\"📊 Feature statistics AFTER normalization:\")\r\nprint(pd.DataFrame(node_features.numpy(), columns=feature_columns).describe())\r\n\r\nprint(f\"✅ Normalization completed. Node feature shape: {node_features.shape}\")\r\n\r\n# Increase spatial threshold slightly to reduce isolated nodes naturally\r\nspatial_threshold = 0.25  # Increased from 0.15\r\ntime_weight = 0.07       # Slightly increased\r\navg_degree = max(5, min(10, int(len(merged_nodes) ** 0.5 / 10)))\r\n\r\ndef time_aware_distance(p1, p2):\r\n    spatial_dist = np.linalg.norm(p1[:2] - p2[:2])  \r\n    time_dist = abs(p1[2] - p2[2]) * time_weight  \r\n    return spatial_dist + time_dist\r\n\r\nprint(\"🔄 Rebuilding KDTree and constructing edges...\")\r\n\r\ncoordinates = merged_nodes[['lat', 'lon', 'dayofyear']].values  \r\ntree = KDTree(coordinates)\r\n\r\ndistances, neighbors = tree.query(coordinates, k=avg_degree+1)\r\n\r\nedge_list = []\r\nedge_weights = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, avg_degree+1):\r\n        p1, p2 = coordinates[i], coordinates[neighbors[i][j]]\r\n        time_diff = abs(p1[2] - p2[2]) / 365  # Normalize by year\r\n        spatial_dist = np.linalg.norm(p1[:2] - p2[:2])  # Euclidean dist\r\n        \r\n        #  Compute edge weight\r\n        weight = 1 / (1 + time_diff + spatial_dist)\r\n        \r\n        edge_list.append((i, neighbors[i][j]))\r\n        edge_weights.append(weight)\r\nedges = np.array(edge_list).T\r\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\nedge_attr = torch.tensor(edge_weights, dtype=torch.float)\r\n# Ensure bidirectional edges\r\nedge_index = to_undirected(edge_index)\r\nedge_attr = torch.cat([edge_attr, edge_attr])\r\n# Recheck isolated nodes\r\ndeg = degree(edge_index[0], num_nodes=merged_nodes.shape[0])\r\nnum_isolated_nodes = (deg == 0).sum().item()\r\n\r\nprint(f\"✅ Isolated Nodes after increasing spatial threshold: {num_isolated_nodes} ({(num_isolated_nodes / merged_nodes.shape[0]) * 100:.2f}%)\")\r\n\r\n# Only add self-loops if isolated nodes remain\r\nif num_isolated_nodes > 0:\r\n    print(\"🔄 Adding self-loops to remaining isolated nodes...\")\r\n    edge_index, _ = add_self_loops(edge_index, num_nodes=merged_nodes.shape[0])\r\n\r\n# Final isolated node check\r\ndeg = degree(edge_index[0], num_nodes=merged_nodes.shape[0])\r\nnum_isolated_nodes = (deg == 0).sum().item()\r\nprint(f\"✅ Final Isolated Nodes: {num_isolated_nodes} ({(num_isolated_nodes / merged_nodes.shape[0]) * 100:.2f}%)\")\r\n\r\n\r\n# Save graph\r\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\ntorch.save(graph_data, \"/media/volume1/gnn_timeaware7.pt\")\r\n\r\nprint(\"✅ Graph successfully saved.\")\r\n",
  "history_output" : "Loading dataset...\nTotal rows in raw dataset: 6155644\n Rows remaining after lat/lon filtering: 620662\n Missing values count per column:\ndate         0\nlat          0\nlon          0\nswe_value    0\ndtype: int64\n Found 2431 duplicate rows.\n Dataset size after deduplication: 618231\n🔍 Possible duplicates based on same location & date: 7482\n🔍 Possible duplicates based on same location (ignoring time): 617460\n🔍 Possible duplicates with small numerical variations (rounded to 3 decimals): 229\n🔄 Merging records for the same station on the same day...\n✅ Merged dataset size (after fixing same-location duplicates): 610749\n✅ Restored `dayofyear`. Sample values:\n        date  dayofyear\n0 2018-10-03        276\n1 2018-10-05        278\n2 2018-10-07        280\n3 2018-10-11        284\n4 2018-10-12        285\n🔄 Rounding numerical values to 3 decimals...\n✅ Final duplicate count after fixes: 0\n🔄 Merging nodes to reduce graph size...\n✅ Unique Latitude Bins: 220\n✅ Unique Longitude Bins: 296\n✅ Unique Time Bins: 8\n✅ Unique Spatial-Temporal IDs: 4920\n✅ Merged nodes: 4920 from 610749 original records.\n🔄 Normalizing features using StandardScaler...\n📊 Feature statistics BEFORE normalization:\n         Elevation  ...  cumulative_precipitation_amount\ncount  4920.000000  ...                      4920.000000\nmean    674.823352  ...                         0.958055\nstd     691.493622  ...                         1.469437\nmin       0.000000  ...                        -1.000000\n25%       0.000000  ...                        -1.000000\n50%     416.331317  ...                         1.320110\n75%    1318.830250  ...                         2.156448\nmax    2721.439000  ...                         3.489622\n[8 rows x 11 columns]\n📊 Feature statistics AFTER normalization:\n          Elevation  ...  cumulative_precipitation_amount\ncount  4.920000e+03  ...                     4.920000e+03\nmean   6.202760e-09  ...                     1.550690e-09\nstd    1.000108e+00  ...                     1.000095e+00\nmin   -9.759915e-01  ...                    -1.332656e+00\n25%   -9.759915e-01  ...                    -1.332656e+00\n50%   -3.738549e-01  ...                     2.464152e-01\n75%    9.314220e-01  ...                     8.156283e-01\nmax    2.960004e+00  ...                     1.722989e+00\n[8 rows x 11 columns]\n✅ Normalization completed. Node feature shape: torch.Size([4920, 11])\n🔄 Rebuilding KDTree and constructing edges...\n✅ Isolated Nodes after increasing spatial threshold: 0 (0.00%)\n✅ Final Isolated Nodes: 0 (0.00%)\n✅ Graph successfully saved.\n",
  "history_begin_time" : 1741579205894,
  "history_end_time" : 1741579241416,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "SZU15WZCAwfd",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom torch_geometric.utils import degree, to_undirected\r\n\r\n# Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\"Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\n\r\ndf_list = []\r\ntotal_rows = 0\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, parse_dates=['date'], chunksize=chunksize):\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear  \r\n    total_rows +=len(chunk)\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Total rows in raw dataset: {total_rows}\")\r\nprint(f\" Rows remaining after lat/lon filtering: {data.shape[0]}\")\r\n\r\n# Check for missing values in critical columns\r\nmissing_values = data[['date', 'lat', 'lon', 'swe_value']].isnull().sum()\r\nprint(f\" Missing values count per column:\\n{missing_values}\")\r\n\r\n# Remove duplicates\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\ndata = data.drop_duplicates()\r\nprint(f\" Dataset size after deduplication: {data.shape[0]}\")\r\n\r\n# Check duplicates based only on ('lat', 'lon', 'date')\r\nlocation_time_duplicates = data.duplicated(subset=['lat', 'lon', 'date']).sum()\r\nprint(f\"🔍 Possible duplicates based on same location & date: {location_time_duplicates}\")\r\n\r\n# Check duplicates based on ('lat', 'lon') only (ignoring time)\r\nlocation_duplicates = data.duplicated(subset=['lat', 'lon']).sum()\r\nprint(f\"🔍 Possible duplicates based on same location (ignoring time): {location_duplicates}\")\r\n\r\n# Check for small variations in numerical values (rounding to 3 decimals)\r\nnumerical_cols = ['Elevation', 'Slope', 'SWE', 'snow_depth',\r\n                  'air_temperature_observed_f', 'precipitation_amount',\r\n                  'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n                  'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value']\r\n\r\n# Ensure we only use columns that exist in the dataframe\r\nexisting_numerical_cols = list(set(numerical_cols) & set(data.columns))\r\n\r\n# Apply rounding to check for floating-point duplicates\r\nrounded_data = data.copy()\r\nrounded_data[existing_numerical_cols] = rounded_data[existing_numerical_cols].round(3)\r\nnumerical_variation_duplicates = rounded_data.duplicated().sum()\r\n\r\nprint(f\"🔍 Possible duplicates with small numerical variations (rounded to 3 decimals): {numerical_variation_duplicates}\")\r\n\r\n#  Merge records with the same station on the same day (average values)\r\nprint(\"🔄 Merging records for the same station on the same day...\")\r\nmerge_columns = ['Elevation', 'Slope', 'SWE', 'snow_depth',\r\n                 'air_temperature_observed_f', 'precipitation_amount',\r\n                 'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n                 'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value']\r\n\r\ndata = data.groupby(['lat', 'lon', 'date'], as_index=False)[merge_columns].mean()\r\ndata['dayofyear'] = pd.to_datetime(data['date']).dt.dayofyear\r\nprint(f\"✅ Merged dataset size (after fixing same-location duplicates): {data.shape[0]}\")\r\nprint(f\"✅ Restored `dayofyear`. Sample values:\\n{data[['date', 'dayofyear']].head()}\")\r\n\r\n# Fix 2️⃣: Round numerical values to avoid float precision issues\r\nprint(\"🔄 Rounding numerical values to 3 decimals...\")\r\nnumerical_cols = list(set(merge_columns) & set(data.columns))  # Ensure existing columns only\r\ndata[numerical_cols] = data[numerical_cols].round(3)\r\n\r\n# Check duplicates again after merging & rounding\r\nfinal_duplicates = data.duplicated().sum()\r\nprint(f\"✅ Final duplicate count after fixes: {final_duplicates}\")\r\n\r\nprint(\"🔄 Merging nodes to reduce graph size...\")\r\n\r\ngrid_size = 0.01  # Spatial binning granularity\r\ntime_bins = 10    # Temporal binning granularity\r\n\r\n# Assign bins for lat/lon\r\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\ndata['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\n\r\n# Create unique spatial-temporal ID\r\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\n# Print unique bin counts\r\nprint(f\"✅ Unique Latitude Bins: {data['lat_bin'].nunique()}\")\r\nprint(f\"✅ Unique Longitude Bins: {data['lon_bin'].nunique()}\")\r\nprint(f\"✅ Unique Time Bins: {data['time_bin'].nunique()}\")\r\nprint(f\"✅ Unique Spatial-Temporal IDs: {data['grid_id'].nunique()}\")\r\n\r\n# Aggregate by grid_id\r\nmerged_nodes = data.groupby('grid_id').agg({\r\n    'Elevation': 'mean', 'Slope': 'mean', 'SWE': 'mean', 'snow_depth': 'mean',\r\n    'air_temperature_observed_f': 'mean', 'precipitation_amount': 'mean',\r\n    'relative_humidity_rmin': 'mean', 'relative_humidity_rmax': 'mean', 'wind_speed': 'mean',\r\n    'cumulative_SWE': 'mean', 'cumulative_precipitation_amount': 'mean', 'swe_value': 'mean',\r\n    'lat': 'mean', 'lon': 'mean', 'dayofyear': 'mean'\r\n}).reset_index()\r\n\r\n# Print merging stats\r\nprint(f\"✅ Merged nodes: {merged_nodes.shape[0]} from {data.shape[0]} original records.\")\r\n\r\nprint(\"🔄 Normalizing features using StandardScaler...\")\r\n\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\n\r\n# Ensure no missing values before normalization\r\nmerged_nodes[feature_columns] = merged_nodes[feature_columns].fillna(merged_nodes[feature_columns].median())\r\n\r\n# Print stats before normalization\r\nprint(\"📊 Feature statistics BEFORE normalization:\")\r\nprint(merged_nodes[feature_columns].describe())\r\n\r\n# Normalize node features\r\nscaler = StandardScaler()\r\nmerged_nodes[feature_columns] = scaler.fit_transform(merged_nodes[feature_columns].values)\r\n\r\n# Convert to tensor\r\nnode_features = torch.tensor(merged_nodes[feature_columns].values, dtype=torch.float)\r\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\n\r\n# Print stats after normalization\r\nprint(\"📊 Feature statistics AFTER normalization:\")\r\nprint(pd.DataFrame(node_features.numpy(), columns=feature_columns).describe())\r\n\r\nprint(f\"✅ Normalization completed. Node feature shape: {node_features.shape}\")\r\n\r\nprint(\"🔄 Building KDTree and constructing edges...\")\r\n\r\ncoordinates = merged_nodes[['lat', 'lon', 'dayofyear']].values  \r\ntree = KDTree(coordinates)\r\n\r\n# Adaptive k-value (between 5 and 10 based on graph density)\r\navg_degree = max(5, min(10, int(len(merged_nodes) ** 0.5 / 10)))\r\nspatial_threshold = 0.15\r\ntime_weight = 0.05  \r\n\r\ndef time_aware_distance(p1, p2):\r\n    spatial_dist = np.linalg.norm(p1[:2] - p2[:2])  \r\n    time_dist = abs(p1[2] - p2[2]) * time_weight  \r\n    return spatial_dist + time_dist\r\n\r\ndistances, neighbors = tree.query(coordinates, k=avg_degree+1)\r\n\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, avg_degree+1):\r\n        if time_aware_distance(coordinates[i], coordinates[neighbors[i][j]]) < spatial_threshold:\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T\r\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\n# Ensure bidirectional edges\r\nedge_index = to_undirected(edge_index)\r\n\r\nprint(f\"✅ Graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\r\n\r\n# Check for isolated nodes (nodes with 0 connections)\r\ndeg = degree(edge_index[0], num_nodes=merged_nodes.shape[0])\r\nnum_isolated_nodes = (deg == 0).sum().item()\r\nprint(f\"✅ Isolated Nodes: {num_isolated_nodes} ({(num_isolated_nodes / merged_nodes.shape[0]) * 100:.2f}%)\")\r\n\r\n# Save graph\r\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\ntorch.save(graph_data, \"/media/volume1/gnn_timeaware4.pt\")\r\n\r\nprint(\"✅ Graph successfully saved.\")\r\n",
  "history_output" : "Loading dataset...\nTotal rows in raw dataset: 6155644\n Rows remaining after lat/lon filtering: 620662\n Missing values count per column:\ndate         0\nlat          0\nlon          0\nswe_value    0\ndtype: int64\n Found 2431 duplicate rows.\n Dataset size after deduplication: 618231\n🔍 Possible duplicates based on same location & date: 7482\n🔍 Possible duplicates based on same location (ignoring time): 617460\n🔍 Possible duplicates with small numerical variations (rounded to 3 decimals): 229\n🔄 Merging records for the same station on the same day...\n✅ Merged dataset size (after fixing same-location duplicates): 610749\n✅ Restored `dayofyear`. Sample values:\n        date  dayofyear\n0 2018-10-03        276\n1 2018-10-05        278\n2 2018-10-07        280\n3 2018-10-11        284\n4 2018-10-12        285\n🔄 Rounding numerical values to 3 decimals...\n✅ Final duplicate count after fixes: 0\n🔄 Merging nodes to reduce graph size...\n✅ Unique Latitude Bins: 293\n✅ Unique Longitude Bins: 369\n✅ Unique Time Bins: 11\n✅ Unique Spatial-Temporal IDs: 7375\n✅ Merged nodes: 7375 from 610749 original records.\n🔄 Normalizing features using StandardScaler...\n📊 Feature statistics BEFORE normalization:\n         Elevation  ...  cumulative_precipitation_amount\ncount  7375.000000  ...                      7375.000000\nmean    718.756401  ...                         1.060205\nstd     699.109205  ...                         1.469682\nmin       0.000000  ...                        -1.000000\n25%       0.000000  ...                        -1.000000\n50%     532.372000  ...                         1.417670\n75%    1348.370000  ...                         2.273114\nmax    2721.439000  ...                         3.489204\n[8 rows x 11 columns]\n📊 Feature statistics AFTER normalization:\n          Elevation  ...  cumulative_precipitation_amount\ncount  7.375000e+03  ...                     7.375000e+03\nmean   8.275953e-09  ...                     6.206965e-09\nstd    1.000068e+00  ...                     1.000070e+00\nmin   -1.028173e+00  ...                    -1.401898e+00\n25%   -1.028173e+00  ...                    -1.401898e+00\n50%   -2.666208e-01  ...                     2.432426e-01\n75%    9.006552e-01  ...                     8.253428e-01\nmax    2.864815e+00  ...                     1.652850e+00\n[8 rows x 11 columns]\n✅ Normalization completed. Node feature shape: torch.Size([7375, 11])\n🔄 Building KDTree and constructing edges...\n✅ Graph constructed with 7375 nodes and 16420 edges.\n✅ Isolated Nodes: 1754 (23.78%)\n✅ Graph successfully saved.\n",
  "history_begin_time" : 1741564136008,
  "history_end_time" : 1741564172030,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "ycPc4fkNErRt",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom torch_geometric.utils import degree, to_undirected, add_self_loops\r\n\r\n# Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\"Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\n\r\ndf_list = []\r\ntotal_rows = 0\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, parse_dates=['date'], chunksize=chunksize):\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear  \r\n    total_rows +=len(chunk)\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Total rows in raw dataset: {total_rows}\")\r\nprint(f\" Rows remaining after lat/lon filtering: {data.shape[0]}\")\r\n\r\n# Check for missing values in critical columns\r\nmissing_values = data[['date', 'lat', 'lon', 'swe_value']].isnull().sum()\r\nprint(f\" Missing values count per column:\\n{missing_values}\")\r\n\r\n# Remove duplicates\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\ndata = data.drop_duplicates()\r\nprint(f\" Dataset size after deduplication: {data.shape[0]}\")\r\n\r\n# Check duplicates based only on ('lat', 'lon', 'date')\r\nlocation_time_duplicates = data.duplicated(subset=['lat', 'lon', 'date']).sum()\r\nprint(f\"🔍 Possible duplicates based on same location & date: {location_time_duplicates}\")\r\n\r\n# Check duplicates based on ('lat', 'lon') only (ignoring time)\r\nlocation_duplicates = data.duplicated(subset=['lat', 'lon']).sum()\r\nprint(f\"🔍 Possible duplicates based on same location (ignoring time): {location_duplicates}\")\r\n\r\n# Check for small variations in numerical values (rounding to 3 decimals)\r\nnumerical_cols = ['Elevation', 'Slope', 'SWE', 'snow_depth',\r\n                  'air_temperature_observed_f', 'precipitation_amount',\r\n                  'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n                  'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value']\r\n\r\n# Ensure we only use columns that exist in the dataframe\r\nexisting_numerical_cols = list(set(numerical_cols) & set(data.columns))\r\n\r\n# Apply rounding to check for floating-point duplicates\r\nrounded_data = data.copy()\r\nrounded_data[existing_numerical_cols] = rounded_data[existing_numerical_cols].round(3)\r\nnumerical_variation_duplicates = rounded_data.duplicated().sum()\r\n\r\nprint(f\"🔍 Possible duplicates with small numerical variations (rounded to 3 decimals): {numerical_variation_duplicates}\")\r\n\r\n#  Merge records with the same station on the same day (average values)\r\nprint(\"🔄 Merging records for the same station on the same day...\")\r\nmerge_columns = ['Elevation', 'Slope', 'SWE', 'snow_depth',\r\n                 'air_temperature_observed_f', 'precipitation_amount',\r\n                 'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n                 'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value']\r\n\r\ndata = data.groupby(['lat', 'lon', 'date'], as_index=False)[merge_columns].mean()\r\ndata['dayofyear'] = pd.to_datetime(data['date']).dt.dayofyear\r\nprint(f\"✅ Merged dataset size (after fixing same-location duplicates): {data.shape[0]}\")\r\nprint(f\"✅ Restored `dayofyear`. Sample values:\\n{data[['date', 'dayofyear']].head()}\")\r\n\r\n# Fix 2️⃣: Round numerical values to avoid float precision issues\r\nprint(\"🔄 Rounding numerical values to 3 decimals...\")\r\nnumerical_cols = list(set(merge_columns) & set(data.columns))  # Ensure existing columns only\r\ndata[numerical_cols] = data[numerical_cols].round(3)\r\n\r\n# Check duplicates again after merging & rounding\r\nfinal_duplicates = data.duplicated().sum()\r\nprint(f\"✅ Final duplicate count after fixes: {final_duplicates}\")\r\n\r\nprint(\"🔄 Merging nodes to reduce graph size...\")\r\n\r\ngrid_size = 0.01  # Spatial binning granularity\r\ntime_bins = 10    # Temporal binning granularity\r\n\r\n# Assign bins for lat/lon\r\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\ndata['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\n\r\n# Create unique spatial-temporal ID\r\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\n# Print unique bin counts\r\nprint(f\"✅ Unique Latitude Bins: {data['lat_bin'].nunique()}\")\r\nprint(f\"✅ Unique Longitude Bins: {data['lon_bin'].nunique()}\")\r\nprint(f\"✅ Unique Time Bins: {data['time_bin'].nunique()}\")\r\nprint(f\"✅ Unique Spatial-Temporal IDs: {data['grid_id'].nunique()}\")\r\n\r\n# Aggregate by grid_id\r\nmerged_nodes = data.groupby('grid_id').agg({\r\n    'Elevation': 'mean', 'Slope': 'mean', 'SWE': 'mean', 'snow_depth': 'mean',\r\n    'air_temperature_observed_f': 'mean', 'precipitation_amount': 'mean',\r\n    'relative_humidity_rmin': 'mean', 'relative_humidity_rmax': 'mean', 'wind_speed': 'mean',\r\n    'cumulative_SWE': 'mean', 'cumulative_precipitation_amount': 'mean', 'swe_value': 'mean',\r\n    'lat': 'mean', 'lon': 'mean', 'dayofyear': 'mean'\r\n}).reset_index()\r\n\r\n# Print merging stats\r\nprint(f\"✅ Merged nodes: {merged_nodes.shape[0]} from {data.shape[0]} original records.\")\r\n\r\nprint(\"🔄 Normalizing features using StandardScaler...\")\r\n\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\n\r\n# Ensure no missing values before normalization\r\nmerged_nodes[feature_columns] = merged_nodes[feature_columns].fillna(merged_nodes[feature_columns].median())\r\n\r\n# Print stats before normalization\r\nprint(\"📊 Feature statistics BEFORE normalization:\")\r\nprint(merged_nodes[feature_columns].describe())\r\n\r\n# Normalize node features\r\nscaler = StandardScaler()\r\nmerged_nodes[feature_columns] = scaler.fit_transform(merged_nodes[feature_columns].values)\r\n\r\n# Convert to tensor\r\nnode_features = torch.tensor(merged_nodes[feature_columns].values, dtype=torch.float)\r\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\n\r\n# Print stats after normalization\r\nprint(\"📊 Feature statistics AFTER normalization:\")\r\nprint(pd.DataFrame(node_features.numpy(), columns=feature_columns).describe())\r\n\r\nprint(f\"✅ Normalization completed. Node feature shape: {node_features.shape}\")\r\n\r\n# Increase spatial threshold slightly to reduce isolated nodes naturally\r\nspatial_threshold = 0.2  # Increased from 0.15\r\ntime_weight = 0.07       # Slightly increased\r\navg_degree = max(5, min(10, int(len(merged_nodes) ** 0.5 / 10)))\r\n\r\ndef time_aware_distance(p1, p2):\r\n    spatial_dist = np.linalg.norm(p1[:2] - p2[:2])  \r\n    time_dist = abs(p1[2] - p2[2]) * time_weight  \r\n    return spatial_dist + time_dist\r\n\r\nprint(\"🔄 Rebuilding KDTree and constructing edges...\")\r\n\r\ncoordinates = merged_nodes[['lat', 'lon', 'dayofyear']].values  \r\ntree = KDTree(coordinates)\r\n\r\ndistances, neighbors = tree.query(coordinates, k=avg_degree+1)\r\n\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, avg_degree+1):\r\n        if time_aware_distance(coordinates[i], coordinates[neighbors[i][j]]) < spatial_threshold:\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T\r\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\n# Ensure bidirectional edges\r\nedge_index = to_undirected(edge_index)\r\n\r\n# Recheck isolated nodes\r\ndeg = degree(edge_index[0], num_nodes=merged_nodes.shape[0])\r\nnum_isolated_nodes = (deg == 0).sum().item()\r\n\r\nprint(f\"✅ Isolated Nodes after increasing spatial threshold: {num_isolated_nodes} ({(num_isolated_nodes / merged_nodes.shape[0]) * 100:.2f}%)\")\r\n\r\n\r\n# Save graph\r\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\ntorch.save(graph_data, \"/media/volume1/gnn_timeaware3.pt\")\r\n\r\nprint(\"✅ Graph successfully saved.\")\r\n",
  "history_output" : "Loading dataset...\nTotal rows in raw dataset: 6155644\n Rows remaining after lat/lon filtering: 620662\n Missing values count per column:\ndate         0\nlat          0\nlon          0\nswe_value    0\ndtype: int64\n Found 2431 duplicate rows.\n Dataset size after deduplication: 618231\n🔍 Possible duplicates based on same location & date: 7482\n🔍 Possible duplicates based on same location (ignoring time): 617460\n🔍 Possible duplicates with small numerical variations (rounded to 3 decimals): 229\n🔄 Merging records for the same station on the same day...\n✅ Merged dataset size (after fixing same-location duplicates): 610749\n✅ Restored `dayofyear`. Sample values:\n        date  dayofyear\n0 2018-10-03        276\n1 2018-10-05        278\n2 2018-10-07        280\n3 2018-10-11        284\n4 2018-10-12        285\n🔄 Rounding numerical values to 3 decimals...\n✅ Final duplicate count after fixes: 0\n🔄 Merging nodes to reduce graph size...\n✅ Unique Latitude Bins: 293\n✅ Unique Longitude Bins: 369\n✅ Unique Time Bins: 11\n✅ Unique Spatial-Temporal IDs: 7375\n✅ Merged nodes: 7375 from 610749 original records.\n🔄 Normalizing features using StandardScaler...\n📊 Feature statistics BEFORE normalization:\n         Elevation  ...  cumulative_precipitation_amount\ncount  7375.000000  ...                      7375.000000\nmean    718.756401  ...                         1.060205\nstd     699.109205  ...                         1.469682\nmin       0.000000  ...                        -1.000000\n25%       0.000000  ...                        -1.000000\n50%     532.372000  ...                         1.417670\n75%    1348.370000  ...                         2.273114\nmax    2721.439000  ...                         3.489204\n[8 rows x 11 columns]\n📊 Feature statistics AFTER normalization:\n          Elevation  ...  cumulative_precipitation_amount\ncount  7.375000e+03  ...                     7.375000e+03\nmean   8.275953e-09  ...                     6.206965e-09\nstd    1.000068e+00  ...                     1.000070e+00\nmin   -1.028173e+00  ...                    -1.401898e+00\n25%   -1.028173e+00  ...                    -1.401898e+00\n50%   -2.666208e-01  ...                     2.432426e-01\n75%    9.006552e-01  ...                     8.253428e-01\nmax    2.864815e+00  ...                     1.652850e+00\n[8 rows x 11 columns]\n✅ Normalization completed. Node feature shape: torch.Size([7375, 11])\n🔄 Rebuilding KDTree and constructing edges...\n✅ Isolated Nodes after increasing spatial threshold: 1347 (18.26%)\n✅ Graph successfully saved.\n",
  "history_begin_time" : 1741563485275,
  "history_end_time" : 1741563520902,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "QapXQ9WxBVi1",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom torch_geometric.utils import degree, to_undirected, add_self_loops\r\n\r\n# Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\"Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\n\r\ndf_list = []\r\ntotal_rows = 0\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, parse_dates=['date'], chunksize=chunksize):\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear  \r\n    total_rows +=len(chunk)\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Total rows in raw dataset: {total_rows}\")\r\nprint(f\" Rows remaining after lat/lon filtering: {data.shape[0]}\")\r\n\r\n# Check for missing values in critical columns\r\nmissing_values = data[['date', 'lat', 'lon', 'swe_value']].isnull().sum()\r\nprint(f\" Missing values count per column:\\n{missing_values}\")\r\n\r\n# Remove duplicates\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\ndata = data.drop_duplicates()\r\nprint(f\" Dataset size after deduplication: {data.shape[0]}\")\r\n\r\n# Check duplicates based only on ('lat', 'lon', 'date')\r\nlocation_time_duplicates = data.duplicated(subset=['lat', 'lon', 'date']).sum()\r\nprint(f\"🔍 Possible duplicates based on same location & date: {location_time_duplicates}\")\r\n\r\n# Check duplicates based on ('lat', 'lon') only (ignoring time)\r\nlocation_duplicates = data.duplicated(subset=['lat', 'lon']).sum()\r\nprint(f\"🔍 Possible duplicates based on same location (ignoring time): {location_duplicates}\")\r\n\r\n# Check for small variations in numerical values (rounding to 3 decimals)\r\nnumerical_cols = ['Elevation', 'Slope', 'SWE', 'snow_depth',\r\n                  'air_temperature_observed_f', 'precipitation_amount',\r\n                  'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n                  'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value']\r\n\r\n# Ensure we only use columns that exist in the dataframe\r\nexisting_numerical_cols = list(set(numerical_cols) & set(data.columns))\r\n\r\n# Apply rounding to check for floating-point duplicates\r\nrounded_data = data.copy()\r\nrounded_data[existing_numerical_cols] = rounded_data[existing_numerical_cols].round(3)\r\nnumerical_variation_duplicates = rounded_data.duplicated().sum()\r\n\r\nprint(f\"🔍 Possible duplicates with small numerical variations (rounded to 3 decimals): {numerical_variation_duplicates}\")\r\n\r\n#  Merge records with the same station on the same day (average values)\r\nprint(\"🔄 Merging records for the same station on the same day...\")\r\nmerge_columns = ['Elevation', 'Slope', 'SWE', 'snow_depth',\r\n                 'air_temperature_observed_f', 'precipitation_amount',\r\n                 'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n                 'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value']\r\n\r\ndata = data.groupby(['lat', 'lon', 'date'], as_index=False)[merge_columns].mean()\r\ndata['dayofyear'] = pd.to_datetime(data['date']).dt.dayofyear\r\nprint(f\"✅ Merged dataset size (after fixing same-location duplicates): {data.shape[0]}\")\r\nprint(f\"✅ Restored `dayofyear`. Sample values:\\n{data[['date', 'dayofyear']].head()}\")\r\n\r\n# Fix 2️⃣: Round numerical values to avoid float precision issues\r\nprint(\"🔄 Rounding numerical values to 3 decimals...\")\r\nnumerical_cols = list(set(merge_columns) & set(data.columns))  # Ensure existing columns only\r\ndata[numerical_cols] = data[numerical_cols].round(3)\r\n\r\n# Check duplicates again after merging & rounding\r\nfinal_duplicates = data.duplicated().sum()\r\nprint(f\"✅ Final duplicate count after fixes: {final_duplicates}\")\r\n\r\nprint(\"🔄 Merging nodes to reduce graph size...\")\r\n\r\ngrid_size = 0.01  # Spatial binning granularity\r\ntime_bins = 10    # Temporal binning granularity\r\n\r\n# Assign bins for lat/lon\r\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\ndata['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\n\r\n# Create unique spatial-temporal ID\r\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\n# Print unique bin counts\r\nprint(f\"✅ Unique Latitude Bins: {data['lat_bin'].nunique()}\")\r\nprint(f\"✅ Unique Longitude Bins: {data['lon_bin'].nunique()}\")\r\nprint(f\"✅ Unique Time Bins: {data['time_bin'].nunique()}\")\r\nprint(f\"✅ Unique Spatial-Temporal IDs: {data['grid_id'].nunique()}\")\r\n\r\n# Aggregate by grid_id\r\nmerged_nodes = data.groupby('grid_id').agg({\r\n    'Elevation': 'mean', 'Slope': 'mean', 'SWE': 'mean', 'snow_depth': 'mean',\r\n    'air_temperature_observed_f': 'mean', 'precipitation_amount': 'mean',\r\n    'relative_humidity_rmin': 'mean', 'relative_humidity_rmax': 'mean', 'wind_speed': 'mean',\r\n    'cumulative_SWE': 'mean', 'cumulative_precipitation_amount': 'mean', 'swe_value': 'mean',\r\n    'lat': 'mean', 'lon': 'mean', 'dayofyear': 'mean'\r\n}).reset_index()\r\n\r\n# Print merging stats\r\nprint(f\"✅ Merged nodes: {merged_nodes.shape[0]} from {data.shape[0]} original records.\")\r\n\r\nprint(\"🔄 Normalizing features using StandardScaler...\")\r\n\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\n\r\n# Ensure no missing values before normalization\r\nmerged_nodes[feature_columns] = merged_nodes[feature_columns].fillna(merged_nodes[feature_columns].median())\r\n\r\n# Print stats before normalization\r\nprint(\"📊 Feature statistics BEFORE normalization:\")\r\nprint(merged_nodes[feature_columns].describe())\r\n\r\n# Normalize node features\r\nscaler = StandardScaler()\r\nmerged_nodes[feature_columns] = scaler.fit_transform(merged_nodes[feature_columns].values)\r\n\r\n# Convert to tensor\r\nnode_features = torch.tensor(merged_nodes[feature_columns].values, dtype=torch.float)\r\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\n\r\n# Print stats after normalization\r\nprint(\"📊 Feature statistics AFTER normalization:\")\r\nprint(pd.DataFrame(node_features.numpy(), columns=feature_columns).describe())\r\n\r\nprint(f\"✅ Normalization completed. Node feature shape: {node_features.shape}\")\r\n\r\n# Increase spatial threshold slightly to reduce isolated nodes naturally\r\nspatial_threshold = 0.2  # Increased from 0.15\r\ntime_weight = 0.07       # Slightly increased\r\navg_degree = max(5, min(10, int(len(merged_nodes) ** 0.5 / 10)))\r\n\r\ndef time_aware_distance(p1, p2):\r\n    spatial_dist = np.linalg.norm(p1[:2] - p2[:2])  \r\n    time_dist = abs(p1[2] - p2[2]) * time_weight  \r\n    return spatial_dist + time_dist\r\n\r\nprint(\"🔄 Rebuilding KDTree and constructing edges...\")\r\n\r\ncoordinates = merged_nodes[['lat', 'lon', 'dayofyear']].values  \r\ntree = KDTree(coordinates)\r\n\r\ndistances, neighbors = tree.query(coordinates, k=avg_degree+1)\r\n\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, avg_degree+1):\r\n        if time_aware_distance(coordinates[i], coordinates[neighbors[i][j]]) < spatial_threshold:\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T\r\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\n# Ensure bidirectional edges\r\nedge_index = to_undirected(edge_index)\r\n\r\n# Recheck isolated nodes\r\ndeg = degree(edge_index[0], num_nodes=merged_nodes.shape[0])\r\nnum_isolated_nodes = (deg == 0).sum().item()\r\n\r\nprint(f\"✅ Isolated Nodes after increasing spatial threshold: {num_isolated_nodes} ({(num_isolated_nodes / merged_nodes.shape[0]) * 100:.2f}%)\")\r\n\r\n\r\n# Save graph\r\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\ntorch.save(graph_data, \"/media/volume1/gnn_timeaware0.pt\")\r\n\r\nprint(\"✅ Graph successfully saved.\")\r\n",
  "history_output" : "Loading dataset...\nTotal rows in raw dataset: 6155644\n Rows remaining after lat/lon filtering: 620662\n Missing values count per column:\ndate         0\nlat          0\nlon          0\nswe_value    0\ndtype: int64\n Found 2431 duplicate rows.\n Dataset size after deduplication: 618231\n🔍 Possible duplicates based on same location & date: 7482\n🔍 Possible duplicates based on same location (ignoring time): 617460\n🔍 Possible duplicates with small numerical variations (rounded to 3 decimals): 229\n🔄 Merging records for the same station on the same day...\n✅ Merged dataset size (after fixing same-location duplicates): 610749\n✅ Restored `dayofyear`. Sample values:\n        date  dayofyear\n0 2018-10-03        276\n1 2018-10-05        278\n2 2018-10-07        280\n3 2018-10-11        284\n4 2018-10-12        285\n🔄 Rounding numerical values to 3 decimals...\n✅ Final duplicate count after fixes: 0\n🔄 Merging nodes to reduce graph size...\n✅ Unique Latitude Bins: 293\n✅ Unique Longitude Bins: 369\n✅ Unique Time Bins: 11\n✅ Unique Spatial-Temporal IDs: 7375\n✅ Merged nodes: 7375 from 610749 original records.\n🔄 Normalizing features using StandardScaler...\n📊 Feature statistics BEFORE normalization:\n         Elevation  ...  cumulative_precipitation_amount\ncount  7375.000000  ...                      7375.000000\nmean    718.756401  ...                         1.060205\nstd     699.109205  ...                         1.469682\nmin       0.000000  ...                        -1.000000\n25%       0.000000  ...                        -1.000000\n50%     532.372000  ...                         1.417670\n75%    1348.370000  ...                         2.273114\nmax    2721.439000  ...                         3.489204\n[8 rows x 11 columns]\n📊 Feature statistics AFTER normalization:\n          Elevation  ...  cumulative_precipitation_amount\ncount  7.375000e+03  ...                     7.375000e+03\nmean   8.275953e-09  ...                     6.206965e-09\nstd    1.000068e+00  ...                     1.000070e+00\nmin   -1.028173e+00  ...                    -1.401898e+00\n25%   -1.028173e+00  ...                    -1.401898e+00\n50%   -2.666208e-01  ...                     2.432426e-01\n75%    9.006552e-01  ...                     8.253428e-01\nmax    2.864815e+00  ...                     1.652850e+00\n[8 rows x 11 columns]\n✅ Normalization completed. Node feature shape: torch.Size([7375, 11])\n🔄 Rebuilding KDTree and constructing edges...\n✅ Isolated Nodes after increasing spatial threshold: 1347 (18.26%)\n✅ Graph successfully saved.\n",
  "history_begin_time" : 1741563330846,
  "history_end_time" : 1741563366636,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "MZ7O1q6cmwq1",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom torch_geometric.utils import degree, to_undirected, add_self_loops\r\n\r\n# Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\"Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\n\r\ndf_list = []\r\ntotal_rows = 0\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, parse_dates=['date'], chunksize=chunksize):\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear  \r\n    total_rows +=len(chunk)\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Total rows in raw dataset: {total_rows}\")\r\nprint(f\" Rows remaining after lat/lon filtering: {data.shape[0]}\")\r\n\r\n# Check for missing values in critical columns\r\nmissing_values = data[['date', 'lat', 'lon', 'swe_value']].isnull().sum()\r\nprint(f\" Missing values count per column:\\n{missing_values}\")\r\n\r\n# Remove duplicates\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\ndata = data.drop_duplicates()\r\nprint(f\" Dataset size after deduplication: {data.shape[0]}\")\r\n\r\n# Check duplicates based only on ('lat', 'lon', 'date')\r\nlocation_time_duplicates = data.duplicated(subset=['lat', 'lon', 'date']).sum()\r\nprint(f\"🔍 Possible duplicates based on same location & date: {location_time_duplicates}\")\r\n\r\n# Check duplicates based on ('lat', 'lon') only (ignoring time)\r\nlocation_duplicates = data.duplicated(subset=['lat', 'lon']).sum()\r\nprint(f\"🔍 Possible duplicates based on same location (ignoring time): {location_duplicates}\")\r\n\r\n# Check for small variations in numerical values (rounding to 3 decimals)\r\nnumerical_cols = ['Elevation', 'Slope', 'SWE', 'snow_depth',\r\n                  'air_temperature_observed_f', 'precipitation_amount',\r\n                  'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n                  'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value']\r\n\r\n# Ensure we only use columns that exist in the dataframe\r\nexisting_numerical_cols = list(set(numerical_cols) & set(data.columns))\r\n\r\n# Apply rounding to check for floating-point duplicates\r\nrounded_data = data.copy()\r\nrounded_data[existing_numerical_cols] = rounded_data[existing_numerical_cols].round(3)\r\nnumerical_variation_duplicates = rounded_data.duplicated().sum()\r\n\r\nprint(f\"🔍 Possible duplicates with small numerical variations (rounded to 3 decimals): {numerical_variation_duplicates}\")\r\n\r\n#  Merge records with the same station on the same day (average values)\r\nprint(\"🔄 Merging records for the same station on the same day...\")\r\nmerge_columns = ['Elevation', 'Slope', 'SWE', 'snow_depth',\r\n                 'air_temperature_observed_f', 'precipitation_amount',\r\n                 'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n                 'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value']\r\n\r\ndata = data.groupby(['lat', 'lon', 'date'], as_index=False)[merge_columns].mean()\r\ndata['dayofyear'] = pd.to_datetime(data['date']).dt.dayofyear\r\nprint(f\"✅ Merged dataset size (after fixing same-location duplicates): {data.shape[0]}\")\r\nprint(f\"✅ Restored `dayofyear`. Sample values:\\n{data[['date', 'dayofyear']].head()}\")\r\n\r\n# Fix 2️⃣: Round numerical values to avoid float precision issues\r\nprint(\"🔄 Rounding numerical values to 3 decimals...\")\r\nnumerical_cols = list(set(merge_columns) & set(data.columns))  # Ensure existing columns only\r\ndata[numerical_cols] = data[numerical_cols].round(3)\r\n\r\n# Check duplicates again after merging & rounding\r\nfinal_duplicates = data.duplicated().sum()\r\nprint(f\"✅ Final duplicate count after fixes: {final_duplicates}\")\r\n\r\nprint(\"🔄 Merging nodes to reduce graph size...\")\r\n\r\ngrid_size = 0.01  # Spatial binning granularity\r\ntime_bins = 10    # Temporal binning granularity\r\n\r\n# Assign bins for lat/lon\r\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\ndata['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\n\r\n# Create unique spatial-temporal ID\r\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\n# Print unique bin counts\r\nprint(f\"✅ Unique Latitude Bins: {data['lat_bin'].nunique()}\")\r\nprint(f\"✅ Unique Longitude Bins: {data['lon_bin'].nunique()}\")\r\nprint(f\"✅ Unique Time Bins: {data['time_bin'].nunique()}\")\r\nprint(f\"✅ Unique Spatial-Temporal IDs: {data['grid_id'].nunique()}\")\r\n\r\n# Aggregate by grid_id\r\nmerged_nodes = data.groupby('grid_id').agg({\r\n    'Elevation': 'mean', 'Slope': 'mean', 'SWE': 'mean', 'snow_depth': 'mean',\r\n    'air_temperature_observed_f': 'mean', 'precipitation_amount': 'mean',\r\n    'relative_humidity_rmin': 'mean', 'relative_humidity_rmax': 'mean', 'wind_speed': 'mean',\r\n    'cumulative_SWE': 'mean', 'cumulative_precipitation_amount': 'mean', 'swe_value': 'mean',\r\n    'lat': 'mean', 'lon': 'mean', 'dayofyear': 'mean'\r\n}).reset_index()\r\n\r\n# Print merging stats\r\nprint(f\"✅ Merged nodes: {merged_nodes.shape[0]} from {data.shape[0]} original records.\")\r\n\r\nprint(\"🔄 Normalizing features using StandardScaler...\")\r\n\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\n\r\n# Ensure no missing values before normalization\r\nmerged_nodes[feature_columns] = merged_nodes[feature_columns].fillna(merged_nodes[feature_columns].median())\r\n\r\n# Print stats before normalization\r\nprint(\"📊 Feature statistics BEFORE normalization:\")\r\nprint(merged_nodes[feature_columns].describe())\r\n\r\n# Normalize node features\r\nscaler = StandardScaler()\r\nmerged_nodes[feature_columns] = scaler.fit_transform(merged_nodes[feature_columns].values)\r\n\r\n# Convert to tensor\r\nnode_features = torch.tensor(merged_nodes[feature_columns].values, dtype=torch.float)\r\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\n\r\n# Print stats after normalization\r\nprint(\"📊 Feature statistics AFTER normalization:\")\r\nprint(pd.DataFrame(node_features.numpy(), columns=feature_columns).describe())\r\n\r\nprint(f\"✅ Normalization completed. Node feature shape: {node_features.shape}\")\r\n\r\n# Increase spatial threshold slightly to reduce isolated nodes naturally\r\nspatial_threshold = 0.2  # Increased from 0.15\r\ntime_weight = 0.07       # Slightly increased\r\navg_degree = max(5, min(10, int(len(merged_nodes) ** 0.5 / 10)))\r\n\r\ndef time_aware_distance(p1, p2):\r\n    spatial_dist = np.linalg.norm(p1[:2] - p2[:2])  \r\n    time_dist = abs(p1[2] - p2[2]) * time_weight  \r\n    return spatial_dist + time_dist\r\n\r\nprint(\"🔄 Rebuilding KDTree and constructing edges...\")\r\n\r\ncoordinates = merged_nodes[['lat', 'lon', 'dayofyear']].values  \r\ntree = KDTree(coordinates)\r\n\r\ndistances, neighbors = tree.query(coordinates, k=avg_degree+1)\r\n\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, avg_degree+1):\r\n        if time_aware_distance(coordinates[i], coordinates[neighbors[i][j]]) < spatial_threshold:\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T\r\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\n# Ensure bidirectional edges\r\nedge_index = to_undirected(edge_index)\r\n\r\n# Recheck isolated nodes\r\ndeg = degree(edge_index[0], num_nodes=merged_nodes.shape[0])\r\nnum_isolated_nodes = (deg == 0).sum().item()\r\n\r\nprint(f\"✅ Isolated Nodes after increasing spatial threshold: {num_isolated_nodes} ({(num_isolated_nodes / merged_nodes.shape[0]) * 100:.2f}%)\")\r\n\r\n# Only add self-loops if isolated nodes remain\r\nif num_isolated_nodes > 0:\r\n    print(\"🔄 Adding self-loops to remaining isolated nodes...\")\r\n    edge_index, _ = add_self_loops(edge_index, num_nodes=merged_nodes.shape[0])\r\n\r\n# Final isolated node check\r\ndeg = degree(edge_index[0], num_nodes=merged_nodes.shape[0])\r\nnum_isolated_nodes = (deg == 0).sum().item()\r\nprint(f\"✅ Final Isolated Nodes: {num_isolated_nodes} ({(num_isolated_nodes / merged_nodes.shape[0]) * 100:.2f}%)\")\r\n\r\n\r\n# Save graph\r\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\ntorch.save(graph_data, \"/media/volume1/gnn_timeaware0.pt\")\r\n\r\nprint(\"✅ Graph successfully saved.\")\r\n",
  "history_output" : "Loading dataset...\nTotal rows in raw dataset: 6155644\n Rows remaining after lat/lon filtering: 620662\n Missing values count per column:\ndate         0\nlat          0\nlon          0\nswe_value    0\ndtype: int64\n Found 2431 duplicate rows.\n Dataset size after deduplication: 618231\n🔍 Possible duplicates based on same location & date: 7482\n🔍 Possible duplicates based on same location (ignoring time): 617460\n🔍 Possible duplicates with small numerical variations (rounded to 3 decimals): 229\n🔄 Merging records for the same station on the same day...\n✅ Merged dataset size (after fixing same-location duplicates): 610749\n✅ Restored `dayofyear`. Sample values:\n        date  dayofyear\n0 2018-10-03        276\n1 2018-10-05        278\n2 2018-10-07        280\n3 2018-10-11        284\n4 2018-10-12        285\n🔄 Rounding numerical values to 3 decimals...\n✅ Final duplicate count after fixes: 0\n🔄 Merging nodes to reduce graph size...\n✅ Unique Latitude Bins: 293\n✅ Unique Longitude Bins: 369\n✅ Unique Time Bins: 11\n✅ Unique Spatial-Temporal IDs: 7375\n✅ Merged nodes: 7375 from 610749 original records.\n🔄 Normalizing features using StandardScaler...\n📊 Feature statistics BEFORE normalization:\n         Elevation  ...  cumulative_precipitation_amount\ncount  7375.000000  ...                      7375.000000\nmean    718.756401  ...                         1.060205\nstd     699.109205  ...                         1.469682\nmin       0.000000  ...                        -1.000000\n25%       0.000000  ...                        -1.000000\n50%     532.372000  ...                         1.417670\n75%    1348.370000  ...                         2.273114\nmax    2721.439000  ...                         3.489204\n[8 rows x 11 columns]\n📊 Feature statistics AFTER normalization:\n          Elevation  ...  cumulative_precipitation_amount\ncount  7.375000e+03  ...                     7.375000e+03\nmean   8.275953e-09  ...                     6.206965e-09\nstd    1.000068e+00  ...                     1.000070e+00\nmin   -1.028173e+00  ...                    -1.401898e+00\n25%   -1.028173e+00  ...                    -1.401898e+00\n50%   -2.666208e-01  ...                     2.432426e-01\n75%    9.006552e-01  ...                     8.253428e-01\nmax    2.864815e+00  ...                     1.652850e+00\n[8 rows x 11 columns]\n✅ Normalization completed. Node feature shape: torch.Size([7375, 11])\n🔄 Rebuilding KDTree and constructing edges...\n✅ Isolated Nodes after increasing spatial threshold: 1347 (18.26%)\n🔄 Adding self-loops to remaining isolated nodes...\n✅ Final Isolated Nodes: 0 (0.00%)\n✅ Graph successfully saved.\n",
  "history_begin_time" : 1741563130832,
  "history_end_time" : 1741563167602,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "LIGPXuhWCnH2",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom torch_geometric.utils import degree, to_undirected, add_self_loops\r\n\r\n# Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\"Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\n\r\ndf_list = []\r\ntotal_rows = 0\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, parse_dates=['date'], chunksize=chunksize):\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear  \r\n    total_rows +=len(chunk)\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Total rows in raw dataset: {total_rows}\")\r\nprint(f\" Rows remaining after lat/lon filtering: {data.shape[0]}\")\r\n\r\n# Check for missing values in critical columns\r\nmissing_values = data[['date', 'lat', 'lon', 'swe_value']].isnull().sum()\r\nprint(f\" Missing values count per column:\\n{missing_values}\")\r\n\r\n# Remove duplicates\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\ndata = data.drop_duplicates()\r\nprint(f\" Dataset size after deduplication: {data.shape[0]}\")\r\n\r\n# Check duplicates based only on ('lat', 'lon', 'date')\r\nlocation_time_duplicates = data.duplicated(subset=['lat', 'lon', 'date']).sum()\r\nprint(f\"🔍 Possible duplicates based on same location & date: {location_time_duplicates}\")\r\n\r\n# Check duplicates based on ('lat', 'lon') only (ignoring time)\r\nlocation_duplicates = data.duplicated(subset=['lat', 'lon']).sum()\r\nprint(f\"🔍 Possible duplicates based on same location (ignoring time): {location_duplicates}\")\r\n\r\n# Check for small variations in numerical values (rounding to 3 decimals)\r\nnumerical_cols = ['Elevation', 'Slope', 'SWE', 'snow_depth',\r\n                  'air_temperature_observed_f', 'precipitation_amount',\r\n                  'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n                  'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value']\r\n\r\n# Ensure we only use columns that exist in the dataframe\r\nexisting_numerical_cols = list(set(numerical_cols) & set(data.columns))\r\n\r\n# Apply rounding to check for floating-point duplicates\r\nrounded_data = data.copy()\r\nrounded_data[existing_numerical_cols] = rounded_data[existing_numerical_cols].round(3)\r\nnumerical_variation_duplicates = rounded_data.duplicated().sum()\r\n\r\nprint(f\"🔍 Possible duplicates with small numerical variations (rounded to 3 decimals): {numerical_variation_duplicates}\")\r\n\r\n#  Merge records with the same station on the same day (average values)\r\nprint(\"🔄 Merging records for the same station on the same day...\")\r\nmerge_columns = ['Elevation', 'Slope', 'SWE', 'snow_depth',\r\n                 'air_temperature_observed_f', 'precipitation_amount',\r\n                 'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n                 'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value']\r\n\r\ndata = data.groupby(['lat', 'lon', 'date'], as_index=False)[merge_columns].mean()\r\ndata['dayofyear'] = pd.to_datetime(data['date']).dt.dayofyear\r\nprint(f\"✅ Merged dataset size (after fixing same-location duplicates): {data.shape[0]}\")\r\nprint(f\"✅ Restored `dayofyear`. Sample values:\\n{data[['date', 'dayofyear']].head()}\")\r\n\r\n# Fix 2️⃣: Round numerical values to avoid float precision issues\r\nprint(\"🔄 Rounding numerical values to 3 decimals...\")\r\nnumerical_cols = list(set(merge_columns) & set(data.columns))  # Ensure existing columns only\r\ndata[numerical_cols] = data[numerical_cols].round(3)\r\n\r\n# Check duplicates again after merging & rounding\r\nfinal_duplicates = data.duplicated().sum()\r\nprint(f\"✅ Final duplicate count after fixes: {final_duplicates}\")\r\n\r\nprint(\"🔄 Merging nodes to reduce graph size...\")\r\n\r\ngrid_size = 0.01  # Spatial binning granularity\r\ntime_bins = 10    # Temporal binning granularity\r\n\r\n# Assign bins for lat/lon\r\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\ndata['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\n\r\n# Create unique spatial-temporal ID\r\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\n# Print unique bin counts\r\nprint(f\"✅ Unique Latitude Bins: {data['lat_bin'].nunique()}\")\r\nprint(f\"✅ Unique Longitude Bins: {data['lon_bin'].nunique()}\")\r\nprint(f\"✅ Unique Time Bins: {data['time_bin'].nunique()}\")\r\nprint(f\"✅ Unique Spatial-Temporal IDs: {data['grid_id'].nunique()}\")\r\n\r\n# Aggregate by grid_id\r\nmerged_nodes = data.groupby('grid_id').agg({\r\n    'Elevation': 'mean', 'Slope': 'mean', 'SWE': 'mean', 'snow_depth': 'mean',\r\n    'air_temperature_observed_f': 'mean', 'precipitation_amount': 'mean',\r\n    'relative_humidity_rmin': 'mean', 'relative_humidity_rmax': 'mean', 'wind_speed': 'mean',\r\n    'cumulative_SWE': 'mean', 'cumulative_precipitation_amount': 'mean', 'swe_value': 'mean',\r\n    'lat': 'mean', 'lon': 'mean', 'dayofyear': 'mean'\r\n}).reset_index()\r\n\r\n# Print merging stats\r\nprint(f\"✅ Merged nodes: {merged_nodes.shape[0]} from {data.shape[0]} original records.\")\r\n\r\nprint(\"🔄 Normalizing features using StandardScaler...\")\r\n\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\n\r\n# Ensure no missing values before normalization\r\nmerged_nodes[feature_columns] = merged_nodes[feature_columns].fillna(merged_nodes[feature_columns].median())\r\n\r\n# Print stats before normalization\r\nprint(\"📊 Feature statistics BEFORE normalization:\")\r\nprint(merged_nodes[feature_columns].describe())\r\n\r\n# Normalize node features\r\nscaler = StandardScaler()\r\nmerged_nodes[feature_columns] = scaler.fit_transform(merged_nodes[feature_columns].values)\r\n\r\n# Convert to tensor\r\nnode_features = torch.tensor(merged_nodes[feature_columns].values, dtype=torch.float)\r\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\n\r\n# Print stats after normalization\r\nprint(\"📊 Feature statistics AFTER normalization:\")\r\nprint(pd.DataFrame(node_features.numpy(), columns=feature_columns).describe())\r\n\r\nprint(f\"✅ Normalization completed. Node feature shape: {node_features.shape}\")\r\n\r\n# Increase spatial threshold slightly to reduce isolated nodes naturally\r\nspatial_threshold = 0.2  # Increased from 0.15\r\ntime_weight = 0.07       # Slightly increased\r\navg_degree = max(5, min(10, int(len(merged_nodes) ** 0.5 / 10)))\r\n\r\ndef time_aware_distance(p1, p2):\r\n    spatial_dist = np.linalg.norm(p1[:2] - p2[:2])  \r\n    time_dist = abs(p1[2] - p2[2]) * time_weight  \r\n    return spatial_dist + time_dist\r\n\r\nprint(\"🔄 Rebuilding KDTree and constructing edges...\")\r\n\r\ncoordinates = merged_nodes[['lat', 'lon', 'dayofyear']].values  \r\ntree = KDTree(coordinates)\r\n\r\ndistances, neighbors = tree.query(coordinates, k=avg_degree+1)\r\n\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, avg_degree+1):\r\n        if time_aware_distance(coordinates[i], coordinates[neighbors[i][j]]) < spatial_threshold:\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T\r\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\n# Ensure bidirectional edges\r\nedge_index = to_undirected(edge_index)\r\n\r\n# Recheck isolated nodes\r\ndeg = degree(edge_index[0], num_nodes=merged_nodes.shape[0])\r\nnum_isolated_nodes = (deg == 0).sum().item()\r\n\r\nprint(f\"✅ Isolated Nodes after increasing spatial threshold: {num_isolated_nodes} ({(num_isolated_nodes / merged_nodes.shape[0]) * 100:.2f}%)\")\r\n\r\n# Only add self-loops if isolated nodes remain\r\nif num_isolated_nodes > 0:\r\n    print(\"🔄 Adding self-loops to remaining isolated nodes...\")\r\n    edge_index, _ = add_self_loops(edge_index, num_nodes=merged_nodes.shape[0])\r\n\r\n# Final isolated node check\r\ndeg = degree(edge_index[0], num_nodes=merged_nodes.shape[0])\r\nnum_isolated_nodes = (deg == 0).sum().item()\r\nprint(f\"✅ Final Isolated Nodes: {num_isolated_nodes} ({(num_isolated_nodes / merged_nodes.shape[0]) * 100:.2f}%)\")\r\n\r\n\r\n# # Save graph\r\n# graph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n# torch.save(graph_data, \"/media/volume1/gnn_timeaware.pt\")\r\n\r\n# print(\"✅ Graph successfully saved.\")\r\n",
  "history_output" : "Loading dataset...\nTotal rows in raw dataset: 6155644\n Rows remaining after lat/lon filtering: 620662\n Missing values count per column:\ndate         0\nlat          0\nlon          0\nswe_value    0\ndtype: int64\n Found 2431 duplicate rows.\n Dataset size after deduplication: 618231\n🔍 Possible duplicates based on same location & date: 7482\n🔍 Possible duplicates based on same location (ignoring time): 617460\n🔍 Possible duplicates with small numerical variations (rounded to 3 decimals): 229\n🔄 Merging records for the same station on the same day...\n✅ Merged dataset size (after fixing same-location duplicates): 610749\n✅ Restored `dayofyear`. Sample values:\n        date  dayofyear\n0 2018-10-03        276\n1 2018-10-05        278\n2 2018-10-07        280\n3 2018-10-11        284\n4 2018-10-12        285\n🔄 Rounding numerical values to 3 decimals...\n✅ Final duplicate count after fixes: 0\n🔄 Merging nodes to reduce graph size...\n✅ Unique Latitude Bins: 293\n✅ Unique Longitude Bins: 369\n✅ Unique Time Bins: 11\n✅ Unique Spatial-Temporal IDs: 7375\n✅ Merged nodes: 7375 from 610749 original records.\n🔄 Normalizing features using StandardScaler...\n📊 Feature statistics BEFORE normalization:\n         Elevation  ...  cumulative_precipitation_amount\ncount  7375.000000  ...                      7375.000000\nmean    718.756401  ...                         1.060205\nstd     699.109205  ...                         1.469682\nmin       0.000000  ...                        -1.000000\n25%       0.000000  ...                        -1.000000\n50%     532.372000  ...                         1.417670\n75%    1348.370000  ...                         2.273114\nmax    2721.439000  ...                         3.489204\n[8 rows x 11 columns]\n📊 Feature statistics AFTER normalization:\n          Elevation  ...  cumulative_precipitation_amount\ncount  7.375000e+03  ...                     7.375000e+03\nmean   8.275953e-09  ...                     6.206965e-09\nstd    1.000068e+00  ...                     1.000070e+00\nmin   -1.028173e+00  ...                    -1.401898e+00\n25%   -1.028173e+00  ...                    -1.401898e+00\n50%   -2.666208e-01  ...                     2.432426e-01\n75%    9.006552e-01  ...                     8.253428e-01\nmax    2.864815e+00  ...                     1.652850e+00\n[8 rows x 11 columns]\n✅ Normalization completed. Node feature shape: torch.Size([7375, 11])\n🔄 Rebuilding KDTree and constructing edges...\n✅ Isolated Nodes after increasing spatial threshold: 1347 (18.26%)\n🔄 Adding self-loops to remaining isolated nodes...\n✅ Final Isolated Nodes: 0 (0.00%)\n",
  "history_begin_time" : 1741563021266,
  "history_end_time" : 1741563057065,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "cYLjxvOCcroT",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom torch_geometric.utils import degree, to_undirected\r\n\r\n# Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\"Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\n\r\ndf_list = []\r\ntotal_rows = 0\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, parse_dates=['date'], chunksize=chunksize):\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear  \r\n    total_rows +=len(chunk)\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Total rows in raw dataset: {total_rows}\")\r\nprint(f\" Rows remaining after lat/lon filtering: {data.shape[0]}\")\r\n\r\n# Check for missing values in critical columns\r\nmissing_values = data[['date', 'lat', 'lon', 'swe_value']].isnull().sum()\r\nprint(f\" Missing values count per column:\\n{missing_values}\")\r\n\r\n# Remove duplicates\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\ndata = data.drop_duplicates()\r\nprint(f\" Dataset size after deduplication: {data.shape[0]}\")\r\n\r\n# Check duplicates based only on ('lat', 'lon', 'date')\r\nlocation_time_duplicates = data.duplicated(subset=['lat', 'lon', 'date']).sum()\r\nprint(f\"🔍 Possible duplicates based on same location & date: {location_time_duplicates}\")\r\n\r\n# Check duplicates based on ('lat', 'lon') only (ignoring time)\r\nlocation_duplicates = data.duplicated(subset=['lat', 'lon']).sum()\r\nprint(f\"🔍 Possible duplicates based on same location (ignoring time): {location_duplicates}\")\r\n\r\n# Check for small variations in numerical values (rounding to 3 decimals)\r\nnumerical_cols = ['Elevation', 'Slope', 'SWE', 'snow_depth',\r\n                  'air_temperature_observed_f', 'precipitation_amount',\r\n                  'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n                  'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value']\r\n\r\n# Ensure we only use columns that exist in the dataframe\r\nexisting_numerical_cols = list(set(numerical_cols) & set(data.columns))\r\n\r\n# Apply rounding to check for floating-point duplicates\r\nrounded_data = data.copy()\r\nrounded_data[existing_numerical_cols] = rounded_data[existing_numerical_cols].round(3)\r\nnumerical_variation_duplicates = rounded_data.duplicated().sum()\r\n\r\nprint(f\"🔍 Possible duplicates with small numerical variations (rounded to 3 decimals): {numerical_variation_duplicates}\")\r\n\r\n#  Merge records with the same station on the same day (average values)\r\nprint(\"🔄 Merging records for the same station on the same day...\")\r\nmerge_columns = ['Elevation', 'Slope', 'SWE', 'snow_depth',\r\n                 'air_temperature_observed_f', 'precipitation_amount',\r\n                 'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n                 'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value']\r\n\r\ndata = data.groupby(['lat', 'lon', 'date'], as_index=False)[merge_columns].mean()\r\ndata['dayofyear'] = pd.to_datetime(data['date']).dt.dayofyear\r\nprint(f\"✅ Merged dataset size (after fixing same-location duplicates): {data.shape[0]}\")\r\nprint(f\"✅ Restored `dayofyear`. Sample values:\\n{data[['date', 'dayofyear']].head()}\")\r\n\r\n# Fix 2️⃣: Round numerical values to avoid float precision issues\r\nprint(\"🔄 Rounding numerical values to 3 decimals...\")\r\nnumerical_cols = list(set(merge_columns) & set(data.columns))  # Ensure existing columns only\r\ndata[numerical_cols] = data[numerical_cols].round(3)\r\n\r\n# Check duplicates again after merging & rounding\r\nfinal_duplicates = data.duplicated().sum()\r\nprint(f\"✅ Final duplicate count after fixes: {final_duplicates}\")\r\n\r\nprint(\"🔄 Merging nodes to reduce graph size...\")\r\n\r\ngrid_size = 0.01  # Spatial binning granularity\r\ntime_bins = 10    # Temporal binning granularity\r\n\r\n# Assign bins for lat/lon\r\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\ndata['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\n\r\n# Create unique spatial-temporal ID\r\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\n# Print unique bin counts\r\nprint(f\"✅ Unique Latitude Bins: {data['lat_bin'].nunique()}\")\r\nprint(f\"✅ Unique Longitude Bins: {data['lon_bin'].nunique()}\")\r\nprint(f\"✅ Unique Time Bins: {data['time_bin'].nunique()}\")\r\nprint(f\"✅ Unique Spatial-Temporal IDs: {data['grid_id'].nunique()}\")\r\n\r\n# Aggregate by grid_id\r\nmerged_nodes = data.groupby('grid_id').agg({\r\n    'Elevation': 'mean', 'Slope': 'mean', 'SWE': 'mean', 'snow_depth': 'mean',\r\n    'air_temperature_observed_f': 'mean', 'precipitation_amount': 'mean',\r\n    'relative_humidity_rmin': 'mean', 'relative_humidity_rmax': 'mean', 'wind_speed': 'mean',\r\n    'cumulative_SWE': 'mean', 'cumulative_precipitation_amount': 'mean', 'swe_value': 'mean',\r\n    'lat': 'mean', 'lon': 'mean', 'dayofyear': 'mean'\r\n}).reset_index()\r\n\r\n# Print merging stats\r\nprint(f\"✅ Merged nodes: {merged_nodes.shape[0]} from {data.shape[0]} original records.\")\r\n\r\nprint(\"🔄 Normalizing features using StandardScaler...\")\r\n\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\n\r\n# Ensure no missing values before normalization\r\nmerged_nodes[feature_columns] = merged_nodes[feature_columns].fillna(merged_nodes[feature_columns].median())\r\n\r\n# Print stats before normalization\r\nprint(\"📊 Feature statistics BEFORE normalization:\")\r\nprint(merged_nodes[feature_columns].describe())\r\n\r\n# Normalize node features\r\nscaler = StandardScaler()\r\nmerged_nodes[feature_columns] = scaler.fit_transform(merged_nodes[feature_columns].values)\r\n\r\n# Convert to tensor\r\nnode_features = torch.tensor(merged_nodes[feature_columns].values, dtype=torch.float)\r\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\n\r\n# Print stats after normalization\r\nprint(\"📊 Feature statistics AFTER normalization:\")\r\nprint(pd.DataFrame(node_features.numpy(), columns=feature_columns).describe())\r\n\r\nprint(f\"✅ Normalization completed. Node feature shape: {node_features.shape}\")\r\n\r\n# Increase spatial threshold slightly to reduce isolated nodes naturally\r\nspatial_threshold = 0.2  # Increased from 0.15\r\ntime_weight = 0.07       # Slightly increased\r\navg_degree = max(5, min(10, int(len(merged_nodes) ** 0.5 / 10)))\r\n\r\ndef time_aware_distance(p1, p2):\r\n    spatial_dist = np.linalg.norm(p1[:2] - p2[:2])  \r\n    time_dist = abs(p1[2] - p2[2]) * time_weight  \r\n    return spatial_dist + time_dist\r\n\r\nprint(\"🔄 Rebuilding KDTree and constructing edges...\")\r\n\r\ncoordinates = merged_nodes[['lat', 'lon', 'dayofyear']].values  \r\ntree = KDTree(coordinates)\r\n\r\ndistances, neighbors = tree.query(coordinates, k=avg_degree+1)\r\n\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, avg_degree+1):\r\n        if time_aware_distance(coordinates[i], coordinates[neighbors[i][j]]) < spatial_threshold:\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T\r\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\n# Ensure bidirectional edges\r\nedge_index = to_undirected(edge_index)\r\n\r\n# Recheck isolated nodes\r\ndeg = degree(edge_index[0], num_nodes=merged_nodes.shape[0])\r\nnum_isolated_nodes = (deg == 0).sum().item()\r\n\r\nprint(f\"✅ Isolated Nodes after increasing spatial threshold: {num_isolated_nodes} ({(num_isolated_nodes / merged_nodes.shape[0]) * 100:.2f}%)\")\r\n\r\n# Only add self-loops if isolated nodes remain\r\nif num_isolated_nodes > 0:\r\n    print(\"🔄 Adding self-loops to remaining isolated nodes...\")\r\n    edge_index, _ = add_self_loops(edge_index, num_nodes=merged_nodes.shape[0])\r\n\r\n# Final isolated node check\r\ndeg = degree(edge_index[0], num_nodes=merged_nodes.shape[0])\r\nnum_isolated_nodes = (deg == 0).sum().item()\r\nprint(f\"✅ Final Isolated Nodes: {num_isolated_nodes} ({(num_isolated_nodes / merged_nodes.shape[0]) * 100:.2f}%)\")\r\n\r\n\r\n# # Save graph\r\n# graph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n# torch.save(graph_data, \"/media/volume1/gnn_timeaware.pt\")\r\n\r\n# print(\"✅ Graph successfully saved.\")\r\n",
  "history_output" : "Loading dataset...\nTotal rows in raw dataset: 6155644\n Rows remaining after lat/lon filtering: 620662\n Missing values count per column:\ndate         0\nlat          0\nlon          0\nswe_value    0\ndtype: int64\n Found 2431 duplicate rows.\n Dataset size after deduplication: 618231\n🔍 Possible duplicates based on same location & date: 7482\n🔍 Possible duplicates based on same location (ignoring time): 617460\n🔍 Possible duplicates with small numerical variations (rounded to 3 decimals): 229\n🔄 Merging records for the same station on the same day...\n✅ Merged dataset size (after fixing same-location duplicates): 610749\n✅ Restored `dayofyear`. Sample values:\n        date  dayofyear\n0 2018-10-03        276\n1 2018-10-05        278\n2 2018-10-07        280\n3 2018-10-11        284\n4 2018-10-12        285\n🔄 Rounding numerical values to 3 decimals...\n✅ Final duplicate count after fixes: 0\n🔄 Merging nodes to reduce graph size...\n✅ Unique Latitude Bins: 293\n✅ Unique Longitude Bins: 369\n✅ Unique Time Bins: 11\n✅ Unique Spatial-Temporal IDs: 7375\n✅ Merged nodes: 7375 from 610749 original records.\n🔄 Normalizing features using StandardScaler...\n📊 Feature statistics BEFORE normalization:\n         Elevation  ...  cumulative_precipitation_amount\ncount  7375.000000  ...                      7375.000000\nmean    718.756401  ...                         1.060205\nstd     699.109205  ...                         1.469682\nmin       0.000000  ...                        -1.000000\n25%       0.000000  ...                        -1.000000\n50%     532.372000  ...                         1.417670\n75%    1348.370000  ...                         2.273114\nmax    2721.439000  ...                         3.489204\n[8 rows x 11 columns]\n📊 Feature statistics AFTER normalization:\n          Elevation  ...  cumulative_precipitation_amount\ncount  7.375000e+03  ...                     7.375000e+03\nmean   8.275953e-09  ...                     6.206965e-09\nstd    1.000068e+00  ...                     1.000070e+00\nmin   -1.028173e+00  ...                    -1.401898e+00\n25%   -1.028173e+00  ...                    -1.401898e+00\n50%   -2.666208e-01  ...                     2.432426e-01\n75%    9.006552e-01  ...                     8.253428e-01\nmax    2.864815e+00  ...                     1.652850e+00\n[8 rows x 11 columns]\n✅ Normalization completed. Node feature shape: torch.Size([7375, 11])\n🔄 Rebuilding KDTree and constructing edges...\n✅ Isolated Nodes after increasing spatial threshold: 1347 (18.26%)\n🔄 Adding self-loops to remaining isolated nodes...\nTraceback (most recent call last):\n  File \"/media/volume1/gw-workspace/cYLjxvOCcroT/graph_creation.py\", line 195, in <module>\n    edge_index, _ = add_self_loops(edge_index, num_nodes=merged_nodes.shape[0])\n                    ^^^^^^^^^^^^^^\nNameError: name 'add_self_loops' is not defined\n",
  "history_begin_time" : 1741562962304,
  "history_end_time" : 1741562998854,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "Jb05bZD4PHIO",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom torch_geometric.utils import degree, to_undirected\r\n\r\n# Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\"Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\n\r\ndf_list = []\r\ntotal_rows = 0\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, parse_dates=['date'], chunksize=chunksize):\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear  \r\n    total_rows +=len(chunk)\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Total rows in raw dataset: {total_rows}\")\r\nprint(f\" Rows remaining after lat/lon filtering: {data.shape[0]}\")\r\n\r\n# Check for missing values in critical columns\r\nmissing_values = data[['date', 'lat', 'lon', 'swe_value']].isnull().sum()\r\nprint(f\" Missing values count per column:\\n{missing_values}\")\r\n\r\n# Remove duplicates\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\ndata = data.drop_duplicates()\r\nprint(f\" Dataset size after deduplication: {data.shape[0]}\")\r\n\r\n# Check duplicates based only on ('lat', 'lon', 'date')\r\nlocation_time_duplicates = data.duplicated(subset=['lat', 'lon', 'date']).sum()\r\nprint(f\"🔍 Possible duplicates based on same location & date: {location_time_duplicates}\")\r\n\r\n# Check duplicates based on ('lat', 'lon') only (ignoring time)\r\nlocation_duplicates = data.duplicated(subset=['lat', 'lon']).sum()\r\nprint(f\"🔍 Possible duplicates based on same location (ignoring time): {location_duplicates}\")\r\n\r\n# Check for small variations in numerical values (rounding to 3 decimals)\r\nnumerical_cols = ['Elevation', 'Slope', 'SWE', 'snow_depth',\r\n                  'air_temperature_observed_f', 'precipitation_amount',\r\n                  'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n                  'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value']\r\n\r\n# Ensure we only use columns that exist in the dataframe\r\nexisting_numerical_cols = list(set(numerical_cols) & set(data.columns))\r\n\r\n# Apply rounding to check for floating-point duplicates\r\nrounded_data = data.copy()\r\nrounded_data[existing_numerical_cols] = rounded_data[existing_numerical_cols].round(3)\r\nnumerical_variation_duplicates = rounded_data.duplicated().sum()\r\n\r\nprint(f\"🔍 Possible duplicates with small numerical variations (rounded to 3 decimals): {numerical_variation_duplicates}\")\r\n\r\n#  Merge records with the same station on the same day (average values)\r\nprint(\"🔄 Merging records for the same station on the same day...\")\r\nmerge_columns = ['Elevation', 'Slope', 'SWE', 'snow_depth',\r\n                 'air_temperature_observed_f', 'precipitation_amount',\r\n                 'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n                 'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value']\r\n\r\ndata = data.groupby(['lat', 'lon', 'date'], as_index=False)[merge_columns].mean()\r\ndata['dayofyear'] = pd.to_datetime(data['date']).dt.dayofyear\r\nprint(f\"✅ Merged dataset size (after fixing same-location duplicates): {data.shape[0]}\")\r\nprint(f\"✅ Restored `dayofyear`. Sample values:\\n{data[['date', 'dayofyear']].head()}\")\r\n\r\n# Fix 2️⃣: Round numerical values to avoid float precision issues\r\nprint(\"🔄 Rounding numerical values to 3 decimals...\")\r\nnumerical_cols = list(set(merge_columns) & set(data.columns))  # Ensure existing columns only\r\ndata[numerical_cols] = data[numerical_cols].round(3)\r\n\r\n# Check duplicates again after merging & rounding\r\nfinal_duplicates = data.duplicated().sum()\r\nprint(f\"✅ Final duplicate count after fixes: {final_duplicates}\")\r\n\r\nprint(\"🔄 Merging nodes to reduce graph size...\")\r\n\r\ngrid_size = 0.01  # Spatial binning granularity\r\ntime_bins = 10    # Temporal binning granularity\r\n\r\n# Assign bins for lat/lon\r\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\ndata['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\n\r\n# Create unique spatial-temporal ID\r\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\n# Print unique bin counts\r\nprint(f\"✅ Unique Latitude Bins: {data['lat_bin'].nunique()}\")\r\nprint(f\"✅ Unique Longitude Bins: {data['lon_bin'].nunique()}\")\r\nprint(f\"✅ Unique Time Bins: {data['time_bin'].nunique()}\")\r\nprint(f\"✅ Unique Spatial-Temporal IDs: {data['grid_id'].nunique()}\")\r\n\r\n# Aggregate by grid_id\r\nmerged_nodes = data.groupby('grid_id').agg({\r\n    'Elevation': 'mean', 'Slope': 'mean', 'SWE': 'mean', 'snow_depth': 'mean',\r\n    'air_temperature_observed_f': 'mean', 'precipitation_amount': 'mean',\r\n    'relative_humidity_rmin': 'mean', 'relative_humidity_rmax': 'mean', 'wind_speed': 'mean',\r\n    'cumulative_SWE': 'mean', 'cumulative_precipitation_amount': 'mean', 'swe_value': 'mean',\r\n    'lat': 'mean', 'lon': 'mean', 'dayofyear': 'mean'\r\n}).reset_index()\r\n\r\n# Print merging stats\r\nprint(f\"✅ Merged nodes: {merged_nodes.shape[0]} from {data.shape[0]} original records.\")\r\n\r\nprint(\"🔄 Normalizing features using StandardScaler...\")\r\n\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\n\r\n# Ensure no missing values before normalization\r\nmerged_nodes[feature_columns] = merged_nodes[feature_columns].fillna(merged_nodes[feature_columns].median())\r\n\r\n# Print stats before normalization\r\nprint(\"📊 Feature statistics BEFORE normalization:\")\r\nprint(merged_nodes[feature_columns].describe())\r\n\r\n# Normalize node features\r\nscaler = StandardScaler()\r\nmerged_nodes[feature_columns] = scaler.fit_transform(merged_nodes[feature_columns].values)\r\n\r\n# Convert to tensor\r\nnode_features = torch.tensor(merged_nodes[feature_columns].values, dtype=torch.float)\r\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\n\r\n# Print stats after normalization\r\nprint(\"📊 Feature statistics AFTER normalization:\")\r\nprint(pd.DataFrame(node_features.numpy(), columns=feature_columns).describe())\r\n\r\nprint(f\"✅ Normalization completed. Node feature shape: {node_features.shape}\")\r\n\r\n# Increase spatial threshold slightly to reduce isolated nodes naturally\r\nspatial_threshold = 0.2  # Increased from 0.15\r\ntime_weight = 0.07       # Slightly increased\r\navg_degree = max(5, min(10, int(len(merged_nodes) ** 0.5 / 10)))\r\n\r\nprint(\"🔄 Rebuilding KDTree and constructing edges...\")\r\n\r\ncoordinates = merged_nodes[['lat', 'lon', 'dayofyear']].values  \r\ntree = KDTree(coordinates)\r\n\r\ndistances, neighbors = tree.query(coordinates, k=avg_degree+1)\r\n\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, avg_degree+1):\r\n        if time_aware_distance(coordinates[i], coordinates[neighbors[i][j]]) < spatial_threshold:\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T\r\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\n# Ensure bidirectional edges\r\nedge_index = to_undirected(edge_index)\r\n\r\n# Recheck isolated nodes\r\ndeg = degree(edge_index[0], num_nodes=merged_nodes.shape[0])\r\nnum_isolated_nodes = (deg == 0).sum().item()\r\n\r\nprint(f\"✅ Isolated Nodes after increasing spatial threshold: {num_isolated_nodes} ({(num_isolated_nodes / merged_nodes.shape[0]) * 100:.2f}%)\")\r\n\r\n# Only add self-loops if isolated nodes remain\r\nif num_isolated_nodes > 0:\r\n    print(\"🔄 Adding self-loops to remaining isolated nodes...\")\r\n    edge_index, _ = add_self_loops(edge_index, num_nodes=merged_nodes.shape[0])\r\n\r\n# Final isolated node check\r\ndeg = degree(edge_index[0], num_nodes=merged_nodes.shape[0])\r\nnum_isolated_nodes = (deg == 0).sum().item()\r\nprint(f\"✅ Final Isolated Nodes: {num_isolated_nodes} ({(num_isolated_nodes / merged_nodes.shape[0]) * 100:.2f}%)\")\r\n\r\n\r\n# # Save graph\r\n# graph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n# torch.save(graph_data, \"/media/volume1/gnn_timeaware.pt\")\r\n\r\n# print(\"✅ Graph successfully saved.\")\r\n",
  "history_output" : "Loading dataset...\nTotal rows in raw dataset: 6155644\n Rows remaining after lat/lon filtering: 620662\n Missing values count per column:\ndate         0\nlat          0\nlon          0\nswe_value    0\ndtype: int64\n Found 2431 duplicate rows.\n Dataset size after deduplication: 618231\n🔍 Possible duplicates based on same location & date: 7482\n🔍 Possible duplicates based on same location (ignoring time): 617460\n🔍 Possible duplicates with small numerical variations (rounded to 3 decimals): 229\n🔄 Merging records for the same station on the same day...\n✅ Merged dataset size (after fixing same-location duplicates): 610749\n✅ Restored `dayofyear`. Sample values:\n        date  dayofyear\n0 2018-10-03        276\n1 2018-10-05        278\n2 2018-10-07        280\n3 2018-10-11        284\n4 2018-10-12        285\n🔄 Rounding numerical values to 3 decimals...\n✅ Final duplicate count after fixes: 0\n🔄 Merging nodes to reduce graph size...\n✅ Unique Latitude Bins: 293\n✅ Unique Longitude Bins: 369\n✅ Unique Time Bins: 11\n✅ Unique Spatial-Temporal IDs: 7375\n✅ Merged nodes: 7375 from 610749 original records.\n🔄 Normalizing features using StandardScaler...\n📊 Feature statistics BEFORE normalization:\n         Elevation  ...  cumulative_precipitation_amount\ncount  7375.000000  ...                      7375.000000\nmean    718.756401  ...                         1.060205\nstd     699.109205  ...                         1.469682\nmin       0.000000  ...                        -1.000000\n25%       0.000000  ...                        -1.000000\n50%     532.372000  ...                         1.417670\n75%    1348.370000  ...                         2.273114\nmax    2721.439000  ...                         3.489204\n[8 rows x 11 columns]\n📊 Feature statistics AFTER normalization:\n          Elevation  ...  cumulative_precipitation_amount\ncount  7.375000e+03  ...                     7.375000e+03\nmean   8.275953e-09  ...                     6.206965e-09\nstd    1.000068e+00  ...                     1.000070e+00\nmin   -1.028173e+00  ...                    -1.401898e+00\n25%   -1.028173e+00  ...                    -1.401898e+00\n50%   -2.666208e-01  ...                     2.432426e-01\n75%    9.006552e-01  ...                     8.253428e-01\nmax    2.864815e+00  ...                     1.652850e+00\n[8 rows x 11 columns]\n✅ Normalization completed. Node feature shape: torch.Size([7375, 11])\n🔄 Rebuilding KDTree and constructing edges...\nTraceback (most recent call last):\n  File \"/media/volume1/gw-workspace/Jb05bZD4PHIO/graph_creation.py\", line 172, in <module>\n    if time_aware_distance(coordinates[i], coordinates[neighbors[i][j]]) < spatial_threshold:\n       ^^^^^^^^^^^^^^^^^^^\nNameError: name 'time_aware_distance' is not defined\n",
  "history_begin_time" : 1741562783506,
  "history_end_time" : 1741562818103,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "imiHUnGobaln",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom torch_geometric.utils import degree, to_undirected\r\n\r\n# Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\"Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\n\r\ndf_list = []\r\ntotal_rows = 0\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, parse_dates=['date'], chunksize=chunksize):\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear  \r\n    total_rows +=len(chunk)\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Total rows in raw dataset: {total_rows}\")\r\nprint(f\" Rows remaining after lat/lon filtering: {data.shape[0]}\")\r\n\r\n# Check for missing values in critical columns\r\nmissing_values = data[['date', 'lat', 'lon', 'swe_value']].isnull().sum()\r\nprint(f\" Missing values count per column:\\n{missing_values}\")\r\n\r\n# Remove duplicates\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\ndata = data.drop_duplicates()\r\nprint(f\" Dataset size after deduplication: {data.shape[0]}\")\r\n\r\n# Check duplicates based only on ('lat', 'lon', 'date')\r\nlocation_time_duplicates = data.duplicated(subset=['lat', 'lon', 'date']).sum()\r\nprint(f\"🔍 Possible duplicates based on same location & date: {location_time_duplicates}\")\r\n\r\n# Check duplicates based on ('lat', 'lon') only (ignoring time)\r\nlocation_duplicates = data.duplicated(subset=['lat', 'lon']).sum()\r\nprint(f\"🔍 Possible duplicates based on same location (ignoring time): {location_duplicates}\")\r\n\r\n# Check for small variations in numerical values (rounding to 3 decimals)\r\nnumerical_cols = ['Elevation', 'Slope', 'SWE', 'snow_depth',\r\n                  'air_temperature_observed_f', 'precipitation_amount',\r\n                  'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n                  'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value']\r\n\r\n# Ensure we only use columns that exist in the dataframe\r\nexisting_numerical_cols = list(set(numerical_cols) & set(data.columns))\r\n\r\n# Apply rounding to check for floating-point duplicates\r\nrounded_data = data.copy()\r\nrounded_data[existing_numerical_cols] = rounded_data[existing_numerical_cols].round(3)\r\nnumerical_variation_duplicates = rounded_data.duplicated().sum()\r\n\r\nprint(f\"🔍 Possible duplicates with small numerical variations (rounded to 3 decimals): {numerical_variation_duplicates}\")\r\n\r\n#  Merge records with the same station on the same day (average values)\r\nprint(\"🔄 Merging records for the same station on the same day...\")\r\nmerge_columns = ['Elevation', 'Slope', 'SWE', 'snow_depth',\r\n                 'air_temperature_observed_f', 'precipitation_amount',\r\n                 'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n                 'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value']\r\n\r\ndata = data.groupby(['lat', 'lon', 'date'], as_index=False)[merge_columns].mean()\r\ndata['dayofyear'] = pd.to_datetime(data['date']).dt.dayofyear\r\nprint(f\"✅ Merged dataset size (after fixing same-location duplicates): {data.shape[0]}\")\r\nprint(f\"✅ Restored `dayofyear`. Sample values:\\n{data[['date', 'dayofyear']].head()}\")\r\n\r\n# Fix 2️⃣: Round numerical values to avoid float precision issues\r\nprint(\"🔄 Rounding numerical values to 3 decimals...\")\r\nnumerical_cols = list(set(merge_columns) & set(data.columns))  # Ensure existing columns only\r\ndata[numerical_cols] = data[numerical_cols].round(3)\r\n\r\n# Check duplicates again after merging & rounding\r\nfinal_duplicates = data.duplicated().sum()\r\nprint(f\"✅ Final duplicate count after fixes: {final_duplicates}\")\r\n\r\nprint(\"🔄 Merging nodes to reduce graph size...\")\r\n\r\ngrid_size = 0.01  # Spatial binning granularity\r\ntime_bins = 10    # Temporal binning granularity\r\n\r\n# Assign bins for lat/lon\r\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\ndata['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\n\r\n# Create unique spatial-temporal ID\r\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\n# Print unique bin counts\r\nprint(f\"✅ Unique Latitude Bins: {data['lat_bin'].nunique()}\")\r\nprint(f\"✅ Unique Longitude Bins: {data['lon_bin'].nunique()}\")\r\nprint(f\"✅ Unique Time Bins: {data['time_bin'].nunique()}\")\r\nprint(f\"✅ Unique Spatial-Temporal IDs: {data['grid_id'].nunique()}\")\r\n\r\n# Aggregate by grid_id\r\nmerged_nodes = data.groupby('grid_id').agg({\r\n    'Elevation': 'mean', 'Slope': 'mean', 'SWE': 'mean', 'snow_depth': 'mean',\r\n    'air_temperature_observed_f': 'mean', 'precipitation_amount': 'mean',\r\n    'relative_humidity_rmin': 'mean', 'relative_humidity_rmax': 'mean', 'wind_speed': 'mean',\r\n    'cumulative_SWE': 'mean', 'cumulative_precipitation_amount': 'mean', 'swe_value': 'mean',\r\n    'lat': 'mean', 'lon': 'mean', 'dayofyear': 'mean'\r\n}).reset_index()\r\n\r\n# Print merging stats\r\nprint(f\"✅ Merged nodes: {merged_nodes.shape[0]} from {data.shape[0]} original records.\")\r\n\r\nprint(\"🔄 Normalizing features using StandardScaler...\")\r\n\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\n\r\n# Ensure no missing values before normalization\r\nmerged_nodes[feature_columns] = merged_nodes[feature_columns].fillna(merged_nodes[feature_columns].median())\r\n\r\n# Print stats before normalization\r\nprint(\"📊 Feature statistics BEFORE normalization:\")\r\nprint(merged_nodes[feature_columns].describe())\r\n\r\n# Normalize node features\r\nscaler = StandardScaler()\r\nmerged_nodes[feature_columns] = scaler.fit_transform(merged_nodes[feature_columns].values)\r\n\r\n# Convert to tensor\r\nnode_features = torch.tensor(merged_nodes[feature_columns].values, dtype=torch.float)\r\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\n\r\n# Print stats after normalization\r\nprint(\"📊 Feature statistics AFTER normalization:\")\r\nprint(pd.DataFrame(node_features.numpy(), columns=feature_columns).describe())\r\n\r\nprint(f\"✅ Normalization completed. Node feature shape: {node_features.shape}\")\r\n\r\n# Increase spatial threshold slightly to reduce isolated nodes naturally\r\nspatial_threshold = 0.2  # Increased from 0.15\r\ntime_weight = 0.07       # Slightly increased\r\n\r\nprint(\"🔄 Rebuilding KDTree and constructing edges...\")\r\n\r\ncoordinates = merged_nodes[['lat', 'lon', 'dayofyear']].values  \r\ntree = KDTree(coordinates)\r\n\r\ndistances, neighbors = tree.query(coordinates, k=avg_degree+1)\r\n\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, avg_degree+1):\r\n        if time_aware_distance(coordinates[i], coordinates[neighbors[i][j]]) < spatial_threshold:\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T\r\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\n# Ensure bidirectional edges\r\nedge_index = to_undirected(edge_index)\r\n\r\n# Recheck isolated nodes\r\ndeg = degree(edge_index[0], num_nodes=merged_nodes.shape[0])\r\nnum_isolated_nodes = (deg == 0).sum().item()\r\n\r\nprint(f\"✅ Isolated Nodes after increasing spatial threshold: {num_isolated_nodes} ({(num_isolated_nodes / merged_nodes.shape[0]) * 100:.2f}%)\")\r\n\r\n# Only add self-loops if isolated nodes remain\r\nif num_isolated_nodes > 0:\r\n    print(\"🔄 Adding self-loops to remaining isolated nodes...\")\r\n    edge_index, _ = add_self_loops(edge_index, num_nodes=merged_nodes.shape[0])\r\n\r\n# Final isolated node check\r\ndeg = degree(edge_index[0], num_nodes=merged_nodes.shape[0])\r\nnum_isolated_nodes = (deg == 0).sum().item()\r\nprint(f\"✅ Final Isolated Nodes: {num_isolated_nodes} ({(num_isolated_nodes / merged_nodes.shape[0]) * 100:.2f}%)\")\r\n\r\n\r\n# # Save graph\r\n# graph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n# torch.save(graph_data, \"/media/volume1/gnn_timeaware.pt\")\r\n\r\n# print(\"✅ Graph successfully saved.\")\r\n",
  "history_output" : "Loading dataset...\nTotal rows in raw dataset: 6155644\n Rows remaining after lat/lon filtering: 620662\n Missing values count per column:\ndate         0\nlat          0\nlon          0\nswe_value    0\ndtype: int64\n Found 2431 duplicate rows.\n Dataset size after deduplication: 618231\n🔍 Possible duplicates based on same location & date: 7482\n🔍 Possible duplicates based on same location (ignoring time): 617460\n🔍 Possible duplicates with small numerical variations (rounded to 3 decimals): 229\n🔄 Merging records for the same station on the same day...\n✅ Merged dataset size (after fixing same-location duplicates): 610749\n✅ Restored `dayofyear`. Sample values:\n        date  dayofyear\n0 2018-10-03        276\n1 2018-10-05        278\n2 2018-10-07        280\n3 2018-10-11        284\n4 2018-10-12        285\n🔄 Rounding numerical values to 3 decimals...\n✅ Final duplicate count after fixes: 0\n🔄 Merging nodes to reduce graph size...\n✅ Unique Latitude Bins: 293\n✅ Unique Longitude Bins: 369\n✅ Unique Time Bins: 11\n✅ Unique Spatial-Temporal IDs: 7375\n✅ Merged nodes: 7375 from 610749 original records.\n🔄 Normalizing features using StandardScaler...\n📊 Feature statistics BEFORE normalization:\n         Elevation  ...  cumulative_precipitation_amount\ncount  7375.000000  ...                      7375.000000\nmean    718.756401  ...                         1.060205\nstd     699.109205  ...                         1.469682\nmin       0.000000  ...                        -1.000000\n25%       0.000000  ...                        -1.000000\n50%     532.372000  ...                         1.417670\n75%    1348.370000  ...                         2.273114\nmax    2721.439000  ...                         3.489204\n[8 rows x 11 columns]\n📊 Feature statistics AFTER normalization:\n          Elevation  ...  cumulative_precipitation_amount\ncount  7.375000e+03  ...                     7.375000e+03\nmean   8.275953e-09  ...                     6.206965e-09\nstd    1.000068e+00  ...                     1.000070e+00\nmin   -1.028173e+00  ...                    -1.401898e+00\n25%   -1.028173e+00  ...                    -1.401898e+00\n50%   -2.666208e-01  ...                     2.432426e-01\n75%    9.006552e-01  ...                     8.253428e-01\nmax    2.864815e+00  ...                     1.652850e+00\n[8 rows x 11 columns]\n✅ Normalization completed. Node feature shape: torch.Size([7375, 11])\n🔄 Rebuilding KDTree and constructing edges...\nTraceback (most recent call last):\n  File \"/media/volume1/gw-workspace/imiHUnGobaln/graph_creation.py\", line 166, in <module>\n    distances, neighbors = tree.query(coordinates, k=avg_degree+1)\n                                                     ^^^^^^^^^^\nNameError: name 'avg_degree' is not defined\n",
  "history_begin_time" : 1741562644648,
  "history_end_time" : 1741562680010,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "rE2e0Ro0nuEe",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom torch_geometric.utils import degree, to_undirected\r\nfrom torch_geometric.utils import add_self_loops, degree\r\n\r\n# Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\"Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\n\r\ndf_list = []\r\ntotal_rows = 0\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, parse_dates=['date'], chunksize=chunksize):\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear  \r\n    total_rows +=len(chunk)\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Total rows in raw dataset: {total_rows}\")\r\nprint(f\" Rows remaining after lat/lon filtering: {data.shape[0]}\")\r\n\r\n# Check for missing values in critical columns\r\nmissing_values = data[['date', 'lat', 'lon', 'swe_value']].isnull().sum()\r\nprint(f\" Missing values count per column:\\n{missing_values}\")\r\n\r\n# Remove duplicates\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\ndata = data.drop_duplicates()\r\nprint(f\" Dataset size after deduplication: {data.shape[0]}\")\r\n\r\n# Check duplicates based only on ('lat', 'lon', 'date')\r\nlocation_time_duplicates = data.duplicated(subset=['lat', 'lon', 'date']).sum()\r\nprint(f\"🔍 Possible duplicates based on same location & date: {location_time_duplicates}\")\r\n\r\n# Check duplicates based on ('lat', 'lon') only (ignoring time)\r\nlocation_duplicates = data.duplicated(subset=['lat', 'lon']).sum()\r\nprint(f\"🔍 Possible duplicates based on same location (ignoring time): {location_duplicates}\")\r\n\r\n# Check for small variations in numerical values (rounding to 3 decimals)\r\nnumerical_cols = ['Elevation', 'Slope', 'SWE', 'snow_depth',\r\n                  'air_temperature_observed_f', 'precipitation_amount',\r\n                  'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n                  'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value']\r\n\r\n# Ensure we only use columns that exist in the dataframe\r\nexisting_numerical_cols = list(set(numerical_cols) & set(data.columns))\r\n\r\n# Apply rounding to check for floating-point duplicates\r\nrounded_data = data.copy()\r\nrounded_data[existing_numerical_cols] = rounded_data[existing_numerical_cols].round(3)\r\nnumerical_variation_duplicates = rounded_data.duplicated().sum()\r\n\r\nprint(f\"🔍 Possible duplicates with small numerical variations (rounded to 3 decimals): {numerical_variation_duplicates}\")\r\n\r\n#  Merge records with the same station on the same day (average values)\r\nprint(\"🔄 Merging records for the same station on the same day...\")\r\nmerge_columns = ['Elevation', 'Slope', 'SWE', 'snow_depth',\r\n                 'air_temperature_observed_f', 'precipitation_amount',\r\n                 'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n                 'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value']\r\n\r\ndata = data.groupby(['lat', 'lon', 'date'], as_index=False)[merge_columns].mean()\r\ndata['dayofyear'] = pd.to_datetime(data['date']).dt.dayofyear\r\nprint(f\"✅ Merged dataset size (after fixing same-location duplicates): {data.shape[0]}\")\r\nprint(f\"✅ Restored `dayofyear`. Sample values:\\n{data[['date', 'dayofyear']].head()}\")\r\n\r\n# Fix 2️⃣: Round numerical values to avoid float precision issues\r\nprint(\"🔄 Rounding numerical values to 3 decimals...\")\r\nnumerical_cols = list(set(merge_columns) & set(data.columns))  # Ensure existing columns only\r\ndata[numerical_cols] = data[numerical_cols].round(3)\r\n\r\n# Check duplicates again after merging & rounding\r\nfinal_duplicates = data.duplicated().sum()\r\nprint(f\"✅ Final duplicate count after fixes: {final_duplicates}\")\r\n\r\nprint(\"🔄 Merging nodes to reduce graph size...\")\r\n\r\ngrid_size = 0.01  # Spatial binning granularity\r\ntime_bins = 10    # Temporal binning granularity\r\n\r\n# Assign bins for lat/lon\r\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\ndata['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\n\r\n# Create unique spatial-temporal ID\r\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\n# Print unique bin counts\r\nprint(f\"✅ Unique Latitude Bins: {data['lat_bin'].nunique()}\")\r\nprint(f\"✅ Unique Longitude Bins: {data['lon_bin'].nunique()}\")\r\nprint(f\"✅ Unique Time Bins: {data['time_bin'].nunique()}\")\r\nprint(f\"✅ Unique Spatial-Temporal IDs: {data['grid_id'].nunique()}\")\r\n\r\n# Aggregate by grid_id\r\nmerged_nodes = data.groupby('grid_id').agg({\r\n    'Elevation': 'mean', 'Slope': 'mean', 'SWE': 'mean', 'snow_depth': 'mean',\r\n    'air_temperature_observed_f': 'mean', 'precipitation_amount': 'mean',\r\n    'relative_humidity_rmin': 'mean', 'relative_humidity_rmax': 'mean', 'wind_speed': 'mean',\r\n    'cumulative_SWE': 'mean', 'cumulative_precipitation_amount': 'mean', 'swe_value': 'mean',\r\n    'lat': 'mean', 'lon': 'mean', 'dayofyear': 'mean'\r\n}).reset_index()\r\n\r\n# Print merging stats\r\nprint(f\"✅ Merged nodes: {merged_nodes.shape[0]} from {data.shape[0]} original records.\")\r\n\r\nprint(\"🔄 Normalizing features using StandardScaler...\")\r\n\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\n\r\n# Ensure no missing values before normalization\r\nmerged_nodes[feature_columns] = merged_nodes[feature_columns].fillna(merged_nodes[feature_columns].median())\r\n\r\n# Print stats before normalization\r\nprint(\"📊 Feature statistics BEFORE normalization:\")\r\nprint(merged_nodes[feature_columns].describe())\r\n\r\n# Normalize node features\r\nscaler = StandardScaler()\r\nmerged_nodes[feature_columns] = scaler.fit_transform(merged_nodes[feature_columns].values)\r\n\r\n# Convert to tensor\r\nnode_features = torch.tensor(merged_nodes[feature_columns].values, dtype=torch.float)\r\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\n\r\n# Print stats after normalization\r\nprint(\"📊 Feature statistics AFTER normalization:\")\r\nprint(pd.DataFrame(node_features.numpy(), columns=feature_columns).describe())\r\n\r\nprint(f\"✅ Normalization completed. Node feature shape: {node_features.shape}\")\r\n\r\nfrom torch_geometric.utils import add_self_loops, degree\r\n\r\nprint(\"🔄 Building KDTree and constructing edges...\")\r\n\r\ncoordinates = merged_nodes[['lat', 'lon', 'dayofyear']].values  \r\ntree = KDTree(coordinates)\r\n\r\n# Adjusted parameters to reduce isolated nodes\r\nspatial_threshold = 0.2  # Increase spatial connection range slightly\r\ntime_weight = 0.07       # Increase time influence slightly\r\navg_degree = max(5, min(10, int(len(merged_nodes) ** 0.5 / 10)))\r\n\r\ndef time_aware_distance(p1, p2):\r\n    spatial_dist = np.linalg.norm(p1[:2] - p2[:2])  \r\n    time_dist = abs(p1[2] - p2[2]) * time_weight  \r\n    return spatial_dist + time_dist\r\n\r\ndistances, neighbors = tree.query(coordinates, k=avg_degree+1)\r\n\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, avg_degree+1):\r\n        if time_aware_distance(coordinates[i], coordinates[neighbors[i][j]]) < spatial_threshold:\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T\r\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\n# Ensure bidirectional edges\r\nedge_index = to_undirected(edge_index)\r\n\r\n# Add self-loops to isolated nodes\r\nedge_index, _ = add_self_loops(edge_index, num_nodes=merged_nodes.shape[0])\r\n\r\nprint(f\"✅ Graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\r\n\r\n# Check for isolated nodes (should be lower now)\r\ndeg = degree(edge_index[0], num_nodes=merged_nodes.shape[0])\r\nnum_isolated_nodes = (deg == 0).sum().item()\r\nprint(f\"✅ Isolated Nodes after fixes: {num_isolated_nodes} ({(num_isolated_nodes / merged_nodes.shape[0]) * 100:.2f}%)\")\r\n\r\n\r\n# # Save graph\r\n# graph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n# torch.save(graph_data, \"/media/volume1/gnn_timeaware.pt\")\r\n\r\n# print(\"✅ Graph successfully saved.\")\r\n",
  "history_output" : "Loading dataset...\nTotal rows in raw dataset: 6155644\n Rows remaining after lat/lon filtering: 620662\n Missing values count per column:\ndate         0\nlat          0\nlon          0\nswe_value    0\ndtype: int64\n Found 2431 duplicate rows.\n Dataset size after deduplication: 618231\n🔍 Possible duplicates based on same location & date: 7482\n🔍 Possible duplicates based on same location (ignoring time): 617460\n🔍 Possible duplicates with small numerical variations (rounded to 3 decimals): 229\n🔄 Merging records for the same station on the same day...\n✅ Merged dataset size (after fixing same-location duplicates): 610749\n✅ Restored `dayofyear`. Sample values:\n        date  dayofyear\n0 2018-10-03        276\n1 2018-10-05        278\n2 2018-10-07        280\n3 2018-10-11        284\n4 2018-10-12        285\n🔄 Rounding numerical values to 3 decimals...\n✅ Final duplicate count after fixes: 0\n🔄 Merging nodes to reduce graph size...\n✅ Unique Latitude Bins: 293\n✅ Unique Longitude Bins: 369\n✅ Unique Time Bins: 11\n✅ Unique Spatial-Temporal IDs: 7375\n✅ Merged nodes: 7375 from 610749 original records.\n🔄 Normalizing features using StandardScaler...\n📊 Feature statistics BEFORE normalization:\n         Elevation  ...  cumulative_precipitation_amount\ncount  7375.000000  ...                      7375.000000\nmean    718.756401  ...                         1.060205\nstd     699.109205  ...                         1.469682\nmin       0.000000  ...                        -1.000000\n25%       0.000000  ...                        -1.000000\n50%     532.372000  ...                         1.417670\n75%    1348.370000  ...                         2.273114\nmax    2721.439000  ...                         3.489204\n[8 rows x 11 columns]\n📊 Feature statistics AFTER normalization:\n          Elevation  ...  cumulative_precipitation_amount\ncount  7.375000e+03  ...                     7.375000e+03\nmean   8.275953e-09  ...                     6.206965e-09\nstd    1.000068e+00  ...                     1.000070e+00\nmin   -1.028173e+00  ...                    -1.401898e+00\n25%   -1.028173e+00  ...                    -1.401898e+00\n50%   -2.666208e-01  ...                     2.432426e-01\n75%    9.006552e-01  ...                     8.253428e-01\nmax    2.864815e+00  ...                     1.652850e+00\n[8 rows x 11 columns]\n✅ Normalization completed. Node feature shape: torch.Size([7375, 11])\n🔄 Building KDTree and constructing edges...\n✅ Graph constructed with 7375 nodes and 32285 edges.\n✅ Isolated Nodes after fixes: 0 (0.00%)\n",
  "history_begin_time" : 1741562257724,
  "history_end_time" : 1741562293447,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "7NQuI8C9Y8sH",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom torch_geometric.utils import add_self_loops, degree\r\n\r\n# Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\"Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\n\r\ndf_list = []\r\ntotal_rows = 0\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, parse_dates=['date'], chunksize=chunksize):\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear  \r\n    total_rows +=len(chunk)\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Total rows in raw dataset: {total_rows}\")\r\nprint(f\" Rows remaining after lat/lon filtering: {data.shape[0]}\")\r\n\r\n# Check for missing values in critical columns\r\nmissing_values = data[['date', 'lat', 'lon', 'swe_value']].isnull().sum()\r\nprint(f\" Missing values count per column:\\n{missing_values}\")\r\n\r\n# Remove duplicates\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\ndata = data.drop_duplicates()\r\nprint(f\" Dataset size after deduplication: {data.shape[0]}\")\r\n\r\n# Check duplicates based only on ('lat', 'lon', 'date')\r\nlocation_time_duplicates = data.duplicated(subset=['lat', 'lon', 'date']).sum()\r\nprint(f\"🔍 Possible duplicates based on same location & date: {location_time_duplicates}\")\r\n\r\n# Check duplicates based on ('lat', 'lon') only (ignoring time)\r\nlocation_duplicates = data.duplicated(subset=['lat', 'lon']).sum()\r\nprint(f\"🔍 Possible duplicates based on same location (ignoring time): {location_duplicates}\")\r\n\r\n# Check for small variations in numerical values (rounding to 3 decimals)\r\nnumerical_cols = ['Elevation', 'Slope', 'SWE', 'snow_depth',\r\n                  'air_temperature_observed_f', 'precipitation_amount',\r\n                  'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n                  'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value']\r\n\r\n# Ensure we only use columns that exist in the dataframe\r\nexisting_numerical_cols = list(set(numerical_cols) & set(data.columns))\r\n\r\n# Apply rounding to check for floating-point duplicates\r\nrounded_data = data.copy()\r\nrounded_data[existing_numerical_cols] = rounded_data[existing_numerical_cols].round(3)\r\nnumerical_variation_duplicates = rounded_data.duplicated().sum()\r\n\r\nprint(f\"🔍 Possible duplicates with small numerical variations (rounded to 3 decimals): {numerical_variation_duplicates}\")\r\n\r\n#  Merge records with the same station on the same day (average values)\r\nprint(\"🔄 Merging records for the same station on the same day...\")\r\nmerge_columns = ['Elevation', 'Slope', 'SWE', 'snow_depth',\r\n                 'air_temperature_observed_f', 'precipitation_amount',\r\n                 'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n                 'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value']\r\n\r\ndata = data.groupby(['lat', 'lon', 'date'], as_index=False)[merge_columns].mean()\r\ndata['dayofyear'] = pd.to_datetime(data['date']).dt.dayofyear\r\nprint(f\"✅ Merged dataset size (after fixing same-location duplicates): {data.shape[0]}\")\r\nprint(f\"✅ Restored `dayofyear`. Sample values:\\n{data[['date', 'dayofyear']].head()}\")\r\n\r\n# Fix 2️⃣: Round numerical values to avoid float precision issues\r\nprint(\"🔄 Rounding numerical values to 3 decimals...\")\r\nnumerical_cols = list(set(merge_columns) & set(data.columns))  # Ensure existing columns only\r\ndata[numerical_cols] = data[numerical_cols].round(3)\r\n\r\n# Check duplicates again after merging & rounding\r\nfinal_duplicates = data.duplicated().sum()\r\nprint(f\"✅ Final duplicate count after fixes: {final_duplicates}\")\r\n\r\nprint(\"🔄 Merging nodes to reduce graph size...\")\r\n\r\ngrid_size = 0.01  # Spatial binning granularity\r\ntime_bins = 10    # Temporal binning granularity\r\n\r\n# Assign bins for lat/lon\r\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\ndata['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\n\r\n# Create unique spatial-temporal ID\r\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\n# Print unique bin counts\r\nprint(f\"✅ Unique Latitude Bins: {data['lat_bin'].nunique()}\")\r\nprint(f\"✅ Unique Longitude Bins: {data['lon_bin'].nunique()}\")\r\nprint(f\"✅ Unique Time Bins: {data['time_bin'].nunique()}\")\r\nprint(f\"✅ Unique Spatial-Temporal IDs: {data['grid_id'].nunique()}\")\r\n\r\n# Aggregate by grid_id\r\nmerged_nodes = data.groupby('grid_id').agg({\r\n    'Elevation': 'mean', 'Slope': 'mean', 'SWE': 'mean', 'snow_depth': 'mean',\r\n    'air_temperature_observed_f': 'mean', 'precipitation_amount': 'mean',\r\n    'relative_humidity_rmin': 'mean', 'relative_humidity_rmax': 'mean', 'wind_speed': 'mean',\r\n    'cumulative_SWE': 'mean', 'cumulative_precipitation_amount': 'mean', 'swe_value': 'mean',\r\n    'lat': 'mean', 'lon': 'mean', 'dayofyear': 'mean'\r\n}).reset_index()\r\n\r\n# Print merging stats\r\nprint(f\"✅ Merged nodes: {merged_nodes.shape[0]} from {data.shape[0]} original records.\")\r\n\r\nprint(\"🔄 Normalizing features using StandardScaler...\")\r\n\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\n\r\n# Ensure no missing values before normalization\r\nmerged_nodes[feature_columns] = merged_nodes[feature_columns].fillna(merged_nodes[feature_columns].median())\r\n\r\n# Print stats before normalization\r\nprint(\"📊 Feature statistics BEFORE normalization:\")\r\nprint(merged_nodes[feature_columns].describe())\r\n\r\n# Normalize node features\r\nscaler = StandardScaler()\r\nmerged_nodes[feature_columns] = scaler.fit_transform(merged_nodes[feature_columns].values)\r\n\r\n# Convert to tensor\r\nnode_features = torch.tensor(merged_nodes[feature_columns].values, dtype=torch.float)\r\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\n\r\n# Print stats after normalization\r\nprint(\"📊 Feature statistics AFTER normalization:\")\r\nprint(pd.DataFrame(node_features.numpy(), columns=feature_columns).describe())\r\n\r\nprint(f\"✅ Normalization completed. Node feature shape: {node_features.shape}\")\r\n\r\nfrom torch_geometric.utils import add_self_loops, degree\r\n\r\nprint(\"🔄 Building KDTree and constructing edges...\")\r\n\r\ncoordinates = merged_nodes[['lat', 'lon', 'dayofyear']].values  \r\ntree = KDTree(coordinates)\r\n\r\n# Adjusted parameters to reduce isolated nodes\r\nspatial_threshold = 0.2  # Increase spatial connection range slightly\r\ntime_weight = 0.07       # Increase time influence slightly\r\navg_degree = max(5, min(10, int(len(merged_nodes) ** 0.5 / 10)))\r\n\r\ndef time_aware_distance(p1, p2):\r\n    spatial_dist = np.linalg.norm(p1[:2] - p2[:2])  \r\n    time_dist = abs(p1[2] - p2[2]) * time_weight  \r\n    return spatial_dist + time_dist\r\n\r\ndistances, neighbors = tree.query(coordinates, k=avg_degree+1)\r\n\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, avg_degree+1):\r\n        if time_aware_distance(coordinates[i], coordinates[neighbors[i][j]]) < spatial_threshold:\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T\r\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\n# Ensure bidirectional edges\r\nedge_index = to_undirected(edge_index)\r\n\r\n# Add self-loops to isolated nodes\r\nedge_index, _ = add_self_loops(edge_index, num_nodes=merged_nodes.shape[0])\r\n\r\nprint(f\"✅ Graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\r\n\r\n# Check for isolated nodes (should be lower now)\r\ndeg = degree(edge_index[0], num_nodes=merged_nodes.shape[0])\r\nnum_isolated_nodes = (deg == 0).sum().item()\r\nprint(f\"✅ Isolated Nodes after fixes: {num_isolated_nodes} ({(num_isolated_nodes / merged_nodes.shape[0]) * 100:.2f}%)\")\r\n\r\n\r\n# # Save graph\r\n# graph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n# torch.save(graph_data, \"/media/volume1/gnn_timeaware.pt\")\r\n\r\n# print(\"✅ Graph successfully saved.\")\r\n",
  "history_output" : "Loading dataset...\nTotal rows in raw dataset: 6155644\n Rows remaining after lat/lon filtering: 620662\n Missing values count per column:\ndate         0\nlat          0\nlon          0\nswe_value    0\ndtype: int64\n Found 2431 duplicate rows.\n Dataset size after deduplication: 618231\n🔍 Possible duplicates based on same location & date: 7482\n🔍 Possible duplicates based on same location (ignoring time): 617460\n🔍 Possible duplicates with small numerical variations (rounded to 3 decimals): 229\n🔄 Merging records for the same station on the same day...\n✅ Merged dataset size (after fixing same-location duplicates): 610749\n✅ Restored `dayofyear`. Sample values:\n        date  dayofyear\n0 2018-10-03        276\n1 2018-10-05        278\n2 2018-10-07        280\n3 2018-10-11        284\n4 2018-10-12        285\n🔄 Rounding numerical values to 3 decimals...\n✅ Final duplicate count after fixes: 0\n🔄 Merging nodes to reduce graph size...\n✅ Unique Latitude Bins: 293\n✅ Unique Longitude Bins: 369\n✅ Unique Time Bins: 11\n✅ Unique Spatial-Temporal IDs: 7375\n✅ Merged nodes: 7375 from 610749 original records.\n🔄 Normalizing features using StandardScaler...\n📊 Feature statistics BEFORE normalization:\n         Elevation  ...  cumulative_precipitation_amount\ncount  7375.000000  ...                      7375.000000\nmean    718.756401  ...                         1.060205\nstd     699.109205  ...                         1.469682\nmin       0.000000  ...                        -1.000000\n25%       0.000000  ...                        -1.000000\n50%     532.372000  ...                         1.417670\n75%    1348.370000  ...                         2.273114\nmax    2721.439000  ...                         3.489204\n[8 rows x 11 columns]\n📊 Feature statistics AFTER normalization:\n          Elevation  ...  cumulative_precipitation_amount\ncount  7.375000e+03  ...                     7.375000e+03\nmean   8.275953e-09  ...                     6.206965e-09\nstd    1.000068e+00  ...                     1.000070e+00\nmin   -1.028173e+00  ...                    -1.401898e+00\n25%   -1.028173e+00  ...                    -1.401898e+00\n50%   -2.666208e-01  ...                     2.432426e-01\n75%    9.006552e-01  ...                     8.253428e-01\nmax    2.864815e+00  ...                     1.652850e+00\n[8 rows x 11 columns]\n✅ Normalization completed. Node feature shape: torch.Size([7375, 11])\n🔄 Building KDTree and constructing edges...\nTraceback (most recent call last):\n  File \"/media/volume1/gw-workspace/7NQuI8C9Y8sH/graph_creation.py\", line 186, in <module>\n    edge_index = to_undirected(edge_index)\n                 ^^^^^^^^^^^^^\nNameError: name 'to_undirected' is not defined\n",
  "history_begin_time" : 1741562204644,
  "history_end_time" : 1741562240408,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "OSH3YhwcJTWm",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom torch_geometric.utils import degree, to_undirected\r\n\r\n# Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\"Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\n\r\ndf_list = []\r\ntotal_rows = 0\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, parse_dates=['date'], chunksize=chunksize):\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear  \r\n    total_rows +=len(chunk)\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Total rows in raw dataset: {total_rows}\")\r\nprint(f\" Rows remaining after lat/lon filtering: {data.shape[0]}\")\r\n\r\n# Check for missing values in critical columns\r\nmissing_values = data[['date', 'lat', 'lon', 'swe_value']].isnull().sum()\r\nprint(f\" Missing values count per column:\\n{missing_values}\")\r\n\r\n# Remove duplicates\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\ndata = data.drop_duplicates()\r\nprint(f\" Dataset size after deduplication: {data.shape[0]}\")\r\n\r\n# Check duplicates based only on ('lat', 'lon', 'date')\r\nlocation_time_duplicates = data.duplicated(subset=['lat', 'lon', 'date']).sum()\r\nprint(f\"🔍 Possible duplicates based on same location & date: {location_time_duplicates}\")\r\n\r\n# Check duplicates based on ('lat', 'lon') only (ignoring time)\r\nlocation_duplicates = data.duplicated(subset=['lat', 'lon']).sum()\r\nprint(f\"🔍 Possible duplicates based on same location (ignoring time): {location_duplicates}\")\r\n\r\n# Check for small variations in numerical values (rounding to 3 decimals)\r\nnumerical_cols = ['Elevation', 'Slope', 'SWE', 'snow_depth',\r\n                  'air_temperature_observed_f', 'precipitation_amount',\r\n                  'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n                  'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value']\r\n\r\n# Ensure we only use columns that exist in the dataframe\r\nexisting_numerical_cols = list(set(numerical_cols) & set(data.columns))\r\n\r\n# Apply rounding to check for floating-point duplicates\r\nrounded_data = data.copy()\r\nrounded_data[existing_numerical_cols] = rounded_data[existing_numerical_cols].round(3)\r\nnumerical_variation_duplicates = rounded_data.duplicated().sum()\r\n\r\nprint(f\"🔍 Possible duplicates with small numerical variations (rounded to 3 decimals): {numerical_variation_duplicates}\")\r\n\r\n#  Merge records with the same station on the same day (average values)\r\nprint(\"🔄 Merging records for the same station on the same day...\")\r\nmerge_columns = ['Elevation', 'Slope', 'SWE', 'snow_depth',\r\n                 'air_temperature_observed_f', 'precipitation_amount',\r\n                 'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n                 'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value']\r\n\r\ndata = data.groupby(['lat', 'lon', 'date'], as_index=False)[merge_columns].mean()\r\ndata['dayofyear'] = pd.to_datetime(data['date']).dt.dayofyear\r\nprint(f\"✅ Merged dataset size (after fixing same-location duplicates): {data.shape[0]}\")\r\nprint(f\"✅ Restored `dayofyear`. Sample values:\\n{data[['date', 'dayofyear']].head()}\")\r\n\r\n# Fix 2️⃣: Round numerical values to avoid float precision issues\r\nprint(\"🔄 Rounding numerical values to 3 decimals...\")\r\nnumerical_cols = list(set(merge_columns) & set(data.columns))  # Ensure existing columns only\r\ndata[numerical_cols] = data[numerical_cols].round(3)\r\n\r\n# Check duplicates again after merging & rounding\r\nfinal_duplicates = data.duplicated().sum()\r\nprint(f\"✅ Final duplicate count after fixes: {final_duplicates}\")\r\n\r\nprint(\"🔄 Merging nodes to reduce graph size...\")\r\n\r\ngrid_size = 0.01  # Spatial binning granularity\r\ntime_bins = 10    # Temporal binning granularity\r\n\r\n# Assign bins for lat/lon\r\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\ndata['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\n\r\n# Create unique spatial-temporal ID\r\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\n# Print unique bin counts\r\nprint(f\"✅ Unique Latitude Bins: {data['lat_bin'].nunique()}\")\r\nprint(f\"✅ Unique Longitude Bins: {data['lon_bin'].nunique()}\")\r\nprint(f\"✅ Unique Time Bins: {data['time_bin'].nunique()}\")\r\nprint(f\"✅ Unique Spatial-Temporal IDs: {data['grid_id'].nunique()}\")\r\n\r\n# Aggregate by grid_id\r\nmerged_nodes = data.groupby('grid_id').agg({\r\n    'Elevation': 'mean', 'Slope': 'mean', 'SWE': 'mean', 'snow_depth': 'mean',\r\n    'air_temperature_observed_f': 'mean', 'precipitation_amount': 'mean',\r\n    'relative_humidity_rmin': 'mean', 'relative_humidity_rmax': 'mean', 'wind_speed': 'mean',\r\n    'cumulative_SWE': 'mean', 'cumulative_precipitation_amount': 'mean', 'swe_value': 'mean',\r\n    'lat': 'mean', 'lon': 'mean', 'dayofyear': 'mean'\r\n}).reset_index()\r\n\r\n# Print merging stats\r\nprint(f\"✅ Merged nodes: {merged_nodes.shape[0]} from {data.shape[0]} original records.\")\r\n\r\nprint(\"🔄 Normalizing features using StandardScaler...\")\r\n\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\n\r\n# Ensure no missing values before normalization\r\nmerged_nodes[feature_columns] = merged_nodes[feature_columns].fillna(merged_nodes[feature_columns].median())\r\n\r\n# Print stats before normalization\r\nprint(\"📊 Feature statistics BEFORE normalization:\")\r\nprint(merged_nodes[feature_columns].describe())\r\n\r\n# Normalize node features\r\nscaler = StandardScaler()\r\nmerged_nodes[feature_columns] = scaler.fit_transform(merged_nodes[feature_columns].values)\r\n\r\n# Convert to tensor\r\nnode_features = torch.tensor(merged_nodes[feature_columns].values, dtype=torch.float)\r\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\n\r\n# Print stats after normalization\r\nprint(\"📊 Feature statistics AFTER normalization:\")\r\nprint(pd.DataFrame(node_features.numpy(), columns=feature_columns).describe())\r\n\r\nprint(f\"✅ Normalization completed. Node feature shape: {node_features.shape}\")\r\n\r\nprint(\"🔄 Building KDTree and constructing edges...\")\r\n\r\ncoordinates = merged_nodes[['lat', 'lon', 'dayofyear']].values  \r\ntree = KDTree(coordinates)\r\n\r\n# Adaptive k-value (between 5 and 10 based on graph density)\r\navg_degree = max(5, min(10, int(len(merged_nodes) ** 0.5 / 10)))\r\nspatial_threshold = 0.15\r\ntime_weight = 0.05  \r\n\r\ndef time_aware_distance(p1, p2):\r\n    spatial_dist = np.linalg.norm(p1[:2] - p2[:2])  \r\n    time_dist = abs(p1[2] - p2[2]) * time_weight  \r\n    return spatial_dist + time_dist\r\n\r\ndistances, neighbors = tree.query(coordinates, k=avg_degree+1)\r\n\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, avg_degree+1):\r\n        if time_aware_distance(coordinates[i], coordinates[neighbors[i][j]]) < spatial_threshold:\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T\r\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\n# Ensure bidirectional edges\r\nedge_index = to_undirected(edge_index)\r\n\r\nprint(f\"✅ Graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\r\n\r\n# Check for isolated nodes (nodes with 0 connections)\r\ndeg = degree(edge_index[0], num_nodes=merged_nodes.shape[0])\r\nnum_isolated_nodes = (deg == 0).sum().item()\r\nprint(f\"✅ Isolated Nodes: {num_isolated_nodes} ({(num_isolated_nodes / merged_nodes.shape[0]) * 100:.2f}%)\")\r\n\r\n# # Save graph\r\n# graph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n# torch.save(graph_data, \"/media/volume1/gnn_timeaware.pt\")\r\n\r\n# print(\"✅ Graph successfully saved.\")\r\n",
  "history_output" : "Loading dataset...\nTotal rows in raw dataset: 6155644\n Rows remaining after lat/lon filtering: 620662\n Missing values count per column:\ndate         0\nlat          0\nlon          0\nswe_value    0\ndtype: int64\n Found 2431 duplicate rows.\n Dataset size after deduplication: 618231\n🔍 Possible duplicates based on same location & date: 7482\n🔍 Possible duplicates based on same location (ignoring time): 617460\n🔍 Possible duplicates with small numerical variations (rounded to 3 decimals): 229\n🔄 Merging records for the same station on the same day...\n✅ Merged dataset size (after fixing same-location duplicates): 610749\n✅ Restored `dayofyear`. Sample values:\n        date  dayofyear\n0 2018-10-03        276\n1 2018-10-05        278\n2 2018-10-07        280\n3 2018-10-11        284\n4 2018-10-12        285\n🔄 Rounding numerical values to 3 decimals...\n✅ Final duplicate count after fixes: 0\n🔄 Merging nodes to reduce graph size...\n✅ Unique Latitude Bins: 293\n✅ Unique Longitude Bins: 369\n✅ Unique Time Bins: 11\n✅ Unique Spatial-Temporal IDs: 7375\n✅ Merged nodes: 7375 from 610749 original records.\n🔄 Normalizing features using StandardScaler...\n📊 Feature statistics BEFORE normalization:\n         Elevation  ...  cumulative_precipitation_amount\ncount  7375.000000  ...                      7375.000000\nmean    718.756401  ...                         1.060205\nstd     699.109205  ...                         1.469682\nmin       0.000000  ...                        -1.000000\n25%       0.000000  ...                        -1.000000\n50%     532.372000  ...                         1.417670\n75%    1348.370000  ...                         2.273114\nmax    2721.439000  ...                         3.489204\n[8 rows x 11 columns]\n📊 Feature statistics AFTER normalization:\n          Elevation  ...  cumulative_precipitation_amount\ncount  7.375000e+03  ...                     7.375000e+03\nmean   8.275953e-09  ...                     6.206965e-09\nstd    1.000068e+00  ...                     1.000070e+00\nmin   -1.028173e+00  ...                    -1.401898e+00\n25%   -1.028173e+00  ...                    -1.401898e+00\n50%   -2.666208e-01  ...                     2.432426e-01\n75%    9.006552e-01  ...                     8.253428e-01\nmax    2.864815e+00  ...                     1.652850e+00\n[8 rows x 11 columns]\n✅ Normalization completed. Node feature shape: torch.Size([7375, 11])\n🔄 Building KDTree and constructing edges...\n✅ Graph constructed with 7375 nodes and 16420 edges.\n✅ Isolated Nodes: 1754 (23.78%)\n",
  "history_begin_time" : 1741561940769,
  "history_end_time" : 1741561976698,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "ycsOEA8B4iHd",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom torch_geometric.utils import degree, to_undirected\r\n\r\n# Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\"Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\n\r\ndf_list = []\r\ntotal_rows = 0\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, parse_dates=['date'], chunksize=chunksize):\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear  \r\n    total_rows +=len(chunk)\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Total rows in raw dataset: {total_rows}\")\r\nprint(f\" Rows remaining after lat/lon filtering: {data.shape[0]}\")\r\n\r\n# Check for missing values in critical columns\r\nmissing_values = data[['date', 'lat', 'lon', 'swe_value']].isnull().sum()\r\nprint(f\" Missing values count per column:\\n{missing_values}\")\r\n\r\n# Remove duplicates\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\ndata = data.drop_duplicates()\r\nprint(f\" Dataset size after deduplication: {data.shape[0]}\")\r\n\r\n# Check duplicates based only on ('lat', 'lon', 'date')\r\nlocation_time_duplicates = data.duplicated(subset=['lat', 'lon', 'date']).sum()\r\nprint(f\"🔍 Possible duplicates based on same location & date: {location_time_duplicates}\")\r\n\r\n# Check duplicates based on ('lat', 'lon') only (ignoring time)\r\nlocation_duplicates = data.duplicated(subset=['lat', 'lon']).sum()\r\nprint(f\"🔍 Possible duplicates based on same location (ignoring time): {location_duplicates}\")\r\n\r\n# Check for small variations in numerical values (rounding to 3 decimals)\r\nnumerical_cols = ['Elevation', 'Slope', 'SWE', 'snow_depth',\r\n                  'air_temperature_observed_f', 'precipitation_amount',\r\n                  'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n                  'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value']\r\n\r\n# Ensure we only use columns that exist in the dataframe\r\nexisting_numerical_cols = list(set(numerical_cols) & set(data.columns))\r\n\r\n# Apply rounding to check for floating-point duplicates\r\nrounded_data = data.copy()\r\nrounded_data[existing_numerical_cols] = rounded_data[existing_numerical_cols].round(3)\r\nnumerical_variation_duplicates = rounded_data.duplicated().sum()\r\n\r\nprint(f\"🔍 Possible duplicates with small numerical variations (rounded to 3 decimals): {numerical_variation_duplicates}\")\r\n\r\n#  Merge records with the same station on the same day (average values)\r\nprint(\"🔄 Merging records for the same station on the same day...\")\r\nmerge_columns = ['Elevation', 'Slope', 'SWE', 'snow_depth',\r\n                 'air_temperature_observed_f', 'precipitation_amount',\r\n                 'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n                 'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value']\r\n\r\ndata = data.groupby(['lat', 'lon', 'date'], as_index=False)[merge_columns].mean()\r\ndata['dayofyear'] = pd.to_datetime(data['date']).dt.dayofyear\r\nprint(f\"✅ Merged dataset size (after fixing same-location duplicates): {data.shape[0]}\")\r\nprint(f\"✅ Restored `dayofyear`. Sample values:\\n{data[['date', 'dayofyear']].head()}\")\r\n\r\n# Fix 2️⃣: Round numerical values to avoid float precision issues\r\nprint(\"🔄 Rounding numerical values to 3 decimals...\")\r\nnumerical_cols = list(set(merge_columns) & set(data.columns))  # Ensure existing columns only\r\ndata[numerical_cols] = data[numerical_cols].round(3)\r\n\r\n# Check duplicates again after merging & rounding\r\nfinal_duplicates = data.duplicated().sum()\r\nprint(f\"✅ Final duplicate count after fixes: {final_duplicates}\")\r\n\r\nprint(\"🔄 Merging nodes to reduce graph size...\")\r\n\r\ngrid_size = 0.01  # Spatial binning granularity\r\ntime_bins = 10    # Temporal binning granularity\r\n\r\n# Assign bins for lat/lon\r\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\ndata['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\n\r\n# Create unique spatial-temporal ID\r\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\n# Print unique bin counts\r\nprint(f\"✅ Unique Latitude Bins: {data['lat_bin'].nunique()}\")\r\nprint(f\"✅ Unique Longitude Bins: {data['lon_bin'].nunique()}\")\r\nprint(f\"✅ Unique Time Bins: {data['time_bin'].nunique()}\")\r\nprint(f\"✅ Unique Spatial-Temporal IDs: {data['grid_id'].nunique()}\")\r\n\r\n# Aggregate by grid_id\r\nmerged_nodes = data.groupby('grid_id').agg({\r\n    'Elevation': 'mean', 'Slope': 'mean', 'SWE': 'mean', 'snow_depth': 'mean',\r\n    'air_temperature_observed_f': 'mean', 'precipitation_amount': 'mean',\r\n    'relative_humidity_rmin': 'mean', 'relative_humidity_rmax': 'mean', 'wind_speed': 'mean',\r\n    'cumulative_SWE': 'mean', 'cumulative_precipitation_amount': 'mean', 'swe_value': 'mean',\r\n    'lat': 'mean', 'lon': 'mean', 'dayofyear': 'mean'\r\n}).reset_index()\r\n\r\n# Print merging stats\r\nprint(f\"✅ Merged nodes: {merged_nodes.shape[0]} from {data.shape[0]} original records.\")\r\n\r\nprint(\"🔄 Normalizing features using StandardScaler...\")\r\n\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\n\r\n# Ensure no missing values before normalization\r\nmerged_nodes[feature_columns] = merged_nodes[feature_columns].fillna(merged_nodes[feature_columns].median())\r\n\r\n# Print stats before normalization\r\nprint(\"📊 Feature statistics BEFORE normalization:\")\r\nprint(merged_nodes[feature_columns].describe())\r\n\r\n# Normalize node features\r\nscaler = StandardScaler()\r\nmerged_nodes[feature_columns] = scaler.fit_transform(merged_nodes[feature_columns].values)\r\n\r\n# Convert to tensor\r\nnode_features = torch.tensor(merged_nodes[feature_columns].values, dtype=torch.float)\r\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\n\r\n# Print stats after normalization\r\nprint(\"📊 Feature statistics AFTER normalization:\")\r\nprint(pd.DataFrame(node_features.numpy(), columns=feature_columns).describe())\r\n\r\nprint(f\"✅ Normalization completed. Node feature shape: {node_features.shape}\")\r\n\r\n\r\n# # Build Graph Using KDTree\r\n# print(\"Building KDTree and constructing edges...\")\r\n\r\n# coordinates = merged_nodes[['lat', 'lon', 'dayofyear']].values  \r\n# tree = KDTree(coordinates)\r\n\r\n# # Adaptive k-value (between 5 and 10 based on graph density)\r\n# avg_degree = max(5, min(10, int(len(merged_nodes) ** 0.5 / 10)))\r\n# spatial_threshold = 0.15\r\n# time_weight = 0.05  \r\n\r\n# def time_aware_distance(p1, p2):\r\n#     spatial_dist = np.linalg.norm(p1[:2] - p2[:2])  \r\n#     time_dist = abs(p1[2] - p2[2]) * time_weight  \r\n#     return spatial_dist + time_dist\r\n\r\n# distances, neighbors = tree.query(coordinates, k=avg_degree+1)\r\n\r\n# edge_list = []\r\n# for i in range(len(neighbors)):\r\n#     for j in range(1, avg_degree+1):\r\n#         if time_aware_distance(coordinates[i], coordinates[neighbors[i][j]]) < spatial_threshold:\r\n#             edge_list.append((i, neighbors[i][j]))\r\n\r\n# edges = np.array(edge_list).T\r\n# edge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\n# edge_index = to_undirected(edge_index)  # Ensure bidirectional edges\r\n# print(f\"Graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\r\n\r\n# graph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n# torch.save(graph_data, \"/media/volume1/gnn_timeaware1.pt\")\r\n# print(\"Graph successfully saved.\")",
  "history_output" : "Loading dataset...\nTotal rows in raw dataset: 6155644\n Rows remaining after lat/lon filtering: 620662\n Missing values count per column:\ndate         0\nlat          0\nlon          0\nswe_value    0\ndtype: int64\n Found 2431 duplicate rows.\n Dataset size after deduplication: 618231\n🔍 Possible duplicates based on same location & date: 7482\n🔍 Possible duplicates based on same location (ignoring time): 617460\n🔍 Possible duplicates with small numerical variations (rounded to 3 decimals): 229\n🔄 Merging records for the same station on the same day...\n✅ Merged dataset size (after fixing same-location duplicates): 610749\n✅ Restored `dayofyear`. Sample values:\n        date  dayofyear\n0 2018-10-03        276\n1 2018-10-05        278\n2 2018-10-07        280\n3 2018-10-11        284\n4 2018-10-12        285\n🔄 Rounding numerical values to 3 decimals...\n✅ Final duplicate count after fixes: 0\n🔄 Merging nodes to reduce graph size...\n✅ Unique Latitude Bins: 293\n✅ Unique Longitude Bins: 369\n✅ Unique Time Bins: 11\n✅ Unique Spatial-Temporal IDs: 7375\n✅ Merged nodes: 7375 from 610749 original records.\n🔄 Normalizing features using StandardScaler...\n📊 Feature statistics BEFORE normalization:\n         Elevation  ...  cumulative_precipitation_amount\ncount  7375.000000  ...                      7375.000000\nmean    718.756401  ...                         1.060205\nstd     699.109205  ...                         1.469682\nmin       0.000000  ...                        -1.000000\n25%       0.000000  ...                        -1.000000\n50%     532.372000  ...                         1.417670\n75%    1348.370000  ...                         2.273114\nmax    2721.439000  ...                         3.489204\n[8 rows x 11 columns]\n📊 Feature statistics AFTER normalization:\n          Elevation  ...  cumulative_precipitation_amount\ncount  7.375000e+03  ...                     7.375000e+03\nmean   8.275953e-09  ...                     6.206965e-09\nstd    1.000068e+00  ...                     1.000070e+00\nmin   -1.028173e+00  ...                    -1.401898e+00\n25%   -1.028173e+00  ...                    -1.401898e+00\n50%   -2.666208e-01  ...                     2.432426e-01\n75%    9.006552e-01  ...                     8.253428e-01\nmax    2.864815e+00  ...                     1.652850e+00\n[8 rows x 11 columns]\n✅ Normalization completed. Node feature shape: torch.Size([7375, 11])\n",
  "history_begin_time" : 1741561740703,
  "history_end_time" : 1741561776605,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "BJq6KytTidKE",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom torch_geometric.utils import degree, to_undirected\r\n\r\n# Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\"Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\n\r\ndf_list = []\r\ntotal_rows = 0\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, parse_dates=['date'], chunksize=chunksize):\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear  \r\n    total_rows +=len(chunk)\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Total rows in raw dataset: {total_rows}\")\r\nprint(f\" Rows remaining after lat/lon filtering: {data.shape[0]}\")\r\n\r\n# Check for missing values in critical columns\r\nmissing_values = data[['date', 'lat', 'lon', 'swe_value']].isnull().sum()\r\nprint(f\" Missing values count per column:\\n{missing_values}\")\r\n\r\n# Remove duplicates\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\ndata = data.drop_duplicates()\r\nprint(f\" Dataset size after deduplication: {data.shape[0]}\")\r\n\r\n# Check duplicates based only on ('lat', 'lon', 'date')\r\nlocation_time_duplicates = data.duplicated(subset=['lat', 'lon', 'date']).sum()\r\nprint(f\"🔍 Possible duplicates based on same location & date: {location_time_duplicates}\")\r\n\r\n# Check duplicates based on ('lat', 'lon') only (ignoring time)\r\nlocation_duplicates = data.duplicated(subset=['lat', 'lon']).sum()\r\nprint(f\"🔍 Possible duplicates based on same location (ignoring time): {location_duplicates}\")\r\n\r\n# Check for small variations in numerical values (rounding to 3 decimals)\r\nnumerical_cols = ['Elevation', 'Slope', 'SWE', 'snow_depth',\r\n                  'air_temperature_observed_f', 'precipitation_amount',\r\n                  'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n                  'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value']\r\n\r\n# Ensure we only use columns that exist in the dataframe\r\nexisting_numerical_cols = list(set(numerical_cols) & set(data.columns))\r\n\r\n# Apply rounding to check for floating-point duplicates\r\nrounded_data = data.copy()\r\nrounded_data[existing_numerical_cols] = rounded_data[existing_numerical_cols].round(3)\r\nnumerical_variation_duplicates = rounded_data.duplicated().sum()\r\n\r\nprint(f\"🔍 Possible duplicates with small numerical variations (rounded to 3 decimals): {numerical_variation_duplicates}\")\r\n\r\n#  Merge records with the same station on the same day (average values)\r\nprint(\"🔄 Merging records for the same station on the same day...\")\r\nmerge_columns = ['Elevation', 'Slope', 'SWE', 'snow_depth',\r\n                 'air_temperature_observed_f', 'precipitation_amount',\r\n                 'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n                 'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value']\r\n\r\ndata = data.groupby(['lat', 'lon', 'date'], as_index=False)[merge_columns].mean()\r\ndata['dayofyear'] = pd.to_datetime(data['date']).dt.dayofyear\r\nprint(f\"✅ Merged dataset size (after fixing same-location duplicates): {data.shape[0]}\")\r\nprint(f\"✅ Restored `dayofyear`. Sample values:\\n{data[['date', 'dayofyear']].head()}\")\r\n\r\n# Fix 2️⃣: Round numerical values to avoid float precision issues\r\nprint(\"🔄 Rounding numerical values to 3 decimals...\")\r\nnumerical_cols = list(set(merge_columns) & set(data.columns))  # Ensure existing columns only\r\ndata[numerical_cols] = data[numerical_cols].round(3)\r\n\r\n# Check duplicates again after merging & rounding\r\nfinal_duplicates = data.duplicated().sum()\r\nprint(f\"✅ Final duplicate count after fixes: {final_duplicates}\")\r\n\r\nprint(\"🔄 Merging nodes to reduce graph size...\")\r\n\r\ngrid_size = 0.01  # Spatial binning granularity\r\ntime_bins = 10    # Temporal binning granularity\r\n\r\n# Assign bins for lat/lon\r\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\ndata['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\n\r\n# Create unique spatial-temporal ID\r\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\n# Print unique bin counts\r\nprint(f\"✅ Unique Latitude Bins: {data['lat_bin'].nunique()}\")\r\nprint(f\"✅ Unique Longitude Bins: {data['lon_bin'].nunique()}\")\r\nprint(f\"✅ Unique Time Bins: {data['time_bin'].nunique()}\")\r\nprint(f\"✅ Unique Spatial-Temporal IDs: {data['grid_id'].nunique()}\")\r\n\r\n# Aggregate by grid_id\r\nmerged_nodes = data.groupby('grid_id').agg({\r\n    'Elevation': 'mean', 'Slope': 'mean', 'SWE': 'mean', 'snow_depth': 'mean',\r\n    'air_temperature_observed_f': 'mean', 'precipitation_amount': 'mean',\r\n    'relative_humidity_rmin': 'mean', 'relative_humidity_rmax': 'mean', 'wind_speed': 'mean',\r\n    'cumulative_SWE': 'mean', 'cumulative_precipitation_amount': 'mean', 'swe_value': 'mean',\r\n    'lat': 'mean', 'lon': 'mean', 'dayofyear': 'mean'\r\n}).reset_index()\r\n\r\n# Print merging stats\r\nprint(f\"✅ Merged nodes: {merged_nodes.shape[0]} from {data.shape[0]} original records.\")\r\n\r\n# # Normalize Features\r\n# print(\"Normalizing features using StandardScaler...\")\r\n# feature_columns = [\r\n#     'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n#     'air_temperature_observed_f', 'precipitation_amount',\r\n#     'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n#     'cumulative_SWE', 'cumulative_precipitation_amount'\r\n# ]\r\n\r\n# scaler = StandardScaler()\r\n# node_features = scaler.fit_transform(merged_nodes[feature_columns])\r\n# node_features = torch.tensor(node_features, dtype=torch.float)\r\n# labels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\n\r\n# print(f\"Normalization completed. Node feature shape: {node_features.shape}\")\r\n\r\n# # Build Graph Using KDTree\r\n# print(\"Building KDTree and constructing edges...\")\r\n\r\n# coordinates = merged_nodes[['lat', 'lon', 'dayofyear']].values  \r\n# tree = KDTree(coordinates)\r\n\r\n# # Adaptive k-value (between 5 and 10 based on graph density)\r\n# avg_degree = max(5, min(10, int(len(merged_nodes) ** 0.5 / 10)))\r\n# spatial_threshold = 0.15\r\n# time_weight = 0.05  \r\n\r\n# def time_aware_distance(p1, p2):\r\n#     spatial_dist = np.linalg.norm(p1[:2] - p2[:2])  \r\n#     time_dist = abs(p1[2] - p2[2]) * time_weight  \r\n#     return spatial_dist + time_dist\r\n\r\n# distances, neighbors = tree.query(coordinates, k=avg_degree+1)\r\n\r\n# edge_list = []\r\n# for i in range(len(neighbors)):\r\n#     for j in range(1, avg_degree+1):\r\n#         if time_aware_distance(coordinates[i], coordinates[neighbors[i][j]]) < spatial_threshold:\r\n#             edge_list.append((i, neighbors[i][j]))\r\n\r\n# edges = np.array(edge_list).T\r\n# edge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\n# edge_index = to_undirected(edge_index)  # Ensure bidirectional edges\r\n# print(f\"Graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\r\n\r\n# graph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n# torch.save(graph_data, \"/media/volume1/gnn_timeaware1.pt\")\r\n# print(\"Graph successfully saved.\")",
  "history_output" : "Loading dataset...\nTotal rows in raw dataset: 6155644\n Rows remaining after lat/lon filtering: 620662\n Missing values count per column:\ndate         0\nlat          0\nlon          0\nswe_value    0\ndtype: int64\n Found 2431 duplicate rows.\n Dataset size after deduplication: 618231\n🔍 Possible duplicates based on same location & date: 7482\n🔍 Possible duplicates based on same location (ignoring time): 617460\n🔍 Possible duplicates with small numerical variations (rounded to 3 decimals): 229\n🔄 Merging records for the same station on the same day...\n✅ Merged dataset size (after fixing same-location duplicates): 610749\n✅ Restored `dayofyear`. Sample values:\n        date  dayofyear\n0 2018-10-03        276\n1 2018-10-05        278\n2 2018-10-07        280\n3 2018-10-11        284\n4 2018-10-12        285\n🔄 Rounding numerical values to 3 decimals...\n✅ Final duplicate count after fixes: 0\n🔄 Merging nodes to reduce graph size...\n✅ Unique Latitude Bins: 293\n✅ Unique Longitude Bins: 369\n✅ Unique Time Bins: 11\n✅ Unique Spatial-Temporal IDs: 7375\n✅ Merged nodes: 7375 from 610749 original records.\n",
  "history_begin_time" : 1741561209334,
  "history_end_time" : 1741561244408,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "H1f9Oq6XRBEH",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom torch_geometric.utils import degree, to_undirected\r\n\r\n# Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\"Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\n\r\ndf_list = []\r\ntotal_rows = 0\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, parse_dates=['date'], chunksize=chunksize):\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear  \r\n    total_rows +=len(chunk)\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Total rows in raw dataset: {total_rows}\")\r\nprint(f\" Rows remaining after lat/lon filtering: {data.shape[0]}\")\r\n\r\n# Check for missing values in critical columns\r\nmissing_values = data[['date', 'lat', 'lon', 'swe_value']].isnull().sum()\r\nprint(f\" Missing values count per column:\\n{missing_values}\")\r\n\r\n# Remove duplicates\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\ndata = data.drop_duplicates()\r\nprint(f\" Dataset size after deduplication: {data.shape[0]}\")\r\n\r\n# Check duplicates based only on ('lat', 'lon', 'date')\r\nlocation_time_duplicates = data.duplicated(subset=['lat', 'lon', 'date']).sum()\r\nprint(f\"🔍 Possible duplicates based on same location & date: {location_time_duplicates}\")\r\n\r\n# Check duplicates based on ('lat', 'lon') only (ignoring time)\r\nlocation_duplicates = data.duplicated(subset=['lat', 'lon']).sum()\r\nprint(f\"🔍 Possible duplicates based on same location (ignoring time): {location_duplicates}\")\r\n\r\n# Check for small variations in numerical values (rounding to 3 decimals)\r\nnumerical_cols = ['Elevation', 'Slope', 'SWE', 'snow_depth',\r\n                  'air_temperature_observed_f', 'precipitation_amount',\r\n                  'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n                  'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value']\r\n\r\n# Ensure we only use columns that exist in the dataframe\r\nexisting_numerical_cols = list(set(numerical_cols) & set(data.columns))\r\n\r\n# Apply rounding to check for floating-point duplicates\r\nrounded_data = data.copy()\r\nrounded_data[existing_numerical_cols] = rounded_data[existing_numerical_cols].round(3)\r\nnumerical_variation_duplicates = rounded_data.duplicated().sum()\r\n\r\nprint(f\"🔍 Possible duplicates with small numerical variations (rounded to 3 decimals): {numerical_variation_duplicates}\")\r\n\r\n# Fix 1️⃣: Merge records with the same station on the same day (average values)\r\nprint(\"🔄 Merging records for the same station on the same day...\")\r\nmerge_columns = ['Elevation', 'Slope', 'SWE', 'snow_depth',\r\n                 'air_temperature_observed_f', 'precipitation_amount',\r\n                 'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n                 'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value']\r\n\r\ndata = data.groupby(['lat', 'lon', 'date'], as_index=False)[merge_columns].mean()\r\nprint(f\"✅ Merged dataset size (after fixing same-location duplicates): {data.shape[0]}\")\r\n\r\n# Fix 2️⃣: Round numerical values to avoid float precision issues\r\nprint(\"🔄 Rounding numerical values to 3 decimals...\")\r\nnumerical_cols = list(set(merge_columns) & set(data.columns))  # Ensure existing columns only\r\ndata[numerical_cols] = data[numerical_cols].round(3)\r\n\r\n# Check duplicates again after merging & rounding\r\nfinal_duplicates = data.duplicated().sum()\r\nprint(f\"✅ Final duplicate count after fixes: {final_duplicates}\")\r\n\r\nprint(\"🔄 Merging nodes to reduce graph size...\")\r\n\r\ngrid_size = 0.01  # Spatial binning granularity\r\ntime_bins = 10    # Temporal binning granularity\r\n\r\n# Assign bins for lat/lon\r\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\ndata['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\n\r\n# Create unique spatial-temporal ID\r\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\n# Print unique bin counts\r\nprint(f\"✅ Unique Latitude Bins: {data['lat_bin'].nunique()}\")\r\nprint(f\"✅ Unique Longitude Bins: {data['lon_bin'].nunique()}\")\r\nprint(f\"✅ Unique Time Bins: {data['time_bin'].nunique()}\")\r\nprint(f\"✅ Unique Spatial-Temporal IDs: {data['grid_id'].nunique()}\")\r\n\r\n# Aggregate by grid_id\r\nmerged_nodes = data.groupby('grid_id').agg({\r\n    'Elevation': 'mean', 'Slope': 'mean', 'SWE': 'mean', 'snow_depth': 'mean',\r\n    'air_temperature_observed_f': 'mean', 'precipitation_amount': 'mean',\r\n    'relative_humidity_rmin': 'mean', 'relative_humidity_rmax': 'mean', 'wind_speed': 'mean',\r\n    'cumulative_SWE': 'mean', 'cumulative_precipitation_amount': 'mean', 'swe_value': 'mean',\r\n    'lat': 'mean', 'lon': 'mean', 'dayofyear': 'mean'\r\n}).reset_index()\r\n\r\n# Print merging stats\r\nprint(f\"✅ Merged nodes: {merged_nodes.shape[0]} from {data.shape[0]} original records.\")\r\n\r\n# # Normalize Features\r\n# print(\"Normalizing features using StandardScaler...\")\r\n# feature_columns = [\r\n#     'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n#     'air_temperature_observed_f', 'precipitation_amount',\r\n#     'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n#     'cumulative_SWE', 'cumulative_precipitation_amount'\r\n# ]\r\n\r\n# scaler = StandardScaler()\r\n# node_features = scaler.fit_transform(merged_nodes[feature_columns])\r\n# node_features = torch.tensor(node_features, dtype=torch.float)\r\n# labels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\n\r\n# print(f\"Normalization completed. Node feature shape: {node_features.shape}\")\r\n\r\n# # Build Graph Using KDTree\r\n# print(\"Building KDTree and constructing edges...\")\r\n\r\n# coordinates = merged_nodes[['lat', 'lon', 'dayofyear']].values  \r\n# tree = KDTree(coordinates)\r\n\r\n# # Adaptive k-value (between 5 and 10 based on graph density)\r\n# avg_degree = max(5, min(10, int(len(merged_nodes) ** 0.5 / 10)))\r\n# spatial_threshold = 0.15\r\n# time_weight = 0.05  \r\n\r\n# def time_aware_distance(p1, p2):\r\n#     spatial_dist = np.linalg.norm(p1[:2] - p2[:2])  \r\n#     time_dist = abs(p1[2] - p2[2]) * time_weight  \r\n#     return spatial_dist + time_dist\r\n\r\n# distances, neighbors = tree.query(coordinates, k=avg_degree+1)\r\n\r\n# edge_list = []\r\n# for i in range(len(neighbors)):\r\n#     for j in range(1, avg_degree+1):\r\n#         if time_aware_distance(coordinates[i], coordinates[neighbors[i][j]]) < spatial_threshold:\r\n#             edge_list.append((i, neighbors[i][j]))\r\n\r\n# edges = np.array(edge_list).T\r\n# edge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\n# edge_index = to_undirected(edge_index)  # Ensure bidirectional edges\r\n# print(f\"Graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\r\n\r\n# graph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n# torch.save(graph_data, \"/media/volume1/gnn_timeaware1.pt\")\r\n# print(\"Graph successfully saved.\")",
  "history_output" : "Loading dataset...\nTotal rows in raw dataset: 6155644\n Rows remaining after lat/lon filtering: 620662\n Missing values count per column:\ndate         0\nlat          0\nlon          0\nswe_value    0\ndtype: int64\n Found 2431 duplicate rows.\n Dataset size after deduplication: 618231\n🔍 Possible duplicates based on same location & date: 7482\n🔍 Possible duplicates based on same location (ignoring time): 617460\n🔍 Possible duplicates with small numerical variations (rounded to 3 decimals): 229\n🔄 Merging records for the same station on the same day...\n✅ Merged dataset size (after fixing same-location duplicates): 610749\n🔄 Rounding numerical values to 3 decimals...\n✅ Final duplicate count after fixes: 0\n🔄 Merging nodes to reduce graph size...\nTraceback (most recent call last):\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py\", line 3805, in get_loc\n    return self._engine.get_loc(casted_key)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"index.pyx\", line 167, in pandas._libs.index.IndexEngine.get_loc\n  File \"index.pyx\", line 196, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7081, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7089, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'dayofyear'\nThe above exception was the direct cause of the following exception:\nTraceback (most recent call last):\n  File \"/media/volume1/gw-workspace/H1f9Oq6XRBEH/graph_creation.py\", line 102, in <module>\n    data['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\n                        ~~~~^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py\", line 4102, in __getitem__\n    indexer = self.columns.get_loc(key)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py\", line 3812, in get_loc\n    raise KeyError(key) from err\nKeyError: 'dayofyear'\n",
  "history_begin_time" : 1741561034342,
  "history_end_time" : 1741561068530,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "4snJKDLH8zDh",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom torch_geometric.utils import degree, to_undirected\r\n\r\n# Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\"Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\n\r\ndf_list = []\r\ntotal_rows = 0\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, parse_dates=['date'], chunksize=chunksize):\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear  \r\n    total_rows +=len(chunk)\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Total rows in raw dataset: {total_rows}\")\r\nprint(f\" Rows remaining after lat/lon filtering: {data.shape[0]}\")\r\n\r\n# Check for missing values in critical columns\r\nmissing_values = data[['date', 'lat', 'lon', 'swe_value']].isnull().sum()\r\nprint(f\" Missing values count per column:\\n{missing_values}\")\r\n\r\n# Remove duplicates\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\ndata = data.drop_duplicates()\r\nprint(f\" Dataset size after deduplication: {data.shape[0]}\")\r\n\r\n# Check duplicates based only on ('lat', 'lon', 'date')\r\nlocation_time_duplicates = data.duplicated(subset=['lat', 'lon', 'date']).sum()\r\nprint(f\"🔍 Possible duplicates based on same location & date: {location_time_duplicates}\")\r\n\r\n# Check duplicates based on ('lat', 'lon') only (ignoring time)\r\nlocation_duplicates = data.duplicated(subset=['lat', 'lon']).sum()\r\nprint(f\"🔍 Possible duplicates based on same location (ignoring time): {location_duplicates}\")\r\n\r\n# Check for small variations in numerical values (rounding to 3 decimals)\r\nnumerical_cols = ['Elevation', 'Slope', 'SWE', 'snow_depth',\r\n                  'air_temperature_observed_f', 'precipitation_amount',\r\n                  'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n                  'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value']\r\n\r\n# Ensure we only use columns that exist in the dataframe\r\nexisting_numerical_cols = list(set(numerical_cols) & set(data.columns))\r\n\r\n# Apply rounding to check for floating-point duplicates\r\nrounded_data = data.copy()\r\nrounded_data[existing_numerical_cols] = rounded_data[existing_numerical_cols].round(3)\r\nnumerical_variation_duplicates = rounded_data.duplicated().sum()\r\n\r\nprint(f\"🔍 Possible duplicates with small numerical variations (rounded to 3 decimals): {numerical_variation_duplicates}\")\r\n\r\n# Fix 1️⃣: Merge records with the same station on the same day (average values)\r\nprint(\"🔄 Merging records for the same station on the same day...\")\r\nmerge_columns = ['Elevation', 'Slope', 'SWE', 'snow_depth',\r\n                 'air_temperature_observed_f', 'precipitation_amount',\r\n                 'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n                 'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value']\r\n\r\ndata = data.groupby(['lat', 'lon', 'date'], as_index=False)[merge_columns].mean()\r\nprint(f\"✅ Merged dataset size (after fixing same-location duplicates): {data.shape[0]}\")\r\n\r\n# Fix 2️⃣: Round numerical values to avoid float precision issues\r\nprint(\"🔄 Rounding numerical values to 3 decimals...\")\r\nnumerical_cols = list(set(merge_columns) & set(data.columns))  # Ensure existing columns only\r\ndata[numerical_cols] = data[numerical_cols].round(3)\r\n\r\n# Check duplicates again after merging & rounding\r\nfinal_duplicates = data.duplicated().sum()\r\nprint(f\"✅ Final duplicate count after fixes: {final_duplicates}\")\r\n\r\n\r\n# # Merge Nodes (Spatial & Time Binning)\r\n# print(\"Merging nodes to reduce graph size...\")\r\n\r\n# grid_size = 0.01\r\n# time_bins = 10\r\n\r\n# data['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\n# data['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\n# data['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\n# data['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\n# merged_nodes = data.groupby('grid_id').agg({\r\n#     'Elevation': 'mean', 'Slope': 'mean', 'SWE': 'mean', 'snow_depth': 'mean',\r\n#     'air_temperature_observed_f': 'mean', 'precipitation_amount': 'mean',\r\n#     'relative_humidity_rmin': 'mean', 'relative_humidity_rmax': 'mean', 'wind_speed': 'mean',\r\n#     'cumulative_SWE': 'mean', 'cumulative_precipitation_amount': 'mean', 'swe_value': 'mean',\r\n#     'lat': 'mean', 'lon': 'mean', 'dayofyear': 'mean'\r\n# }).reset_index()\r\n\r\n# print(f\"Merged nodes: {merged_nodes.shape[0]} from {data.shape[0]} original records.\")\r\n\r\n# # Normalize Features\r\n# print(\"Normalizing features using StandardScaler...\")\r\n# feature_columns = [\r\n#     'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n#     'air_temperature_observed_f', 'precipitation_amount',\r\n#     'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n#     'cumulative_SWE', 'cumulative_precipitation_amount'\r\n# ]\r\n\r\n# scaler = StandardScaler()\r\n# node_features = scaler.fit_transform(merged_nodes[feature_columns])\r\n# node_features = torch.tensor(node_features, dtype=torch.float)\r\n# labels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\n\r\n# print(f\"Normalization completed. Node feature shape: {node_features.shape}\")\r\n\r\n# # Build Graph Using KDTree\r\n# print(\"Building KDTree and constructing edges...\")\r\n\r\n# coordinates = merged_nodes[['lat', 'lon', 'dayofyear']].values  \r\n# tree = KDTree(coordinates)\r\n\r\n# # Adaptive k-value (between 5 and 10 based on graph density)\r\n# avg_degree = max(5, min(10, int(len(merged_nodes) ** 0.5 / 10)))\r\n# spatial_threshold = 0.15\r\n# time_weight = 0.05  \r\n\r\n# def time_aware_distance(p1, p2):\r\n#     spatial_dist = np.linalg.norm(p1[:2] - p2[:2])  \r\n#     time_dist = abs(p1[2] - p2[2]) * time_weight  \r\n#     return spatial_dist + time_dist\r\n\r\n# distances, neighbors = tree.query(coordinates, k=avg_degree+1)\r\n\r\n# edge_list = []\r\n# for i in range(len(neighbors)):\r\n#     for j in range(1, avg_degree+1):\r\n#         if time_aware_distance(coordinates[i], coordinates[neighbors[i][j]]) < spatial_threshold:\r\n#             edge_list.append((i, neighbors[i][j]))\r\n\r\n# edges = np.array(edge_list).T\r\n# edge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\n# edge_index = to_undirected(edge_index)  # Ensure bidirectional edges\r\n# print(f\"Graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\r\n\r\n# graph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n# torch.save(graph_data, \"/media/volume1/gnn_timeaware1.pt\")\r\n# print(\"Graph successfully saved.\")",
  "history_output" : "Loading dataset...\nTotal rows in raw dataset: 6155644\n Rows remaining after lat/lon filtering: 620662\n Missing values count per column:\ndate         0\nlat          0\nlon          0\nswe_value    0\ndtype: int64\n Found 2431 duplicate rows.\n Dataset size after deduplication: 618231\n🔍 Possible duplicates based on same location & date: 7482\n🔍 Possible duplicates based on same location (ignoring time): 617460\n🔍 Possible duplicates with small numerical variations (rounded to 3 decimals): 229\n🔄 Merging records for the same station on the same day...\n✅ Merged dataset size (after fixing same-location duplicates): 610749\n🔄 Rounding numerical values to 3 decimals...\n✅ Final duplicate count after fixes: 0\n",
  "history_begin_time" : 1741560757798,
  "history_end_time" : 1741560792392,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "NPBLqGAhjtkt",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom torch_geometric.utils import degree, to_undirected\r\n\r\n# Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\"Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\n\r\ndf_list = []\r\ntotal_rows = 0\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, parse_dates=['date'], chunksize=chunksize):\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear  \r\n    total_rows +=len(chunk)\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Total rows in raw dataset: {total_rows}\")\r\nprint(f\" Rows remaining after lat/lon filtering: {data.shape[0]}\")\r\n\r\n# Check for missing values in critical columns\r\nmissing_values = data[['date', 'lat', 'lon', 'swe_value']].isnull().sum()\r\nprint(f\" Missing values count per column:\\n{missing_values}\")\r\n\r\n# Remove duplicates\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\ndata = data.drop_duplicates()\r\nprint(f\" Dataset size after deduplication: {data.shape[0]}\")\r\n\r\n# Check duplicates based only on ('lat', 'lon', 'date')\r\nlocation_time_duplicates = data.duplicated(subset=['lat', 'lon', 'date']).sum()\r\nprint(f\"🔍 Possible duplicates based on same location & date: {location_time_duplicates}\")\r\n\r\n# Check duplicates based on ('lat', 'lon') only (ignoring time)\r\nlocation_duplicates = data.duplicated(subset=['lat', 'lon']).sum()\r\nprint(f\"🔍 Possible duplicates based on same location (ignoring time): {location_duplicates}\")\r\n\r\n# Check for small variations in numerical values (rounding to 3 decimals)\r\nnumerical_cols = ['Elevation', 'Slope', 'SWE', 'snow_depth',\r\n                  'air_temperature_observed_f', 'precipitation_amount',\r\n                  'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n                  'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value']\r\n\r\n# Ensure we only use columns that exist in the dataframe\r\nexisting_numerical_cols = list(set(numerical_cols) & set(data.columns))\r\n\r\n# Apply rounding to check for floating-point duplicates\r\nrounded_data = data.copy()\r\nrounded_data[existing_numerical_cols] = rounded_data[existing_numerical_cols].round(3)\r\nnumerical_variation_duplicates = rounded_data.duplicated().sum()\r\n\r\nprint(f\"🔍 Possible duplicates with small numerical variations (rounded to 3 decimals): {numerical_variation_duplicates}\")\r\n\r\n# # Merge Nodes (Spatial & Time Binning)\r\n# print(\"Merging nodes to reduce graph size...\")\r\n\r\n# grid_size = 0.01\r\n# time_bins = 10\r\n\r\n# data['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\n# data['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\n# data['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\n# data['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\n# merged_nodes = data.groupby('grid_id').agg({\r\n#     'Elevation': 'mean', 'Slope': 'mean', 'SWE': 'mean', 'snow_depth': 'mean',\r\n#     'air_temperature_observed_f': 'mean', 'precipitation_amount': 'mean',\r\n#     'relative_humidity_rmin': 'mean', 'relative_humidity_rmax': 'mean', 'wind_speed': 'mean',\r\n#     'cumulative_SWE': 'mean', 'cumulative_precipitation_amount': 'mean', 'swe_value': 'mean',\r\n#     'lat': 'mean', 'lon': 'mean', 'dayofyear': 'mean'\r\n# }).reset_index()\r\n\r\n# print(f\"Merged nodes: {merged_nodes.shape[0]} from {data.shape[0]} original records.\")\r\n\r\n# # Normalize Features\r\n# print(\"Normalizing features using StandardScaler...\")\r\n# feature_columns = [\r\n#     'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n#     'air_temperature_observed_f', 'precipitation_amount',\r\n#     'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n#     'cumulative_SWE', 'cumulative_precipitation_amount'\r\n# ]\r\n\r\n# scaler = StandardScaler()\r\n# node_features = scaler.fit_transform(merged_nodes[feature_columns])\r\n# node_features = torch.tensor(node_features, dtype=torch.float)\r\n# labels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\n\r\n# print(f\"Normalization completed. Node feature shape: {node_features.shape}\")\r\n\r\n# # Build Graph Using KDTree\r\n# print(\"Building KDTree and constructing edges...\")\r\n\r\n# coordinates = merged_nodes[['lat', 'lon', 'dayofyear']].values  \r\n# tree = KDTree(coordinates)\r\n\r\n# # Adaptive k-value (between 5 and 10 based on graph density)\r\n# avg_degree = max(5, min(10, int(len(merged_nodes) ** 0.5 / 10)))\r\n# spatial_threshold = 0.15\r\n# time_weight = 0.05  \r\n\r\n# def time_aware_distance(p1, p2):\r\n#     spatial_dist = np.linalg.norm(p1[:2] - p2[:2])  \r\n#     time_dist = abs(p1[2] - p2[2]) * time_weight  \r\n#     return spatial_dist + time_dist\r\n\r\n# distances, neighbors = tree.query(coordinates, k=avg_degree+1)\r\n\r\n# edge_list = []\r\n# for i in range(len(neighbors)):\r\n#     for j in range(1, avg_degree+1):\r\n#         if time_aware_distance(coordinates[i], coordinates[neighbors[i][j]]) < spatial_threshold:\r\n#             edge_list.append((i, neighbors[i][j]))\r\n\r\n# edges = np.array(edge_list).T\r\n# edge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\n# edge_index = to_undirected(edge_index)  # Ensure bidirectional edges\r\n# print(f\"Graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\r\n\r\n# graph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n# torch.save(graph_data, \"/media/volume1/gnn_timeaware1.pt\")\r\n# print(\"Graph successfully saved.\")",
  "history_output" : "Loading dataset...\nTotal rows in raw dataset: 6155644\n Rows remaining after lat/lon filtering: 620662\n Missing values count per column:\ndate         0\nlat          0\nlon          0\nswe_value    0\ndtype: int64\n Found 2431 duplicate rows.\n Dataset size after deduplication: 618231\n🔍 Possible duplicates based on same location & date: 7482\n🔍 Possible duplicates based on same location (ignoring time): 617460\n🔍 Possible duplicates with small numerical variations (rounded to 3 decimals): 229\n",
  "history_begin_time" : 1741560252265,
  "history_end_time" : 1741560285909,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "nYHdzfZdv4bT",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom torch_geometric.utils import degree, to_undirected\r\n\r\n# Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\"Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\n\r\ndf_list = []\r\ntotal_rows = 0\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, parse_dates=['date'], chunksize=chunksize):\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear  \r\n    total_rows +=len(chunk)\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Total rows in raw dataset: {total_rows}\")\r\nprint(f\" Rows remaining after lat/lon filtering: {data.shape[0]}\")\r\n\r\n# Check for missing values in critical columns\r\nmissing_values = data[['date', 'lat', 'lon', 'swe_value']].isnull().sum()\r\nprint(f\" Missing values count per column:\\n{missing_values}\")\r\n\r\n# Remove duplicates\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\ndata = data.drop_duplicates()\r\nprint(f\" Dataset size after deduplication: {data.shape[0]}\")\r\n\r\n# Check duplicates based only on ('lat', 'lon', 'date')\r\nlocation_time_duplicates = data.duplicated(subset=['lat', 'lon', 'date']).sum()\r\nprint(f\"🔍 Possible duplicates based on same location & date: {location_time_duplicates}\")\r\n\r\n# Check duplicates based on ('lat', 'lon') only (ignoring time)\r\nlocation_duplicates = data.duplicated(subset=['lat', 'lon']).sum()\r\nprint(f\"🔍 Possible duplicates based on same location (ignoring time): {location_duplicates}\")\r\n\r\n# Check for small variations in numerical values (rounding to 3 decimals)\r\nrounded_data = data.copy()\r\nnumerical_cols = ['Elevation', 'Slope', 'SWE', 'snow_depth', 'air_temperature_observed_f', 'precipitation_amount''relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed','cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value']\r\n\r\nrounded_data[numerical_cols] = rounded_data[numerical_cols].round(3)\r\nnumerical_variation_duplicates = rounded_data.duplicated().sum()\r\nprint(f\"🔍 Possible duplicates with small numerical variations (rounded to 3 decimals): {numerical_variation_duplicates}\")\r\n\r\n\r\n# # Merge Nodes (Spatial & Time Binning)\r\n# print(\"Merging nodes to reduce graph size...\")\r\n\r\n# grid_size = 0.01\r\n# time_bins = 10\r\n\r\n# data['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\n# data['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\n# data['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\n# data['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\n# merged_nodes = data.groupby('grid_id').agg({\r\n#     'Elevation': 'mean', 'Slope': 'mean', 'SWE': 'mean', 'snow_depth': 'mean',\r\n#     'air_temperature_observed_f': 'mean', 'precipitation_amount': 'mean',\r\n#     'relative_humidity_rmin': 'mean', 'relative_humidity_rmax': 'mean', 'wind_speed': 'mean',\r\n#     'cumulative_SWE': 'mean', 'cumulative_precipitation_amount': 'mean', 'swe_value': 'mean',\r\n#     'lat': 'mean', 'lon': 'mean', 'dayofyear': 'mean'\r\n# }).reset_index()\r\n\r\n# print(f\"Merged nodes: {merged_nodes.shape[0]} from {data.shape[0]} original records.\")\r\n\r\n# # Normalize Features\r\n# print(\"Normalizing features using StandardScaler...\")\r\n# feature_columns = [\r\n#     'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n#     'air_temperature_observed_f', 'precipitation_amount',\r\n#     'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n#     'cumulative_SWE', 'cumulative_precipitation_amount'\r\n# ]\r\n\r\n# scaler = StandardScaler()\r\n# node_features = scaler.fit_transform(merged_nodes[feature_columns])\r\n# node_features = torch.tensor(node_features, dtype=torch.float)\r\n# labels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\n\r\n# print(f\"Normalization completed. Node feature shape: {node_features.shape}\")\r\n\r\n# # Build Graph Using KDTree\r\n# print(\"Building KDTree and constructing edges...\")\r\n\r\n# coordinates = merged_nodes[['lat', 'lon', 'dayofyear']].values  \r\n# tree = KDTree(coordinates)\r\n\r\n# # Adaptive k-value (between 5 and 10 based on graph density)\r\n# avg_degree = max(5, min(10, int(len(merged_nodes) ** 0.5 / 10)))\r\n# spatial_threshold = 0.15\r\n# time_weight = 0.05  \r\n\r\n# def time_aware_distance(p1, p2):\r\n#     spatial_dist = np.linalg.norm(p1[:2] - p2[:2])  \r\n#     time_dist = abs(p1[2] - p2[2]) * time_weight  \r\n#     return spatial_dist + time_dist\r\n\r\n# distances, neighbors = tree.query(coordinates, k=avg_degree+1)\r\n\r\n# edge_list = []\r\n# for i in range(len(neighbors)):\r\n#     for j in range(1, avg_degree+1):\r\n#         if time_aware_distance(coordinates[i], coordinates[neighbors[i][j]]) < spatial_threshold:\r\n#             edge_list.append((i, neighbors[i][j]))\r\n\r\n# edges = np.array(edge_list).T\r\n# edge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\n# edge_index = to_undirected(edge_index)  # Ensure bidirectional edges\r\n# print(f\"Graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\r\n\r\n# graph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n# torch.save(graph_data, \"/media/volume1/gnn_timeaware1.pt\")\r\n# print(\"Graph successfully saved.\")",
  "history_output" : "Loading dataset...\nTotal rows in raw dataset: 6155644\n Rows remaining after lat/lon filtering: 620662\n Missing values count per column:\ndate         0\nlat          0\nlon          0\nswe_value    0\ndtype: int64\n Found 2431 duplicate rows.\n Dataset size after deduplication: 618231\n🔍 Possible duplicates based on same location & date: 7482\n🔍 Possible duplicates based on same location (ignoring time): 617460\nTraceback (most recent call last):\n  File \"/media/volume1/gw-workspace/nYHdzfZdv4bT/graph_creation.py\", line 63, in <module>\n    rounded_data[numerical_cols] = rounded_data[numerical_cols].round(3)\n                                   ~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py\", line 4108, in __getitem__\n    indexer = self.columns._get_indexer_strict(key, \"columns\")[1]\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py\", line 6200, in _get_indexer_strict\n    self._raise_if_missing(keyarr, indexer, axis_name)\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py\", line 6252, in _raise_if_missing\n    raise KeyError(f\"{not_found} not in index\")\nKeyError: \"['precipitation_amountrelative_humidity_rmin'] not in index\"\n",
  "history_begin_time" : 1741560056730,
  "history_end_time" : 1741560091266,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "1z4lU9ikunvB",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom torch_geometric.utils import degree, to_undirected\r\n\r\n# Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\"Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\n\r\ndf_list = []\r\ntotal_rows = 0\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, parse_dates=['date'], chunksize=chunksize):\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear  \r\n    total_rows +=len(chunk)\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Total rows in raw dataset: {total_rows}\")\r\nprint(f\" Rows remaining after lat/lon filtering: {data.shape[0]}\")\r\n\r\n# Check for missing values in critical columns\r\nmissing_values = data[['date', 'lat', 'lon', 'swe_value']].isnull().sum()\r\nprint(f\" Missing values count per column:\\n{missing_values}\")\r\n\r\n# Remove duplicates\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\ndata = data.drop_duplicates()\r\nprint(f\" Dataset size after deduplication: {data.shape[0]}\")\r\n\r\n# # Merge Nodes (Spatial & Time Binning)\r\n# print(\"Merging nodes to reduce graph size...\")\r\n\r\n# grid_size = 0.01\r\n# time_bins = 10\r\n\r\n# data['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\n# data['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\n# data['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\n# data['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\n# merged_nodes = data.groupby('grid_id').agg({\r\n#     'Elevation': 'mean', 'Slope': 'mean', 'SWE': 'mean', 'snow_depth': 'mean',\r\n#     'air_temperature_observed_f': 'mean', 'precipitation_amount': 'mean',\r\n#     'relative_humidity_rmin': 'mean', 'relative_humidity_rmax': 'mean', 'wind_speed': 'mean',\r\n#     'cumulative_SWE': 'mean', 'cumulative_precipitation_amount': 'mean', 'swe_value': 'mean',\r\n#     'lat': 'mean', 'lon': 'mean', 'dayofyear': 'mean'\r\n# }).reset_index()\r\n\r\n# print(f\"Merged nodes: {merged_nodes.shape[0]} from {data.shape[0]} original records.\")\r\n\r\n# # Normalize Features\r\n# print(\"Normalizing features using StandardScaler...\")\r\n# feature_columns = [\r\n#     'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n#     'air_temperature_observed_f', 'precipitation_amount',\r\n#     'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n#     'cumulative_SWE', 'cumulative_precipitation_amount'\r\n# ]\r\n\r\n# scaler = StandardScaler()\r\n# node_features = scaler.fit_transform(merged_nodes[feature_columns])\r\n# node_features = torch.tensor(node_features, dtype=torch.float)\r\n# labels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\n\r\n# print(f\"Normalization completed. Node feature shape: {node_features.shape}\")\r\n\r\n# # Build Graph Using KDTree\r\n# print(\"Building KDTree and constructing edges...\")\r\n\r\n# coordinates = merged_nodes[['lat', 'lon', 'dayofyear']].values  \r\n# tree = KDTree(coordinates)\r\n\r\n# # Adaptive k-value (between 5 and 10 based on graph density)\r\n# avg_degree = max(5, min(10, int(len(merged_nodes) ** 0.5 / 10)))\r\n# spatial_threshold = 0.15\r\n# time_weight = 0.05  \r\n\r\n# def time_aware_distance(p1, p2):\r\n#     spatial_dist = np.linalg.norm(p1[:2] - p2[:2])  \r\n#     time_dist = abs(p1[2] - p2[2]) * time_weight  \r\n#     return spatial_dist + time_dist\r\n\r\n# distances, neighbors = tree.query(coordinates, k=avg_degree+1)\r\n\r\n# edge_list = []\r\n# for i in range(len(neighbors)):\r\n#     for j in range(1, avg_degree+1):\r\n#         if time_aware_distance(coordinates[i], coordinates[neighbors[i][j]]) < spatial_threshold:\r\n#             edge_list.append((i, neighbors[i][j]))\r\n\r\n# edges = np.array(edge_list).T\r\n# edge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\n# edge_index = to_undirected(edge_index)  # Ensure bidirectional edges\r\n# print(f\"Graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\r\n\r\n# graph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n# torch.save(graph_data, \"/media/volume1/gnn_timeaware1.pt\")\r\n# print(\"Graph successfully saved.\")",
  "history_output" : "Loading dataset...\nTotal rows in raw dataset: 6155644\n Rows remaining after lat/lon filtering: 620662\n Missing values count per column:\ndate         0\nlat          0\nlon          0\nswe_value    0\ndtype: int64\n Found 2431 duplicate rows.\n Dataset size after deduplication: 618231\n",
  "history_begin_time" : 1741559908104,
  "history_end_time" : 1741559941883,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "JVcikwrTWh4g",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom torch_geometric.utils import degree, to_undirected\r\n\r\n# Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\"Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\n\r\ndf_list = []\r\ntotal_rows = 0\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, parse_dates=['date'], chunksize=chunksize):\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear  \r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Total rows in raw dataset: {total_rows}\")\r\nprint(f\" Rows remaining after lat/lon filtering: {data.shape[0]}\")\r\n\r\n# Check for missing values in critical columns\r\nmissing_values = data[['date', 'lat', 'lon', 'swe_value']].isnull().sum()\r\nprint(f\" Missing values count per column:\\n{missing_values}\")\r\n\r\n# Remove duplicates\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\ndata = data.drop_duplicates()\r\nprint(f\" Dataset size after deduplication: {data.shape[0]}\")\r\n\r\n# # Merge Nodes (Spatial & Time Binning)\r\n# print(\"Merging nodes to reduce graph size...\")\r\n\r\n# grid_size = 0.01\r\n# time_bins = 10\r\n\r\n# data['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\n# data['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\n# data['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\n# data['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\n# merged_nodes = data.groupby('grid_id').agg({\r\n#     'Elevation': 'mean', 'Slope': 'mean', 'SWE': 'mean', 'snow_depth': 'mean',\r\n#     'air_temperature_observed_f': 'mean', 'precipitation_amount': 'mean',\r\n#     'relative_humidity_rmin': 'mean', 'relative_humidity_rmax': 'mean', 'wind_speed': 'mean',\r\n#     'cumulative_SWE': 'mean', 'cumulative_precipitation_amount': 'mean', 'swe_value': 'mean',\r\n#     'lat': 'mean', 'lon': 'mean', 'dayofyear': 'mean'\r\n# }).reset_index()\r\n\r\n# print(f\"Merged nodes: {merged_nodes.shape[0]} from {data.shape[0]} original records.\")\r\n\r\n# # Normalize Features\r\n# print(\"Normalizing features using StandardScaler...\")\r\n# feature_columns = [\r\n#     'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n#     'air_temperature_observed_f', 'precipitation_amount',\r\n#     'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n#     'cumulative_SWE', 'cumulative_precipitation_amount'\r\n# ]\r\n\r\n# scaler = StandardScaler()\r\n# node_features = scaler.fit_transform(merged_nodes[feature_columns])\r\n# node_features = torch.tensor(node_features, dtype=torch.float)\r\n# labels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\n\r\n# print(f\"Normalization completed. Node feature shape: {node_features.shape}\")\r\n\r\n# # Build Graph Using KDTree\r\n# print(\"Building KDTree and constructing edges...\")\r\n\r\n# coordinates = merged_nodes[['lat', 'lon', 'dayofyear']].values  \r\n# tree = KDTree(coordinates)\r\n\r\n# # Adaptive k-value (between 5 and 10 based on graph density)\r\n# avg_degree = max(5, min(10, int(len(merged_nodes) ** 0.5 / 10)))\r\n# spatial_threshold = 0.15\r\n# time_weight = 0.05  \r\n\r\n# def time_aware_distance(p1, p2):\r\n#     spatial_dist = np.linalg.norm(p1[:2] - p2[:2])  \r\n#     time_dist = abs(p1[2] - p2[2]) * time_weight  \r\n#     return spatial_dist + time_dist\r\n\r\n# distances, neighbors = tree.query(coordinates, k=avg_degree+1)\r\n\r\n# edge_list = []\r\n# for i in range(len(neighbors)):\r\n#     for j in range(1, avg_degree+1):\r\n#         if time_aware_distance(coordinates[i], coordinates[neighbors[i][j]]) < spatial_threshold:\r\n#             edge_list.append((i, neighbors[i][j]))\r\n\r\n# edges = np.array(edge_list).T\r\n# edge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\n# edge_index = to_undirected(edge_index)  # Ensure bidirectional edges\r\n# print(f\"Graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\r\n\r\n# graph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n# torch.save(graph_data, \"/media/volume1/gnn_timeaware1.pt\")\r\n# print(\"Graph successfully saved.\")",
  "history_output" : "Loading dataset...\nTotal rows in raw dataset: 0\n Rows remaining after lat/lon filtering: 620662\n Missing values count per column:\ndate         0\nlat          0\nlon          0\nswe_value    0\ndtype: int64\n Found 2431 duplicate rows.\n Dataset size after deduplication: 618231\n",
  "history_begin_time" : 1741559692505,
  "history_end_time" : 1741559725435,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "V7BZSxg7QZrA",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom torch_geometric.utils import degree, to_undirected\r\n\r\n# Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\"Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\n\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, parse_dates=['date'], chunksize=chunksize):\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear  \r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Filtered dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\n# Remove duplicates\r\ndata = data.drop_duplicates()\r\nprint(f\"New dataset size after removing duplicates: {data.shape[0]} rows.\")\r\n\r\n# Merge Nodes (Spatial & Time Binning)\r\nprint(\"Merging nodes to reduce graph size...\")\r\n\r\ngrid_size = 0.01\r\ntime_bins = 10\r\n\r\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\ndata['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\nmerged_nodes = data.groupby('grid_id').agg({\r\n    'Elevation': 'mean', 'Slope': 'mean', 'SWE': 'mean', 'snow_depth': 'mean',\r\n    'air_temperature_observed_f': 'mean', 'precipitation_amount': 'mean',\r\n    'relative_humidity_rmin': 'mean', 'relative_humidity_rmax': 'mean', 'wind_speed': 'mean',\r\n    'cumulative_SWE': 'mean', 'cumulative_precipitation_amount': 'mean', 'swe_value': 'mean',\r\n    'lat': 'mean', 'lon': 'mean', 'dayofyear': 'mean'\r\n}).reset_index()\r\n\r\nprint(f\"Merged nodes: {merged_nodes.shape[0]} from {data.shape[0]} original records.\")\r\n\r\n# Normalize Features\r\nprint(\"Normalizing features using StandardScaler...\")\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\n\r\nscaler = StandardScaler()\r\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\r\nnode_features = torch.tensor(node_features, dtype=torch.float)\r\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\n\r\nprint(f\"Normalization completed. Node feature shape: {node_features.shape}\")\r\n\r\n# Build Graph Using KDTree\r\nprint(\"Building KDTree and constructing edges...\")\r\n\r\ncoordinates = merged_nodes[['lat', 'lon', 'dayofyear']].values  \r\ntree = KDTree(coordinates)\r\n\r\n# Adaptive k-value (between 5 and 10 based on graph density)\r\navg_degree = max(5, min(10, int(len(merged_nodes) ** 0.5 / 10)))\r\nspatial_threshold = 0.15\r\ntime_weight = 0.05  \r\n\r\ndef time_aware_distance(p1, p2):\r\n    spatial_dist = np.linalg.norm(p1[:2] - p2[:2])  \r\n    time_dist = abs(p1[2] - p2[2]) * time_weight  \r\n    return spatial_dist + time_dist\r\n\r\ndistances, neighbors = tree.query(coordinates, k=avg_degree+1)\r\n\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, avg_degree+1):\r\n        if time_aware_distance(coordinates[i], coordinates[neighbors[i][j]]) < spatial_threshold:\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T\r\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\nedge_index = to_undirected(edge_index)  # Ensure bidirectional edges\r\nprint(f\"Graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\r\n\r\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\ntorch.save(graph_data, \"/media/volume1/gnn_timeaware1.pt\")\r\nprint(\"Graph successfully saved.\")",
  "history_output" : "Loading dataset...\nFiltered dataset loaded with 620662 rows and 16 columns.\nNew dataset size after removing duplicates: 618231 rows.\nMerging nodes to reduce graph size...\nMerged nodes: 7375 from 618231 original records.\nNormalizing features using StandardScaler...\nNormalization completed. Node feature shape: torch.Size([7375, 11])\nBuilding KDTree and constructing edges...\nGraph constructed with 7375 nodes and 16350 edges.\nGraph successfully saved.\n",
  "history_begin_time" : 1741555449642,
  "history_end_time" : 1741555484847,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "JIWH6Tv0Du7L",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom torch_geometric.utils import degree \r\n\r\n# Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\"Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\n\r\n# Step 1: Load and filter the dataset\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, parse_dates=['date'], chunksize=chunksize):\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear  \r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\n# Combine filtered chunks\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Filtered dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\n# Remove duplicates\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\"Found {num_duplicates} duplicate rows.\")\r\ndata = data.drop_duplicates()\r\nprint(f\"Removed duplicates. New dataset size: {data.shape[0]} rows.\")\r\n\r\n# Merge Nodes (Spatial & Time Binning)\r\nprint(\"Merging nodes to reduce graph size...\")\r\n\r\ngrid_size = 0.01  # Reduced merging size (preserve more detail)\r\ntime_bins = 10  # Slightly increased for better time granularity\r\n\r\n# Assign spatial bins\r\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\n\r\n# Assign temporal bins\r\ndata['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\n\r\n# Create unique spatial-temporal ID\r\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\n# Aggregate nodes by grid_id\r\nmerged_nodes = data.groupby('grid_id').agg({\r\n    'Elevation': 'mean',\r\n    'Slope': 'mean',\r\n    'SWE': 'mean',\r\n    'snow_depth': 'mean',\r\n    'air_temperature_observed_f': 'mean',\r\n    'precipitation_amount': 'mean',\r\n    'relative_humidity_rmin': 'mean',\r\n    'relative_humidity_rmax': 'mean',\r\n    'wind_speed': 'mean',\r\n    'cumulative_SWE': 'mean',\r\n    'cumulative_precipitation_amount': 'mean',\r\n    'swe_value': 'mean',  \r\n    'lat': 'mean',\r\n    'lon': 'mean',\r\n    'dayofyear': 'mean'\r\n}).reset_index()\r\n\r\nprint(f\"Merged nodes: {merged_nodes.shape[0]} from {data.shape[0]} original records.\")\r\n\r\n# Normalize Features\r\nprint(\"Normalizing features using StandardScaler...\")\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\n\r\nscaler = StandardScaler()\r\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\r\nnode_features = torch.tensor(node_features, dtype=torch.float)\r\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\n\r\nprint(f\"Normalization completed. Node feature shape: {node_features.shape}\")\r\n\r\n# Build Graph Using Time-Aware KDTree\r\nprint(\"Building KDTree and constructing edges...\")\r\n\r\ncoordinates = merged_nodes[['lat', 'lon', 'dayofyear']].values  \r\ntree = KDTree(coordinates)\r\n\r\n# Graph Parameters\r\nk = 5  # Keep same k-value for stable neighborhood relations\r\nspatial_threshold = 0.1  # Slightly increased to improve connectivity\r\ntime_weight = 0.02  # Lowered to prevent too much focus on time\r\n\r\n# Define time-aware distance function\r\ndef time_aware_distance(p1, p2):\r\n    spatial_dist = np.linalg.norm(p1[:2] - p2[:2])  \r\n    time_dist = abs(p1[2] - p2[2]) * time_weight  \r\n    return spatial_dist + time_dist\r\n\r\n# Query k-NN\r\ndistances, neighbors = tree.query(coordinates, k=k+1)  \r\n\r\n# Convert to edge index format with distance filtering\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, k+1):  \r\n        if time_aware_distance(coordinates[i], coordinates[neighbors[i][j]]) < spatial_threshold:\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T\r\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\nprint(f\"Graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\r\n\r\n# Create PyTorch Geometric Graph Object\r\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n\r\n# Validate Graph Before Saving\r\nprint(\"\\nGraph Data Summary:\")\r\nprint(graph_data)\r\nprint(f\"Nodes: {graph_data.num_nodes}\")\r\nprint(f\"Edges: {graph_data.num_edges}\")\r\nprint(f\"Feature shape: {graph_data.x.shape}\")\r\nprint(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n\r\n# Compute Degree Statistics\r\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\r\nprint(\"\\nDegree Stats:\")\r\nprint(f\"- Min degree: {deg.min().item()}\")\r\nprint(f\"- Max degree: {deg.max().item()}\")\r\nprint(f\"- Mean degree: {deg.float().mean().item()}\")\r\nprint(f\"- Standard deviation: {deg.float().std().item()}\")\r\n\r\n# Count Isolated Nodes\r\nnum_isolated_nodes = (deg == 0).sum().item()\r\npercentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\r\nprint(f\"\\nIsolated nodes: {num_isolated_nodes} ({percentage_isolated:.4f}%)\")\r\n\r\n# Save the Graph\r\ngraph_data_output_path = \"/media/volume1/gnn_timeaware.pt\"\r\ntorch.save(graph_data, graph_data_output_path)\r\n\r\nprint(f\"\\nPyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\r\n",
  "history_output" : "Loading dataset...\nFiltered dataset loaded with 620662 rows and 16 columns.\nFound 2431 duplicate rows.\nRemoved duplicates. New dataset size: 618231 rows.\nMerging nodes to reduce graph size...\nMerged nodes: 7375 from 618231 original records.\nNormalizing features using StandardScaler...\nNormalization completed. Node feature shape: torch.Size([7375, 11])\nBuilding KDTree and constructing edges...\nGraph constructed with 7375 nodes and 8275 edges.\nGraph Data Summary:\nData(x=[7375, 11], edge_index=[2, 8275], y=[7375])\nNodes: 7375\nEdges: 8275\nFeature shape: torch.Size([7375, 11])\nEdge index shape: torch.Size([2, 8275])\nDegree Stats:\n- Min degree: 0.0\n- Max degree: 5.0\n- Mean degree: 1.122033953666687\n- Standard deviation: 1.1639513969421387\nIsolated nodes: 2703 (36.6508%)\nPyTorch Geometric Graph Data saved at: /media/volume1/gnn_timeaware.pt\n",
  "history_begin_time" : 1741551946373,
  "history_end_time" : 1741551980463,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "7VoGeRfCV6v4",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom torch_geometric.utils import degree \r\n\r\n# Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\"Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\n\r\n# Step 1: Load and filter the dataset\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, parse_dates=['date'], chunksize=chunksize):\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear  \r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\n# Combine filtered chunks\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Filtered dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\n# Remove duplicates\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\"Found {num_duplicates} duplicate rows.\")\r\ndata = data.drop_duplicates()\r\nprint(f\"Removed duplicates. New dataset size: {data.shape[0]} rows.\")\r\n\r\n# Merge Nodes (Spatial & Time Binning)\r\nprint(\"Merging nodes to reduce graph size...\")\r\n\r\ngrid_size = 0.0125  # Slightly tighter merging (better than 0.015)\r\ntime_bins = 7  # Further reduced from 10 to 7\r\n\r\n# Assign spatial bins\r\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\n\r\n# Assign temporal bins\r\ndata['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\n\r\n# Create unique spatial-temporal ID\r\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\n# Aggregate nodes by grid_id\r\nmerged_nodes = data.groupby('grid_id').agg({\r\n    'Elevation': 'mean',\r\n    'Slope': 'mean',\r\n    'SWE': 'mean',\r\n    'snow_depth': 'mean',\r\n    'air_temperature_observed_f': 'mean',\r\n    'precipitation_amount': 'mean',\r\n    'relative_humidity_rmin': 'mean',\r\n    'relative_humidity_rmax': 'mean',\r\n    'wind_speed': 'mean',\r\n    'cumulative_SWE': 'mean',\r\n    'cumulative_precipitation_amount': 'mean',\r\n    'swe_value': 'mean',  \r\n    'lat': 'mean',\r\n    'lon': 'mean',\r\n    'dayofyear': 'mean'\r\n}).reset_index()\r\n\r\nprint(f\"Merged nodes: {merged_nodes.shape[0]} from {data.shape[0]} original records.\")\r\n\r\n# Normalize Features\r\nprint(\"Normalizing features using StandardScaler...\")\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\n\r\nscaler = StandardScaler()\r\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\r\nnode_features = torch.tensor(node_features, dtype=torch.float)\r\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\n\r\nprint(f\"Normalization completed. Node feature shape: {node_features.shape}\")\r\n\r\n# Build Graph Using Time-Aware KDTree\r\nprint(\"Building KDTree and constructing edges...\")\r\n\r\ncoordinates = merged_nodes[['lat', 'lon', 'dayofyear']].values  \r\ntree = KDTree(coordinates)\r\n\r\n# Graph Parameters\r\nk = 6  # Increased from 5 → 6 to improve connectivity\r\nspatial_threshold = 0.08  \r\ntime_weight = 0.03  # Increased from 0.02 → 0.03 (slightly more flexibility)\r\n\r\n# Define time-aware distance function\r\ndef time_aware_distance(p1, p2):\r\n    spatial_dist = np.linalg.norm(p1[:2] - p2[:2])  \r\n    time_dist = abs(p1[2] - p2[2]) * time_weight  \r\n    return spatial_dist + time_dist\r\n\r\n# Query k-NN\r\ndistances, neighbors = tree.query(coordinates, k=k+1)  \r\n\r\n# Convert to edge index format with distance filtering\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, k+1):  \r\n        if time_aware_distance(coordinates[i], coordinates[neighbors[i][j]]) < spatial_threshold:\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T\r\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\nprint(f\"Graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\r\n\r\n# Create PyTorch Geometric Graph Object\r\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n\r\n# Validate Graph Before Saving\r\nprint(\"\\nGraph Data Summary:\")\r\nprint(graph_data)\r\nprint(f\"Nodes: {graph_data.num_nodes}\")\r\nprint(f\"Edges: {graph_data.num_edges}\")\r\nprint(f\"Feature shape: {graph_data.x.shape}\")\r\nprint(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n\r\n# Compute Degree Statistics\r\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\r\nprint(\"\\nDegree Stats:\")\r\nprint(f\"- Min degree: {deg.min().item()}\")\r\nprint(f\"- Max degree: {deg.max().item()}\")\r\nprint(f\"- Mean degree: {deg.float().mean().item()}\")\r\nprint(f\"- Standard deviation: {deg.float().std().item()}\")\r\n\r\n# Count Isolated Nodes\r\nnum_isolated_nodes = (deg == 0).sum().item()\r\npercentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\r\nprint(f\"\\nIsolated nodes: {num_isolated_nodes} ({percentage_isolated:.4f}%)\")\r\n\r\n# Save the Graph\r\ngraph_data_output_path = \"/media/volume1/gnn_timeaware.pt\"\r\ntorch.save(graph_data, graph_data_output_path)\r\n\r\nprint(f\"\\nPyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\r\n",
  "history_output" : "Loading dataset...\nFiltered dataset loaded with 620662 rows and 16 columns.\nFound 2431 duplicate rows.\nRemoved duplicates. New dataset size: 618231 rows.\nMerging nodes to reduce graph size...\nMerged nodes: 5051 from 618231 original records.\nNormalizing features using StandardScaler...\nNormalization completed. Node feature shape: torch.Size([5051, 11])\nBuilding KDTree and constructing edges...\nGraph constructed with 5051 nodes and 3670 edges.\nGraph Data Summary:\nData(x=[5051, 11], edge_index=[2, 3670], y=[5051])\nNodes: 5051\nEdges: 3670\nFeature shape: torch.Size([5051, 11])\nEdge index shape: torch.Size([2, 3670])\nDegree Stats:\n- Min degree: 0.0\n- Max degree: 6.0\n- Mean degree: 0.726588785648346\n- Standard deviation: 0.9503953456878662\nIsolated nodes: 2611 (51.6927%)\nPyTorch Geometric Graph Data saved at: /media/volume1/gnn_timeaware.pt\n",
  "history_begin_time" : 1741551363896,
  "history_end_time" : 1741551397535,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "DttcaDB5LP4z",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom torch_geometric.utils import degree \r\n\r\n# Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\"Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\n\r\n# Step 1: Load and filter the dataset\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, parse_dates=['date'], chunksize=chunksize):\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear  # Extract day of year\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\n# Combine filtered chunks\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Filtered dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\n# Remove duplicates\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\"Found {num_duplicates} duplicate rows.\")\r\ndata = data.drop_duplicates()\r\nprint(f\"Removed duplicates. New dataset size: {data.shape[0]} rows.\")\r\n\r\n# Merge Nodes (Spatial & Time Binning)\r\nprint(\"Merging nodes to reduce graph size...\")\r\n\r\ngrid_size = 0.015  # Increased merging (0.02 was too sparse in Graph 2)\r\ntime_bins = 10  # Reduced from 20 → 10 for less fragmentation\r\n\r\n# Assign spatial bins\r\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\n\r\n# Assign temporal bins\r\ndata['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\n\r\n# Create unique spatial-temporal ID\r\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\n# Aggregate nodes by grid_id\r\nmerged_nodes = data.groupby('grid_id').agg({\r\n    'Elevation': 'mean',\r\n    'Slope': 'mean',\r\n    'SWE': 'mean',\r\n    'snow_depth': 'mean',\r\n    'air_temperature_observed_f': 'mean',\r\n    'precipitation_amount': 'mean',\r\n    'relative_humidity_rmin': 'mean',\r\n    'relative_humidity_rmax': 'mean',\r\n    'wind_speed': 'mean',\r\n    'cumulative_SWE': 'mean',\r\n    'cumulative_precipitation_amount': 'mean',\r\n    'swe_value': 'mean',  \r\n    'lat': 'mean',\r\n    'lon': 'mean',\r\n    'dayofyear': 'mean'\r\n}).reset_index()\r\n\r\nprint(f\"Merged nodes: {merged_nodes.shape[0]} from {data.shape[0]} original records.\")\r\n\r\n# Normalize Features\r\nprint(\"Normalizing features using StandardScaler...\")\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\n\r\nscaler = StandardScaler()\r\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\r\nnode_features = torch.tensor(node_features, dtype=torch.float)\r\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\n\r\nprint(f\"Normalization completed. Node feature shape: {node_features.shape}\")\r\n\r\n# Build Graph Using Time-Aware KDTree\r\nprint(\"Building KDTree and constructing edges...\")\r\n\r\ncoordinates = merged_nodes[['lat', 'lon', 'dayofyear']].values  \r\ntree = KDTree(coordinates)\r\n\r\n# Graph Parameters\r\nk = 5  # Keeping k=5 as increasing it reduced accuracy\r\nspatial_threshold = 0.08  # Keeping it stable for better connections\r\ntime_weight = 0.02  # Balancing time influence\r\n\r\n# Define time-aware distance function\r\ndef time_aware_distance(p1, p2):\r\n    spatial_dist = np.linalg.norm(p1[:2] - p2[:2])  \r\n    time_dist = abs(p1[2] - p2[2]) * time_weight  \r\n    return spatial_dist + time_dist\r\n\r\n# Query k-NN\r\ndistances, neighbors = tree.query(coordinates, k=k+1)  \r\n\r\n# Convert to edge index format with distance filtering\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, k+1):  \r\n        if time_aware_distance(coordinates[i], coordinates[neighbors[i][j]]) < spatial_threshold:\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T\r\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\nprint(f\"Graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\r\n\r\n# Create PyTorch Geometric Graph Object\r\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n\r\n# Validate Graph Before Saving\r\nprint(\"\\nGraph Data Summary:\")\r\nprint(graph_data)\r\nprint(f\"Nodes: {graph_data.num_nodes}\")\r\nprint(f\"Edges: {graph_data.num_edges}\")\r\nprint(f\"Feature shape: {graph_data.x.shape}\")\r\nprint(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n\r\n# Compute Degree Statistics\r\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\r\nprint(\"\\nDegree Stats:\")\r\nprint(f\"- Min degree: {deg.min().item()}\")\r\nprint(f\"- Max degree: {deg.max().item()}\")\r\nprint(f\"- Mean degree: {deg.float().mean().item()}\")\r\nprint(f\"- Standard deviation: {deg.float().std().item()}\")\r\n\r\n# Count Isolated Nodes\r\nnum_isolated_nodes = (deg == 0).sum().item()\r\npercentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\r\nprint(f\"\\nIsolated nodes: {num_isolated_nodes} ({percentage_isolated:.4f}%)\")\r\n\r\n# Save the Graph\r\ngraph_data_output_path = \"/media/volume1/gnn_timeaware.pt\"\r\ntorch.save(graph_data, graph_data_output_path)\r\n\r\nprint(f\"\\nPyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\r\n",
  "history_output" : "Loading dataset...\nFiltered dataset loaded with 620662 rows and 16 columns.\nFound 2431 duplicate rows.\nRemoved duplicates. New dataset size: 618231 rows.\nMerging nodes to reduce graph size...\nMerged nodes: 6815 from 618231 original records.\nNormalizing features using StandardScaler...\nNormalization completed. Node feature shape: torch.Size([6815, 11])\nBuilding KDTree and constructing edges...\nGraph constructed with 6815 nodes and 4989 edges.\nGraph Data Summary:\nData(x=[6815, 11], edge_index=[2, 4989], y=[6815])\nNodes: 6815\nEdges: 4989\nFeature shape: torch.Size([6815, 11])\nEdge index shape: torch.Size([2, 4989])\nDegree Stats:\n- Min degree: 0.0\n- Max degree: 5.0\n- Mean degree: 0.7320616245269775\n- Standard deviation: 0.9417075514793396\nIsolated nodes: 3559 (52.2230%)\nPyTorch Geometric Graph Data saved at: /media/volume1/gnn_timeaware.pt\n",
  "history_begin_time" : 1741502494321,
  "history_end_time" : 1741502528351,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "v5bObmI3OJ0D",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom torch_geometric.utils import degree \r\n\r\n# Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\"Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\n\r\n# Step 1: Load and filter the dataset\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, parse_dates=['date'], chunksize=chunksize):\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear  # Extract day of year\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\n# Combine filtered chunks\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Filtered dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\n# Remove duplicates\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\"Found {num_duplicates} duplicate rows.\")\r\ndata = data.drop_duplicates()\r\nprint(f\"Removed duplicates. New dataset size: {data.shape[0]} rows.\")\r\n\r\n# Merge Nodes (over time & space)\r\nprint(\"Merging nodes to reduce graph size...\")\r\n\r\ngrid_size = 0.005  # Smaller grid size for better merging\r\ntime_bins = 5  # Reduced from 20 to 5 to prevent fragmentation\r\n\r\n# Assign spatial bins\r\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\n\r\n# Assign temporal bins\r\ndata['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\n\r\n# Create unique spatial-temporal ID\r\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\n# Aggregate nodes by grid_id\r\nmerged_nodes = data.groupby('grid_id').agg({\r\n    'Elevation': 'mean',\r\n    'Slope': 'mean',\r\n    'SWE': 'mean',\r\n    'snow_depth': 'mean',\r\n    'air_temperature_observed_f': 'mean',\r\n    'precipitation_amount': 'mean',\r\n    'relative_humidity_rmin': 'mean',\r\n    'relative_humidity_rmax': 'mean',\r\n    'wind_speed': 'mean',\r\n    'cumulative_SWE': 'mean',\r\n    'cumulative_precipitation_amount': 'mean',\r\n    'swe_value': 'mean',  \r\n    'lat': 'mean',\r\n    'lon': 'mean',\r\n    'dayofyear': 'mean'\r\n}).reset_index()\r\n\r\nprint(f\"Merged nodes: {merged_nodes.shape[0]} from {data.shape[0]} original records.\")\r\n\r\n# Normalize Features\r\nprint(\"Normalizing features using StandardScaler...\")\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\n\r\nscaler = StandardScaler()\r\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\r\nnode_features = torch.tensor(node_features, dtype=torch.float)\r\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\n\r\nprint(f\"Normalization completed. Node feature shape: {node_features.shape}\")\r\n\r\n# Build Graph Using Time-Aware KDTree\r\nprint(\"Building KDTree and constructing edges...\")\r\n\r\ncoordinates = merged_nodes[['lat', 'lon', 'dayofyear']].values  \r\ntree = KDTree(coordinates)\r\n\r\n# Graph Parameters\r\nk = 9  # Increased from 7 to 9 to improve connectivity\r\nspatial_threshold = 0.10  # Increased from 0.08 to 0.10 for more connections\r\ntime_weight = 0.01  # Reduce time impact further\r\n\r\n# Define time-aware distance function\r\ndef time_aware_distance(p1, p2):\r\n    spatial_dist = np.linalg.norm(p1[:2] - p2[:2])  \r\n    time_dist = abs(p1[2] - p2[2]) * time_weight  \r\n    return spatial_dist + time_dist\r\n\r\n# Query k-NN\r\ndistances, neighbors = tree.query(coordinates, k=k+1)  \r\n\r\n# Convert to edge index format with distance filtering\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, k+1):  \r\n        if time_aware_distance(coordinates[i], coordinates[neighbors[i][j]]) < spatial_threshold:\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T\r\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\nprint(f\"Graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\r\n\r\n# Create PyTorch Geometric Graph Object\r\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n\r\n# Validate Graph Before Saving\r\nprint(\"\\nGraph Data Summary:\")\r\nprint(graph_data)\r\nprint(f\"Nodes: {graph_data.num_nodes}\")\r\nprint(f\"Edges: {graph_data.num_edges}\")\r\nprint(f\"Feature shape: {graph_data.x.shape}\")\r\nprint(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n\r\n# Compute Degree Statistics\r\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\r\nprint(\"\\nDegree Stats:\")\r\nprint(f\"- Min degree: {deg.min().item()}\")\r\nprint(f\"- Max degree: {deg.max().item()}\")\r\nprint(f\"- Mean degree: {deg.float().mean().item()}\")\r\nprint(f\"- Standard deviation: {deg.float().std().item()}\")\r\n\r\n# Count Isolated Nodes\r\nnum_isolated_nodes = (deg == 0).sum().item()\r\npercentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\r\nprint(f\"\\nIsolated nodes: {num_isolated_nodes} ({percentage_isolated:.4f}%)\")\r\n\r\n# Save the Graph\r\ngraph_data_output_path = \"/media/volume1/gnn_oregontime.pt\"\r\ntorch.save(graph_data, graph_data_output_path)\r\n\r\nprint(f\"\\nPyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\r\n",
  "history_output" : "Loading dataset...\nFiltered dataset loaded with 620662 rows and 16 columns.\nFound 2431 duplicate rows.\nRemoved duplicates. New dataset size: 618231 rows.\nMerging nodes to reduce graph size...\nMerged nodes: 4073 from 618231 original records.\nNormalizing features using StandardScaler...\nNormalization completed. Node feature shape: torch.Size([4073, 11])\nBuilding KDTree and constructing edges...\nGraph constructed with 4073 nodes and 4550 edges.\nGraph Data Summary:\nData(x=[4073, 11], edge_index=[2, 4550], y=[4073])\nNodes: 4073\nEdges: 4550\nFeature shape: torch.Size([4073, 11])\nEdge index shape: torch.Size([2, 4550])\nDegree Stats:\n- Min degree: 0.0\n- Max degree: 8.0\n- Mean degree: 1.117112636566162\n- Standard deviation: 1.269749402999878\nIsolated nodes: 1593 (39.1112%)\nPyTorch Geometric Graph Data saved at: /media/volume1/gnn_oregontime.pt\n",
  "history_begin_time" : 1741501739119,
  "history_end_time" : 1741501772870,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "YQELWVsYvsfD",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom torch_geometric.utils import degree \r\n\r\n# Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\"Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\n\r\n# Step 1: Load and filter the dataset\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, parse_dates=['date'], chunksize=chunksize):\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear  \r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\n# Combine filtered chunks\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Filtered dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\n# Drop the original 'date' column after extracting 'dayofyear'\r\ndata = data.drop(columns=['date'])\r\n\r\n# Remove duplicates\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\"Found {num_duplicates} duplicate rows.\")\r\n\r\ndata = data.drop_duplicates()\r\nprint(f\"Removed duplicates. New dataset size: {data.shape[0]} rows.\")\r\n\r\n# Merge Nodes\r\nprint(\"Merging nodes to reduce graph size...\")\r\n\r\ngrid_size = 0.01  # Changed from 0.02 to 0.01 for more merging\r\ntime_bins = 5  # Changed from 20 to 5 to avoid excessive fragmentation\r\n\r\n# Assign spatial bins\r\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\n\r\n# Assign temporal bins (optional)\r\ndata['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\n\r\n# Create unique spatial-temporal ID\r\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\n# Aggregate nodes by grid_id\r\nmerged_nodes = data.groupby('grid_id').agg({\r\n    'Elevation': 'mean',\r\n    'Slope': 'mean',\r\n    'SWE': 'mean',\r\n    'snow_depth': 'mean',\r\n    'air_temperature_observed_f': 'mean',\r\n    'precipitation_amount': 'mean',\r\n    'relative_humidity_rmin': 'mean',\r\n    'relative_humidity_rmax': 'mean',\r\n    'wind_speed': 'mean',\r\n    'cumulative_SWE': 'mean',\r\n    'cumulative_precipitation_amount': 'mean',\r\n    'swe_value': 'mean',  \r\n    'lat': 'mean',\r\n    'lon': 'mean',\r\n    'dayofyear': 'mean'\r\n}).reset_index()\r\n\r\nprint(f\"Merged nodes: {merged_nodes.shape[0]} from {data.shape[0]} original records.\")\r\n\r\n# Normalize Features\r\nprint(\"Normalizing features using StandardScaler...\")\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\n\r\nscaler = StandardScaler()\r\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\r\nnode_features = torch.tensor(node_features, dtype=torch.float)\r\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\n\r\nprint(f\"Normalization completed. Node feature shape: {node_features.shape}\")\r\n\r\n# Build Graph Using Time-Aware KDTree\r\nprint(\"Building KDTree and constructing edges...\")\r\n\r\ncoordinates = merged_nodes[['lat', 'lon', 'dayofyear']].values  \r\ntree = KDTree(coordinates)\r\n\r\n# Graph Parameters\r\nk = 7  # Increased from 5 to 7 for better connectivity\r\nspatial_threshold = 0.08  # Increased from 0.06 to 0.08\r\ntime_weight = 0.02  # Reduced from 0.04 to 0.02 to reduce time fragmentation\r\n\r\n# Define time-aware distance function\r\ndef time_aware_distance(p1, p2):\r\n    spatial_dist = np.linalg.norm(p1[:2] - p2[:2])  \r\n    time_dist = abs(p1[2] - p2[2]) * time_weight  \r\n    return spatial_dist + time_dist\r\n\r\n# Query k-NN\r\ndistances, neighbors = tree.query(coordinates, k=k+1)  \r\n\r\n# Convert to edge index format with distance filtering\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, k+1):  \r\n        if time_aware_distance(coordinates[i], coordinates[neighbors[i][j]]) < spatial_threshold:\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T\r\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\nprint(f\"Graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\r\n\r\n# Create PyTorch Geometric Graph Object\r\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n\r\n# Validate Graph Before Saving\r\nprint(\"\\nGraph Data Summary:\")\r\nprint(graph_data)\r\nprint(f\"Nodes: {graph_data.num_nodes}\")\r\nprint(f\"Edges: {graph_data.num_edges}\")\r\nprint(f\"Feature shape: {graph_data.x.shape}\")\r\nprint(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n\r\n# Compute Degree Statistics\r\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\r\nprint(\"\\nDegree Stats:\")\r\nprint(f\"- Min degree: {deg.min().item()}\")\r\nprint(f\"- Max degree: {deg.max().item()}\")\r\nprint(f\"- Mean degree: {deg.float().mean().item()}\")\r\nprint(f\"- Standard deviation: {deg.float().std().item()}\")\r\n\r\n# Count Isolated Nodes\r\nnum_isolated_nodes = (deg == 0).sum().item()\r\npercentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\r\nprint(f\"\\nIsolated nodes: {num_isolated_nodes} ({percentage_isolated:.4f}%)\")\r\n\r\n# Save the Graph\r\ngraph_data_output_path = \"/media/volume1/gnn_dategraph.pt\"\r\ntorch.save(graph_data, graph_data_output_path)\r\n\r\nprint(f\"\\nPyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\r\n",
  "history_output" : "Loading dataset...\nFiltered dataset loaded with 620662 rows and 16 columns.\nFound 97913 duplicate rows.\nRemoved duplicates. New dataset size: 522749 rows.\nMerging nodes to reduce graph size...\nMerged nodes: 3939 from 522749 original records.\nNormalizing features using StandardScaler...\nNormalization completed. Node feature shape: torch.Size([3939, 11])\nBuilding KDTree and constructing edges...\nGraph constructed with 3939 nodes and 2961 edges.\nGraph Data Summary:\nData(x=[3939, 11], edge_index=[2, 2961], y=[3939])\nNodes: 3939\nEdges: 2961\nFeature shape: torch.Size([3939, 11])\nEdge index shape: torch.Size([2, 2961])\nDegree Stats:\n- Min degree: 0.0\n- Max degree: 6.0\n- Mean degree: 0.7517136335372925\n- Standard deviation: 0.9850553870201111\nIsolated nodes: 2031 (51.5613%)\nPyTorch Geometric Graph Data saved at: /media/volume1/gnn_dategraph.pt\n",
  "history_begin_time" : 1741500994351,
  "history_end_time" : 1741501027800,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "6YDPUQDGA1g7",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom torch_geometric.utils import degree \r\n\r\n# Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\n# Oregon state\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\"Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\n\r\n# Step 1: Load and filter the dataset\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, parse_dates=['date'], chunksize=chunksize):\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear  \r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\n# Combine filtered chunks\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Filtered dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\n# Drop the original 'date' column after extracting 'dayofyear'\r\ndata = data.drop(columns=['date'])\r\n\r\n# Remove duplicates\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\"Found {num_duplicates} duplicate rows.\")\r\n\r\ndata = data.drop_duplicates()\r\nprint(f\"Removed duplicates. New dataset size: {data.shape[0]} rows.\")\r\n\r\n# Merge Nodes\r\nprint(\"Merging nodes to reduce graph size...\")\r\n\r\ngrid_size = 0.02  \r\ntime_bins = 20  \r\n\r\n# Assign spatial bins\r\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\n\r\n# Assign temporal bins\r\ndata['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\n\r\n# Create unique spatial-temporal ID\r\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\n# Aggregate nodes by grid_id\r\nmerged_nodes = data.groupby('grid_id').agg({\r\n    'Elevation': 'mean',\r\n    'Slope': 'mean',\r\n    'SWE': 'mean',\r\n    'snow_depth': 'mean',\r\n    'air_temperature_observed_f': 'mean',\r\n    'precipitation_amount': 'mean',\r\n    'relative_humidity_rmin': 'mean',\r\n    'relative_humidity_rmax': 'mean',\r\n    'wind_speed': 'mean',\r\n    'cumulative_SWE': 'mean',\r\n    'cumulative_precipitation_amount': 'mean',\r\n    'swe_value': 'mean',  \r\n    'lat': 'mean',\r\n    'lon': 'mean',\r\n    'dayofyear': 'mean'\r\n}).reset_index()\r\n\r\nprint(f\"Merged nodes: {merged_nodes.shape[0]} from {data.shape[0]} original records.\")\r\n\r\n# Normalize Features\r\nprint(\"Normalizing features using StandardScaler...\")\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\n\r\nscaler = StandardScaler()\r\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\r\nnode_features = torch.tensor(node_features, dtype=torch.float)\r\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\n\r\nprint(f\"Normalization completed. Node feature shape: {node_features.shape}\")\r\n\r\n# Build Graph Using Time-Aware KDTree\r\nprint(\"Building KDTree and constructing edges...\")\r\n\r\ncoordinates = merged_nodes[['lat', 'lon', 'dayofyear']].values  \r\ntree = KDTree(coordinates)\r\n\r\n# Graph Parameters\r\nk = 5 \r\nspatial_threshold = 0.06\r\ntime_weight = 0.02  \r\n\r\n# Define time-aware distance function\r\ndef time_aware_distance(p1, p2):\r\n    spatial_dist = np.linalg.norm(p1[:2] - p2[:2])  \r\n    time_dist = abs(p1[2] - p2[2]) * time_weight  \r\n    return spatial_dist + time_dist\r\n\r\n# Query k-NN\r\ndistances, neighbors = tree.query(coordinates, k=k+1)  \r\n\r\n# Convert to edge index format with distance filtering\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, k+1):  \r\n        if time_aware_distance(coordinates[i], coordinates[neighbors[i][j]]) < spatial_threshold:\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T\r\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\nprint(f\"Graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\r\n\r\n# Create PyTorch Geometric Graph Object\r\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n\r\n# Validate Graph Before Saving\r\nprint(\"\\nGraph Data Summary:\")\r\nprint(graph_data)\r\nprint(f\"Nodes: {graph_data.num_nodes}\")\r\nprint(f\"Edges: {graph_data.num_edges}\")\r\nprint(f\"Feature shape: {graph_data.x.shape}\")\r\nprint(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n\r\n# Sample Nodes & Edges\r\nprint(\"\\nSample of Node Features:\")\r\nprint(graph_data.x[:5])\r\nprint(\"\\nSample of Edge Index:\")\r\nprint(graph_data.edge_index[:, :10])\r\n\r\n# Compute Degree Statistics\r\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\r\nprint(\"\\nDegree Stats:\")\r\nprint(f\"- Min degree: {deg.min().item()}\")\r\nprint(f\"- Max degree: {deg.max().item()}\")\r\nprint(f\"- Mean degree: {deg.float().mean().item()}\")\r\nprint(f\"- Standard deviation: {deg.float().std().item()}\")\r\n\r\n# Count Isolated Nodes\r\nnum_isolated_nodes = (deg == 0).sum().item()\r\npercentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\r\nprint(f\"\\nIsolated nodes: {num_isolated_nodes} ({percentage_isolated:.4f}%)\")\r\n\r\n# Save the Graph\r\ngraph_data_output_path = \"/media/volume1/gnn_date.pt\"\r\ntorch.save(graph_data, graph_data_output_path)\r\n\r\nprint(f\"\\nPyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\r\n",
  "history_output" : "Loading dataset...\nFiltered dataset loaded with 620662 rows and 16 columns.\nFound 97913 duplicate rows.\nRemoved duplicates. New dataset size: 522749 rows.\nMerging nodes to reduce graph size...\nMerged nodes: 12664 from 522749 original records.\nNormalizing features using StandardScaler...\nNormalization completed. Node feature shape: torch.Size([12664, 11])\nBuilding KDTree and constructing edges...\nGraph constructed with 12664 nodes and 7266 edges.\nGraph Data Summary:\nData(x=[12664, 11], edge_index=[2, 7266], y=[12664])\nNodes: 12664\nEdges: 7266\nFeature shape: torch.Size([12664, 11])\nEdge index shape: torch.Size([2, 7266])\nSample of Node Features:\ntensor([[ 2.1246e+00,  3.6312e-01, -1.7096e-01, -1.9966e-01,  4.3298e+00,\n         -5.6344e-01,  3.1638e-01, -2.3069e-01,  3.4011e-01, -4.0331e-01,\n          5.2565e-01],\n        [ 2.1246e+00,  3.6312e-01, -1.7096e-01, -1.9966e-01,  4.6841e+00,\n         -5.0281e-01,  2.5040e-01, -5.8181e-02,  3.1789e-01, -4.0331e-01,\n          5.1284e-01],\n        [ 2.1246e+00,  3.6312e-01, -1.7096e-01, -1.9966e-01,  4.3848e+00,\n         -5.2042e-01,  4.5102e-02, -5.1295e-02,  3.1096e-01, -4.0331e-01,\n          5.5758e-01],\n        [ 2.1246e+00,  3.6312e-01, -1.7096e-01, -1.9966e-01,  4.3297e+00,\n         -4.9235e-01, -1.4487e-02,  4.3373e-02,  2.9749e-01, -4.0331e-01,\n          5.5662e-01],\n        [ 2.1246e+00,  3.6312e-01, -1.7096e-01, -1.9966e-01,  3.7627e+00,\n          3.1852e-03,  3.5454e-01,  2.9798e-01,  5.0949e-01, -4.0331e-01,\n          6.4640e-01]])\nSample of Edge Index:\ntensor([[ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73],\n        [182, 183, 184, 185, 186, 187, 188, 189, 190, 191]])\nDegree Stats:\n- Min degree: 0.0\n- Max degree: 5.0\n- Mean degree: 0.5737523436546326\n- Standard deviation: 0.8438504934310913\nIsolated nodes: 7618 (60.1548%)\nPyTorch Geometric Graph Data saved at: /media/volume1/gnn_date.pt\n",
  "history_begin_time" : 1741492855763,
  "history_end_time" : 1741492890973,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "cJKJHWz1acHD",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom torch_geometric.utils import degree \r\n\r\n# Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\n# Oregon state\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\"Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\n\r\n# Step 1: Load and filter the dataset\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, parse_dates=['date'], chunksize=chunksize):\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear  \r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\n# Combine filtered chunks\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Filtered dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\n# Drop the original 'date' column after extracting 'dayofyear'\r\ndata = data.drop(columns=['date'])\r\n\r\n# Remove duplicates\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\"Found {num_duplicates} duplicate rows.\")\r\n\r\ndata = data.drop_duplicates()\r\nprint(f\"Removed duplicates. New dataset size: {data.shape[0]} rows.\")\r\n\r\n# Merge Nodes\r\nprint(\"Merging nodes to reduce graph size...\")\r\n\r\ngrid_size = 0.02  \r\ntime_bins = 20  \r\n\r\n# Assign spatial bins\r\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\n\r\n# Assign temporal bins\r\ndata['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\n\r\n# Create unique spatial-temporal ID\r\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\n# Aggregate nodes by grid_id\r\nmerged_nodes = data.groupby('grid_id').agg({\r\n    'Elevation': 'mean',\r\n    'Slope': 'mean',\r\n    'SWE': 'mean',\r\n    'snow_depth': 'mean',\r\n    'air_temperature_observed_f': 'mean',\r\n    'precipitation_amount': 'mean',\r\n    'relative_humidity_rmin': 'mean',\r\n    'relative_humidity_rmax': 'mean',\r\n    'wind_speed': 'mean',\r\n    'cumulative_SWE': 'mean',\r\n    'cumulative_precipitation_amount': 'mean',\r\n    'swe_value': 'mean',  \r\n    'lat': 'mean',\r\n    'lon': 'mean',\r\n    'dayofyear': 'mean'\r\n}).reset_index()\r\n\r\nprint(f\"Merged nodes: {merged_nodes.shape[0]} from {data.shape[0]} original records.\")\r\n\r\n# Normalize Features\r\nprint(\"Normalizing features using StandardScaler...\")\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\n\r\nscaler = StandardScaler()\r\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\r\nnode_features = torch.tensor(node_features, dtype=torch.float)\r\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\n\r\nprint(f\"Normalization completed. Node feature shape: {node_features.shape}\")\r\n\r\n# Build Graph Using Time-Aware KDTree\r\nprint(\"Building KDTree and constructing edges...\")\r\n\r\ncoordinates = merged_nodes[['lat', 'lon', 'dayofyear']].values  \r\ntree = KDTree(coordinates)\r\n\r\n# Graph Parameters\r\nk = 5 \r\nspatial_threshold = 0.06\r\ntime_weight = 0.02  \r\n\r\n# Define time-aware distance function\r\ndef time_aware_distance(p1, p2):\r\n    spatial_dist = np.linalg.norm(p1[:2] - p2[:2])  \r\n    time_dist = abs(p1[2] - p2[2]) * time_weight  \r\n    return spatial_dist + time_dist\r\n\r\n# Query k-NN\r\ndistances, neighbors = tree.query(coordinates, k=k+1)  \r\n\r\n# Convert to edge index format with distance filtering\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, k+1):  \r\n        if time_aware_distance(coordinates[i], coordinates[neighbors[i][j]]) < spatial_threshold:\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T\r\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\nprint(f\"Graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\r\n\r\n# Create PyTorch Geometric Graph Object\r\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n\r\n# Validate Graph Before Saving\r\nprint(\"\\nGraph Data Summary:\")\r\nprint(graph_data)\r\nprint(f\"Nodes: {graph_data.num_nodes}\")\r\nprint(f\"Edges: {graph_data.num_edges}\")\r\nprint(f\"Feature shape: {graph_data.x.shape}\")\r\nprint(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n\r\n# Sample Nodes & Edges\r\nprint(\"\\nSample of Node Features:\")\r\nprint(graph_data.x[:5])\r\nprint(\"\\nSample of Edge Index:\")\r\nprint(graph_data.edge_index[:, :10])\r\n\r\n# Compute Degree Statistics\r\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\r\nprint(\"\\nDegree Stats:\")\r\nprint(f\"- Min degree: {deg.min().item()}\")\r\nprint(f\"- Max degree: {deg.max().item()}\")\r\nprint(f\"- Mean degree: {deg.float().mean().item()}\")\r\nprint(f\"- Standard deviation: {deg.float().std().item()}\")\r\n\r\n# Count Isolated Nodes\r\nnum_isolated_nodes = (deg == 0).sum().item()\r\npercentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\r\nprint(f\"\\nIsolated nodes: {num_isolated_nodes} ({percentage_isolated:.4f}%)\")\r\n\r\n# Save the Graph\r\ngraph_data_output_path = \"/media/volume1/gnn_graph_fixed.pt\"\r\ntorch.save(graph_data, graph_data_output_path)\r\n\r\nprint(f\"\\nPyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\r\n",
  "history_output" : "Loading dataset...\nFiltered dataset loaded with 620662 rows and 16 columns.\nFound 97913 duplicate rows.\nRemoved duplicates. New dataset size: 522749 rows.\nMerging nodes to reduce graph size...\nMerged nodes: 12664 from 522749 original records.\nNormalizing features using StandardScaler...\nNormalization completed. Node feature shape: torch.Size([12664, 11])\nBuilding KDTree and constructing edges...\nGraph constructed with 12664 nodes and 7266 edges.\nGraph Data Summary:\nData(x=[12664, 11], edge_index=[2, 7266], y=[12664])\nNodes: 12664\nEdges: 7266\nFeature shape: torch.Size([12664, 11])\nEdge index shape: torch.Size([2, 7266])\nSample of Node Features:\ntensor([[ 2.1246e+00,  3.6312e-01, -1.7096e-01, -1.9966e-01,  4.3298e+00,\n         -5.6344e-01,  3.1638e-01, -2.3069e-01,  3.4011e-01, -4.0331e-01,\n          5.2565e-01],\n        [ 2.1246e+00,  3.6312e-01, -1.7096e-01, -1.9966e-01,  4.6841e+00,\n         -5.0281e-01,  2.5040e-01, -5.8181e-02,  3.1789e-01, -4.0331e-01,\n          5.1284e-01],\n        [ 2.1246e+00,  3.6312e-01, -1.7096e-01, -1.9966e-01,  4.3848e+00,\n         -5.2042e-01,  4.5102e-02, -5.1295e-02,  3.1096e-01, -4.0331e-01,\n          5.5758e-01],\n        [ 2.1246e+00,  3.6312e-01, -1.7096e-01, -1.9966e-01,  4.3297e+00,\n         -4.9235e-01, -1.4487e-02,  4.3373e-02,  2.9749e-01, -4.0331e-01,\n          5.5662e-01],\n        [ 2.1246e+00,  3.6312e-01, -1.7096e-01, -1.9966e-01,  3.7627e+00,\n          3.1852e-03,  3.5454e-01,  2.9798e-01,  5.0949e-01, -4.0331e-01,\n          6.4640e-01]])\nSample of Edge Index:\ntensor([[ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73],\n        [182, 183, 184, 185, 186, 187, 188, 189, 190, 191]])\nDegree Stats:\n- Min degree: 0.0\n- Max degree: 5.0\n- Mean degree: 0.5737523436546326\n- Standard deviation: 0.8438504934310913\nIsolated nodes: 7618 (60.1548%)\nPyTorch Geometric Graph Data saved at: /media/volume1/gnn_graph_fixed.pt\n",
  "history_begin_time" : 1741492773117,
  "history_end_time" : 1741492807844,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "x4NWN69uwRTs",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom torch_geometric.utils import degree \r\n\r\n# Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\"Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\n\r\n# Step 1: Load and filter the dataset\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, parse_dates=['date'], chunksize=chunksize):\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear  \r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\n# Combine filtered chunks\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Filtered dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\n# Drop the original 'date' column after extracting 'dayofyear'\r\ndata = data.drop(columns=['date'])\r\n\r\n# Remove duplicates\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\"Found {num_duplicates} duplicate rows.\")\r\n\r\ndata = data.drop_duplicates()\r\nprint(f\"Removed duplicates. New dataset size: {data.shape[0]} rows.\")\r\n\r\n# Merge Nodes\r\nprint(\"Merging nodes to reduce graph size...\")\r\n\r\ngrid_size = 0.02  \r\ntime_bins = 20  \r\n\r\n# Assign spatial bins\r\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\n\r\n# Assign temporal bins\r\ndata['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\n\r\n# Create unique spatial-temporal ID\r\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\n# Aggregate nodes by grid_id\r\nmerged_nodes = data.groupby('grid_id').agg({\r\n    'Elevation': 'mean',\r\n    'Slope': 'mean',\r\n    'SWE': 'mean',\r\n    'snow_depth': 'mean',\r\n    'air_temperature_observed_f': 'mean',\r\n    'precipitation_amount': 'mean',\r\n    'relative_humidity_rmin': 'mean',\r\n    'relative_humidity_rmax': 'mean',\r\n    'wind_speed': 'mean',\r\n    'cumulative_SWE': 'mean',\r\n    'cumulative_precipitation_amount': 'mean',\r\n    'swe_value': 'mean',  \r\n    'lat': 'mean',\r\n    'lon': 'mean',\r\n    'dayofyear': 'mean'\r\n}).reset_index()\r\n\r\nprint(f\"Merged nodes: {merged_nodes.shape[0]} from {data.shape[0]} original records.\")\r\n\r\n# Normalize Features\r\nprint(\"Normalizing features using StandardScaler...\")\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\n\r\nscaler = StandardScaler()\r\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\r\nnode_features = torch.tensor(node_features, dtype=torch.float)\r\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\n\r\nprint(f\"Normalization completed. Node feature shape: {node_features.shape}\")\r\n\r\n# Build Graph Using Time-Aware KDTree\r\nprint(\"Building KDTree and constructing edges...\")\r\n\r\ncoordinates = merged_nodes[['lat', 'lon', 'dayofyear']].values  \r\ntree = KDTree(coordinates)\r\n\r\n# Graph Parameters\r\nk = 3  \r\nspatial_threshold = 0.04  \r\ntime_weight = 0.04  \r\n\r\n# Define time-aware distance function\r\ndef time_aware_distance(p1, p2):\r\n    spatial_dist = np.linalg.norm(p1[:2] - p2[:2])  \r\n    time_dist = abs(p1[2] - p2[2]) * time_weight  \r\n    return spatial_dist + time_dist\r\n\r\n# Query k-NN\r\ndistances, neighbors = tree.query(coordinates, k=k+1)  \r\n\r\n# Convert to edge index format with distance filtering\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, k+1):  \r\n        if time_aware_distance(coordinates[i], coordinates[neighbors[i][j]]) < spatial_threshold:\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T\r\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\nprint(f\"Graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\r\n\r\n# Create PyTorch Geometric Graph Object\r\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n\r\n# Validate Graph Before Saving\r\nprint(\"\\nGraph Data Summary:\")\r\nprint(graph_data)\r\nprint(f\"Nodes: {graph_data.num_nodes}\")\r\nprint(f\"Edges: {graph_data.num_edges}\")\r\nprint(f\"Feature shape: {graph_data.x.shape}\")\r\nprint(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n\r\n# Sample Nodes & Edges\r\nprint(\"\\nSample of Node Features:\")\r\nprint(graph_data.x[:5])\r\nprint(\"\\nSample of Edge Index:\")\r\nprint(graph_data.edge_index[:, :10])\r\n\r\n# Compute Degree Statistics\r\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\r\nprint(\"\\nDegree Stats:\")\r\nprint(f\"- Min degree: {deg.min().item()}\")\r\nprint(f\"- Max degree: {deg.max().item()}\")\r\nprint(f\"- Mean degree: {deg.float().mean().item()}\")\r\nprint(f\"- Standard deviation: {deg.float().std().item()}\")\r\n\r\n# Count Isolated Nodes\r\nnum_isolated_nodes = (deg == 0).sum().item()\r\npercentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\r\nprint(f\"\\nIsolated nodes: {num_isolated_nodes} ({percentage_isolated:.4f}%)\")\r\n\r\n# Save the Graph\r\ngraph_data_output_path = \"/media/volume1/gnn_graph_fixed.pt\"\r\ntorch.save(graph_data, graph_data_output_path)\r\n\r\nprint(f\"\\nPyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\r\n",
  "history_output" : "Loading dataset...\nFiltered dataset loaded with 620662 rows and 16 columns.\nFound 97913 duplicate rows.\nRemoved duplicates. New dataset size: 522749 rows.\nMerging nodes to reduce graph size...\nMerged nodes: 12664 from 522749 original records.\nNormalizing features using StandardScaler...\nNormalization completed. Node feature shape: torch.Size([12664, 11])\nBuilding KDTree and constructing edges...\nGraph constructed with 12664 nodes and 4226 edges.\nGraph Data Summary:\nData(x=[12664, 11], edge_index=[2, 4226], y=[12664])\nNodes: 12664\nEdges: 4226\nFeature shape: torch.Size([12664, 11])\nEdge index shape: torch.Size([2, 4226])\nSample of Node Features:\ntensor([[ 2.1246e+00,  3.6312e-01, -1.7096e-01, -1.9966e-01,  4.3298e+00,\n         -5.6344e-01,  3.1638e-01, -2.3069e-01,  3.4011e-01, -4.0331e-01,\n          5.2565e-01],\n        [ 2.1246e+00,  3.6312e-01, -1.7096e-01, -1.9966e-01,  4.6841e+00,\n         -5.0281e-01,  2.5040e-01, -5.8181e-02,  3.1789e-01, -4.0331e-01,\n          5.1284e-01],\n        [ 2.1246e+00,  3.6312e-01, -1.7096e-01, -1.9966e-01,  4.3848e+00,\n         -5.2042e-01,  4.5102e-02, -5.1295e-02,  3.1096e-01, -4.0331e-01,\n          5.5758e-01],\n        [ 2.1246e+00,  3.6312e-01, -1.7096e-01, -1.9966e-01,  4.3297e+00,\n         -4.9235e-01, -1.4487e-02,  4.3373e-02,  2.9749e-01, -4.0331e-01,\n          5.5662e-01],\n        [ 2.1246e+00,  3.6312e-01, -1.7096e-01, -1.9966e-01,  3.7627e+00,\n          3.1852e-03,  3.5454e-01,  2.9798e-01,  5.0949e-01, -4.0331e-01,\n          6.4640e-01]])\nSample of Edge Index:\ntensor([[ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73],\n        [182, 183, 184, 185, 186, 187, 188, 189, 190, 191]])\nDegree Stats:\n- Min degree: 0.0\n- Max degree: 3.0\n- Mean degree: 0.33370181918144226\n- Standard deviation: 0.6055741310119629\nIsolated nodes: 9207 (72.7021%)\nPyTorch Geometric Graph Data saved at: /media/volume1/gnn_graph_fixed.pt\n",
  "history_begin_time" : 1741492098359,
  "history_end_time" : 1741492132023,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "YQs0noE5CFiO",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom torch_geometric.utils import degree \r\n\r\n# Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\"Loading dataset...\")\r\n\r\n# Use 'date' to compute 'dayofyear' instead of assuming it exists in the CSV\r\nuseful_columns = [\r\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\r\n]\r\n\r\n# Step 1: Load and filter the dataset\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, parse_dates=['date'], chunksize=chunksize):\r\n    chunk['dayofyear'] = chunk['date'].dt.dayofyear  \r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\n# Combine filtered chunks\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Filtered dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\n# Drop the original 'date' column after extracting 'dayofyear'\r\ndata = data.drop(columns=['date'])\r\n\r\n# Remove duplicates\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\"Found {num_duplicates} duplicate rows.\")\r\n\r\ndata = data.drop_duplicates()\r\nprint(f\"Removed duplicates. New dataset size: {data.shape[0]} rows.\")\r\n\r\n# Merge Nodes\r\nprint(\"Merging nodes to reduce graph size...\")\r\n\r\ngrid_size = 0.02  \r\ntime_bins = 20  \r\n\r\n# Assign spatial bins\r\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\n\r\n# Assign temporal bins\r\ndata['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\n\r\n# Create unique spatial-temporal ID\r\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\n# Aggregate nodes by grid_id\r\nmerged_nodes = data.groupby('grid_id').agg({\r\n    'Elevation': 'mean',\r\n    'Slope': 'mean',\r\n    'SWE': 'mean',\r\n    'snow_depth': 'mean',\r\n    'air_temperature_observed_f': 'mean',\r\n    'precipitation_amount': 'mean',\r\n    'relative_humidity_rmin': 'mean',\r\n    'relative_humidity_rmax': 'mean',\r\n    'wind_speed': 'mean',\r\n    'cumulative_SWE': 'mean',\r\n    'cumulative_precipitation_amount': 'mean',\r\n    'swe_value': 'mean',  \r\n    'lat': 'mean',\r\n    'lon': 'mean',\r\n    'dayofyear': 'mean'\r\n}).reset_index()\r\n\r\nprint(f\"Merged nodes: {merged_nodes.shape[0]} from {data.shape[0]} original records.\")\r\n\r\n# Normalize Features\r\nprint(\"Normalizing features using StandardScaler...\")\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\n\r\nscaler = StandardScaler()\r\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\r\nnode_features = torch.tensor(node_features, dtype=torch.float)\r\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\n\r\nprint(f\"Normalization completed. Node feature shape: {node_features.shape}\")\r\n\r\n# Build Graph Using Time-Aware KDTree\r\nprint(\"Building KDTree and constructing edges...\")\r\n\r\ncoordinates = merged_nodes[['lat', 'lon', 'dayofyear']].values  \r\ntree = KDTree(coordinates)\r\n\r\n# Graph Parameters\r\nk = 3  \r\nspatial_threshold = 0.04  \r\ntime_weight = 0.04  \r\n\r\n# Define time-aware distance function\r\ndef time_aware_distance(p1, p2):\r\n    spatial_dist = np.linalg.norm(p1[:2] - p2[:2])  \r\n    time_dist = abs(p1[2] - p2[2]) * time_weight  \r\n    return spatial_dist + time_dist\r\n\r\n# Query k-NN\r\ndistances, neighbors = tree.query(coordinates, k=k+1)  \r\n\r\n# Convert to edge index format with distance filtering\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, k+1):  \r\n        if time_aware_distance(coordinates[i], coordinates[neighbors[i][j]]) < spatial_threshold:\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T\r\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\nprint(f\"Graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\r\n\r\n# Create PyTorch Geometric Graph Object\r\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n\r\n# Validate Graph Before Saving\r\nprint(\"\\nGraph Data Summary:\")\r\nprint(graph_data)\r\nprint(f\"Nodes: {graph_data.num_nodes}\")\r\nprint(f\"Edges: {graph_data.num_edges}\")\r\nprint(f\"Feature shape: {graph_data.x.shape}\")\r\nprint(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n\r\n# Sample Nodes & Edges\r\nprint(\"\\nSample of Node Features:\")\r\nprint(graph_data.x[:5])\r\nprint(\"\\nSample of Edge Index:\")\r\nprint(graph_data.edge_index[:, :10])\r\n\r\n# Compute Degree Statistics\r\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\r\nprint(\"\\nDegree Stats:\")\r\nprint(f\"- Min degree: {deg.min().item()}\")\r\nprint(f\"- Max degree: {deg.max().item()}\")\r\nprint(f\"- Mean degree: {deg.float().mean().item()}\")\r\nprint(f\"- Standard deviation: {deg.float().std().item()}\")\r\n\r\n# Count Isolated Nodes\r\nnum_isolated_nodes = (deg == 0).sum().item()\r\npercentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\r\nprint(f\"\\nIsolated nodes: {num_isolated_nodes} ({percentage_isolated:.4f}%)\")\r\n\r\n# Save the Graph\r\ngraph_data_output_path = \"/media/volume1/gnn_graph_fixed.pt\"\r\ntorch.save(graph_data, graph_data_output_path)\r\n\r\nprint(f\"\\nPyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\r\n",
  "history_output" : "Loading dataset...\n",
  "history_begin_time" : 1741492067027,
  "history_end_time" : 1741492076631,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "tnjMPrEijm59",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom torch_geometric.utils import degree \r\n\r\n#  Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\n# Load only the first few rows to check available columns\r\ndf_check = pd.read_csv(file_path, nrows=5)\r\n\r\n# Print column names from the CSV file\r\nprint(\"🔎 Available columns in CSV:\", df_check.columns.tolist())\r\n\r\nexit()  # Stop execution here for debugging\r\n\r\n\r\n# chunksize = 500000\r\n# min_lat, max_lat = 42.020065, 45.768982\r\n# min_lon, max_lon = -124.212205, -116.940392\r\n\r\n# print(\" Loading dataset...\")\r\n\r\n# useful_columns = [\r\n#     'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n#     'air_temperature_observed_f', 'precipitation_amount',\r\n#     'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n#     'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value',\r\n#     'dayofyear'  \r\n# ]\r\n\r\n# # Step 1: Load and filter the dataset\r\n# df_list = []\r\n# for chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n#     filtered_chunk = chunk[\r\n#         (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n#         (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n#     ]\r\n#     df_list.append(filtered_chunk)\r\n\r\n# # Combine filtered chunks\r\n# data = pd.concat(df_list, ignore_index=True)\r\n# print(f\" Filtered dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\n# # Remove duplicates\r\n# num_duplicates = data.duplicated().sum()\r\n# print(f\" Found {num_duplicates} duplicate rows.\")\r\n\r\n# data = data.drop_duplicates()\r\n# print(f\" Removed duplicates. New dataset size: {data.shape[0]} rows.\")\r\n\r\n# # Check if 'dayofyear' exists\r\n# if 'dayofyear' not in data.columns:\r\n#     print(\" ERROR: 'dayofyear' column not found in dataset. Exiting.\")\r\n#     exit()\r\n\r\n# #  Merge Nodes (Reduce Graph Size)\r\n# print(\" Merging nodes to reduce graph size...\")\r\n\r\n# grid_size = 0.02  # Spatial bin size\r\n# time_bins = 20  # Temporal bin size\r\n\r\n# # Assign spatial bins\r\n# data['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\n# data['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\n\r\n# # Assign temporal bins\r\n# data['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\n\r\n# # Create unique spatial-temporal ID\r\n# data['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\n# # Aggregate nodes by grid_id (reducing the number of nodes)\r\n# merged_nodes = data.groupby('grid_id').agg({\r\n#     'Elevation': 'mean',\r\n#     'Slope': 'mean',\r\n#     'SWE': 'mean',\r\n#     'snow_depth': 'mean',\r\n#     'air_temperature_observed_f': 'mean',\r\n#     'precipitation_amount': 'mean',\r\n#     'relative_humidity_rmin': 'mean',\r\n#     'relative_humidity_rmax': 'mean',\r\n#     'wind_speed': 'mean',\r\n#     'cumulative_SWE': 'mean',\r\n#     'cumulative_precipitation_amount': 'mean',\r\n#     'swe_value': 'mean',  \r\n#     'lat': 'mean',\r\n#     'lon': 'mean',\r\n#     'dayofyear': 'mean'\r\n# }).reset_index()\r\n\r\n# print(f\" Merged nodes: {merged_nodes.shape[0]} from {data.shape[0]} original records.\")\r\n\r\n# #  Normalize Features\r\n# print(\" Normalizing features using StandardScaler...\")\r\n# feature_columns = [\r\n#     'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n#     'air_temperature_observed_f', 'precipitation_amount',\r\n#     'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n#     'cumulative_SWE', 'cumulative_precipitation_amount'\r\n# ]\r\n\r\n# scaler = StandardScaler()\r\n# node_features = scaler.fit_transform(merged_nodes[feature_columns])\r\n# node_features = torch.tensor(node_features, dtype=torch.float)\r\n# labels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\n\r\n# print(f\" Normalization completed. Node feature shape: {node_features.shape}\")\r\n\r\n# #  Build Graph Using Time-Aware KDTree\r\n# print(\" Building KDTree and constructing edges...\")\r\n\r\n# coordinates = merged_nodes[['lat', 'lon', 'dayofyear']].values  \r\n# tree = KDTree(coordinates)\r\n\r\n# # Graph Parameters\r\n# k = 3  # Number of nearest neighbors\r\n# spatial_threshold = 0.04  # Max spatial connection distance\r\n# time_weight = 0.04  # Reduce time impact\r\n\r\n# # Define time-aware distance function\r\n# def time_aware_distance(p1, p2):\r\n#     spatial_dist = np.linalg.norm(p1[:2] - p2[:2])  # Spatial distance\r\n#     time_dist = abs(p1[2] - p2[2]) * time_weight  # Time difference scaled\r\n#     return spatial_dist + time_dist\r\n\r\n# # Query k-NN\r\n# distances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-node\r\n\r\n# # Convert to edge index format with distance filtering\r\n# edge_list = []\r\n# for i in range(len(neighbors)):\r\n#     for j in range(1, k+1):  # Skip self-node\r\n#         if time_aware_distance(coordinates[i], coordinates[neighbors[i][j]]) < spatial_threshold:\r\n#             edge_list.append((i, neighbors[i][j]))\r\n\r\n# edges = np.array(edge_list).T\r\n# edge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\n# print(f\" Graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\r\n\r\n# #  Create PyTorch Geometric Graph Object\r\n# graph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n\r\n# #  Validate Graph Before Saving\r\n# print(\"\\n Graph Data Summary:\")\r\n# print(graph_data)\r\n# print(f\" Nodes: {graph_data.num_nodes}\")\r\n# print(f\" Edges: {graph_data.num_edges}\")\r\n# print(f\" Feature shape: {graph_data.x.shape}\")\r\n# print(f\" Edge index shape: {graph_data.edge_index.shape}\")\r\n\r\n# # Sample Nodes & Edges\r\n# print(\"\\n Sample of Node Features:\")\r\n# print(graph_data.x[:5])\r\n# print(\"\\n Sample of Edge Index:\")\r\n# print(graph_data.edge_index[:, :10])\r\n\r\n# # Compute Degree Statistics\r\n# deg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\r\n# print(\"\\n Degree Stats:\")\r\n# print(f\"- Min degree: {deg.min().item()}\")\r\n# print(f\"- Max degree: {deg.max().item()}\")\r\n# print(f\"- Mean degree: {deg.float().mean().item()}\")\r\n# print(f\"- Standard deviation: {deg.float().std().item()}\")\r\n\r\n# # Count Isolated Nodes\r\n# num_isolated_nodes = (deg == 0).sum().item()\r\n# percentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\r\n# print(f\"\\nIsolated nodes: {num_isolated_nodes} ({percentage_isolated:.4f}%)\")\r\n\r\n# # #  Save the Graph\r\n# # graph_data_output_path = \"/media/volume1/gnn_graph_fixed.pt\"\r\n# # torch.save(graph_data, graph_data_output_path)\r\n\r\n# # print(f\"\\n✅ PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\r\n",
  "history_output" : "🔎 Available columns in CSV: ['date', 'lat', 'lon', 'SWE', 'station_name', 'swe_value', 'change_in_swe_inch', 'snow_depth', 'air_temperature_observed_f', 'precipitation_amount', 'relative_humidity_rmin', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'mean_vapor_pressure_deficit', 'air_temperature_tmmn', 'wind_speed', 'Elevation', 'Aspect', 'Curvature', 'Northness', 'Eastness', 'fsca', 'Slope', 'SWE_1', 'air_temperature_tmmn_1', 'potential_evapotranspiration_1', 'mean_vapor_pressure_deficit_1', 'relative_humidity_rmax_1', 'relative_humidity_rmin_1', 'precipitation_amount_1', 'air_temperature_tmmx_1', 'wind_speed_1', 'fsca_1', 'SWE_2', 'air_temperature_tmmn_2', 'potential_evapotranspiration_2', 'mean_vapor_pressure_deficit_2', 'relative_humidity_rmax_2', 'relative_humidity_rmin_2', 'precipitation_amount_2', 'air_temperature_tmmx_2', 'wind_speed_2', 'fsca_2', 'SWE_3', 'air_temperature_tmmn_3', 'potential_evapotranspiration_3', 'mean_vapor_pressure_deficit_3', 'relative_humidity_rmax_3', 'relative_humidity_rmin_3', 'precipitation_amount_3', 'air_temperature_tmmx_3', 'wind_speed_3', 'fsca_3', 'SWE_4', 'air_temperature_tmmn_4', 'potential_evapotranspiration_4', 'mean_vapor_pressure_deficit_4', 'relative_humidity_rmax_4', 'relative_humidity_rmin_4', 'precipitation_amount_4', 'air_temperature_tmmx_4', 'wind_speed_4', 'fsca_4', 'SWE_5', 'air_temperature_tmmn_5', 'potential_evapotranspiration_5', 'mean_vapor_pressure_deficit_5', 'relative_humidity_rmax_5', 'relative_humidity_rmin_5', 'precipitation_amount_5', 'air_temperature_tmmx_5', 'wind_speed_5', 'fsca_5', 'SWE_6', 'air_temperature_tmmn_6', 'potential_evapotranspiration_6', 'mean_vapor_pressure_deficit_6', 'relative_humidity_rmax_6', 'relative_humidity_rmin_6', 'precipitation_amount_6', 'air_temperature_tmmx_6', 'wind_speed_6', 'fsca_6', 'SWE_7', 'air_temperature_tmmn_7', 'potential_evapotranspiration_7', 'mean_vapor_pressure_deficit_7', 'relative_humidity_rmax_7', 'relative_humidity_rmin_7', 'precipitation_amount_7', 'air_temperature_tmmx_7', 'wind_speed_7', 'fsca_7', 'water_year', 'cumulative_SWE', 'cumulative_air_temperature_tmmn', 'cumulative_potential_evapotranspiration', 'cumulative_mean_vapor_pressure_deficit', 'cumulative_relative_humidity_rmax', 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount', 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'cumulative_fsca']\n",
  "history_begin_time" : 1741491833541,
  "history_end_time" : 1741491838815,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "t0mXwBqX8OV1",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom torch_geometric.utils import degree \r\n\r\n#  Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\" Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value',\r\n    'dayofyear'  \r\n]\r\n\r\n# Step 1: Load and filter the dataset\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\n# Combine filtered chunks\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\" Filtered dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\n# Remove duplicates\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\n\r\ndata = data.drop_duplicates()\r\nprint(f\" Removed duplicates. New dataset size: {data.shape[0]} rows.\")\r\n\r\n# Check if 'dayofyear' exists\r\nif 'dayofyear' not in data.columns:\r\n    print(\" ERROR: 'dayofyear' column not found in dataset. Exiting.\")\r\n    exit()\r\n\r\n#  Merge Nodes (Reduce Graph Size)\r\nprint(\" Merging nodes to reduce graph size...\")\r\n\r\ngrid_size = 0.02  # Spatial bin size\r\ntime_bins = 20  # Temporal bin size\r\n\r\n# Assign spatial bins\r\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\n\r\n# Assign temporal bins\r\ndata['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\n\r\n# Create unique spatial-temporal ID\r\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\n# Aggregate nodes by grid_id (reducing the number of nodes)\r\nmerged_nodes = data.groupby('grid_id').agg({\r\n    'Elevation': 'mean',\r\n    'Slope': 'mean',\r\n    'SWE': 'mean',\r\n    'snow_depth': 'mean',\r\n    'air_temperature_observed_f': 'mean',\r\n    'precipitation_amount': 'mean',\r\n    'relative_humidity_rmin': 'mean',\r\n    'relative_humidity_rmax': 'mean',\r\n    'wind_speed': 'mean',\r\n    'cumulative_SWE': 'mean',\r\n    'cumulative_precipitation_amount': 'mean',\r\n    'swe_value': 'mean',  \r\n    'lat': 'mean',\r\n    'lon': 'mean',\r\n    'dayofyear': 'mean'\r\n}).reset_index()\r\n\r\nprint(f\" Merged nodes: {merged_nodes.shape[0]} from {data.shape[0]} original records.\")\r\n\r\n#  Normalize Features\r\nprint(\" Normalizing features using StandardScaler...\")\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\n\r\nscaler = StandardScaler()\r\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\r\nnode_features = torch.tensor(node_features, dtype=torch.float)\r\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\n\r\nprint(f\" Normalization completed. Node feature shape: {node_features.shape}\")\r\n\r\n#  Build Graph Using Time-Aware KDTree\r\nprint(\" Building KDTree and constructing edges...\")\r\n\r\ncoordinates = merged_nodes[['lat', 'lon', 'dayofyear']].values  \r\ntree = KDTree(coordinates)\r\n\r\n# Graph Parameters\r\nk = 3  # Number of nearest neighbors\r\nspatial_threshold = 0.04  # Max spatial connection distance\r\ntime_weight = 0.04  # Reduce time impact\r\n\r\n# Define time-aware distance function\r\ndef time_aware_distance(p1, p2):\r\n    spatial_dist = np.linalg.norm(p1[:2] - p2[:2])  # Spatial distance\r\n    time_dist = abs(p1[2] - p2[2]) * time_weight  # Time difference scaled\r\n    return spatial_dist + time_dist\r\n\r\n# Query k-NN\r\ndistances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-node\r\n\r\n# Convert to edge index format with distance filtering\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, k+1):  # Skip self-node\r\n        if time_aware_distance(coordinates[i], coordinates[neighbors[i][j]]) < spatial_threshold:\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T\r\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\nprint(f\" Graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\r\n\r\n#  Create PyTorch Geometric Graph Object\r\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n\r\n#  Validate Graph Before Saving\r\nprint(\"\\n Graph Data Summary:\")\r\nprint(graph_data)\r\nprint(f\" Nodes: {graph_data.num_nodes}\")\r\nprint(f\" Edges: {graph_data.num_edges}\")\r\nprint(f\" Feature shape: {graph_data.x.shape}\")\r\nprint(f\" Edge index shape: {graph_data.edge_index.shape}\")\r\n\r\n# Sample Nodes & Edges\r\nprint(\"\\n Sample of Node Features:\")\r\nprint(graph_data.x[:5])\r\nprint(\"\\n Sample of Edge Index:\")\r\nprint(graph_data.edge_index[:, :10])\r\n\r\n# Compute Degree Statistics\r\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\r\nprint(\"\\n Degree Stats:\")\r\nprint(f\"- Min degree: {deg.min().item()}\")\r\nprint(f\"- Max degree: {deg.max().item()}\")\r\nprint(f\"- Mean degree: {deg.float().mean().item()}\")\r\nprint(f\"- Standard deviation: {deg.float().std().item()}\")\r\n\r\n# Count Isolated Nodes\r\nnum_isolated_nodes = (deg == 0).sum().item()\r\npercentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\r\nprint(f\"\\nIsolated nodes: {num_isolated_nodes} ({percentage_isolated:.4f}%)\")\r\n\r\n# #  Save the Graph\r\n# graph_data_output_path = \"/media/volume1/gnn_graph_fixed.pt\"\r\n# torch.save(graph_data, graph_data_output_path)\r\n\r\n# print(f\"\\n✅ PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\r\n",
  "history_output" : " Loading dataset...\nTraceback (most recent call last):\n  File \"/media/volume1/gw-workspace/t0mXwBqX8OV1/graph_creation.py\", line 28, in <module>\n    for chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 1026, in read_csv\n    return _read(filepath_or_buffer, kwds)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 620, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 1620, in __init__\n    self._engine = self._make_engine(f, self.engine)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 1898, in _make_engine\n    return mapping[engine](f, **self.options)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 140, in __init__\n    self._validate_usecols_names(usecols, self.orig_names)\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/base_parser.py\", line 979, in _validate_usecols_names\n    raise ValueError(\nValueError: Usecols do not match columns, columns expected but not found: ['dayofyear']\n",
  "history_begin_time" : 1741491685242,
  "history_end_time" : 1741491690597,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "5oum6H4S0Sma",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom torch_geometric.utils import degree \r\n\r\n\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\" Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount','swe_value'\r\n]  \r\n\r\n# Step 1: Load and filter the dataset\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\n# Combine filtered chunks\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\" Filtered dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\n\r\ndata = data.drop_duplicates()\r\n\r\nprint(f\"Removed duplicates. New dataset size: {data.shape[0]} rows.\") \r\n\r\n#Merge nodes\r\nprint(\"Merging nodes to reduce graph size:\")\r\ngrid_size= 0.02\r\ntime_bins=20\r\n\r\n# Assign spatial bins\r\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\n\r\n# Assign temporal bins\r\ndata['dayofyear'] = data['date'].dt.dayofyear\r\ndata['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\n\r\n# Create unique spatial-temporal ID\r\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\n# Aggregate nodes by grid_id (reducing the number of nodes)\r\nmerged_nodes = data.groupby('grid_id').agg({\r\n    'Elevation': 'mean',\r\n    'Slope': 'mean',\r\n    'SWE': 'mean',\r\n    'snow_depth': 'mean',\r\n    'air_temperature_observed_f': 'mean',\r\n    'precipitation_amount': 'mean',\r\n    'relative_humidity_rmin': 'mean',\r\n    'relative_humidity_rmax': 'mean',\r\n    'wind_speed': 'mean',\r\n    'cumulative_SWE': 'mean',\r\n    'cumulative_precipitation_amount': 'mean',\r\n    'swe_value': 'mean',  \r\n    'lat': 'mean',\r\n    'lon': 'mean',\r\n    'dayofyear': 'mean'\r\n}).reset_index()\r\n\r\nprint(f\" Merged nodes: {merged_nodes.shape[0]} from {data.shape[0]} original records.\")\r\n\r\n\r\nprint(\" Normalizing features using StandardScaler...\")\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\nscaler = StandardScaler()\r\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\r\nnode_features = torch.tensor(node_features, dtype=torch.float)\r\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\nprint(\" Normalization completed. Node feature shape:{node_features.shape}\")\r\n\r\nprint(\"Building KDTree and constructing edges\")\r\n\r\ncoordinates = merged_nodes[['lat', 'lon','dayofyear']].values\r\ntree = KDTree(coordinates)\r\n\r\nk = 3\r\nspatial_threshold = 0.04 \r\ntime_weight =0.04\r\ndef time_aware_distance(p1,p2):\r\n    spatial_dist = np.linalg.norm(p1[:2] - p2[:2])\r\n    time_dist = abs(p1[2] - p2[2]) * time_weight\r\n    return spatial_dist + time_dist\r\n\r\n# Query k-nearest neighbors\r\ndistances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\r\n\r\n# Convert to edge index format with distance filtering\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, k+1):  # Skip first \r\n        if time_aware_distance(coordinates[i], coordinates[neighbors[i][j]]) < spatial_threshold:\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T  # Convert list to NumPy array and transpose\r\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\nprint(f\"✅ Graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\r\n\r\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n\r\n\r\n#  checking graph before saving\r\nprint(\"\\n Graph Data Summary:\")\r\nprint(graph_data)  # This prints node & edge count\r\nprint(f\"Number of nodes: {graph_data.num_nodes}\")\r\nprint(f\"Number of edges: {graph_data.num_edges}\")\r\nprint(f\"Node feature shape: {graph_data.x.shape}\")\r\nprint(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n# Sample a few edges and nodes\r\nprint(\"\\n Sample of Node Features:\")\r\nprint(graph_data.x[:5])  # Show first 5 node features\r\nprint(\"\\n Sample of Edge Index:\")\r\nprint(graph_data.edge_index[:, :10])  # Show first 10 edges\r\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)  # Compute degrees for each node\r\nprint(f\" Degree stats:\\n\")\r\nprint(f\" - Min degree: {deg.min().item()}\")\r\nprint(f\" - Max degree: {deg.max().item()}\")\r\nprint(f\" - Mean degree: {deg.float().mean().item()}\")\r\nprint(f\" - Standard deviation: {deg.float().std().item()}\")\r\n# Count isolated nodes\r\nnum_isolated_nodes = (deg == 0).sum().item()\r\n\r\nprint(f\" Number of isolated nodes (degree=0): {num_isolated_nodes}\")\r\n\r\n# Percentage of isolated nodes\r\npercentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\r\nprint(f\" Percentage of isolated nodes: {percentage_isolated:.4f}%\")\r\n\r\n\r\n# #saving the graph\r\n# graph_data_output_path = \"/media/volume1/gnn_graph.pt\"\r\n\r\n# # Save the graph data object\r\n# torch.save(graph_data, graph_data_output_path)\r\n\r\n# print(f\" PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\r\n",
  "history_output" : " Loading dataset...\n Filtered dataset loaded with 620662 rows and 14 columns.\n Found 185867 duplicate rows.\nRemoved duplicates. New dataset size: 434795 rows.\nMerging nodes to reduce graph size:\nTraceback (most recent call last):\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py\", line 3805, in get_loc\n    return self._engine.get_loc(casted_key)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"index.pyx\", line 167, in pandas._libs.index.IndexEngine.get_loc\n  File \"index.pyx\", line 196, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7081, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7089, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'date'\nThe above exception was the direct cause of the following exception:\nTraceback (most recent call last):\n  File \"/media/volume1/gw-workspace/5oum6H4S0Sma/graph_creation.py\", line 57, in <module>\n    data['dayofyear'] = data['date'].dt.dayofyear\n                        ~~~~^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py\", line 4102, in __getitem__\n    indexer = self.columns.get_loc(key)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py\", line 3812, in get_loc\n    raise KeyError(key) from err\nKeyError: 'date'\n",
  "history_begin_time" : 1741491229905,
  "history_end_time" : 1741491261111,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "TIcw7KZ87kOa",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\nfrom sklearn.decomposition import PCA\r\nfrom torch_geometric.utils import degree \r\n\r\n\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\" Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount','swe_value'\r\n]  \r\n\r\n# Step 1: Load and filter the dataset\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\n# Combine filtered chunks\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\" Filtered dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\n\r\ndata = data.drop_duplicates()\r\n\r\nprint(f\"Removed duplicates. New dataset size: {data.shape[0]} rows.\") \r\n\r\n#Merge nodes\r\nprint(\"Merging nodes to reduce graph size:\")\r\ngrid_size= 0.02\r\ntime_bins=20\r\n\r\n# Assign spatial bins\r\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\r\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\r\n\r\n# Assign temporal bins\r\ndata['dayofyear'] = data['date'].dt.dayofyear\r\ndata['time_bin'] = (data['dayofyear'] // (365 // time_bins)).astype(int)\r\n\r\n# Create unique spatial-temporal ID\r\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['time_bin'].astype(str)\r\n\r\n# Aggregate nodes by grid_id (reducing the number of nodes)\r\nmerged_nodes = data.groupby('grid_id').agg({\r\n    'Elevation': 'mean',\r\n    'Slope': 'mean',\r\n    'SWE': 'mean',\r\n    'snow_depth': 'mean',\r\n    'air_temperature_observed_f': 'mean',\r\n    'precipitation_amount': 'mean',\r\n    'relative_humidity_rmin': 'mean',\r\n    'relative_humidity_rmax': 'mean',\r\n    'wind_speed': 'mean',\r\n    'cumulative_SWE': 'mean',\r\n    'cumulative_precipitation_amount': 'mean',\r\n    'swe_value': 'mean',  \r\n    'lat': 'mean',\r\n    'lon': 'mean',\r\n    'dayofyear': 'mean'\r\n}).reset_index()\r\n\r\nprint(f\" Merged nodes: {merged_nodes.shape[0]} from {data.shape[0]} original records.\")\r\n\r\n\r\nprint(\" Normalizing features using StandardScaler...\")\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\nscaler = StandardScaler()\r\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\r\nnode_features = torch.tensor(node_features, dtype=torch.float)\r\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\r\nprint(\" Normalization completed. Node feature shape:{node_features.shape}\")\r\n\r\nprint(\"Building KDTree and constructing edges\")\r\n\r\ncoordinates = data[['lat', 'lon','dayofyear']].values\r\ntree = KDTree(coordinates)\r\n\r\nk = 3\r\nspatial_threshold = 0.04 \r\ntime_weight =0.04\r\ndef time_aware_distance(p1,p2):\r\n    spatial_dist = np.linalg.norm(p1[:2] - p2[:2])\r\n    time_dist = abs(p1[2] - p2[2]) * time_weight\r\n    return spatial_dist + time_dist\r\n\r\n# Query k-nearest neighbors\r\ndistances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\r\n\r\n# Convert to edge index format with distance filtering\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, k+1):  # Skip first \r\n        if time_aware_distance(coordinates[i], coordinates[neighbors[i][j]]) < spatial_threshold:\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T  # Convert list to NumPy array and transpose\r\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\r\n\r\nprint(f\"✅ Graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\r\n\r\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n\r\n\r\n#  checking graph before saving\r\nprint(\"\\n Graph Data Summary:\")\r\nprint(graph_data)  # This prints node & edge count\r\nprint(f\"Number of nodes: {graph_data.num_nodes}\")\r\nprint(f\"Number of edges: {graph_data.num_edges}\")\r\nprint(f\"Node feature shape: {graph_data.x.shape}\")\r\nprint(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n# Sample a few edges and nodes\r\nprint(\"\\n Sample of Node Features:\")\r\nprint(graph_data.x[:5])  # Show first 5 node features\r\nprint(\"\\n Sample of Edge Index:\")\r\nprint(graph_data.edge_index[:, :10])  # Show first 10 edges\r\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)  # Compute degrees for each node\r\nprint(f\" Degree stats:\\n\")\r\nprint(f\" - Min degree: {deg.min().item()}\")\r\nprint(f\" - Max degree: {deg.max().item()}\")\r\nprint(f\" - Mean degree: {deg.float().mean().item()}\")\r\nprint(f\" - Standard deviation: {deg.float().std().item()}\")\r\n# Count isolated nodes\r\nnum_isolated_nodes = (deg == 0).sum().item()\r\n\r\nprint(f\" Number of isolated nodes (degree=0): {num_isolated_nodes}\")\r\n\r\n# Percentage of isolated nodes\r\npercentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\r\nprint(f\" Percentage of isolated nodes: {percentage_isolated:.4f}%\")\r\n\r\n\r\n# #saving the graph\r\n# graph_data_output_path = \"/media/volume1/gnn_graph.pt\"\r\n\r\n# # Save the graph data object\r\n# torch.save(graph_data, graph_data_output_path)\r\n\r\n# print(f\" PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\r\n",
  "history_output" : " Loading dataset...\n Filtered dataset loaded with 620662 rows and 14 columns.\n Found 185867 duplicate rows.\nRemoved duplicates. New dataset size: 434795 rows.\nMerging nodes to reduce graph size:\nTraceback (most recent call last):\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py\", line 3805, in get_loc\n    return self._engine.get_loc(casted_key)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"index.pyx\", line 167, in pandas._libs.index.IndexEngine.get_loc\n  File \"index.pyx\", line 196, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7081, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7089, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'date'\nThe above exception was the direct cause of the following exception:\nTraceback (most recent call last):\n  File \"/media/volume1/gw-workspace/TIcw7KZ87kOa/graph_creation.py\", line 59, in <module>\n    data['dayofyear'] = data['date'].dt.dayofyear\n                        ~~~~^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py\", line 4102, in __getitem__\n    indexer = self.columns.get_loc(key)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py\", line 3812, in get_loc\n    raise KeyError(key) from err\nKeyError: 'date'\n",
  "history_begin_time" : 1741491037222,
  "history_end_time" : 1741491069475,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "TNno9w1QFa6e",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\nfrom sklearn.decomposition import PCA\r\nfrom torch_geometric.utils import degree \r\n\r\n\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\" Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount','swe_value'\r\n]  \r\n\r\n# Step 1: Load and filter the dataset\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\n# Combine filtered chunks\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\" Filtered dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\n\r\ndata = data.drop_duplicates()\r\n\r\nprint(f\"Removed duplicates. New dataset size: {data.shape[0]} rows.\") \r\n\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\n\r\nprint(\" Normalizing features using StandardScaler...\")\r\nscaler = StandardScaler()\r\ndata[feature_columns] = scaler.fit_transform(data[feature_columns])\r\nprint(\" Normalization complete.\")\r\n\r\n\r\nnode_features = torch.tensor(data[feature_columns].values, dtype=torch.float)\r\nprint(f\"P Node feature shape: {node_features.shape}\")\r\n\r\nlabels = torch.tensor(data[\"swe_value\"].values, dtype=torch.float)\r\nprint(f\" `y` (target variable) successfully assigned! Shape: {labels.shape}\")\r\n\r\nprint(\"Building KDTree and constructing edges (k-NN + Distance Threshold)...\")\r\n\r\ncoordinates = data[['lat', 'lon']].values\r\ntree = KDTree(coordinates)\r\n\r\nk = 5\r\nthreshold = 0.0075  # Max connection distance (adjust as needed)\r\n\r\n# Query k-nearest neighbors\r\ndistances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\r\n\r\n# Convert to edge index format with distance filtering\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, k+1):  # Skip first entry (self-node)\r\n        if distances[i][j] < threshold:  # Only add edge if within threshold\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T  # Convert list to NumPy array and transpose\r\n\r\nif edges.shape[1] > 0:\r\n    edge_index = torch.tensor(edges, dtype=torch.long)  # Convert to PyTorch tensor\r\n    print(f\"Number of edges created: {edge_index.shape[1]}\")\r\nelse:\r\n    print(\"No edges were created. Consider increasing `threshold` or `k`.\")\r\n    edge_index = torch.empty((2, 0), dtype=torch.long)  # Empty tensor\r\n\r\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n\r\n\r\n#  checking graph before saving\r\nprint(\"\\n Graph Data Summary:\")\r\nprint(graph_data)  # This prints node & edge count\r\nprint(f\"Number of nodes: {graph_data.num_nodes}\")\r\nprint(f\"Number of edges: {graph_data.num_edges}\")\r\nprint(f\"Node feature shape: {graph_data.x.shape}\")\r\nprint(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n# Sample a few edges and nodes\r\nprint(\"\\n Sample of Node Features:\")\r\nprint(graph_data.x[:5])  # Show first 5 node features\r\nprint(\"\\n Sample of Edge Index:\")\r\nprint(graph_data.edge_index[:, :10])  # Show first 10 edges\r\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)  # Compute degrees for each node\r\nprint(f\" Degree stats:\\n\")\r\nprint(f\" - Min degree: {deg.min().item()}\")\r\nprint(f\" - Max degree: {deg.max().item()}\")\r\nprint(f\" - Mean degree: {deg.float().mean().item()}\")\r\nprint(f\" - Standard deviation: {deg.float().std().item()}\")\r\n# Count isolated nodes\r\nnum_isolated_nodes = (deg == 0).sum().item()\r\n\r\nprint(f\" Number of isolated nodes (degree=0): {num_isolated_nodes}\")\r\n\r\n# Percentage of isolated nodes\r\npercentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\r\nprint(f\" Percentage of isolated nodes: {percentage_isolated:.4f}%\")\r\n\r\n\r\n# #saving the graph\r\n# graph_data_output_path = \"/media/volume1/gnn_graph.pt\"\r\n\r\n# # Save the graph data object\r\n# torch.save(graph_data, graph_data_output_path)\r\n\r\n# print(f\" PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\r\n",
  "history_output" : " Loading dataset...\n Filtered dataset loaded with 620662 rows and 14 columns.\n Found 185867 duplicate rows.\nRemoved duplicates. New dataset size: 434795 rows.\n Normalizing features using StandardScaler...\n Normalization complete.\nP Node feature shape: torch.Size([434795, 11])\n `y` (target variable) successfully assigned! Shape: torch.Size([434795])\nBuilding KDTree and constructing edges (k-NN + Distance Threshold)...\nNumber of edges created: 2173386\n Graph Data Summary:\nData(x=[434795, 11], edge_index=[2, 2173386], y=[434795])\nNumber of nodes: 434795\nNumber of edges: 2173386\nNode feature shape: torch.Size([434795, 11])\nEdge index shape: torch.Size([2, 2173386])\n Sample of Node Features:\ntensor([[ 1.1495,  0.4163, -0.1368, -0.2474, -0.5183,  1.1568, -0.7856,  0.0584,\n         -0.9254, -0.4367, -1.2362],\n        [ 1.1495,  0.4163, -0.1368, -0.2474, -0.5183,  1.2954, -1.0640, -0.4703,\n         -0.2924, -0.4367, -0.8971],\n        [ 1.1495,  0.4163, -0.1368, -0.2474, -0.5183, -0.3509, -0.9663,  0.2743,\n         -0.6377, -0.4367, -0.8971],\n        [ 1.1495,  0.4163, -0.1368, -0.2474, -0.5183, -0.3509,  0.6844, -0.0710,\n         -0.6377, -0.4367, -0.8971],\n        [ 1.1495,  0.4163, -0.1368, -0.2474, -0.5183, -0.3509,  0.3572,  0.0099,\n          1.7216, -0.4367, -0.8971]])\n Sample of Edge Index:\ntensor([[  0,   0,   0,   0,   0,   1,   1,   1,   1,   1],\n        [163, 161, 160, 159, 219, 163, 161, 160, 159, 219]])\n Degree stats:\n - Min degree: 0.0\n - Max degree: 5.0\n - Mean degree: 4.998645305633545\n - Standard deviation: 0.06900246441364288\n Number of isolated nodes (degree=0): 24\n Percentage of isolated nodes: 0.0055%\n",
  "history_begin_time" : 1741489943510,
  "history_end_time" : 1741489982129,
  "history_notes" : "before merging",
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "9j7gK82YSJtN",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\nfrom sklearn.decomposition import PCA\r\nfrom torch_geometric.utils import degree \r\n\r\n\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\" Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount','swe_value'\r\n]  \r\n\r\n# Step 1: Load and filter the dataset\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\n# Combine filtered chunks\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\" Filtered dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\n\r\ndata = data.drop_duplicates()\r\n\r\nprint(f\"Removed duplicates. New dataset size: {data.shape[0]} rows.\") \r\n\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\n\r\nprint(\" Normalizing features using StandardScaler...\")\r\nscaler = StandardScaler()\r\ndata[feature_columns] = scaler.fit_transform(data[feature_columns])\r\nprint(\" Normalization complete.\")\r\n\r\n\r\nnode_features = torch.tensor(node_features, dtype=torch.float)\r\nprint(f\"P Node feature shape: {node_features.shape}\")\r\n\r\nlabels = torch.tensor(data[\"swe_value\"].values, dtype=torch.float)\r\nprint(f\" `y` (target variable) successfully assigned! Shape: {labels.shape}\")\r\n\r\nprint(\"Building KDTree and constructing edges (k-NN + Distance Threshold)...\")\r\n\r\ncoordinates = data[['lat', 'lon']].values\r\ntree = KDTree(coordinates)\r\n\r\nk = 5\r\nthreshold = 0.0075  # Max connection distance (adjust as needed)\r\n\r\n# Query k-nearest neighbors\r\ndistances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\r\n\r\n# Convert to edge index format with distance filtering\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, k+1):  # Skip first entry (self-node)\r\n        if distances[i][j] < threshold:  # Only add edge if within threshold\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T  # Convert list to NumPy array and transpose\r\n\r\nif edges.shape[1] > 0:\r\n    edge_index = torch.tensor(edges, dtype=torch.long)  # Convert to PyTorch tensor\r\n    print(f\"Number of edges created: {edge_index.shape[1]}\")\r\nelse:\r\n    print(\"No edges were created. Consider increasing `threshold` or `k`.\")\r\n    edge_index = torch.empty((2, 0), dtype=torch.long)  # Empty tensor\r\n\r\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n\r\n\r\n#  checking graph before saving\r\nprint(\"\\n Graph Data Summary:\")\r\nprint(graph_data)  # This prints node & edge count\r\nprint(f\"Number of nodes: {graph_data.num_nodes}\")\r\nprint(f\"Number of edges: {graph_data.num_edges}\")\r\nprint(f\"Node feature shape: {graph_data.x.shape}\")\r\nprint(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n# Sample a few edges and nodes\r\nprint(\"\\n Sample of Node Features:\")\r\nprint(graph_data.x[:5])  # Show first 5 node features\r\nprint(\"\\n Sample of Edge Index:\")\r\nprint(graph_data.edge_index[:, :10])  # Show first 10 edges\r\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)  # Compute degrees for each node\r\nprint(f\" Degree stats:\\n\")\r\nprint(f\" - Min degree: {deg.min().item()}\")\r\nprint(f\" - Max degree: {deg.max().item()}\")\r\nprint(f\" - Mean degree: {deg.float().mean().item()}\")\r\nprint(f\" - Standard deviation: {deg.float().std().item()}\")\r\n# Count isolated nodes\r\nnum_isolated_nodes = (deg == 0).sum().item()\r\n\r\nprint(f\" Number of isolated nodes (degree=0): {num_isolated_nodes}\")\r\n\r\n# Percentage of isolated nodes\r\npercentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\r\nprint(f\" Percentage of isolated nodes: {percentage_isolated:.4f}%\")\r\n\r\n\r\n# #saving the graph\r\n# graph_data_output_path = \"/media/volume1/gnn_graph.pt\"\r\n\r\n# # Save the graph data object\r\n# torch.save(graph_data, graph_data_output_path)\r\n\r\n# print(f\" PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\r\n",
  "history_output" : " Loading dataset...\n Filtered dataset loaded with 620662 rows and 14 columns.\n Found 185867 duplicate rows.\nRemoved duplicates. New dataset size: 434795 rows.\n Normalizing features using StandardScaler...\n Normalization complete.\nTraceback (most recent call last):\n  File \"/media/volume1/gw-workspace/9j7gK82YSJtN/graph_creation.py\", line 62, in <module>\n    node_features = torch.tensor(node_features, dtype=torch.float)\n                                 ^^^^^^^^^^^^^\nNameError: name 'node_features' is not defined\n",
  "history_begin_time" : 1741489823994,
  "history_end_time" : 1741489856643,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "4ydb7dejphe",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\nfrom sklearn.decomposition import PCA\r\nfrom torch_geometric.utils import degree \r\n\r\n\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\" Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount','swe_value'\r\n]  \r\n\r\n# Step 1: Load and filter the dataset\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\n# Combine filtered chunks\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\" Filtered dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\n\r\ndata = data.drop_duplicates()\r\n\r\nprint(f\"Removed duplicates. New dataset size: {data.shape[0]} rows.\") \r\n\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\n\r\nprint(\" Normalizing features using StandardScaler...\")\r\nscaler = StandardScaler()\r\ndata[feature_columns] = scaler.fit_transform(data[feature_columns])\r\nprint(\" Normalization complete.\")\r\n\r\n# # to check how many components to use\r\n# pca = PCA()\r\n# pca.fit(data[useful_columns])\r\n# # Print explained variance\r\n# explained_variance = np.cumsum(pca.explained_variance_ratio_)\r\n# print(\"Explained variance by each component:\\n\", explained_variance)\r\n# # Find the number of components that retain 95% variance\r\n# n_components_95 = np.argmax(explained_variance >= 0.95) + 1\r\n# print(f\"To retain 95% of variance, we need {n_components_95} components.\")\r\n\r\nprint(\"Applying PCA to reduce dimensionality...\")\r\npca = PCA(n_components=11)  # Reduce to 11 main features\r\nnode_features = pca.fit_transform(data[feature_columns])\r\nnode_features = torch.tensor(node_features, dtype=torch.float)\r\nprint(f\"PCA applied. Node feature shape: {node_features.shape}\")\r\n\r\nlabels = torch.tensor(data[\"swe_value\"].values, dtype=torch.float)\r\nprint(f\" `y` (target variable) successfully assigned! Shape: {labels.shape}\")\r\n\r\nprint(\"Building KDTree and constructing edges (k-NN + Distance Threshold)...\")\r\n\r\ncoordinates = data[['lat', 'lon']].values\r\ntree = KDTree(coordinates)\r\n\r\nk = 5\r\nthreshold = 0.0075  # Max connection distance (adjust as needed)\r\n\r\n# Query k-nearest neighbors\r\ndistances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\r\n\r\n# Convert to edge index format with distance filtering\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, k+1):  # Skip first entry (self-node)\r\n        if distances[i][j] < threshold:  # Only add edge if within threshold\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T  # Convert list to NumPy array and transpose\r\n\r\nif edges.shape[1] > 0:\r\n    edge_index = torch.tensor(edges, dtype=torch.long)  # Convert to PyTorch tensor\r\n    print(f\"Number of edges created: {edge_index.shape[1]}\")\r\nelse:\r\n    print(\"No edges were created. Consider increasing `threshold` or `k`.\")\r\n    edge_index = torch.empty((2, 0), dtype=torch.long)  # Empty tensor\r\n\r\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n\r\n\r\n# #  checking graph before saving\r\n# print(\"\\n Graph Data Summary:\")\r\n# print(graph_data)  # This prints node & edge count\r\n# print(f\"Number of nodes: {graph_data.num_nodes}\")\r\n# print(f\"Number of edges: {graph_data.num_edges}\")\r\n# print(f\"Node feature shape: {graph_data.x.shape}\")\r\n# print(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n# # Sample a few edges and nodes\r\n# print(\"\\n Sample of Node Features:\")\r\n# print(graph_data.x[:5])  # Show first 5 node features\r\n# print(\"\\n Sample of Edge Index:\")\r\n# print(graph_data.edge_index[:, :10])  # Show first 10 edges\r\n# deg = degree(edge_index[0], num_nodes=graph_data.num_nodes)  # Compute degrees for each node\r\n# print(f\" Degree stats:\\n\")\r\n# print(f\" - Min degree: {deg.min().item()}\")\r\n# print(f\" - Max degree: {deg.max().item()}\")\r\n# print(f\" - Mean degree: {deg.float().mean().item()}\")\r\n# print(f\" - Standard deviation: {deg.float().std().item()}\")\r\n# # Count isolated nodes\r\n# num_isolated_nodes = (deg == 0).sum().item()\r\n\r\n# print(f\" Number of isolated nodes (degree=0): {num_isolated_nodes}\")\r\n\r\n# # Percentage of isolated nodes\r\n# percentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\r\n# print(f\" Percentage of isolated nodes: {percentage_isolated:.4f}%\")\r\n\r\n\r\n#saving the graph\r\ngraph_data_output_path = \"/media/volume1/gnn_graph.pt\"\r\n\r\n# Save the graph data object\r\ntorch.save(graph_data, graph_data_output_path)\r\n\r\nprint(f\" PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\r\n",
  "history_output" : " Loading dataset...\n Filtered dataset loaded with 620662 rows and 14 columns.\n Found 185867 duplicate rows.\nRemoved duplicates. New dataset size: 434795 rows.\n Normalizing features using StandardScaler...\n Normalization complete.\nApplying PCA to reduce dimensionality...\nPCA applied. Node feature shape: torch.Size([434795, 11])\n `y` (target variable) successfully assigned! Shape: torch.Size([434795])\nBuilding KDTree and constructing edges (k-NN + Distance Threshold)...\nNumber of edges created: 2173386\n PyTorch Geometric Graph Data saved at: /media/volume1/gnn_graph.pt\n",
  "history_begin_time" : 1741212676828,
  "history_end_time" : 1741212719798,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "TXC3R0Fhh7H3",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\nfrom sklearn.decomposition import PCA\r\nfrom torch_geometric.utils import degree \r\n\r\n\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\" Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount','swe_value'\r\n]  \r\n\r\n# Step 1: Load and filter the dataset\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\n# Combine filtered chunks\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\" Filtered dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\n\r\ndata = data.drop_duplicates()\r\n\r\nprint(f\"Removed duplicates. New dataset size: {data.shape[0]} rows.\") \r\n\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\n\r\nprint(\" Normalizing features using StandardScaler...\")\r\nscaler = StandardScaler()\r\ndata[feature_columns] = scaler.fit_transform(data[feature_columns])\r\nprint(\" Normalization complete.\")\r\n\r\n# # to check how many components to use\r\n# pca = PCA()\r\n# pca.fit(data[useful_columns])\r\n# # Print explained variance\r\n# explained_variance = np.cumsum(pca.explained_variance_ratio_)\r\n# print(\"Explained variance by each component:\\n\", explained_variance)\r\n# # Find the number of components that retain 95% variance\r\n# n_components_95 = np.argmax(explained_variance >= 0.95) + 1\r\n# print(f\"To retain 95% of variance, we need {n_components_95} components.\")\r\n\r\nprint(\"Applying PCA to reduce dimensionality...\")\r\npca = PCA(n_components=11)  # Reduce to 11 main features\r\nnode_features = pca.fit_transform(data[feature_columns])\r\nnode_features = torch.tensor(node_features, dtype=torch.float)\r\nprint(f\"PCA applied. Node feature shape: {node_features.shape}\")\r\n\r\nlabels = torch.tensor(data[\"swe_value\"].values, dtype=torch.float)\r\nprint(f\" `y` (target variable) successfully assigned! Shape: {labels.shape}\")\r\n\r\nprint(\"Building KDTree and constructing edges (k-NN + Distance Threshold)...\")\r\n\r\ncoordinates = data[['lat', 'lon']].values\r\ntree = KDTree(coordinates)\r\n\r\nk = 5\r\nthreshold = 0.0075  # Max connection distance (adjust as needed)\r\n\r\n# Query k-nearest neighbors\r\ndistances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\r\n\r\n# Convert to edge index format with distance filtering\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, k+1):  # Skip first entry (self-node)\r\n        if distances[i][j] < threshold:  # Only add edge if within threshold\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T  # Convert list to NumPy array and transpose\r\n\r\nif edges.shape[1] > 0:\r\n    edge_index = torch.tensor(edges, dtype=torch.long)  # Convert to PyTorch tensor\r\n    print(f\"Number of edges created: {edge_index.shape[1]}\")\r\nelse:\r\n    print(\"No edges were created. Consider increasing `threshold` or `k`.\")\r\n    edge_index = torch.empty((2, 0), dtype=torch.long)  # Empty tensor\r\n\r\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n\r\n\r\n# #  checking graph before saving\r\n# print(\"\\n Graph Data Summary:\")\r\n# print(graph_data)  # This prints node & edge count\r\n# print(f\"Number of nodes: {graph_data.num_nodes}\")\r\n# print(f\"Number of edges: {graph_data.num_edges}\")\r\n# print(f\"Node feature shape: {graph_data.x.shape}\")\r\n# print(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n# # Sample a few edges and nodes\r\n# print(\"\\n Sample of Node Features:\")\r\n# print(graph_data.x[:5])  # Show first 5 node features\r\n# print(\"\\n Sample of Edge Index:\")\r\n# print(graph_data.edge_index[:, :10])  # Show first 10 edges\r\n# deg = degree(edge_index[0], num_nodes=graph_data.num_nodes)  # Compute degrees for each node\r\n# print(f\" Degree stats:\\n\")\r\n# print(f\" - Min degree: {deg.min().item()}\")\r\n# print(f\" - Max degree: {deg.max().item()}\")\r\n# print(f\" - Mean degree: {deg.float().mean().item()}\")\r\n# print(f\" - Standard deviation: {deg.float().std().item()}\")\r\n# # Count isolated nodes\r\n# num_isolated_nodes = (deg == 0).sum().item()\r\n# print(f\" Number of isolated nodes (degree=0): {num_isolated_nodes}\")\r\n# # Percentage of isolated nodes\r\n# percentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\r\n# print(f\" Percentage of isolated nodes: {percentage_isolated:.4f}%\")\r\n\r\n\r\n#saving the graph\r\ngraph_data_output_path = \"/media/volume1/gnn_graph.pt\"\r\n\r\n# Save the graph data object\r\ntorch.save(graph_data, graph_data_output_path)\r\n\r\nprint(f\" PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\r\n",
  "history_output" : " Loading dataset...\n Filtered dataset loaded with 620662 rows and 14 columns.\n Found 185867 duplicate rows.\nRemoved duplicates. New dataset size: 434795 rows.\n Normalizing features using StandardScaler...\n Normalization complete.\nApplying PCA to reduce dimensionality...\nPCA applied. Node feature shape: torch.Size([434795, 11])\n `y` (target variable) successfully assigned! Shape: torch.Size([434795])\nBuilding KDTree and constructing edges (k-NN + Distance Threshold)...\nNumber of edges created: 2173386\n PyTorch Geometric Graph Data saved at: /media/volume1/gnn_graph.pt\n",
  "history_begin_time" : 1740779470078,
  "history_end_time" : 1740779505961,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "CVrGu4ADH8kW",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\nfrom sklearn.decomposition import PCA\r\nfrom torch_geometric.utils import degree \r\n\r\n\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\" Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount','swe_value'\r\n]  \r\n\r\n# Step 1: Load and filter the dataset\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\n# Combine filtered chunks\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\" Filtered dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\n\r\ndata = data.drop_duplicates()\r\n\r\nprint(f\"Removed duplicates. New dataset size: {data.shape[0]} rows.\") \r\n\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\n\r\nprint(\" Normalizing features using StandardScaler...\")\r\nscaler = StandardScaler()\r\ndata[feature_columns] = scaler.fit_transform(data[feature_columns])\r\nprint(\" Normalization complete.\")\r\n\r\n# # to check how many components to use\r\n# pca = PCA()\r\n# pca.fit(data[useful_columns])\r\n# # Print explained variance\r\n# explained_variance = np.cumsum(pca.explained_variance_ratio_)\r\n# print(\"Explained variance by each component:\\n\", explained_variance)\r\n# # Find the number of components that retain 95% variance\r\n# n_components_95 = np.argmax(explained_variance >= 0.95) + 1\r\n# print(f\"To retain 95% of variance, we need {n_components_95} components.\")\r\n\r\nprint(\"Applying PCA to reduce dimensionality...\")\r\npca = PCA(n_components=11)  # Reduce to 11 main features\r\nnode_features = pca.fit_transform(data[feature_columns])\r\nnode_features = torch.tensor(node_features, dtype=torch.float)\r\nprint(f\"PCA applied. Node feature shape: {node_features.shape}\")\r\n\r\nlabels = torch.tensor(data[\"swe_value\"].values, dtype=torch.float)\r\nprint(f\" `y` (target variable) successfully assigned! Shape: {labels.shape}\")\r\n\r\nprint(\"Building KDTree and constructing edges (k-NN + Distance Threshold)...\")\r\n\r\ncoordinates = data[['lat', 'lon']].values\r\ntree = KDTree(coordinates)\r\n\r\nk = 5\r\nthreshold = 0.0075  # Max connection distance (adjust as needed)\r\n\r\n# Query k-nearest neighbors\r\ndistances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\r\n\r\n# Convert to edge index format with distance filtering\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, k+1):  # Skip first entry (self-node)\r\n        if distances[i][j] < threshold:  # Only add edge if within threshold\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T  # Convert list to NumPy array and transpose\r\n\r\nif edges.shape[1] > 0:\r\n    edge_index = torch.tensor(edges, dtype=torch.long)  # Convert to PyTorch tensor\r\n    print(f\"Number of edges created: {edge_index.shape[1]}\")\r\nelse:\r\n    print(\"No edges were created. Consider increasing `threshold` or `k`.\")\r\n    edge_index = torch.empty((2, 0), dtype=torch.long)  # Empty tensor\r\n\r\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n\r\n\r\n#  checking graph before saving\r\nprint(\"\\n Graph Data Summary:\")\r\nprint(graph_data)  # This prints node & edge count\r\nprint(f\"Number of nodes: {graph_data.num_nodes}\")\r\nprint(f\"Number of edges: {graph_data.num_edges}\")\r\nprint(f\"Node feature shape: {graph_data.x.shape}\")\r\nprint(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n# Sample a few edges and nodes\r\nprint(\"\\n Sample of Node Features:\")\r\nprint(graph_data.x[:5])  # Show first 5 node features\r\nprint(\"\\n Sample of Edge Index:\")\r\nprint(graph_data.edge_index[:, :10])  # Show first 10 edges\r\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)  # Compute degrees for each node\r\nprint(f\" Degree stats:\\n\")\r\nprint(f\" - Min degree: {deg.min().item()}\")\r\nprint(f\" - Max degree: {deg.max().item()}\")\r\nprint(f\" - Mean degree: {deg.float().mean().item()}\")\r\nprint(f\" - Standard deviation: {deg.float().std().item()}\")\r\n# Count isolated nodes\r\nnum_isolated_nodes = (deg == 0).sum().item()\r\nprint(f\" Number of isolated nodes (degree=0): {num_isolated_nodes}\")\r\n# Percentage of isolated nodes\r\npercentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\r\nprint(f\" Percentage of isolated nodes: {percentage_isolated:.4f}%\")\r\n\r\n\r\n# #saving the graph\r\n# graph_data_output_path = \"/media/volume1/gnn_graph.pt\"\r\n\r\n# # Save the graph data object\r\n# torch.save(graph_data, graph_data_output_path)\r\n\r\n# print(f\" PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\r\n",
  "history_output" : " Loading dataset...\n Filtered dataset loaded with 620662 rows and 14 columns.\n Found 185867 duplicate rows.\nRemoved duplicates. New dataset size: 434795 rows.\n Normalizing features using StandardScaler...\n Normalization complete.\nApplying PCA to reduce dimensionality...\nPCA applied. Node feature shape: torch.Size([434795, 11])\n `y` (target variable) successfully assigned! Shape: torch.Size([434795])\nBuilding KDTree and constructing edges (k-NN + Distance Threshold)...\nNumber of edges created: 2173386\n Graph Data Summary:\nData(x=[434795, 11], edge_index=[2, 2173386], y=[434795])\nNumber of nodes: 434795\nNumber of edges: 2173386\nNode feature shape: torch.Size([434795, 11])\nEdge index shape: torch.Size([2, 2173386])\n Sample of Node Features:\ntensor([[ 0.2792, -0.7567, -0.9541, -0.3052,  0.5099, -0.5074,  1.3447, -1.3094,\n          0.0629,  0.4974, -0.6569],\n        [ 0.4489, -0.6079, -1.1101, -0.3320,  0.3971, -0.1794,  1.6179, -0.5869,\n         -0.2096,  0.4449, -0.8051],\n        [ 0.3577, -1.0630, -0.5846, -0.7478,  0.1830, -0.3189,  0.0840, -1.0206,\n         -0.4733,  0.6188, -0.6539],\n        [-0.0106, -0.5313, -0.2694, -1.0909, -0.0676, -0.3332, -0.0755, -0.8150,\n          0.9562,  0.4340, -0.6391],\n        [-0.1901,  0.2463,  0.2993, -2.1108, -0.3736,  0.5511,  0.5872,  0.1211,\n         -0.2615,  0.3625, -0.6393]])\n Sample of Edge Index:\ntensor([[  0,   0,   0,   0,   0,   1,   1,   1,   1,   1],\n        [163, 161, 160, 159, 219, 163, 161, 160, 159, 219]])\n Degree stats:\n - Min degree: 0.0\n - Max degree: 5.0\n - Mean degree: 4.998645305633545\n - Standard deviation: 0.06900246441364288\n Number of isolated nodes (degree=0): 24\n Percentage of isolated nodes: 0.0055%\n",
  "history_begin_time" : 1740779278307,
  "history_end_time" : 1740779314871,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "u1GGaUVq1zgk",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\nfrom sklearn.decomposition import PCA\r\nfrom torch_geometric.utils import degree \r\n\r\n\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\" Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount','swe_value'\r\n]  \r\n\r\n# Step 1: Load and filter the dataset\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\n# Combine filtered chunks\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\" Filtered dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\n\r\ndata = data.drop_duplicates()\r\n\r\nprint(f\"Removed duplicates. New dataset size: {data.shape[0]} rows.\") \r\n\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\n\r\nprint(\" Normalizing features using StandardScaler...\")\r\nscaler = StandardScaler()\r\ndata[feature_columns] = scaler.fit_transform(data[feature_columns])\r\nprint(\" Normalization complete.\")\r\n\r\n# # to check how many components to use\r\n# pca = PCA()\r\n# pca.fit(data[useful_columns])\r\n# # Print explained variance\r\n# explained_variance = np.cumsum(pca.explained_variance_ratio_)\r\n# print(\"Explained variance by each component:\\n\", explained_variance)\r\n# # Find the number of components that retain 95% variance\r\n# n_components_95 = np.argmax(explained_variance >= 0.95) + 1\r\n# print(f\"To retain 95% of variance, we need {n_components_95} components.\")\r\n\r\nprint(\"Applying PCA to reduce dimensionality...\")\r\npca = PCA(n_components=11)  # Reduce to 11 main features\r\nnode_features = pca.fit_transform(data[feature_columns])\r\nnode_features = torch.tensor(node_features, dtype=torch.float)\r\nprint(f\"PCA applied. Node feature shape: {node_features.shape}\")\r\n\r\nlabels = torch.tensor(data[\"swe_value\"].values, dtype=torch.float)\r\nprint(f\" `y` (target variable) successfully assigned! Shape: {labels.shape}\")\r\n\r\nprint(\"Building KDTree and constructing edges (k-NN + Distance Threshold)...\")\r\n\r\ncoordinates = data[['lat', 'lon']].values\r\ntree = KDTree(coordinates)\r\n\r\nk = 5\r\nthreshold = 0.0075  # Max connection distance (adjust as needed)\r\n\r\n# Query k-nearest neighbors\r\ndistances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\r\n\r\n# Convert to edge index format with distance filtering\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, k+1):  # Skip first entry (self-node)\r\n        if distances[i][j] < threshold:  # Only add edge if within threshold\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T  # Convert list to NumPy array and transpose\r\n\r\nif edges.shape[1] > 0:\r\n    edge_index = torch.tensor(edges, dtype=torch.long)  # Convert to PyTorch tensor\r\n    print(f\"Number of edges created: {edge_index.shape[1]}\")\r\nelse:\r\n    print(\"No edges were created. Consider increasing `threshold` or `k`.\")\r\n    edge_index = torch.empty((2, 0), dtype=torch.long)  # Empty tensor\r\n\r\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n\r\n\r\n# #  checking graph before saving\r\n# print(\"\\n Graph Data Summary:\")\r\n# print(graph_data)  # This prints node & edge count\r\n# print(f\"Number of nodes: {graph_data.num_nodes}\")\r\n# print(f\"Number of edges: {graph_data.num_edges}\")\r\n# print(f\"Node feature shape: {graph_data.x.shape}\")\r\n# print(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n# # Sample a few edges and nodes\r\n# print(\"\\n Sample of Node Features:\")\r\n# print(graph_data.x[:5])  # Show first 5 node features\r\n# print(\"\\n Sample of Edge Index:\")\r\n# print(graph_data.edge_index[:, :10])  # Show first 10 edges\r\n# deg = degree(edge_index[0], num_nodes=graph_data.num_nodes)  # Compute degrees for each node\r\n# print(f\" Degree stats:\\n\")\r\n# print(f\" - Min degree: {deg.min().item()}\")\r\n# print(f\" - Max degree: {deg.max().item()}\")\r\n# print(f\" - Mean degree: {deg.float().mean().item()}\")\r\n# print(f\" - Standard deviation: {deg.float().std().item()}\")\r\n# # Count isolated nodes\r\n# num_isolated_nodes = (deg == 0).sum().item()\r\n# print(f\" Number of isolated nodes (degree=0): {num_isolated_nodes}\")\r\n# # Percentage of isolated nodes\r\n# percentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\r\n# print(f\" Percentage of isolated nodes: {percentage_isolated:.4f}%\")\r\n\r\n\r\n# #saving the graph\r\n# graph_data_output_path = \"/media/volume1/gnn_graph.pt\"\r\n\r\n# # Save the graph data object\r\n# torch.save(graph_data, graph_data_output_path)\r\n\r\n# print(f\" PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\r\n",
  "history_output" : " Loading dataset...\n Filtered dataset loaded with 620662 rows and 14 columns.\n Found 185867 duplicate rows.\nRemoved duplicates. New dataset size: 434795 rows.\n Normalizing features using StandardScaler...\n Normalization complete.\nApplying PCA to reduce dimensionality...\nPCA applied. Node feature shape: torch.Size([434795, 11])\n `y` (target variable) successfully assigned! Shape: torch.Size([434795])\nBuilding KDTree and constructing edges (k-NN + Distance Threshold)...\nNumber of edges created: 2173386\n",
  "history_begin_time" : 1740779185003,
  "history_end_time" : 1740779220984,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "XSGxwWjwFmeC",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\nfrom sklearn.decomposition import PCA\r\nfrom torch_geometric.utils import degree \r\n\r\n\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\" Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount','swe_value'\r\n]  \r\n\r\n# Step 1: Load and filter the dataset\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\n# Combine filtered chunks\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\" Filtered dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\n\r\ndata = data.drop_duplicates()\r\n\r\nprint(f\"Removed duplicates. New dataset size: {data.shape[0]} rows.\") \r\n\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\n\r\nprint(\" Normalizing features using StandardScaler...\")\r\nscaler = StandardScaler()\r\ndata[feature_columns] = scaler.fit_transform(data[feature_columns])\r\nprint(\" Normalization complete.\")\r\n\r\n# # to check how many components to use\r\n# pca = PCA()\r\n# pca.fit(data[useful_columns])\r\n# # Print explained variance\r\n# explained_variance = np.cumsum(pca.explained_variance_ratio_)\r\n# print(\"Explained variance by each component:\\n\", explained_variance)\r\n# # Find the number of components that retain 95% variance\r\n# n_components_95 = np.argmax(explained_variance >= 0.95) + 1\r\n# print(f\"To retain 95% of variance, we need {n_components_95} components.\")\r\n\r\nprint(\"Applying PCA to reduce dimensionality...\")\r\npca = PCA(n_components=11)  # Reduce to 11 main features\r\nnode_features = pca.fit_transform(data[feature_columns])\r\nnode_features = torch.tensor(node_features, dtype=torch.float)\r\nprint(f\"PCA applied. Node feature shape: {node_features.shape}\")\r\n\r\nlabels = torch.tensor(data[\"swe_value\"].values, dtype=torch.float)\r\nprint(f\" `y` (target variable) successfully assigned! Shape: {labels.shape}\")\r\n\r\n# print(\"Building KDTree and constructing edges (k-NN + Distance Threshold)...\")\r\n\r\n# coordinates = data[['lat', 'lon']].values\r\n# tree = KDTree(coordinates)\r\n\r\n# k = 5\r\n# threshold = 0.0075  # Max connection distance (adjust as needed)\r\n\r\n# # Query k-nearest neighbors\r\n# distances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\r\n\r\n# # Convert to edge index format with distance filtering\r\n# edge_list = []\r\n# for i in range(len(neighbors)):\r\n#     for j in range(1, k+1):  # Skip first entry (self-node)\r\n#         if distances[i][j] < threshold:  # Only add edge if within threshold\r\n#             edge_list.append((i, neighbors[i][j]))\r\n\r\n# edges = np.array(edge_list).T  # Convert list to NumPy array and transpose\r\n\r\n# if edges.shape[1] > 0:\r\n#     edge_index = torch.tensor(edges, dtype=torch.long)  # Convert to PyTorch tensor\r\n#     print(f\"Number of edges created: {edge_index.shape[1]}\")\r\n# else:\r\n#     print(\"No edges were created. Consider increasing `threshold` or `k`.\")\r\n#     edge_index = torch.empty((2, 0), dtype=torch.long)  # Empty tensor\r\n\r\n# graph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n\r\n\r\n# #  checking graph before saving\r\n# print(\"\\n Graph Data Summary:\")\r\n# print(graph_data)  # This prints node & edge count\r\n# print(f\"Number of nodes: {graph_data.num_nodes}\")\r\n# print(f\"Number of edges: {graph_data.num_edges}\")\r\n# print(f\"Node feature shape: {graph_data.x.shape}\")\r\n# print(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n# # Sample a few edges and nodes\r\n# print(\"\\n Sample of Node Features:\")\r\n# print(graph_data.x[:5])  # Show first 5 node features\r\n# print(\"\\n Sample of Edge Index:\")\r\n# print(graph_data.edge_index[:, :10])  # Show first 10 edges\r\n# deg = degree(edge_index[0], num_nodes=graph_data.num_nodes)  # Compute degrees for each node\r\n# print(f\" Degree stats:\\n\")\r\n# print(f\" - Min degree: {deg.min().item()}\")\r\n# print(f\" - Max degree: {deg.max().item()}\")\r\n# print(f\" - Mean degree: {deg.float().mean().item()}\")\r\n# print(f\" - Standard deviation: {deg.float().std().item()}\")\r\n# # Count isolated nodes\r\n# num_isolated_nodes = (deg == 0).sum().item()\r\n# print(f\" Number of isolated nodes (degree=0): {num_isolated_nodes}\")\r\n# # Percentage of isolated nodes\r\n# percentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\r\n# print(f\" Percentage of isolated nodes: {percentage_isolated:.4f}%\")\r\n\r\n\r\n# #saving the graph\r\n# graph_data_output_path = \"/media/volume1/gnn_graph.pt\"\r\n\r\n# # Save the graph data object\r\n# torch.save(graph_data, graph_data_output_path)\r\n\r\n# print(f\" PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\r\n",
  "history_output" : " Loading dataset...\n Filtered dataset loaded with 620662 rows and 14 columns.\n Found 185867 duplicate rows.\nRemoved duplicates. New dataset size: 434795 rows.\n Normalizing features using StandardScaler...\n Normalization complete.\nApplying PCA to reduce dimensionality...\nPCA applied. Node feature shape: torch.Size([434795, 11])\n `y` (target variable) successfully assigned! Shape: torch.Size([434795])\n",
  "history_begin_time" : 1740778978664,
  "history_end_time" : 1740779009986,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "N5qcNaLMkAPh",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\nfrom sklearn.decomposition import PCA\r\nfrom torch_geometric.utils import degree \r\n\r\n\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\" Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]  \r\n\r\n# Step 1: Load and filter the dataset\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\n# Combine filtered chunks\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\" Filtered dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\n\r\ndata = data.drop_duplicates()\r\n\r\nprint(f\"Removed duplicates. New dataset size: {data.shape[0]} rows.\") \r\n\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\n\r\nprint(\" Normalizing features using StandardScaler...\")\r\nscaler = StandardScaler()\r\ndata[feature_columns] = scaler.fit_transform(data[feature_columns])\r\nprint(\" Normalization complete.\")\r\n\r\n# # to check how many components to use\r\n# pca = PCA()\r\n# pca.fit(data[useful_columns])\r\n# # Print explained variance\r\n# explained_variance = np.cumsum(pca.explained_variance_ratio_)\r\n# print(\"Explained variance by each component:\\n\", explained_variance)\r\n# # Find the number of components that retain 95% variance\r\n# n_components_95 = np.argmax(explained_variance >= 0.95) + 1\r\n# print(f\"To retain 95% of variance, we need {n_components_95} components.\")\r\n\r\nprint(\"Applying PCA to reduce dimensionality...\")\r\npca = PCA(n_components=11)  # Reduce to 11 main features\r\nnode_features = pca.fit_transform(data[feature_columns])\r\nnode_features = torch.tensor(node_features, dtype=torch.float)\r\nprint(f\"PCA applied. Node feature shape: {node_features.shape}\")\r\n\r\nlabels = torch.tensor(data[\"swe_value\"].values, dtype=torch.float)\r\nprint(f\" `y` (target variable) successfully assigned! Shape: {labels.shape}\")\r\n\r\n# print(\"Building KDTree and constructing edges (k-NN + Distance Threshold)...\")\r\n\r\n# coordinates = data[['lat', 'lon']].values\r\n# tree = KDTree(coordinates)\r\n\r\n# k = 5\r\n# threshold = 0.0075  # Max connection distance (adjust as needed)\r\n\r\n# # Query k-nearest neighbors\r\n# distances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\r\n\r\n# # Convert to edge index format with distance filtering\r\n# edge_list = []\r\n# for i in range(len(neighbors)):\r\n#     for j in range(1, k+1):  # Skip first entry (self-node)\r\n#         if distances[i][j] < threshold:  # Only add edge if within threshold\r\n#             edge_list.append((i, neighbors[i][j]))\r\n\r\n# edges = np.array(edge_list).T  # Convert list to NumPy array and transpose\r\n\r\n# if edges.shape[1] > 0:\r\n#     edge_index = torch.tensor(edges, dtype=torch.long)  # Convert to PyTorch tensor\r\n#     print(f\"Number of edges created: {edge_index.shape[1]}\")\r\n# else:\r\n#     print(\"No edges were created. Consider increasing `threshold` or `k`.\")\r\n#     edge_index = torch.empty((2, 0), dtype=torch.long)  # Empty tensor\r\n\r\n# graph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n\r\n\r\n# #  checking graph before saving\r\n# print(\"\\n Graph Data Summary:\")\r\n# print(graph_data)  # This prints node & edge count\r\n# print(f\"Number of nodes: {graph_data.num_nodes}\")\r\n# print(f\"Number of edges: {graph_data.num_edges}\")\r\n# print(f\"Node feature shape: {graph_data.x.shape}\")\r\n# print(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n# # Sample a few edges and nodes\r\n# print(\"\\n Sample of Node Features:\")\r\n# print(graph_data.x[:5])  # Show first 5 node features\r\n# print(\"\\n Sample of Edge Index:\")\r\n# print(graph_data.edge_index[:, :10])  # Show first 10 edges\r\n# deg = degree(edge_index[0], num_nodes=graph_data.num_nodes)  # Compute degrees for each node\r\n# print(f\" Degree stats:\\n\")\r\n# print(f\" - Min degree: {deg.min().item()}\")\r\n# print(f\" - Max degree: {deg.max().item()}\")\r\n# print(f\" - Mean degree: {deg.float().mean().item()}\")\r\n# print(f\" - Standard deviation: {deg.float().std().item()}\")\r\n# # Count isolated nodes\r\n# num_isolated_nodes = (deg == 0).sum().item()\r\n# print(f\" Number of isolated nodes (degree=0): {num_isolated_nodes}\")\r\n# # Percentage of isolated nodes\r\n# percentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\r\n# print(f\" Percentage of isolated nodes: {percentage_isolated:.4f}%\")\r\n\r\n\r\n# #saving the graph\r\n# graph_data_output_path = \"/media/volume1/gnn_graph.pt\"\r\n\r\n# # Save the graph data object\r\n# torch.save(graph_data, graph_data_output_path)\r\n\r\n# print(f\" PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\r\n",
  "history_output" : " Loading dataset...\n Filtered dataset loaded with 620662 rows and 13 columns.\n Found 186037 duplicate rows.\nRemoved duplicates. New dataset size: 434625 rows.\n Normalizing features using StandardScaler...\n Normalization complete.\nApplying PCA to reduce dimensionality...\nPCA applied. Node feature shape: torch.Size([434625, 11])\nTraceback (most recent call last):\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py\", line 3805, in get_loc\n    return self._engine.get_loc(casted_key)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"index.pyx\", line 167, in pandas._libs.index.IndexEngine.get_loc\n  File \"index.pyx\", line 196, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7081, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7089, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'swe_value'\nThe above exception was the direct cause of the following exception:\nTraceback (most recent call last):\n  File \"/media/volume1/gw-workspace/N5qcNaLMkAPh/graph_creation.py\", line 77, in <module>\n    labels = torch.tensor(data[\"swe_value\"].values, dtype=torch.float)\n                          ~~~~^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py\", line 4102, in __getitem__\n    indexer = self.columns.get_loc(key)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py\", line 3812, in get_loc\n    raise KeyError(key) from err\nKeyError: 'swe_value'\n",
  "history_begin_time" : 1740778682642,
  "history_end_time" : 1740778713307,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "hPEwAVo9dAlN",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\nfrom sklearn.decomposition import PCA\r\nfrom torch_geometric.utils import degree \r\n\r\n\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\" Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]  \r\n\r\n# Step 1: Load and filter the dataset\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\n# Combine filtered chunks\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\" Filtered dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\n\r\ndata = data.drop_duplicates()\r\n\r\nprint(f\"Removed duplicates. New dataset size: {data.shape[0]} rows.\") \r\n\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\n\r\nprint(\" Normalizing features using StandardScaler...\")\r\nscaler = StandardScaler()\r\ndata[feature_columns] = scaler.fit_transform(data[feature_columns])\r\nprint(\" Normalization complete.\")\r\n\r\n# to check how many components to use\r\npca = PCA()\r\npca.fit(data[useful_columns])\r\n# Print explained variance\r\nexplained_variance = np.cumsum(pca.explained_variance_ratio_)\r\nprint(\"Explained variance by each component:\\n\", explained_variance)\r\n# Find the number of components that retain 95% variance\r\nn_components_95 = np.argmax(explained_variance >= 0.95) + 1\r\nprint(f\"To retain 95% of variance, we need {n_components_95} components.\")\r\n\r\n# print(\"Applying PCA to reduce dimensionality...\")\r\n# pca = PCA(n_components=12)  # Reduce to 12 main features\r\n# node_features = pca.fit_transform(data[useful_columns])\r\n# node_features = torch.tensor(node_features, dtype=torch.float)\r\n# print(f\"PCA applied. Node feature shape: {node_features.shape}\")\r\n\r\n# labels = torch.tensor(data[\"swe_value\"].values, dtype=torch.float)\r\n# print(f\" `y` (target variable) successfully assigned! Shape: {labels.shape}\")\r\n\r\n# print(\"Building KDTree and constructing edges (k-NN + Distance Threshold)...\")\r\n\r\n# coordinates = data[['lat', 'lon']].values\r\n# tree = KDTree(coordinates)\r\n\r\n# k = 5\r\n# threshold = 0.0075  # Max connection distance (adjust as needed)\r\n\r\n# # Query k-nearest neighbors\r\n# distances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\r\n\r\n# # Convert to edge index format with distance filtering\r\n# edge_list = []\r\n# for i in range(len(neighbors)):\r\n#     for j in range(1, k+1):  # Skip first entry (self-node)\r\n#         if distances[i][j] < threshold:  # Only add edge if within threshold\r\n#             edge_list.append((i, neighbors[i][j]))\r\n\r\n# edges = np.array(edge_list).T  # Convert list to NumPy array and transpose\r\n\r\n# if edges.shape[1] > 0:\r\n#     edge_index = torch.tensor(edges, dtype=torch.long)  # Convert to PyTorch tensor\r\n#     print(f\"Number of edges created: {edge_index.shape[1]}\")\r\n# else:\r\n#     print(\"No edges were created. Consider increasing `threshold` or `k`.\")\r\n#     edge_index = torch.empty((2, 0), dtype=torch.long)  # Empty tensor\r\n\r\n# graph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n\r\n\r\n# #  checking graph before saving\r\n# print(\"\\n Graph Data Summary:\")\r\n# print(graph_data)  # This prints node & edge count\r\n# print(f\"Number of nodes: {graph_data.num_nodes}\")\r\n# print(f\"Number of edges: {graph_data.num_edges}\")\r\n# print(f\"Node feature shape: {graph_data.x.shape}\")\r\n# print(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n# # Sample a few edges and nodes\r\n# print(\"\\n Sample of Node Features:\")\r\n# print(graph_data.x[:5])  # Show first 5 node features\r\n# print(\"\\n Sample of Edge Index:\")\r\n# print(graph_data.edge_index[:, :10])  # Show first 10 edges\r\n# deg = degree(edge_index[0], num_nodes=graph_data.num_nodes)  # Compute degrees for each node\r\n# print(f\" Degree stats:\\n\")\r\n# print(f\" - Min degree: {deg.min().item()}\")\r\n# print(f\" - Max degree: {deg.max().item()}\")\r\n# print(f\" - Mean degree: {deg.float().mean().item()}\")\r\n# print(f\" - Standard deviation: {deg.float().std().item()}\")\r\n# # Count isolated nodes\r\n# num_isolated_nodes = (deg == 0).sum().item()\r\n# print(f\" Number of isolated nodes (degree=0): {num_isolated_nodes}\")\r\n# # Percentage of isolated nodes\r\n# percentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\r\n# print(f\" Percentage of isolated nodes: {percentage_isolated:.4f}%\")\r\n\r\n\r\n# #saving the graph\r\n# graph_data_output_path = \"/media/volume1/gnn_graph.pt\"\r\n\r\n# # Save the graph data object\r\n# torch.save(graph_data, graph_data_output_path)\r\n\r\n# print(f\" PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\r\n",
  "history_output" : " Loading dataset...\n Filtered dataset loaded with 620662 rows and 13 columns.\n Found 186037 duplicate rows.\nRemoved duplicates. New dataset size: 434625 rows.\n Normalizing features using StandardScaler...\n Normalization complete.\nExplained variance by each component:\n [0.28242602 0.40056118 0.49893777 0.58120135 0.65813573 0.7196702\n 0.77752655 0.82969028 0.87831285 0.92113218 0.95438408 0.98519019\n 1.        ]\nTo retain 95% of variance, we need 11 components.\n",
  "history_begin_time" : 1740778566621,
  "history_end_time" : 1740778596983,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "E6CWxG1X5STI",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\nfrom sklearn.decomposition import PCA\r\nfrom torch_geometric.utils import degree \r\n\r\n\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\" Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]  \r\n\r\n# Step 1: Load and filter the dataset\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\n# Combine filtered chunks\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\" Filtered dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\n\r\ndata = data.drop_duplicates()\r\n\r\nprint(f\"Removed duplicates. New dataset size: {data.shape[0]} rows.\") \r\n\r\nfeature_columns = [\r\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]\r\n\r\nprint(\" Normalizing features using StandardScaler...\")\r\nscaler = StandardScaler()\r\ndata[feature_columns] = scaler.fit_transform(data[feature_columns])\r\nprint(\" Normalization complete.\")\r\n\r\n# to check how many components to use\r\n# pca = PCA()\r\n# pca.fit(data[useful_columns])\r\n# # Print explained variance\r\n# explained_variance = np.cumsum(pca.explained_variance_ratio_)\r\n# print(\"Explained variance by each component:\\n\", explained_variance)\r\n# # Find the number of components that retain 95% variance\r\n# n_components_95 = np.argmax(explained_variance >= 0.95) + 1\r\n# print(f\"To retain 95% of variance, we need {n_components_95} components.\")\r\n\r\n# print(\"Applying PCA to reduce dimensionality...\")\r\n# pca = PCA(n_components=12)  # Reduce to 12 main features\r\n# node_features = pca.fit_transform(data[useful_columns])\r\n# node_features = torch.tensor(node_features, dtype=torch.float)\r\n# print(f\"PCA applied. Node feature shape: {node_features.shape}\")\r\n\r\n# labels = torch.tensor(data[\"swe_value\"].values, dtype=torch.float)\r\n# print(f\" `y` (target variable) successfully assigned! Shape: {labels.shape}\")\r\n\r\n# print(\"Building KDTree and constructing edges (k-NN + Distance Threshold)...\")\r\n\r\n# coordinates = data[['lat', 'lon']].values\r\n# tree = KDTree(coordinates)\r\n\r\n# k = 5\r\n# threshold = 0.0075  # Max connection distance (adjust as needed)\r\n\r\n# # Query k-nearest neighbors\r\n# distances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\r\n\r\n# # Convert to edge index format with distance filtering\r\n# edge_list = []\r\n# for i in range(len(neighbors)):\r\n#     for j in range(1, k+1):  # Skip first entry (self-node)\r\n#         if distances[i][j] < threshold:  # Only add edge if within threshold\r\n#             edge_list.append((i, neighbors[i][j]))\r\n\r\n# edges = np.array(edge_list).T  # Convert list to NumPy array and transpose\r\n\r\n# if edges.shape[1] > 0:\r\n#     edge_index = torch.tensor(edges, dtype=torch.long)  # Convert to PyTorch tensor\r\n#     print(f\"Number of edges created: {edge_index.shape[1]}\")\r\n# else:\r\n#     print(\"No edges were created. Consider increasing `threshold` or `k`.\")\r\n#     edge_index = torch.empty((2, 0), dtype=torch.long)  # Empty tensor\r\n\r\n# graph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n\r\n\r\n# #  checking graph before saving\r\n# print(\"\\n Graph Data Summary:\")\r\n# print(graph_data)  # This prints node & edge count\r\n# print(f\"Number of nodes: {graph_data.num_nodes}\")\r\n# print(f\"Number of edges: {graph_data.num_edges}\")\r\n# print(f\"Node feature shape: {graph_data.x.shape}\")\r\n# print(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n# # Sample a few edges and nodes\r\n# print(\"\\n Sample of Node Features:\")\r\n# print(graph_data.x[:5])  # Show first 5 node features\r\n# print(\"\\n Sample of Edge Index:\")\r\n# print(graph_data.edge_index[:, :10])  # Show first 10 edges\r\n# deg = degree(edge_index[0], num_nodes=graph_data.num_nodes)  # Compute degrees for each node\r\n# print(f\" Degree stats:\\n\")\r\n# print(f\" - Min degree: {deg.min().item()}\")\r\n# print(f\" - Max degree: {deg.max().item()}\")\r\n# print(f\" - Mean degree: {deg.float().mean().item()}\")\r\n# print(f\" - Standard deviation: {deg.float().std().item()}\")\r\n# # Count isolated nodes\r\n# num_isolated_nodes = (deg == 0).sum().item()\r\n# print(f\" Number of isolated nodes (degree=0): {num_isolated_nodes}\")\r\n# # Percentage of isolated nodes\r\n# percentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\r\n# print(f\" Percentage of isolated nodes: {percentage_isolated:.4f}%\")\r\n\r\n\r\n# #saving the graph\r\n# graph_data_output_path = \"/media/volume1/gnn_graph.pt\"\r\n\r\n# # Save the graph data object\r\n# torch.save(graph_data, graph_data_output_path)\r\n\r\n# print(f\" PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\r\n",
  "history_output" : " Loading dataset...\n Filtered dataset loaded with 620662 rows and 13 columns.\n Found 186037 duplicate rows.\nRemoved duplicates. New dataset size: 434625 rows.\n Normalizing features using StandardScaler...\n Normalization complete.\n",
  "history_begin_time" : 1740778444927,
  "history_end_time" : 1740778474397,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "J1viYmWpQfqB",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\nfrom sklearn.decomposition import PCA\r\nfrom torch_geometric.utils import degree \r\n\r\n\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\" Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]  \r\n\r\n# Step 1: Load and filter the dataset\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\n# Combine filtered chunks\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\" Filtered dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\n\r\ndata = data.drop_duplicates()\r\n\r\nprint(f\"Removed duplicates. New dataset size: {data.shape[0]} rows.\")\r\n\r\nprint(f\"Min Latitude: {data['lat'].min()}, Max Latitude: {data['lat'].max()}\") \r\nprint(f\" Min Longitude: {data['lon'].min()}, Max Longitude: {data['lon'].max()}\") \r\n\r\n# print(\" Normalizing features using StandardScaler...\")\r\n# scaler = StandardScaler()\r\n# data[useful_columns] = scaler.fit_transform(data[useful_columns])  # Normalize features\r\n# print(\"Normalization complete.\")\r\n\r\n# to check how many components to use\r\n# pca = PCA()\r\n# pca.fit(data[useful_columns])\r\n# # Print explained variance\r\n# explained_variance = np.cumsum(pca.explained_variance_ratio_)\r\n# print(\"Explained variance by each component:\\n\", explained_variance)\r\n# # Find the number of components that retain 95% variance\r\n# n_components_95 = np.argmax(explained_variance >= 0.95) + 1\r\n# print(f\"To retain 95% of variance, we need {n_components_95} components.\")\r\n\r\n# print(\"Applying PCA to reduce dimensionality...\")\r\n# pca = PCA(n_components=12)  # Reduce to 12 main features\r\n# node_features = pca.fit_transform(data[useful_columns])\r\n# node_features = torch.tensor(node_features, dtype=torch.float)\r\n# print(f\"PCA applied. Node feature shape: {node_features.shape}\")\r\n\r\n# labels = torch.tensor(data[\"swe_value\"].values, dtype=torch.float)\r\n# print(f\" `y` (target variable) successfully assigned! Shape: {labels.shape}\")\r\n\r\n# print(\"Building KDTree and constructing edges (k-NN + Distance Threshold)...\")\r\n\r\n# coordinates = data[['lat', 'lon']].values\r\n# tree = KDTree(coordinates)\r\n\r\n# k = 5\r\n# threshold = 0.0075  # Max connection distance (adjust as needed)\r\n\r\n# # Query k-nearest neighbors\r\n# distances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\r\n\r\n# # Convert to edge index format with distance filtering\r\n# edge_list = []\r\n# for i in range(len(neighbors)):\r\n#     for j in range(1, k+1):  # Skip first entry (self-node)\r\n#         if distances[i][j] < threshold:  # Only add edge if within threshold\r\n#             edge_list.append((i, neighbors[i][j]))\r\n\r\n# edges = np.array(edge_list).T  # Convert list to NumPy array and transpose\r\n\r\n# if edges.shape[1] > 0:\r\n#     edge_index = torch.tensor(edges, dtype=torch.long)  # Convert to PyTorch tensor\r\n#     print(f\"Number of edges created: {edge_index.shape[1]}\")\r\n# else:\r\n#     print(\"No edges were created. Consider increasing `threshold` or `k`.\")\r\n#     edge_index = torch.empty((2, 0), dtype=torch.long)  # Empty tensor\r\n\r\n# graph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n\r\n\r\n# #  checking graph before saving\r\n# print(\"\\n Graph Data Summary:\")\r\n# print(graph_data)  # This prints node & edge count\r\n# print(f\"Number of nodes: {graph_data.num_nodes}\")\r\n# print(f\"Number of edges: {graph_data.num_edges}\")\r\n# print(f\"Node feature shape: {graph_data.x.shape}\")\r\n# print(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n# # Sample a few edges and nodes\r\n# print(\"\\n Sample of Node Features:\")\r\n# print(graph_data.x[:5])  # Show first 5 node features\r\n# print(\"\\n Sample of Edge Index:\")\r\n# print(graph_data.edge_index[:, :10])  # Show first 10 edges\r\n# deg = degree(edge_index[0], num_nodes=graph_data.num_nodes)  # Compute degrees for each node\r\n# print(f\" Degree stats:\\n\")\r\n# print(f\" - Min degree: {deg.min().item()}\")\r\n# print(f\" - Max degree: {deg.max().item()}\")\r\n# print(f\" - Mean degree: {deg.float().mean().item()}\")\r\n# print(f\" - Standard deviation: {deg.float().std().item()}\")\r\n# # Count isolated nodes\r\n# num_isolated_nodes = (deg == 0).sum().item()\r\n# print(f\" Number of isolated nodes (degree=0): {num_isolated_nodes}\")\r\n# # Percentage of isolated nodes\r\n# percentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\r\n# print(f\" Percentage of isolated nodes: {percentage_isolated:.4f}%\")\r\n\r\n\r\n# #saving the graph\r\n# graph_data_output_path = \"/media/volume1/gnn_graph.pt\"\r\n\r\n# # Save the graph data object\r\n# torch.save(graph_data, graph_data_output_path)\r\n\r\n# print(f\" PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\r\n",
  "history_output" : " Loading dataset...\n Filtered dataset loaded with 620662 rows and 13 columns.\n Found 186037 duplicate rows.\nRemoved duplicates. New dataset size: 434625 rows.\nMin Latitude: 42.02282, Max Latitude: 45.76666\n Min Longitude: -124.2101666, Max Longitude: -116.95906\n",
  "history_begin_time" : 1740777780963,
  "history_end_time" : 1740777810315,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "vfgpu0e6nm7c",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\nfrom sklearn.decomposition import PCA\r\nfrom torch_geometric.utils import degree \r\n\r\n\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\" Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]  \r\n\r\n# Step 1: Load and filter the dataset\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\n# Combine filtered chunks\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\" Filtered dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\n\r\n# data = data.drop_duplicates()\r\n\r\n# print(f\"Removed duplicates. New dataset size: {data.shape[0]} rows.\")\r\n\r\n# print(\" Normalizing features using StandardScaler...\")\r\n# scaler = StandardScaler()\r\n# data[useful_columns] = scaler.fit_transform(data[useful_columns])  # Normalize features\r\n# print(\"Normalization complete.\")\r\n\r\n# to check how many components to use\r\n# pca = PCA()\r\n# pca.fit(data[useful_columns])\r\n# # Print explained variance\r\n# explained_variance = np.cumsum(pca.explained_variance_ratio_)\r\n# print(\"Explained variance by each component:\\n\", explained_variance)\r\n# # Find the number of components that retain 95% variance\r\n# n_components_95 = np.argmax(explained_variance >= 0.95) + 1\r\n# print(f\"To retain 95% of variance, we need {n_components_95} components.\")\r\n\r\n# print(\"Applying PCA to reduce dimensionality...\")\r\n# pca = PCA(n_components=12)  # Reduce to 12 main features\r\n# node_features = pca.fit_transform(data[useful_columns])\r\n# node_features = torch.tensor(node_features, dtype=torch.float)\r\n# print(f\"PCA applied. Node feature shape: {node_features.shape}\")\r\n\r\n# labels = torch.tensor(data[\"swe_value\"].values, dtype=torch.float)\r\n# print(f\" `y` (target variable) successfully assigned! Shape: {labels.shape}\")\r\n\r\n# print(\"Building KDTree and constructing edges (k-NN + Distance Threshold)...\")\r\n\r\n# coordinates = data[['lat', 'lon']].values\r\n# tree = KDTree(coordinates)\r\n\r\n# k = 5\r\n# threshold = 0.0075  # Max connection distance (adjust as needed)\r\n\r\n# # Query k-nearest neighbors\r\n# distances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\r\n\r\n# # Convert to edge index format with distance filtering\r\n# edge_list = []\r\n# for i in range(len(neighbors)):\r\n#     for j in range(1, k+1):  # Skip first entry (self-node)\r\n#         if distances[i][j] < threshold:  # Only add edge if within threshold\r\n#             edge_list.append((i, neighbors[i][j]))\r\n\r\n# edges = np.array(edge_list).T  # Convert list to NumPy array and transpose\r\n\r\n# if edges.shape[1] > 0:\r\n#     edge_index = torch.tensor(edges, dtype=torch.long)  # Convert to PyTorch tensor\r\n#     print(f\"Number of edges created: {edge_index.shape[1]}\")\r\n# else:\r\n#     print(\"No edges were created. Consider increasing `threshold` or `k`.\")\r\n#     edge_index = torch.empty((2, 0), dtype=torch.long)  # Empty tensor\r\n\r\n# graph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n\r\n\r\n# #  checking graph before saving\r\n# print(\"\\n Graph Data Summary:\")\r\n# print(graph_data)  # This prints node & edge count\r\n# print(f\"Number of nodes: {graph_data.num_nodes}\")\r\n# print(f\"Number of edges: {graph_data.num_edges}\")\r\n# print(f\"Node feature shape: {graph_data.x.shape}\")\r\n# print(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n# # Sample a few edges and nodes\r\n# print(\"\\n Sample of Node Features:\")\r\n# print(graph_data.x[:5])  # Show first 5 node features\r\n# print(\"\\n Sample of Edge Index:\")\r\n# print(graph_data.edge_index[:, :10])  # Show first 10 edges\r\n# deg = degree(edge_index[0], num_nodes=graph_data.num_nodes)  # Compute degrees for each node\r\n# print(f\" Degree stats:\\n\")\r\n# print(f\" - Min degree: {deg.min().item()}\")\r\n# print(f\" - Max degree: {deg.max().item()}\")\r\n# print(f\" - Mean degree: {deg.float().mean().item()}\")\r\n# print(f\" - Standard deviation: {deg.float().std().item()}\")\r\n# # Count isolated nodes\r\n# num_isolated_nodes = (deg == 0).sum().item()\r\n# print(f\" Number of isolated nodes (degree=0): {num_isolated_nodes}\")\r\n# # Percentage of isolated nodes\r\n# percentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\r\n# print(f\" Percentage of isolated nodes: {percentage_isolated:.4f}%\")\r\n\r\n\r\n# #saving the graph\r\n# graph_data_output_path = \"/media/volume1/gnn_graph.pt\"\r\n\r\n# # Save the graph data object\r\n# torch.save(graph_data, graph_data_output_path)\r\n\r\n# print(f\" PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\r\n",
  "history_output" : " Loading dataset...\n Filtered dataset loaded with 620662 rows and 13 columns.\n Found 186037 duplicate rows.\n",
  "history_begin_time" : 1740777250976,
  "history_end_time" : 1740777281005,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "XXZScd0I82ZG",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\nfrom sklearn.decomposition import PCA\r\nfrom torch_geometric.utils import degree \r\n\r\n\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\" Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]  \r\n\r\n# Step 1: Load and filter the dataset\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\n# Combine filtered chunks\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\" Filtered dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\n# num_duplicates = data.duplicated().sum()\r\n# print(f\" Found {num_duplicates} duplicate rows.\")\r\n\r\n# data = data.drop_duplicates()\r\n\r\n# print(f\"Removed duplicates. New dataset size: {data.shape[0]} rows.\")\r\n\r\n# print(\" Normalizing features using StandardScaler...\")\r\n# scaler = StandardScaler()\r\n# data[useful_columns] = scaler.fit_transform(data[useful_columns])  # Normalize features\r\n# print(\"Normalization complete.\")\r\n\r\n# to check how many components to use\r\n# pca = PCA()\r\n# pca.fit(data[useful_columns])\r\n# # Print explained variance\r\n# explained_variance = np.cumsum(pca.explained_variance_ratio_)\r\n# print(\"Explained variance by each component:\\n\", explained_variance)\r\n# # Find the number of components that retain 95% variance\r\n# n_components_95 = np.argmax(explained_variance >= 0.95) + 1\r\n# print(f\"To retain 95% of variance, we need {n_components_95} components.\")\r\n\r\n# print(\"Applying PCA to reduce dimensionality...\")\r\n# pca = PCA(n_components=12)  # Reduce to 12 main features\r\n# node_features = pca.fit_transform(data[useful_columns])\r\n# node_features = torch.tensor(node_features, dtype=torch.float)\r\n# print(f\"PCA applied. Node feature shape: {node_features.shape}\")\r\n\r\n# labels = torch.tensor(data[\"swe_value\"].values, dtype=torch.float)\r\n# print(f\" `y` (target variable) successfully assigned! Shape: {labels.shape}\")\r\n\r\n# print(\"Building KDTree and constructing edges (k-NN + Distance Threshold)...\")\r\n\r\n# coordinates = data[['lat', 'lon']].values\r\n# tree = KDTree(coordinates)\r\n\r\n# k = 5\r\n# threshold = 0.0075  # Max connection distance (adjust as needed)\r\n\r\n# # Query k-nearest neighbors\r\n# distances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\r\n\r\n# # Convert to edge index format with distance filtering\r\n# edge_list = []\r\n# for i in range(len(neighbors)):\r\n#     for j in range(1, k+1):  # Skip first entry (self-node)\r\n#         if distances[i][j] < threshold:  # Only add edge if within threshold\r\n#             edge_list.append((i, neighbors[i][j]))\r\n\r\n# edges = np.array(edge_list).T  # Convert list to NumPy array and transpose\r\n\r\n# if edges.shape[1] > 0:\r\n#     edge_index = torch.tensor(edges, dtype=torch.long)  # Convert to PyTorch tensor\r\n#     print(f\"Number of edges created: {edge_index.shape[1]}\")\r\n# else:\r\n#     print(\"No edges were created. Consider increasing `threshold` or `k`.\")\r\n#     edge_index = torch.empty((2, 0), dtype=torch.long)  # Empty tensor\r\n\r\n# graph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n\r\n\r\n# #  checking graph before saving\r\n# print(\"\\n Graph Data Summary:\")\r\n# print(graph_data)  # This prints node & edge count\r\n# print(f\"Number of nodes: {graph_data.num_nodes}\")\r\n# print(f\"Number of edges: {graph_data.num_edges}\")\r\n# print(f\"Node feature shape: {graph_data.x.shape}\")\r\n# print(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n# # Sample a few edges and nodes\r\n# print(\"\\n Sample of Node Features:\")\r\n# print(graph_data.x[:5])  # Show first 5 node features\r\n# print(\"\\n Sample of Edge Index:\")\r\n# print(graph_data.edge_index[:, :10])  # Show first 10 edges\r\n# deg = degree(edge_index[0], num_nodes=graph_data.num_nodes)  # Compute degrees for each node\r\n# print(f\" Degree stats:\\n\")\r\n# print(f\" - Min degree: {deg.min().item()}\")\r\n# print(f\" - Max degree: {deg.max().item()}\")\r\n# print(f\" - Mean degree: {deg.float().mean().item()}\")\r\n# print(f\" - Standard deviation: {deg.float().std().item()}\")\r\n# # Count isolated nodes\r\n# num_isolated_nodes = (deg == 0).sum().item()\r\n# print(f\" Number of isolated nodes (degree=0): {num_isolated_nodes}\")\r\n# # Percentage of isolated nodes\r\n# percentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\r\n# print(f\" Percentage of isolated nodes: {percentage_isolated:.4f}%\")\r\n\r\n\r\n# #saving the graph\r\n# graph_data_output_path = \"/media/volume1/gnn_graph.pt\"\r\n\r\n# # Save the graph data object\r\n# torch.save(graph_data, graph_data_output_path)\r\n\r\n# print(f\" PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\r\n",
  "history_output" : " Loading dataset...\n Filtered dataset loaded with 620662 rows and 13 columns.\n",
  "history_begin_time" : 1740776960494,
  "history_end_time" : 1740776990275,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "TiTbiXfwrLof",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\nfrom sklearn.decomposition import PCA\r\nfrom torch_geometric.utils import degree \r\n\r\n\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\" Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]  \r\n\r\n# Step 1: Load and filter the dataset\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\n# Combine filtered chunks\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\" Filtered dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\n# num_duplicates = data.duplicated().sum()\r\n# print(f\" Found {num_duplicates} duplicate rows.\")\r\n\r\n# data = data.drop_duplicates()\r\n\r\n# print(f\"Removed duplicates. New dataset size: {data.shape[0]} rows.\")\r\n\r\n# print(\" Normalizing features using StandardScaler...\")\r\n# scaler = StandardScaler()\r\n# data[useful_columns] = scaler.fit_transform(data[useful_columns])  # Normalize features\r\n# print(\"Normalization complete.\")\r\n\r\n# to check how many components to use\r\n# pca = PCA()\r\n# pca.fit(data[useful_columns])\r\n# # Print explained variance\r\n# explained_variance = np.cumsum(pca.explained_variance_ratio_)\r\n# print(\"Explained variance by each component:\\n\", explained_variance)\r\n# # Find the number of components that retain 95% variance\r\n# n_components_95 = np.argmax(explained_variance >= 0.95) + 1\r\n# print(f\"To retain 95% of variance, we need {n_components_95} components.\")\r\n\r\n# print(\"Applying PCA to reduce dimensionality...\")\r\n# pca = PCA(n_components=12)  # Reduce to 12 main features\r\n# node_features = pca.fit_transform(data[useful_columns])\r\n# node_features = torch.tensor(node_features, dtype=torch.float)\r\n# print(f\"PCA applied. Node feature shape: {node_features.shape}\")\r\n\r\n# labels = torch.tensor(data[\"swe_value\"].values, dtype=torch.float)\r\n# print(f\" `y` (target variable) successfully assigned! Shape: {labels.shape}\")\r\n\r\n# print(\"Building KDTree and constructing edges (k-NN + Distance Threshold)...\")\r\n\r\n# coordinates = data[['lat', 'lon']].values\r\n# tree = KDTree(coordinates)\r\n\r\n# k = 5\r\n# threshold = 0.0075  # Max connection distance (adjust as needed)\r\n\r\n# # Query k-nearest neighbors\r\n# distances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\r\n\r\n# # Convert to edge index format with distance filtering\r\n# edge_list = []\r\n# for i in range(len(neighbors)):\r\n#     for j in range(1, k+1):  # Skip first entry (self-node)\r\n#         if distances[i][j] < threshold:  # Only add edge if within threshold\r\n#             edge_list.append((i, neighbors[i][j]))\r\n\r\n# edges = np.array(edge_list).T  # Convert list to NumPy array and transpose\r\n\r\n# if edges.shape[1] > 0:\r\n#     edge_index = torch.tensor(edges, dtype=torch.long)  # Convert to PyTorch tensor\r\n#     print(f\"Number of edges created: {edge_index.shape[1]}\")\r\n# else:\r\n#     print(\"No edges were created. Consider increasing `threshold` or `k`.\")\r\n#     edge_index = torch.empty((2, 0), dtype=torch.long)  # Empty tensor\r\n\r\n# graph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n\r\n\r\n# #  checking graph before saving\r\n# print(\"\\n Graph Data Summary:\")\r\n# print(graph_data)  # This prints node & edge count\r\n# print(f\"Number of nodes: {graph_data.num_nodes}\")\r\n# print(f\"Number of edges: {graph_data.num_edges}\")\r\n# print(f\"Node feature shape: {graph_data.x.shape}\")\r\n# print(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n# # Sample a few edges and nodes\r\n# print(\"\\n Sample of Node Features:\")\r\n# print(graph_data.x[:5])  # Show first 5 node features\r\n# print(\"\\n Sample of Edge Index:\")\r\n# print(graph_data.edge_index[:, :10])  # Show first 10 edges\r\n# deg = degree(edge_index[0], num_nodes=graph_data.num_nodes)  # Compute degrees for each node\r\n# print(f\" Degree stats:\\n\")\r\n# print(f\" - Min degree: {deg.min().item()}\")\r\n# print(f\" - Max degree: {deg.max().item()}\")\r\n# print(f\" - Mean degree: {deg.float().mean().item()}\")\r\n# print(f\" - Standard deviation: {deg.float().std().item()}\")\r\n# # Count isolated nodes\r\n# num_isolated_nodes = (deg == 0).sum().item()\r\n# print(f\" Number of isolated nodes (degree=0): {num_isolated_nodes}\")\r\n# # Percentage of isolated nodes\r\n# percentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\r\n# print(f\" Percentage of isolated nodes: {percentage_isolated:.4f}%\")\r\n\r\n\r\n# #saving the graph\r\n# graph_data_output_path = \"/media/volume1/gnn_graph.pt\"\r\n\r\n# # Save the graph data object\r\n# torch.save(graph_data, graph_data_output_path)\r\n\r\n# print(f\" PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\r\n",
  "history_output" : " Loading dataset...\n Filtered dataset loaded with 620662 rows and 13 columns.\n",
  "history_begin_time" : 1740776929370,
  "history_end_time" : 1740776960910,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "vSjtBWs8vgtH",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\nfrom sklearn.decomposition import PCA\r\nfrom torch_geometric.utils import degree \r\n\r\n\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\n\r\nchunksize = 500000\r\nmin_lat, max_lat = 42.020065, 45.768982\r\nmin_lon, max_lon = -124.212205, -116.940392\r\n\r\nprint(\" Loading dataset...\")\r\n\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]  \r\n\r\n# Step 1: Load and filter the dataset\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    filtered_chunk = chunk[\r\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\r\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\r\n    ]\r\n    df_list.append(filtered_chunk)\r\n\r\n# Combine filtered chunks\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\" Filtered dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\n# num_duplicates = data.duplicated().sum()\r\n# print(f\" Found {num_duplicates} duplicate rows.\")\r\n\r\n# data = data.drop_duplicates()\r\n\r\n# print(f\"Removed duplicates. New dataset size: {data.shape[0]} rows.\")\r\n\r\n# print(\" Normalizing features using StandardScaler...\")\r\n# scaler = StandardScaler()\r\n# data[useful_columns] = scaler.fit_transform(data[useful_columns])  # Normalize features\r\n# print(\"Normalization complete.\")\r\n\r\n# to check how many components to use\r\n# pca = PCA()\r\n# pca.fit(data[useful_columns])\r\n# # Print explained variance\r\n# explained_variance = np.cumsum(pca.explained_variance_ratio_)\r\n# print(\"Explained variance by each component:\\n\", explained_variance)\r\n# # Find the number of components that retain 95% variance\r\n# n_components_95 = np.argmax(explained_variance >= 0.95) + 1\r\n# print(f\"To retain 95% of variance, we need {n_components_95} components.\")\r\n\r\n# print(\"Applying PCA to reduce dimensionality...\")\r\n# pca = PCA(n_components=12)  # Reduce to 12 main features\r\n# node_features = pca.fit_transform(data[useful_columns])\r\n# node_features = torch.tensor(node_features, dtype=torch.float)\r\n# print(f\"PCA applied. Node feature shape: {node_features.shape}\")\r\n\r\n# labels = torch.tensor(data[\"swe_value\"].values, dtype=torch.float)\r\n# print(f\" `y` (target variable) successfully assigned! Shape: {labels.shape}\")\r\n\r\n# print(\"Building KDTree and constructing edges (k-NN + Distance Threshold)...\")\r\n\r\n# coordinates = data[['lat', 'lon']].values\r\n# tree = KDTree(coordinates)\r\n\r\n# k = 5\r\n# threshold = 0.0075  # Max connection distance (adjust as needed)\r\n\r\n# # Query k-nearest neighbors\r\n# distances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\r\n\r\n# # Convert to edge index format with distance filtering\r\n# edge_list = []\r\n# for i in range(len(neighbors)):\r\n#     for j in range(1, k+1):  # Skip first entry (self-node)\r\n#         if distances[i][j] < threshold:  # Only add edge if within threshold\r\n#             edge_list.append((i, neighbors[i][j]))\r\n\r\n# edges = np.array(edge_list).T  # Convert list to NumPy array and transpose\r\n\r\n# if edges.shape[1] > 0:\r\n#     edge_index = torch.tensor(edges, dtype=torch.long)  # Convert to PyTorch tensor\r\n#     print(f\"Number of edges created: {edge_index.shape[1]}\")\r\n# else:\r\n#     print(\"No edges were created. Consider increasing `threshold` or `k`.\")\r\n#     edge_index = torch.empty((2, 0), dtype=torch.long)  # Empty tensor\r\n\r\n# graph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n\r\n\r\n# #  checking graph before saving\r\n# print(\"\\n Graph Data Summary:\")\r\n# print(graph_data)  # This prints node & edge count\r\n# print(f\"Number of nodes: {graph_data.num_nodes}\")\r\n# print(f\"Number of edges: {graph_data.num_edges}\")\r\n# print(f\"Node feature shape: {graph_data.x.shape}\")\r\n# print(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n# # Sample a few edges and nodes\r\n# print(\"\\n Sample of Node Features:\")\r\n# print(graph_data.x[:5])  # Show first 5 node features\r\n# print(\"\\n Sample of Edge Index:\")\r\n# print(graph_data.edge_index[:, :10])  # Show first 10 edges\r\n# deg = degree(edge_index[0], num_nodes=graph_data.num_nodes)  # Compute degrees for each node\r\n# print(f\" Degree stats:\\n\")\r\n# print(f\" - Min degree: {deg.min().item()}\")\r\n# print(f\" - Max degree: {deg.max().item()}\")\r\n# print(f\" - Mean degree: {deg.float().mean().item()}\")\r\n# print(f\" - Standard deviation: {deg.float().std().item()}\")\r\n# # Count isolated nodes\r\n# num_isolated_nodes = (deg == 0).sum().item()\r\n# print(f\" Number of isolated nodes (degree=0): {num_isolated_nodes}\")\r\n# # Percentage of isolated nodes\r\n# percentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\r\n# print(f\" Percentage of isolated nodes: {percentage_isolated:.4f}%\")\r\n\r\n\r\n# #saving the graph\r\n# graph_data_output_path = \"/media/volume1/gnn_graph.pt\"\r\n\r\n# # Save the graph data object\r\n# torch.save(graph_data, graph_data_output_path)\r\n\r\n# print(f\" PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\r\n",
  "history_output" : " Loading dataset...\n Filtered dataset loaded with 620662 rows and 13 columns.\n",
  "history_begin_time" : 1740776926441,
  "history_end_time" : 1740776957747,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "6nQSyXT2qwIM",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\nfrom sklearn.decomposition import PCA\r\nfrom torch_geometric.utils import degree \r\n\r\n\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\nchunksize = 500000\r\n\r\nprint(\"Loading dataset...\")\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]  \r\n\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    df_list.append(chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\n\r\ndata = data.drop_duplicates()\r\n\r\nprint(f\"Removed duplicates. New dataset size: {data.shape[0]} rows.\")\r\n\r\nprint(\" Normalizing features using StandardScaler...\")\r\nscaler = StandardScaler()\r\ndata[useful_columns] = scaler.fit_transform(data[useful_columns])  # Normalize features\r\nprint(\"Normalization complete.\")\r\n\r\n# to check how many components to use\r\n# pca = PCA()\r\n# pca.fit(data[useful_columns])\r\n# # Print explained variance\r\n# explained_variance = np.cumsum(pca.explained_variance_ratio_)\r\n# print(\"Explained variance by each component:\\n\", explained_variance)\r\n# # Find the number of components that retain 95% variance\r\n# n_components_95 = np.argmax(explained_variance >= 0.95) + 1\r\n# print(f\"To retain 95% of variance, we need {n_components_95} components.\")\r\n\r\nprint(\"Applying PCA to reduce dimensionality...\")\r\npca = PCA(n_components=12)  # Reduce to 12 main features\r\nnode_features = pca.fit_transform(data[useful_columns])\r\nnode_features = torch.tensor(node_features, dtype=torch.float)\r\nprint(f\"PCA applied. Node feature shape: {node_features.shape}\")\r\n\r\nlabels = torch.tensor(data[\"swe_value\"].values, dtype=torch.float)\r\nprint(f\" `y` (target variable) successfully assigned! Shape: {labels.shape}\")\r\n\r\nprint(\"Building KDTree and constructing edges (k-NN + Distance Threshold)...\")\r\n\r\ncoordinates = data[['lat', 'lon']].values\r\ntree = KDTree(coordinates)\r\n\r\nk = 5\r\nthreshold = 0.0075  # Max connection distance (adjust as needed)\r\n\r\n# Query k-nearest neighbors\r\ndistances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\r\n\r\n# Convert to edge index format with distance filtering\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, k+1):  # Skip first entry (self-node)\r\n        if distances[i][j] < threshold:  # Only add edge if within threshold\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T  # Convert list to NumPy array and transpose\r\n\r\nif edges.shape[1] > 0:\r\n    edge_index = torch.tensor(edges, dtype=torch.long)  # Convert to PyTorch tensor\r\n    print(f\"Number of edges created: {edge_index.shape[1]}\")\r\nelse:\r\n    print(\"No edges were created. Consider increasing `threshold` or `k`.\")\r\n    edge_index = torch.empty((2, 0), dtype=torch.long)  # Empty tensor\r\n\r\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n\r\n\r\n# #  checking graph before saving\r\n# print(\"\\n Graph Data Summary:\")\r\n# print(graph_data)  # This prints node & edge count\r\n# print(f\"Number of nodes: {graph_data.num_nodes}\")\r\n# print(f\"Number of edges: {graph_data.num_edges}\")\r\n# print(f\"Node feature shape: {graph_data.x.shape}\")\r\n# print(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n# # Sample a few edges and nodes\r\n# print(\"\\n Sample of Node Features:\")\r\n# print(graph_data.x[:5])  # Show first 5 node features\r\n# print(\"\\n Sample of Edge Index:\")\r\n# print(graph_data.edge_index[:, :10])  # Show first 10 edges\r\n# deg = degree(edge_index[0], num_nodes=graph_data.num_nodes)  # Compute degrees for each node\r\n# print(f\" Degree stats:\\n\")\r\n# print(f\" - Min degree: {deg.min().item()}\")\r\n# print(f\" - Max degree: {deg.max().item()}\")\r\n# print(f\" - Mean degree: {deg.float().mean().item()}\")\r\n# print(f\" - Standard deviation: {deg.float().std().item()}\")\r\n# # Count isolated nodes\r\n# num_isolated_nodes = (deg == 0).sum().item()\r\n# print(f\" Number of isolated nodes (degree=0): {num_isolated_nodes}\")\r\n# # Percentage of isolated nodes\r\n# percentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\r\n# print(f\" Percentage of isolated nodes: {percentage_isolated:.4f}%\")\r\n\r\n\r\n# #saving the graph\r\n# graph_data_output_path = \"/media/volume1/gnn_graph.pt\"\r\n\r\n# # Save the graph data object\r\n# torch.save(graph_data, graph_data_output_path)\r\n\r\n# print(f\" PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\r\n",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 13 columns.\n Found 2177342 duplicate rows.\nRemoved duplicates. New dataset size: 3978302 rows.\n Normalizing features using StandardScaler...\nNormalization complete.\nApplying PCA to reduce dimensionality...\nPCA applied. Node feature shape: torch.Size([3978302, 12])\nTraceback (most recent call last):\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py\", line 3805, in get_loc\n    return self._engine.get_loc(casted_key)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"index.pyx\", line 167, in pandas._libs.index.IndexEngine.get_loc\n  File \"index.pyx\", line 196, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7081, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7089, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'swe_value'\nThe above exception was the direct cause of the following exception:\nTraceback (most recent call last):\n  File \"/media/volume1/gw-workspace/6nQSyXT2qwIM/graph_creation.py\", line 60, in <module>\n    labels = torch.tensor(data[\"swe_value\"].values, dtype=torch.float)\n                          ~~~~^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py\", line 4102, in __getitem__\n    indexer = self.columns.get_loc(key)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py\", line 3812, in get_loc\n    raise KeyError(key) from err\nKeyError: 'swe_value'\n",
  "history_begin_time" : 1740759122473,
  "history_end_time" : 1740759166396,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "opSwgdJyaC1B",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\nfrom sklearn.decomposition import PCA\r\nfrom torch_geometric.utils import degree \r\n\r\n\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\nchunksize = 500000\r\n\r\nprint(\"Loading dataset...\")\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]  \r\n\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    df_list.append(chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\n\r\ndata = data.drop_duplicates()\r\n\r\nprint(f\"Removed duplicates. New dataset size: {data.shape[0]} rows.\")\r\n\r\nprint(\" Normalizing features using StandardScaler...\")\r\nscaler = StandardScaler()\r\ndata[useful_columns] = scaler.fit_transform(data[useful_columns])  # Normalize features\r\nprint(\"Normalization complete.\")\r\n\r\n# to check how many components to use\r\n# pca = PCA()\r\n# pca.fit(data[useful_columns])\r\n# # Print explained variance\r\n# explained_variance = np.cumsum(pca.explained_variance_ratio_)\r\n# print(\"Explained variance by each component:\\n\", explained_variance)\r\n# # Find the number of components that retain 95% variance\r\n# n_components_95 = np.argmax(explained_variance >= 0.95) + 1\r\n# print(f\"To retain 95% of variance, we need {n_components_95} components.\")\r\n\r\nprint(\"Applying PCA to reduce dimensionality...\")\r\npca = PCA(n_components=12)  # Reduce to 12 main features\r\nnode_features = pca.fit_transform(data[useful_columns])\r\nnode_features = torch.tensor(node_features, dtype=torch.float)\r\nprint(f\"PCA applied. Node feature shape: {node_features.shape}\")\r\n\r\nlabels = torch.tensor(data[\"swe_value\"].values, dtype=torch.float)\r\nprint(f\" `y` (target variable) successfully assigned! Shape: {labels.shape}\")\r\n\r\nprint(\"Building KDTree and constructing edges (k-NN + Distance Threshold)...\")\r\n\r\ncoordinates = data[['lat', 'lon']].values\r\ntree = KDTree(coordinates)\r\n\r\nk = 5\r\nthreshold = 0.0075  # Max connection distance (adjust as needed)\r\n\r\n# Query k-nearest neighbors\r\ndistances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\r\n\r\n# Convert to edge index format with distance filtering\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, k+1):  # Skip first entry (self-node)\r\n        if distances[i][j] < threshold:  # Only add edge if within threshold\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T  # Convert list to NumPy array and transpose\r\n\r\nif edges.shape[1] > 0:\r\n    edge_index = torch.tensor(edges, dtype=torch.long)  # Convert to PyTorch tensor\r\n    print(f\"Number of edges created: {edge_index.shape[1]}\")\r\nelse:\r\n    print(\"No edges were created. Consider increasing `threshold` or `k`.\")\r\n    edge_index = torch.empty((2, 0), dtype=torch.long)  # Empty tensor\r\n\r\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n\r\n\r\n# #  checking graph before saving\r\n# print(\"\\n Graph Data Summary:\")\r\n# print(graph_data)  # This prints node & edge count\r\n# print(f\"Number of nodes: {graph_data.num_nodes}\")\r\n# print(f\"Number of edges: {graph_data.num_edges}\")\r\n# print(f\"Node feature shape: {graph_data.x.shape}\")\r\n# print(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n# # Sample a few edges and nodes\r\n# print(\"\\n Sample of Node Features:\")\r\n# print(graph_data.x[:5])  # Show first 5 node features\r\n# print(\"\\n Sample of Edge Index:\")\r\n# print(graph_data.edge_index[:, :10])  # Show first 10 edges\r\n# deg = degree(edge_index[0], num_nodes=graph_data.num_nodes)  # Compute degrees for each node\r\n# print(f\" Degree stats:\\n\")\r\n# print(f\" - Min degree: {deg.min().item()}\")\r\n# print(f\" - Max degree: {deg.max().item()}\")\r\n# print(f\" - Mean degree: {deg.float().mean().item()}\")\r\n# print(f\" - Standard deviation: {deg.float().std().item()}\")\r\n# # Count isolated nodes\r\n# num_isolated_nodes = (deg == 0).sum().item()\r\n# print(f\" Number of isolated nodes (degree=0): {num_isolated_nodes}\")\r\n# # Percentage of isolated nodes\r\n# percentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\r\n# print(f\" Percentage of isolated nodes: {percentage_isolated:.4f}%\")\r\n\r\n\r\n# #saving the graph\r\n# graph_data_output_path = \"/media/volume1/gnn_graph.pt\"\r\n\r\n# # Save the graph data object\r\n# torch.save(graph_data, graph_data_output_path)\r\n\r\n# print(f\" PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\r\n",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 13 columns.\n Found 2177342 duplicate rows.\nRemoved duplicates. New dataset size: 3978302 rows.\n Normalizing features using StandardScaler...\nNormalization complete.\nApplying PCA to reduce dimensionality...\nPCA applied. Node feature shape: torch.Size([3978302, 12])\nTraceback (most recent call last):\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py\", line 3805, in get_loc\n    return self._engine.get_loc(casted_key)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"index.pyx\", line 167, in pandas._libs.index.IndexEngine.get_loc\n  File \"index.pyx\", line 196, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7081, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7089, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'swe_value'\nThe above exception was the direct cause of the following exception:\nTraceback (most recent call last):\n  File \"/media/volume1/gw-workspace/opSwgdJyaC1B/graph_creation.py\", line 60, in <module>\n    labels = torch.tensor(data[\"swe_value\"].values, dtype=torch.float)\n                          ~~~~^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py\", line 4102, in __getitem__\n    indexer = self.columns.get_loc(key)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py\", line 3812, in get_loc\n    raise KeyError(key) from err\nKeyError: 'swe_value'\n",
  "history_begin_time" : 1740759117858,
  "history_end_time" : 1740759160919,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "S68ebXH6H95v",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\nfrom sklearn.decomposition import PCA\r\nfrom torch_geometric.utils import degree \r\n\r\n\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\nchunksize = 500000\r\n\r\nprint(\"Loading dataset...\")\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]  \r\n\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    df_list.append(chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\n\r\ndata = data.drop_duplicates()\r\n\r\nprint(f\"Removed duplicates. New dataset size: {data.shape[0]} rows.\")\r\n\r\nprint(\" Normalizing features using StandardScaler...\")\r\nscaler = StandardScaler()\r\ndata[useful_columns] = scaler.fit_transform(data[useful_columns])  # Normalize features\r\nprint(\"Normalization complete.\")\r\n\r\n#to check how many components to use\r\npca = PCA()\r\npca.fit(data[useful_columns])\r\n\r\n# Print explained variance\r\nexplained_variance = np.cumsum(pca.explained_variance_ratio_)\r\nprint(\"Explained variance by each component:\\n\", explained_variance)\r\n\r\n# Find the number of components that retain 95% variance\r\nn_components_95 = np.argmax(explained_variance >= 0.95) + 1\r\nprint(f\"To retain 95% of variance, we need {n_components_95} components.\")\r\n\r\nprint(\"Applying PCA to reduce dimensionality...\")\r\npca = PCA(n_components=12)  # Reduce to 12 main features\r\nnode_features = pca.fit_transform(data[useful_columns])\r\nnode_features = torch.tensor(node_features, dtype=torch.float)\r\nprint(f\"PCA applied. Node feature shape: {node_features.shape}\")\r\n\r\nprint(\"Building KDTree and constructing edges (k-NN + Distance Threshold)...\")\r\n\r\ncoordinates = data[['lat', 'lon']].values\r\ntree = KDTree(coordinates)\r\n\r\nk = 5\r\nthreshold = 0.0075  # Max connection distance (adjust as needed)\r\n\r\n# Query k-nearest neighbors\r\ndistances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\r\n\r\n# Convert to edge index format with distance filtering\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, k+1):  # Skip first entry (self-node)\r\n        if distances[i][j] < threshold:  # Only add edge if within threshold\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T  # Convert list to NumPy array and transpose\r\n\r\nif edges.shape[1] > 0:\r\n    edge_index = torch.tensor(edges, dtype=torch.long)  # Convert to PyTorch tensor\r\n    print(f\"Number of edges created: {edge_index.shape[1]}\")\r\nelse:\r\n    print(\"No edges were created. Consider increasing `threshold` or `k`.\")\r\n    edge_index = torch.empty((2, 0), dtype=torch.long)  # Empty tensor\r\n\r\ngraph_data = Data(x=node_features, edge_index=edge_index)\r\n\r\nprint(\"Checking if `y` exists before saving the graph...\")\r\n\r\nif hasattr(graph_data, \"y\"):\r\n    print(f\" Target variable exists! Shape: {graph_data.y.shape}\")\r\nelse:\r\n    print(\" `y` is missing! Ensure it's correctly assigned before saving.\")\r\n\r\n\r\n# #  checking graph before saving\r\n# print(\"\\n Graph Data Summary:\")\r\n# print(graph_data)  # This prints node & edge count\r\n# print(f\"Number of nodes: {graph_data.num_nodes}\")\r\n# print(f\"Number of edges: {graph_data.num_edges}\")\r\n# print(f\"Node feature shape: {graph_data.x.shape}\")\r\n# print(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n\r\n# # Sample a few edges and nodes\r\n# print(\"\\n Sample of Node Features:\")\r\n# print(graph_data.x[:5])  # Show first 5 node features\r\n\r\n# print(\"\\n Sample of Edge Index:\")\r\n# print(graph_data.edge_index[:, :10])  # Show first 10 edges\r\n\r\n\r\n# deg = degree(edge_index[0], num_nodes=graph_data.num_nodes)  # Compute degrees for each node\r\n# print(f\" Degree stats:\\n\")\r\n# print(f\" - Min degree: {deg.min().item()}\")\r\n# print(f\" - Max degree: {deg.max().item()}\")\r\n# print(f\" - Mean degree: {deg.float().mean().item()}\")\r\n# print(f\" - Standard deviation: {deg.float().std().item()}\")\r\n\r\n# # Count isolated nodes\r\n# num_isolated_nodes = (deg == 0).sum().item()\r\n# print(f\" Number of isolated nodes (degree=0): {num_isolated_nodes}\")\r\n\r\n# # Percentage of isolated nodes\r\n# percentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\r\n# print(f\" Percentage of isolated nodes: {percentage_isolated:.4f}%\")\r\n\r\n\r\n# #saving the graph\r\n# graph_data_output_path = \"/media/volume1/gnn_graph.pt\"\r\n\r\n# # Save the graph data object\r\n# torch.save(graph_data, graph_data_output_path)\r\n\r\n# print(f\" PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\r\n",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 13 columns.\n Found 2177342 duplicate rows.\nRemoved duplicates. New dataset size: 3978302 rows.\n Normalizing features using StandardScaler...\nNormalization complete.\nExplained variance by each component:\n [0.16404866 0.30675171 0.41844088 0.50969481 0.58747902 0.6587108\n 0.72535687 0.78651539 0.84276239 0.8926543  0.93666506 0.96938215\n 1.        ]\nTo retain 95% of variance, we need 12 components.\nApplying PCA to reduce dimensionality...\nPCA applied. Node feature shape: torch.Size([3978302, 12])\nBuilding KDTree and constructing edges (k-NN + Distance Threshold)...\nNumber of edges created: 19888728\nChecking if `y` exists before saving the graph...\nTraceback (most recent call last):\n  File \"/media/volume1/gw-workspace/S68ebXH6H95v/graph_creation.py\", line 94, in <module>\n    print(f\" Target variable exists! Shape: {graph_data.y.shape}\")\n                                             ^^^^^^^^^^^^^^^^^^\nAttributeError: 'NoneType' object has no attribute 'shape'\n",
  "history_begin_time" : 1740722438779,
  "history_end_time" : 1740722532060,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "vzpyupcc3pcs",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\nfrom sklearn.decomposition import PCA\r\nfrom torch_geometric.utils import degree \r\n\r\n\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\nchunksize = 500000\r\n\r\nprint(\"Loading dataset...\")\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]  \r\n\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    df_list.append(chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\n\r\ndata = data.drop_duplicates()\r\n\r\nprint(f\"Removed duplicates. New dataset size: {data.shape[0]} rows.\")\r\n\r\nprint(\" Normalizing features using StandardScaler...\")\r\nscaler = StandardScaler()\r\ndata[useful_columns] = scaler.fit_transform(data[useful_columns])  # Normalize features\r\nprint(\"Normalization complete.\")\r\n\r\n#to check how many components to use\r\npca = PCA()\r\npca.fit(data[useful_columns])\r\n\r\n# Print explained variance\r\nexplained_variance = np.cumsum(pca.explained_variance_ratio_)\r\nprint(\"Explained variance by each component:\\n\", explained_variance)\r\n\r\n# Find the number of components that retain 95% variance\r\nn_components_95 = np.argmax(explained_variance >= 0.95) + 1\r\nprint(f\"To retain 95% of variance, we need {n_components_95} components.\")\r\n\r\nprint(\"Applying PCA to reduce dimensionality...\")\r\npca = PCA(n_components=12)  # Reduce to 12 main features\r\nnode_features = pca.fit_transform(data[useful_columns])\r\nnode_features = torch.tensor(node_features, dtype=torch.float)\r\nprint(f\"PCA applied. Node feature shape: {node_features.shape}\")\r\n\r\nprint(\"Building KDTree and constructing edges (k-NN + Distance Threshold)...\")\r\n\r\ncoordinates = data[['lat', 'lon']].values\r\ntree = KDTree(coordinates)\r\n\r\nk = 5\r\nthreshold = 0.0075  # Max connection distance (adjust as needed)\r\n\r\n# Query k-nearest neighbors\r\ndistances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\r\n\r\n# Convert to edge index format with distance filtering\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, k+1):  # Skip first entry (self-node)\r\n        if distances[i][j] < threshold:  # Only add edge if within threshold\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T  # Convert list to NumPy array and transpose\r\n\r\nif edges.shape[1] > 0:\r\n    edge_index = torch.tensor(edges, dtype=torch.long)  # Convert to PyTorch tensor\r\n    print(f\"Number of edges created: {edge_index.shape[1]}\")\r\nelse:\r\n    print(\"No edges were created. Consider increasing `threshold` or `k`.\")\r\n    edge_index = torch.empty((2, 0), dtype=torch.long)  # Empty tensor\r\n\r\ngraph_data = Data(x=node_features, edge_index=edge_index)\r\n\r\n# #  checking graph before saving\r\n# print(\"\\n Graph Data Summary:\")\r\n# print(graph_data)  # This prints node & edge count\r\n# print(f\"Number of nodes: {graph_data.num_nodes}\")\r\n# print(f\"Number of edges: {graph_data.num_edges}\")\r\n# print(f\"Node feature shape: {graph_data.x.shape}\")\r\n# print(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n\r\n# # Sample a few edges and nodes\r\n# print(\"\\n Sample of Node Features:\")\r\n# print(graph_data.x[:5])  # Show first 5 node features\r\n\r\n# print(\"\\n Sample of Edge Index:\")\r\n# print(graph_data.edge_index[:, :10])  # Show first 10 edges\r\n\r\n\r\n# deg = degree(edge_index[0], num_nodes=graph_data.num_nodes)  # Compute degrees for each node\r\n# print(f\" Degree stats:\\n\")\r\n# print(f\" - Min degree: {deg.min().item()}\")\r\n# print(f\" - Max degree: {deg.max().item()}\")\r\n# print(f\" - Mean degree: {deg.float().mean().item()}\")\r\n# print(f\" - Standard deviation: {deg.float().std().item()}\")\r\n\r\n# # Count isolated nodes\r\n# num_isolated_nodes = (deg == 0).sum().item()\r\n# print(f\" Number of isolated nodes (degree=0): {num_isolated_nodes}\")\r\n\r\n# # Percentage of isolated nodes\r\n# percentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\r\n# print(f\" Percentage of isolated nodes: {percentage_isolated:.4f}%\")\r\n\r\n\r\n#saving the graph\r\ngraph_data_output_path = \"/media/volume1/gnn_graph.pt\"\r\n\r\n# Save the graph data object\r\ntorch.save(graph_data, graph_data_output_path)\r\n\r\nprint(f\" PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\r\n",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 13 columns.\n Found 2177342 duplicate rows.\nRemoved duplicates. New dataset size: 3978302 rows.\n Normalizing features using StandardScaler...\nNormalization complete.\nExplained variance by each component:\n [0.16404866 0.30675171 0.41844088 0.50969481 0.58747902 0.6587108\n 0.72535687 0.78651539 0.84276239 0.8926543  0.93666506 0.96938215\n 1.        ]\nTo retain 95% of variance, we need 12 components.\nApplying PCA to reduce dimensionality...\nPCA applied. Node feature shape: torch.Size([3978302, 12])\nBuilding KDTree and constructing edges (k-NN + Distance Threshold)...\nNumber of edges created: 19888728\n PyTorch Geometric Graph Data saved at: /media/volume1/gnn_graph.pt\n",
  "history_begin_time" : 1740719123610,
  "history_end_time" : 1740719218265,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "pVA00eT1Unh7",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\nfrom sklearn.decomposition import PCA\r\nfrom torch_geometric.utils import degree \r\n\r\n\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\nchunksize = 500000\r\n\r\nprint(\"Loading dataset...\")\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]  \r\n\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    df_list.append(chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\n\r\ndata = data.drop_duplicates()\r\n\r\nprint(f\"Removed duplicates. New dataset size: {data.shape[0]} rows.\")\r\n\r\nprint(\" Normalizing features using StandardScaler...\")\r\nscaler = StandardScaler()\r\ndata[useful_columns] = scaler.fit_transform(data[useful_columns])  # Normalize features\r\nprint(\"Normalization complete.\")\r\n\r\n#to check how many components to use\r\npca = PCA()\r\npca.fit(data[useful_columns])\r\n\r\n# Print explained variance\r\nexplained_variance = np.cumsum(pca.explained_variance_ratio_)\r\nprint(\"Explained variance by each component:\\n\", explained_variance)\r\n\r\n# Find the number of components that retain 95% variance\r\nn_components_95 = np.argmax(explained_variance >= 0.95) + 1\r\nprint(f\"To retain 95% of variance, we need {n_components_95} components.\")\r\n\r\nprint(\"Applying PCA to reduce dimensionality...\")\r\npca = PCA(n_components=12)  # Reduce to 12 main features\r\nnode_features = pca.fit_transform(data[useful_columns])\r\nnode_features = torch.tensor(node_features, dtype=torch.float)\r\nprint(f\"PCA applied. Node feature shape: {node_features.shape}\")\r\n\r\nprint(\"Building KDTree and constructing edges (k-NN + Distance Threshold)...\")\r\n\r\ncoordinates = data[['lat', 'lon']].values\r\ntree = KDTree(coordinates)\r\n\r\nk = 5\r\nthreshold = 0.0075  # Max connection distance (adjust as needed)\r\n\r\n# Query k-nearest neighbors\r\ndistances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\r\n\r\n# Convert to edge index format with distance filtering\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, k+1):  # Skip first entry (self-node)\r\n        if distances[i][j] < threshold:  # Only add edge if within threshold\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T  # Convert list to NumPy array and transpose\r\n\r\nif edges.shape[1] > 0:\r\n    edge_index = torch.tensor(edges, dtype=torch.long)  # Convert to PyTorch tensor\r\n    print(f\"Number of edges created: {edge_index.shape[1]}\")\r\nelse:\r\n    print(\"No edges were created. Consider increasing `threshold` or `k`.\")\r\n    edge_index = torch.empty((2, 0), dtype=torch.long)  # Empty tensor\r\n\r\ngraph_data = Data(x=node_features, edge_index=edge_index)\r\n\r\n# #  checking graph before saving\r\n# print(\"\\n Graph Data Summary:\")\r\n# print(graph_data)  # This prints node & edge count\r\n# print(f\"Number of nodes: {graph_data.num_nodes}\")\r\n# print(f\"Number of edges: {graph_data.num_edges}\")\r\n# print(f\"Node feature shape: {graph_data.x.shape}\")\r\n# print(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n\r\n# # Sample a few edges and nodes\r\n# print(\"\\n Sample of Node Features:\")\r\n# print(graph_data.x[:5])  # Show first 5 node features\r\n\r\n# print(\"\\n Sample of Edge Index:\")\r\n# print(graph_data.edge_index[:, :10])  # Show first 10 edges\r\n\r\n\r\n# deg = degree(edge_index[0], num_nodes=graph_data.num_nodes)  # Compute degrees for each node\r\n# print(f\" Degree stats:\\n\")\r\n# print(f\" - Min degree: {deg.min().item()}\")\r\n# print(f\" - Max degree: {deg.max().item()}\")\r\n# print(f\" - Mean degree: {deg.float().mean().item()}\")\r\n# print(f\" - Standard deviation: {deg.float().std().item()}\")\r\n\r\n# # Count isolated nodes\r\n# num_isolated_nodes = (deg == 0).sum().item()\r\n# print(f\" Number of isolated nodes (degree=0): {num_isolated_nodes}\")\r\n\r\n# # Percentage of isolated nodes\r\n# percentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\r\n# print(f\" Percentage of isolated nodes: {percentage_isolated:.4f}%\")\r\n\r\n\r\n#saving the graph\r\ngraph_data_output_path = \"/media/volume1/gnn_graph.pt\"\r\n\r\n# Save the graph data object\r\ntorch.save(graph_data, graph_data_output_path)\r\n\r\nprint(f\" PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\r\n",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 13 columns.\n Found 2177342 duplicate rows.\nRemoved duplicates. New dataset size: 3978302 rows.\n Normalizing features using StandardScaler...\nNormalization complete.\nExplained variance by each component:\n [0.16404866 0.30675171 0.41844088 0.50969481 0.58747902 0.6587108\n 0.72535687 0.78651539 0.84276239 0.8926543  0.93666506 0.96938215\n 1.        ]\nTo retain 95% of variance, we need 12 components.\nApplying PCA to reduce dimensionality...\nPCA applied. Node feature shape: torch.Size([3978302, 12])\nBuilding KDTree and constructing edges (k-NN + Distance Threshold)...\nNumber of edges created: 19888728\n PyTorch Geometric Graph Data saved at: /media/volume1/gnn_graph.pt\n",
  "history_begin_time" : 1740713769658,
  "history_end_time" : 1740713864559,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "9hvLP8JmX1H3",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\nfrom sklearn.decomposition import PCA\r\nfrom torch_geometric.utils import degree \r\n\r\n\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\nchunksize = 500000\r\n\r\nprint(\"Loading dataset...\")\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]  \r\n\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    df_list.append(chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\n\r\ndata = data.drop_duplicates()\r\n\r\nprint(f\"Removed duplicates. New dataset size: {data.shape[0]} rows.\")\r\n\r\nprint(\" Normalizing features using StandardScaler...\")\r\nscaler = StandardScaler()\r\ndata[useful_columns] = scaler.fit_transform(data[useful_columns])  # Normalize features\r\nprint(\"Normalization complete.\")\r\n\r\n#to check how many components to use\r\npca = PCA()\r\npca.fit(data[useful_columns])\r\n\r\n# Print explained variance\r\nexplained_variance = np.cumsum(pca.explained_variance_ratio_)\r\nprint(\"Explained variance by each component:\\n\", explained_variance)\r\n\r\n# Find the number of components that retain 95% variance\r\nn_components_95 = np.argmax(explained_variance >= 0.95) + 1\r\nprint(f\"To retain 95% of variance, we need {n_components_95} components.\")\r\n\r\nprint(\"Applying PCA to reduce dimensionality...\")\r\npca = PCA(n_components=12)  # Reduce to 12 main features\r\nnode_features = pca.fit_transform(data[useful_columns])\r\nnode_features = torch.tensor(node_features, dtype=torch.float)\r\nprint(f\"PCA applied. Node feature shape: {node_features.shape}\")\r\n\r\n#checking for unique rows\r\nprint(\"Unique rows in node features:\", np.unique(node_features.numpy(), axis=0).shape[0])\r\n\r\nprint(\"Building KDTree and constructing edges (k-NN + Distance Threshold)...\")\r\n\r\ncoordinates = data[['lat', 'lon']].values\r\ntree = KDTree(coordinates)\r\n\r\nk = 5\r\nthreshold = 0.0075  # Max connection distance (adjust as needed)\r\n\r\n# Query k-nearest neighbors\r\ndistances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\r\n\r\n# Convert to edge index format with distance filtering\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, k+1):  # Skip first entry (self-node)\r\n        if distances[i][j] < threshold:  # Only add edge if within threshold\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T  # Convert list to NumPy array and transpose\r\n\r\nif edges.shape[1] > 0:\r\n    edge_index = torch.tensor(edges, dtype=torch.long)  # Convert to PyTorch tensor\r\n    print(f\"Number of edges created: {edge_index.shape[1]}\")\r\nelse:\r\n    print(\"No edges were created. Consider increasing `threshold` or `k`.\")\r\n    edge_index = torch.empty((2, 0), dtype=torch.long)  # Empty tensor\r\n\r\ngraph_data = Data(x=node_features, edge_index=edge_index)\r\n\r\n#  checking graph before saving\r\nprint(\"\\n Graph Data Summary:\")\r\nprint(graph_data)  # This prints node & edge count\r\nprint(f\"Number of nodes: {graph_data.num_nodes}\")\r\nprint(f\"Number of edges: {graph_data.num_edges}\")\r\nprint(f\"Node feature shape: {graph_data.x.shape}\")\r\nprint(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n\r\n# Sample a few edges and nodes\r\nprint(\"\\n Sample of Node Features:\")\r\nprint(graph_data.x[:5])  # Show first 5 node features\r\n\r\nprint(\"\\n Sample of Edge Index:\")\r\nprint(graph_data.edge_index[:, :10])  # Show first 10 edges\r\n\r\n\r\n\r\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)  # Compute degrees for each node\r\nprint(f\" Degree stats:\\n\")\r\nprint(f\" - Min degree: {deg.min().item()}\")\r\nprint(f\" - Max degree: {deg.max().item()}\")\r\nprint(f\" - Mean degree: {deg.float().mean().item()}\")\r\nprint(f\" - Standard deviation: {deg.float().std().item()}\")\r\n\r\n# Count isolated nodes\r\nnum_isolated_nodes = (deg == 0).sum().item()\r\nprint(f\" Number of isolated nodes (degree=0): {num_isolated_nodes}\")\r\n\r\n# Percentage of isolated nodes\r\npercentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\r\nprint(f\" Percentage of isolated nodes: {percentage_isolated:.4f}%\")\r\n",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 13 columns.\n Found 2177342 duplicate rows.\nRemoved duplicates. New dataset size: 3978302 rows.\n Normalizing features using StandardScaler...\nNormalization complete.\nExplained variance by each component:\n [0.16404866 0.30675171 0.41844088 0.50969481 0.58747902 0.6587108\n 0.72535687 0.78651539 0.84276239 0.8926543  0.93666506 0.96938215\n 1.        ]\nTo retain 95% of variance, we need 12 components.\nApplying PCA to reduce dimensionality...\nPCA applied. Node feature shape: torch.Size([3978302, 12])\nUnique rows in node features: 3978302\nBuilding KDTree and constructing edges (k-NN + Distance Threshold)...\nNumber of edges created: 19888728\n Graph Data Summary:\nData(x=[3978302, 12], edge_index=[2, 19888728])\nNumber of nodes: 3978302\nNumber of edges: 19888728\nNode feature shape: torch.Size([3978302, 12])\nEdge index shape: torch.Size([2, 19888728])\n Sample of Node Features:\ntensor([[ 1.2711, -5.4217, -1.7512,  0.9761,  0.1306,  0.8428,  0.9132,  0.6987,\n          1.2156,  0.9624, -0.1536,  2.4352],\n        [ 1.3018, -5.4361, -1.7268,  0.9584,  0.1464,  0.8433,  0.9190,  0.6718,\n          1.1903,  0.9781, -0.2140,  2.4628],\n        [ 1.2942, -5.4298, -1.6888,  0.9831,  0.1535,  0.8583,  0.8874,  0.6719,\n          1.1902,  1.0094, -0.1983,  2.4783],\n        [ 0.5409, -3.1068,  0.5584, -2.2972, -0.5476, -0.0588,  1.4304,  0.6040,\n          0.4775,  0.9376,  0.8436,  1.9350],\n        [ 0.3412, -2.8375,  0.7850, -2.7413, -0.4942,  0.0896,  1.6898,  0.0924,\n         -0.4330,  0.7897,  1.4687,  1.8536]])\n Sample of Edge Index:\ntensor([[  1,   2,   3,   3,   3,   3,   3,   4,   4,   4],\n        [  2,   2, 138, 137, 136, 135, 714, 138, 137, 136]])\n Degree stats:\n - Min degree: 0.0\n - Max degree: 5.0\n - Mean degree: 4.999300479888916\n - Standard deviation: 0.04958681762218475\n Number of isolated nodes (degree=0): 189\n Percentage of isolated nodes: 0.0048%\n",
  "history_begin_time" : 1740713337852,
  "history_end_time" : 1740713442822,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "4dOsWIlq1n8v",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\nfrom sklearn.decomposition import PCA\r\nfrom torch_geometric.utils import degree \r\n\r\n\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\nchunksize = 500000\r\n\r\nprint(\"Loading dataset...\")\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]  \r\n\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    df_list.append(chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\n\r\ndata = data.drop_duplicates()\r\n\r\nprint(f\"Removed duplicates. New dataset size: {data.shape[0]} rows.\")\r\n\r\nprint(\" Normalizing features using StandardScaler...\")\r\nscaler = StandardScaler()\r\ndata[useful_columns] = scaler.fit_transform(data[useful_columns])  # Normalize features\r\nprint(\"Normalization complete.\")\r\n\r\n#to check how many components to use\r\npca = PCA()\r\npca.fit(data[useful_columns])\r\n\r\n# Print explained variance\r\nexplained_variance = np.cumsum(pca.explained_variance_ratio_)\r\nprint(\"Explained variance by each component:\\n\", explained_variance)\r\n\r\n# Find the number of components that retain 95% variance\r\nn_components_95 = np.argmax(explained_variance >= 0.95) + 1\r\nprint(f\"To retain 95% of variance, we need {n_components_95} components.\")\r\n\r\nprint(\"Applying PCA to reduce dimensionality...\")\r\npca = PCA(n_components=12)  # Reduce to 12 main features\r\nnode_features = pca.fit_transform(data[useful_columns])\r\nnode_features = torch.tensor(node_features, dtype=torch.float)\r\nprint(f\"PCA applied. Node feature shape: {node_features.shape}\")\r\n\r\n#checking for unique rows\r\nprint(\"Unique rows in node features:\", np.unique(node_features.numpy(), axis=0).shape[0])\r\n\r\nprint(\"Building KDTree and constructing edges (k-NN + Distance Threshold)...\")\r\n\r\ncoordinates = data[['lat', 'lon']].values\r\ntree = KDTree(coordinates)\r\n\r\nk = 5\r\nthreshold = 0.0075  # Max connection distance (adjust as needed)\r\n\r\n# Query k-nearest neighbors\r\ndistances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\r\n\r\n# Convert to edge index format with distance filtering\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, k+1):  # Skip first entry (self-node)\r\n        if distances[i][j] < threshold:  # Only add edge if within threshold\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T  # Convert list to NumPy array and transpose\r\n\r\nif edges.shape[1] > 0:\r\n    edge_index = torch.tensor(edges, dtype=torch.long)  # Convert to PyTorch tensor\r\n    print(f\"Number of edges created: {edge_index.shape[1]}\")\r\nelse:\r\n    print(\"No edges were created. Consider increasing `threshold` or `k`.\")\r\n    edge_index = torch.empty((2, 0), dtype=torch.long)  # Empty tensor\r\n\r\ngraph_data = Data(x=node_features, edge_index=edge_index)\r\n\r\n#  checking graph before saving\r\nprint(\"\\n Graph Data Summary:\")\r\nprint(graph_data)  # This prints node & edge count\r\nprint(f\"Number of nodes: {graph_data.num_nodes}\")\r\nprint(f\"Number of edges: {graph_data.num_edges}\")\r\nprint(f\"Node feature shape: {graph_data.x.shape}\")\r\nprint(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n\r\n# Sample a few edges and nodes\r\nprint(\"\\n Sample of Node Features:\")\r\nprint(graph_data.x[:5])  # Show first 5 node features\r\n\r\nprint(\"\\n Sample of Edge Index:\")\r\nprint(graph_data.edge_index[:, :10])  # Show first 10 edges\r\n\r\n# Count isolated nodes\r\nnum_isolated_nodes = (deg == 0).sum().item()\r\nprint(f\" Number of isolated nodes (degree=0): {num_isolated_nodes}\")\r\n\r\n# Percentage of isolated nodes\r\npercentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\r\nprint(f\" Percentage of isolated nodes: {percentage_isolated:.4f}%\")\r\n\r\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)  # Compute degrees for each node\r\nprint(f\" Degree stats:\\n\")\r\nprint(f\" - Min degree: {deg.min().item()}\")\r\nprint(f\" - Max degree: {deg.max().item()}\")\r\nprint(f\" - Mean degree: {deg.float().mean().item()}\")\r\nprint(f\" - Standard deviation: {deg.float().std().item()}\")\r\n",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 13 columns.\n Found 2177342 duplicate rows.\nRemoved duplicates. New dataset size: 3978302 rows.\n Normalizing features using StandardScaler...\nNormalization complete.\nExplained variance by each component:\n [0.16404866 0.30675171 0.41844088 0.50969481 0.58747902 0.6587108\n 0.72535687 0.78651539 0.84276239 0.8926543  0.93666506 0.96938215\n 1.        ]\nTo retain 95% of variance, we need 12 components.\nApplying PCA to reduce dimensionality...\nPCA applied. Node feature shape: torch.Size([3978302, 12])\nUnique rows in node features: 3978302\nBuilding KDTree and constructing edges (k-NN + Distance Threshold)...\nNumber of edges created: 19888728\n Graph Data Summary:\nData(x=[3978302, 12], edge_index=[2, 19888728])\nNumber of nodes: 3978302\nNumber of edges: 19888728\nNode feature shape: torch.Size([3978302, 12])\nEdge index shape: torch.Size([2, 19888728])\n Sample of Node Features:\ntensor([[ 1.2711, -5.4217, -1.7512,  0.9761,  0.1306,  0.8428,  0.9132,  0.6987,\n          1.2156,  0.9624, -0.1536,  2.4352],\n        [ 1.3018, -5.4361, -1.7268,  0.9584,  0.1464,  0.8433,  0.9190,  0.6718,\n          1.1903,  0.9781, -0.2140,  2.4628],\n        [ 1.2942, -5.4298, -1.6888,  0.9831,  0.1535,  0.8583,  0.8874,  0.6719,\n          1.1902,  1.0094, -0.1983,  2.4783],\n        [ 0.5409, -3.1068,  0.5584, -2.2972, -0.5476, -0.0588,  1.4304,  0.6040,\n          0.4775,  0.9376,  0.8436,  1.9350],\n        [ 0.3412, -2.8375,  0.7850, -2.7413, -0.4942,  0.0896,  1.6898,  0.0924,\n         -0.4330,  0.7897,  1.4687,  1.8536]])\n Sample of Edge Index:\ntensor([[  1,   2,   3,   3,   3,   3,   3,   4,   4,   4],\n        [  2,   2, 138, 137, 136, 135, 714, 138, 137, 136]])\nTraceback (most recent call last):\n  File \"/media/volume1/gw-workspace/4dOsWIlq1n8v/graph_creation.py\", line 110, in <module>\n    num_isolated_nodes = (deg == 0).sum().item()\n                          ^^^\nNameError: name 'deg' is not defined\n",
  "history_begin_time" : 1740713304614,
  "history_end_time" : 1740713407319,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "9pvKEvkEjsgd",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\nfrom sklearn.decomposition import PCA\r\nfrom torch_geometric.utils import degree \r\n\r\n\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\nchunksize = 500000\r\n\r\nprint(\"Loading dataset...\")\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]  \r\n\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    df_list.append(chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\n\r\ndata = data.drop_duplicates()\r\n\r\nprint(f\"Removed duplicates. New dataset size: {data.shape[0]} rows.\")\r\n\r\nprint(\" Normalizing features using StandardScaler...\")\r\nscaler = StandardScaler()\r\ndata[useful_columns] = scaler.fit_transform(data[useful_columns])  # Normalize features\r\nprint(\"Normalization complete.\")\r\n\r\n#to check how many components to use\r\npca = PCA()\r\npca.fit(data[useful_columns])\r\n\r\n# Print explained variance\r\nexplained_variance = np.cumsum(pca.explained_variance_ratio_)\r\nprint(\"Explained variance by each component:\\n\", explained_variance)\r\n\r\n# Find the number of components that retain 95% variance\r\nn_components_95 = np.argmax(explained_variance >= 0.95) + 1\r\nprint(f\"To retain 95% of variance, we need {n_components_95} components.\")\r\n\r\nprint(\"Applying PCA to reduce dimensionality...\")\r\npca = PCA(n_components=12)  # Reduce to 12 main features\r\nnode_features = pca.fit_transform(data[useful_columns])\r\nnode_features = torch.tensor(node_features, dtype=torch.float)\r\nprint(f\"PCA applied. Node feature shape: {node_features.shape}\")\r\n\r\n#checking for unique rows\r\nprint(\"Unique rows in node features:\", np.unique(node_features.numpy(), axis=0).shape[0])\r\n\r\nprint(\"Building KDTree and constructing edges (k-NN + Distance Threshold)...\")\r\n\r\ncoordinates = data[['lat', 'lon']].values\r\ntree = KDTree(coordinates)\r\n\r\nk = 5\r\nthreshold = 0.0075  # Max connection distance (adjust as needed)\r\n\r\n# Query k-nearest neighbors\r\ndistances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\r\n\r\n# Convert to edge index format with distance filtering\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, k+1):  # Skip first entry (self-node)\r\n        if distances[i][j] < threshold:  # Only add edge if within threshold\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T  # Convert list to NumPy array and transpose\r\n\r\nif edges.shape[1] > 0:\r\n    edge_index = torch.tensor(edges, dtype=torch.long)  # Convert to PyTorch tensor\r\n    print(f\"Number of edges created: {edge_index.shape[1]}\")\r\nelse:\r\n    print(\"No edges were created. Consider increasing `threshold` or `k`.\")\r\n    edge_index = torch.empty((2, 0), dtype=torch.long)  # Empty tensor\r\n\r\ngraph_data = Data(x=node_features, edge_index=edge_index)\r\n\r\n#  checking graph before saving\r\nprint(\"\\n Graph Data Summary:\")\r\nprint(graph_data)  # This prints node & edge count\r\nprint(f\"Number of nodes: {graph_data.num_nodes}\")\r\nprint(f\"Number of edges: {graph_data.num_edges}\")\r\nprint(f\"Node feature shape: {graph_data.x.shape}\")\r\nprint(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n\r\n# Sample a few edges and nodes\r\nprint(\"\\n Sample of Node Features:\")\r\nprint(graph_data.x[:5])  # Show first 5 node features\r\n\r\nprint(\"\\n Sample of Edge Index:\")\r\nprint(graph_data.edge_index[:, :10])  # Show first 10 edges\r\n\r\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)  # Compute degrees for each node\r\nprint(f\" Degree stats:\\n\")\r\nprint(f\" - Min degree: {deg.min().item()}\")\r\nprint(f\" - Max degree: {deg.max().item()}\")\r\nprint(f\" - Mean degree: {deg.float().mean().item()}\")\r\nprint(f\" - Standard deviation: {deg.float().std().item()}\")\r\n",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 13 columns.\n Found 2177342 duplicate rows.\nRemoved duplicates. New dataset size: 3978302 rows.\n Normalizing features using StandardScaler...\nNormalization complete.\nExplained variance by each component:\n [0.16404866 0.30675171 0.41844088 0.50969481 0.58747902 0.6587108\n 0.72535687 0.78651539 0.84276239 0.8926543  0.93666506 0.96938215\n 1.        ]\nTo retain 95% of variance, we need 12 components.\nApplying PCA to reduce dimensionality...\nPCA applied. Node feature shape: torch.Size([3978302, 12])\nUnique rows in node features: 3978302\nBuilding KDTree and constructing edges (k-NN + Distance Threshold)...\nNumber of edges created: 19888728\n Graph Data Summary:\nData(x=[3978302, 12], edge_index=[2, 19888728])\nNumber of nodes: 3978302\nNumber of edges: 19888728\nNode feature shape: torch.Size([3978302, 12])\nEdge index shape: torch.Size([2, 19888728])\n Sample of Node Features:\ntensor([[ 1.2711, -5.4217, -1.7512,  0.9761,  0.1306,  0.8428,  0.9132,  0.6987,\n          1.2156,  0.9624, -0.1536,  2.4352],\n        [ 1.3018, -5.4361, -1.7268,  0.9584,  0.1464,  0.8433,  0.9190,  0.6718,\n          1.1903,  0.9781, -0.2140,  2.4628],\n        [ 1.2942, -5.4298, -1.6888,  0.9831,  0.1535,  0.8583,  0.8874,  0.6719,\n          1.1902,  1.0094, -0.1983,  2.4783],\n        [ 0.5409, -3.1068,  0.5584, -2.2972, -0.5476, -0.0588,  1.4304,  0.6040,\n          0.4775,  0.9376,  0.8436,  1.9350],\n        [ 0.3412, -2.8375,  0.7850, -2.7413, -0.4942,  0.0896,  1.6898,  0.0924,\n         -0.4330,  0.7897,  1.4687,  1.8536]])\n Sample of Edge Index:\ntensor([[  1,   2,   3,   3,   3,   3,   3,   4,   4,   4],\n        [  2,   2, 138, 137, 136, 135, 714, 138, 137, 136]])\n Degree stats:\n - Min degree: 0.0\n - Max degree: 5.0\n - Mean degree: 4.999300479888916\n - Standard deviation: 0.04958681762218475\n",
  "history_begin_time" : 1740713088691,
  "history_end_time" : 1740713188139,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "09xl5s8oSBhI",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\nfrom sklearn.decomposition import PCA\r\nfrom torch_geometric.utils import degree \r\n\r\n\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\nchunksize = 500000\r\n\r\nprint(\"Loading dataset...\")\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]  \r\n\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    df_list.append(chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\n\r\ndata = data.drop_duplicates()\r\n\r\nprint(f\"Removed duplicates. New dataset size: {data.shape[0]} rows.\")\r\n\r\nprint(\" Normalizing features using StandardScaler...\")\r\nscaler = StandardScaler()\r\ndata[useful_columns] = scaler.fit_transform(data[useful_columns])  # Normalize features\r\nprint(\"Normalization complete.\")\r\n\r\n#to check how many components to use\r\npca = PCA()\r\npca.fit(data[useful_columns])\r\n\r\n# Print explained variance\r\nexplained_variance = np.cumsum(pca.explained_variance_ratio_)\r\nprint(\"Explained variance by each component:\\n\", explained_variance)\r\n\r\n# Find the number of components that retain 95% variance\r\nn_components_95 = np.argmax(explained_variance >= 0.95) + 1\r\nprint(f\"To retain 95% of variance, we need {n_components_95} components.\")\r\n\r\nprint(\"Applying PCA to reduce dimensionality...\")\r\npca = PCA(n_components=12)  # Reduce to 12 main features\r\nnode_features = pca.fit_transform(data[useful_columns])\r\nnode_features = torch.tensor(node_features, dtype=torch.float)\r\nprint(f\"PCA applied. Node feature shape: {node_features.shape}\")\r\n\r\n#checking for unique rows\r\nprint(\"Unique rows in node features:\", np.unique(node_features.numpy(), axis=0).shape[0])\r\n\r\nprint(\"Building KDTree and constructing edges (k-NN + Distance Threshold)...\")\r\n\r\ncoordinates = data[['lat', 'lon']].values\r\ntree = KDTree(coordinates)\r\n\r\nk = 5\r\nthreshold = 0.005  # Max connection distance (adjust as needed)\r\n\r\n# Query k-nearest neighbors\r\ndistances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\r\n\r\n# Convert to edge index format with distance filtering\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, k+1):  # Skip first entry (self-node)\r\n        if distances[i][j] < threshold:  # Only add edge if within threshold\r\n            edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T  # Convert list to NumPy array and transpose\r\n\r\nif edges.shape[1] > 0:\r\n    edge_index = torch.tensor(edges, dtype=torch.long)  # Convert to PyTorch tensor\r\n    print(f\"Number of edges created: {edge_index.shape[1]}\")\r\nelse:\r\n    print(\"No edges were created. Consider increasing `threshold` or `k`.\")\r\n    edge_index = torch.empty((2, 0), dtype=torch.long)  # Empty tensor\r\n\r\ngraph_data = Data(x=node_features, edge_index=edge_index)\r\n\r\n#  checking graph before saving\r\nprint(\"\\n Graph Data Summary:\")\r\nprint(graph_data)  # This prints node & edge count\r\nprint(f\"Number of nodes: {graph_data.num_nodes}\")\r\nprint(f\"Number of edges: {graph_data.num_edges}\")\r\nprint(f\"Node feature shape: {graph_data.x.shape}\")\r\nprint(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n\r\n# Sample a few edges and nodes\r\nprint(\"\\n Sample of Node Features:\")\r\nprint(graph_data.x[:5])  # Show first 5 node features\r\n\r\nprint(\"\\n Sample of Edge Index:\")\r\nprint(graph_data.edge_index[:, :10])  # Show first 10 edges\r\n\r\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)  # Compute degrees for each node\r\nprint(f\" Degree stats:\\n\")\r\nprint(f\" - Min degree: {deg.min().item()}\")\r\nprint(f\" - Max degree: {deg.max().item()}\")\r\nprint(f\" - Mean degree: {deg.float().mean().item()}\")\r\nprint(f\" - Standard deviation: {deg.float().std().item()}\")\r\n",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 13 columns.\n Found 2177342 duplicate rows.\nRemoved duplicates. New dataset size: 3978302 rows.\n Normalizing features using StandardScaler...\nNormalization complete.\nExplained variance by each component:\n [0.16404866 0.30675171 0.41844088 0.50969481 0.58747902 0.6587108\n 0.72535687 0.78651539 0.84276239 0.8926543  0.93666506 0.96938215\n 1.        ]\nTo retain 95% of variance, we need 12 components.\nApplying PCA to reduce dimensionality...\nPCA applied. Node feature shape: torch.Size([3978302, 12])\nUnique rows in node features: 3978302\nBuilding KDTree and constructing edges (k-NN + Distance Threshold)...\nNumber of edges created: 19888426\n Graph Data Summary:\nData(x=[3978302, 12], edge_index=[2, 19888426])\nNumber of nodes: 3978302\nNumber of edges: 19888426\nNode feature shape: torch.Size([3978302, 12])\nEdge index shape: torch.Size([2, 19888426])\n Sample of Node Features:\ntensor([[ 1.2711, -5.4217, -1.7512,  0.9761,  0.1306,  0.8428,  0.9132,  0.6987,\n          1.2156,  0.9624, -0.1536,  2.4352],\n        [ 1.3018, -5.4361, -1.7268,  0.9584,  0.1464,  0.8433,  0.9190,  0.6718,\n          1.1903,  0.9781, -0.2140,  2.4628],\n        [ 1.2942, -5.4298, -1.6888,  0.9831,  0.1535,  0.8583,  0.8874,  0.6719,\n          1.1902,  1.0094, -0.1983,  2.4783],\n        [ 0.5409, -3.1068,  0.5584, -2.2972, -0.5476, -0.0588,  1.4304,  0.6040,\n          0.4775,  0.9376,  0.8436,  1.9350],\n        [ 0.3412, -2.8375,  0.7850, -2.7413, -0.4942,  0.0896,  1.6898,  0.0924,\n         -0.4330,  0.7897,  1.4687,  1.8536]])\n Sample of Edge Index:\ntensor([[  1,   2,   3,   3,   3,   3,   3,   4,   4,   4],\n        [  2,   2, 138, 137, 136, 135, 714, 138, 137, 136]])\n Degree stats:\n - Min degree: 0.0\n - Max degree: 5.0\n - Mean degree: 4.99922513961792\n - Standard deviation: 0.052126213908195496\n",
  "history_begin_time" : 1740712387601,
  "history_end_time" : 1740712487196,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "VWQ4bwvcXgG3",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\nfrom sklearn.decomposition import PCA\r\nfrom torch_geometric.utils import degree \r\n\r\n\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\nchunksize = 500000\r\n\r\nprint(\"Loading dataset...\")\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]  \r\n\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    df_list.append(chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\n\r\ndata = data.drop_duplicates()\r\n\r\nprint(f\"Removed duplicates. New dataset size: {data.shape[0]} rows.\")\r\n\r\nprint(\" Normalizing features using StandardScaler...\")\r\nscaler = StandardScaler()\r\ndata[useful_columns] = scaler.fit_transform(data[useful_columns])  # Normalize features\r\nprint(\"Normalization complete.\")\r\n\r\n#to check how many components to use\r\npca = PCA()\r\npca.fit(data[useful_columns])\r\n\r\n# Print explained variance\r\nexplained_variance = np.cumsum(pca.explained_variance_ratio_)\r\nprint(\"Explained variance by each component:\\n\", explained_variance)\r\n\r\n# Find the number of components that retain 95% variance\r\nn_components_95 = np.argmax(explained_variance >= 0.95) + 1\r\nprint(f\"To retain 95% of variance, we need {n_components_95} components.\")\r\n\r\nprint(\"Applying PCA to reduce dimensionality...\")\r\npca = PCA(n_components=12)  # Reduce to 12 main features\r\nnode_features = pca.fit_transform(data[useful_columns])\r\nnode_features = torch.tensor(node_features, dtype=torch.float)\r\nprint(f\"PCA applied. Node feature shape: {node_features.shape}\")\r\n\r\n#checking for unique rows\r\nprint(\"Unique rows in node features:\", np.unique(node_features.numpy(), axis=0).shape[0])\r\n\r\nprint(\"Building KDTree and constructing edges (k-NN approach)...\")\r\n\r\ncoordinates = data[['lat', 'lon']].values\r\ntree = KDTree(coordinates)\r\n\r\nk = 5  # Each node connects to its 5 nearest neighbors\r\ndistances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\r\n\r\n# Convert to edge index format\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, k+1):  # Skip first entry (self-node)\r\n        edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T  # Convert list to NumPy array and transpose\r\n\r\nif edges.shape[1] > 0:\r\n    edge_index = torch.tensor(edges, dtype=torch.long)  # Convert to PyTorch tensor\r\n    print(f\"Number of edges created: {edge_index.shape[1]}\")\r\nelse:\r\n    print(\"No edges were created. Consider increasing `k`.\")\r\n    edge_index = torch.empty((2, 0), dtype=torch.long)  \r\n\r\n\r\ngraph_data = Data(x=node_features, edge_index=edge_index)\r\n\r\n#  checking graph before saving\r\nprint(\"\\n Graph Data Summary:\")\r\nprint(graph_data)  # This prints node & edge count\r\nprint(f\"Number of nodes: {graph_data.num_nodes}\")\r\nprint(f\"Number of edges: {graph_data.num_edges}\")\r\nprint(f\"Node feature shape: {graph_data.x.shape}\")\r\nprint(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n\r\n# Sample a few edges and nodes\r\nprint(\"\\n Sample of Node Features:\")\r\nprint(graph_data.x[:5])  # Show first 5 node features\r\n\r\nprint(\"\\n Sample of Edge Index:\")\r\nprint(graph_data.edge_index[:, :10])  # Show first 10 edges\r\n\r\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)  # Compute degrees for each node\r\nprint(f\" Degree stats:\\n\")\r\nprint(f\" - Min degree: {deg.min().item()}\")\r\nprint(f\" - Max degree: {deg.max().item()}\")\r\nprint(f\" - Mean degree: {deg.float().mean().item()}\")\r\nprint(f\" - Standard deviation: {deg.float().std().item()}\")\r\n",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 13 columns.\n Found 2177342 duplicate rows.\nRemoved duplicates. New dataset size: 3978302 rows.\n Normalizing features using StandardScaler...\nNormalization complete.\nExplained variance by each component:\n [0.16404866 0.30675171 0.41844088 0.50969481 0.58747902 0.6587108\n 0.72535687 0.78651539 0.84276239 0.8926543  0.93666506 0.96938215\n 1.        ]\nTo retain 95% of variance, we need 12 components.\nApplying PCA to reduce dimensionality...\nPCA applied. Node feature shape: torch.Size([3978302, 12])\nUnique rows in node features: 3978302\nBuilding KDTree and constructing edges (k-NN approach)...\nNumber of edges created: 19891510\n Graph Data Summary:\nData(x=[3978302, 12], edge_index=[2, 19891510])\nNumber of nodes: 3978302\nNumber of edges: 19891510\nNode feature shape: torch.Size([3978302, 12])\nEdge index shape: torch.Size([2, 19891510])\n Sample of Node Features:\ntensor([[ 1.2711, -5.4217, -1.7512,  0.9761,  0.1306,  0.8428,  0.9132,  0.6987,\n          1.2156,  0.9624, -0.1536,  2.4352],\n        [ 1.3018, -5.4361, -1.7268,  0.9584,  0.1464,  0.8433,  0.9190,  0.6718,\n          1.1903,  0.9781, -0.2140,  2.4628],\n        [ 1.2942, -5.4298, -1.6888,  0.9831,  0.1535,  0.8583,  0.8874,  0.6719,\n          1.1902,  1.0094, -0.1983,  2.4783],\n        [ 0.5409, -3.1068,  0.5584, -2.2972, -0.5476, -0.0588,  1.4304,  0.6040,\n          0.4775,  0.9376,  0.8436,  1.9350],\n        [ 0.3412, -2.8375,  0.7850, -2.7413, -0.4942,  0.0896,  1.6898,  0.0924,\n         -0.4330,  0.7897,  1.4687,  1.8536]])\n Sample of Edge Index:\ntensor([[   0,    0,    0,    0,    0,    1,    1,    1,    1,    1],\n        [2058, 1944, 1943, 2056, 2057,    2, 2060, 2063, 2062, 2059]])\n Degree stats:\n - Min degree: 5.0\n - Max degree: 5.0\n - Mean degree: 5.000000476837158\n - Standard deviation: 4.768372150465439e-07\n",
  "history_begin_time" : 1740711942396,
  "history_end_time" : 1740712036814,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "B7593NJSZdc3",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\nfrom sklearn.decomposition import PCA\r\nfrom torch_geometric.utils import degree \r\n\r\n\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\nchunksize = 500000\r\n\r\nprint(\"Loading dataset...\")\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]  \r\n\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    df_list.append(chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\n\r\ndata = data.drop_duplicates()\r\n\r\nprint(f\"Removed duplicates. New dataset size: {data.shape[0]} rows.\")\r\n\r\nprint(\" Normalizing features using StandardScaler...\")\r\nscaler = StandardScaler()\r\ndata[useful_columns] = scaler.fit_transform(data[useful_columns])  # Normalize features\r\nprint(\"Normalization complete.\")\r\n\r\n#to check how many components to use\r\npca = PCA()\r\npca.fit(data[useful_columns])\r\n\r\n# Print explained variance\r\nexplained_variance = np.cumsum(pca.explained_variance_ratio_)\r\nprint(\"Explained variance by each component:\\n\", explained_variance)\r\n\r\n# Find the number of components that retain 95% variance\r\nn_components_95 = np.argmax(explained_variance >= 0.95) + 1\r\nprint(f\"To retain 95% of variance, we need {n_components_95} components.\")\r\n\r\nprint(\"Applying PCA to reduce dimensionality...\")\r\npca = PCA(n_components=11)  # Reduce to 11 main features\r\nnode_features = pca.fit_transform(data[useful_columns])\r\nnode_features = torch.tensor(node_features, dtype=torch.float)\r\nprint(f\"PCA applied. Node feature shape: {node_features.shape}\")\r\n\r\n#checking for unique rows\r\nprint(\"Unique rows in node features:\", np.unique(node_features.numpy(), axis=0).shape[0])\r\n\r\nprint(\"Building KDTree and constructing edges (k-NN approach)...\")\r\n\r\ncoordinates = data[['lat', 'lon']].values\r\ntree = KDTree(coordinates)\r\n\r\nk = 5  # Each node connects to its 5 nearest neighbors\r\ndistances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\r\n\r\n# Convert to edge index format\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, k+1):  # Skip first entry (self-node)\r\n        edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T  # Convert list to NumPy array and transpose\r\n\r\nif edges.shape[1] > 0:\r\n    edge_index = torch.tensor(edges, dtype=torch.long)  # Convert to PyTorch tensor\r\n    print(f\"Number of edges created: {edge_index.shape[1]}\")\r\nelse:\r\n    print(\"No edges were created. Consider increasing `k`.\")\r\n    edge_index = torch.empty((2, 0), dtype=torch.long)  \r\n\r\n\r\ngraph_data = Data(x=node_features, edge_index=edge_index)\r\n\r\n#  checking graph before saving\r\nprint(\"\\n Graph Data Summary:\")\r\nprint(graph_data)  # This prints node & edge count\r\nprint(f\"Number of nodes: {graph_data.num_nodes}\")\r\nprint(f\"Number of edges: {graph_data.num_edges}\")\r\nprint(f\"Node feature shape: {graph_data.x.shape}\")\r\nprint(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n\r\n# Sample a few edges and nodes\r\nprint(\"\\n Sample of Node Features:\")\r\nprint(graph_data.x[:5])  # Show first 5 node features\r\n\r\nprint(\"\\n Sample of Edge Index:\")\r\nprint(graph_data.edge_index[:, :10])  # Show first 10 edges\r\n\r\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)  # Compute degrees for each node\r\nprint(f\" Degree stats:\\n\")\r\nprint(f\" - Min degree: {deg.min().item()}\")\r\nprint(f\" - Max degree: {deg.max().item()}\")\r\nprint(f\" - Mean degree: {deg.float().mean().item()}\")\r\nprint(f\" - Standard deviation: {deg.float().std().item()}\")\r\n",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 13 columns.\n Found 2177342 duplicate rows.\nRemoved duplicates. New dataset size: 3978302 rows.\n Normalizing features using StandardScaler...\nNormalization complete.\nExplained variance by each component:\n [0.16404866 0.30675171 0.41844088 0.50969481 0.58747902 0.6587108\n 0.72535687 0.78651539 0.84276239 0.8926543  0.93666506 0.96938215\n 1.        ]\nTo retain 95% of variance, we need 12 components.\nApplying PCA to reduce dimensionality...\nPCA applied. Node feature shape: torch.Size([3978302, 11])\nUnique rows in node features: 3978302\nBuilding KDTree and constructing edges (k-NN approach)...\nNumber of edges created: 19891510\n Graph Data Summary:\nData(x=[3978302, 11], edge_index=[2, 19891510])\nNumber of nodes: 3978302\nNumber of edges: 19891510\nNode feature shape: torch.Size([3978302, 11])\nEdge index shape: torch.Size([2, 19891510])\n Sample of Node Features:\ntensor([[ 1.2711, -5.4217, -1.7512,  0.9761,  0.1306,  0.8428,  0.9132,  0.6987,\n          1.2156,  0.9624, -0.1536],\n        [ 1.3018, -5.4361, -1.7268,  0.9584,  0.1464,  0.8433,  0.9190,  0.6718,\n          1.1903,  0.9781, -0.2140],\n        [ 1.2942, -5.4298, -1.6888,  0.9831,  0.1535,  0.8583,  0.8874,  0.6719,\n          1.1902,  1.0094, -0.1983],\n        [ 0.5409, -3.1068,  0.5584, -2.2972, -0.5476, -0.0588,  1.4304,  0.6040,\n          0.4775,  0.9376,  0.8436],\n        [ 0.3412, -2.8375,  0.7850, -2.7413, -0.4942,  0.0896,  1.6898,  0.0924,\n         -0.4330,  0.7897,  1.4687]])\n Sample of Edge Index:\ntensor([[   0,    0,    0,    0,    0,    1,    1,    1,    1,    1],\n        [2058, 1944, 1943, 2056, 2057,    2, 2060, 2063, 2062, 2059]])\n Degree stats:\n - Min degree: 5.0\n - Max degree: 5.0\n - Mean degree: 5.000000476837158\n - Standard deviation: 4.768372150465439e-07\n",
  "history_begin_time" : 1740711878809,
  "history_end_time" : 1740711973241,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "m77FmfZc8nI8",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\nfrom sklearn.decomposition import PCA\r\nfrom torch_geometric.utils import degree \r\n\r\n\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\nchunksize = 500000\r\n\r\nprint(\"Loading dataset...\")\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]  \r\n\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    df_list.append(chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\n\r\ndata = data.drop_duplicates()\r\n\r\nprint(f\"Removed duplicates. New dataset size: {data.shape[0]} rows.\")\r\n\r\n# print(\" Normalizing features using StandardScaler...\")\r\n# scaler = StandardScaler()\r\n# data[useful_columns] = scaler.fit_transform(data[useful_columns])  # Normalize features\r\n# print(\"Normalization complete.\")\r\n\r\n# #to check how many components to use\r\n# pca = PCA()\r\n# pca.fit(data[useful_columns])\r\n\r\n# # Print explained variance\r\n# explained_variance = np.cumsum(pca.explained_variance_ratio_)\r\n# print(\"Explained variance by each component:\\n\", explained_variance)\r\n\r\n# # Find the number of components that retain 95% variance\r\n# n_components_95 = np.argmax(explained_variance >= 0.95) + 1\r\n# print(f\"To retain 95% of variance, we need {n_components_95} components.\")\r\n\r\n# print(\"Applying PCA to reduce dimensionality...\")\r\n# pca = PCA(n_components=11)  # Reduce to 11 main features\r\n# node_features = pca.fit_transform(data[useful_columns])\r\n# node_features = torch.tensor(node_features, dtype=torch.float)\r\n# print(f\"PCA applied. Node feature shape: {node_features.shape}\")\r\n\r\n# #checking for unique rows\r\n# print(\"Unique rows in node features:\", np.unique(node_features.numpy(), axis=0).shape[0])\r\n\r\n# print(\"Building KDTree and constructing edges (k-NN approach)...\")\r\n\r\n# coordinates = data[['lat', 'lon']].values\r\n# tree = KDTree(coordinates)\r\n\r\n# k = 5  # Each node connects to its 5 nearest neighbors\r\n# distances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\r\n\r\n# # Convert to edge index format\r\n# edge_list = []\r\n# for i in range(len(neighbors)):\r\n#     for j in range(1, k+1):  # Skip first entry (self-node)\r\n#         edge_list.append((i, neighbors[i][j]))\r\n\r\n# edges = np.array(edge_list).T  # Convert list to NumPy array and transpose\r\n\r\n# if edges.shape[1] > 0:\r\n#     edge_index = torch.tensor(edges, dtype=torch.long)  # Convert to PyTorch tensor\r\n#     print(f\"Number of edges created: {edge_index.shape[1]}\")\r\n# else:\r\n#     print(\"No edges were created. Consider increasing `k`.\")\r\n#     edge_index = torch.empty((2, 0), dtype=torch.long)  \r\n\r\n\r\n# graph_data = Data(x=node_features, edge_index=edge_index)\r\n\r\n# #  checking graph before saving\r\n# print(\"\\n Graph Data Summary:\")\r\n# print(graph_data)  # This prints node & edge count\r\n# print(f\"Number of nodes: {graph_data.num_nodes}\")\r\n# print(f\"Number of edges: {graph_data.num_edges}\")\r\n# print(f\"Node feature shape: {graph_data.x.shape}\")\r\n# print(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n\r\n# # Sample a few edges and nodes\r\n# print(\"\\n Sample of Node Features:\")\r\n# print(graph_data.x[:5])  # Show first 5 node features\r\n\r\n# print(\"\\n Sample of Edge Index:\")\r\n# print(graph_data.edge_index[:, :10])  # Show first 10 edges\r\n\r\n# deg = degree(edge_index[0], num_nodes=graph_data.num_nodes)  # Compute degrees for each node\r\n# print(f\" Degree stats:\\n\")\r\n# print(f\" - Min degree: {deg.min().item()}\")\r\n# print(f\" - Max degree: {deg.max().item()}\")\r\n# print(f\" - Mean degree: {deg.float().mean().item()}\")\r\n# print(f\" - Standard deviation: {deg.float().std().item()}\")\r\n",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 13 columns.\n Found 2177342 duplicate rows.\nRemoved duplicates. New dataset size: 3978302 rows.\n",
  "history_begin_time" : 1740711796736,
  "history_end_time" : 1740711833557,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "97HRt2mlQl44",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\nfrom sklearn.decomposition import PCA\r\nfrom torch_geometric.utils import degree \r\n\r\n\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\nchunksize = 500000\r\n\r\nprint(\"Loading dataset...\")\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]  \r\n\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    df_list.append(chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\nnum_duplicates = data.duplicated().sum()\r\nprint(f\" Found {num_duplicates} duplicate rows.\")\r\n\r\n# print(\" Normalizing features using StandardScaler...\")\r\n# scaler = StandardScaler()\r\n# data[useful_columns] = scaler.fit_transform(data[useful_columns])  # Normalize features\r\n# print(\"Normalization complete.\")\r\n\r\n# #to check how many components to use\r\n# pca = PCA()\r\n# pca.fit(data[useful_columns])\r\n\r\n# # Print explained variance\r\n# explained_variance = np.cumsum(pca.explained_variance_ratio_)\r\n# print(\"Explained variance by each component:\\n\", explained_variance)\r\n\r\n# # Find the number of components that retain 95% variance\r\n# n_components_95 = np.argmax(explained_variance >= 0.95) + 1\r\n# print(f\"To retain 95% of variance, we need {n_components_95} components.\")\r\n\r\n# print(\"Applying PCA to reduce dimensionality...\")\r\n# pca = PCA(n_components=11)  # Reduce to 11 main features\r\n# node_features = pca.fit_transform(data[useful_columns])\r\n# node_features = torch.tensor(node_features, dtype=torch.float)\r\n# print(f\"PCA applied. Node feature shape: {node_features.shape}\")\r\n\r\n# #checking for unique rows\r\n# print(\"Unique rows in node features:\", np.unique(node_features.numpy(), axis=0).shape[0])\r\n\r\n# print(\"Building KDTree and constructing edges (k-NN approach)...\")\r\n\r\n# coordinates = data[['lat', 'lon']].values\r\n# tree = KDTree(coordinates)\r\n\r\n# k = 5  # Each node connects to its 5 nearest neighbors\r\n# distances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\r\n\r\n# # Convert to edge index format\r\n# edge_list = []\r\n# for i in range(len(neighbors)):\r\n#     for j in range(1, k+1):  # Skip first entry (self-node)\r\n#         edge_list.append((i, neighbors[i][j]))\r\n\r\n# edges = np.array(edge_list).T  # Convert list to NumPy array and transpose\r\n\r\n# if edges.shape[1] > 0:\r\n#     edge_index = torch.tensor(edges, dtype=torch.long)  # Convert to PyTorch tensor\r\n#     print(f\"Number of edges created: {edge_index.shape[1]}\")\r\n# else:\r\n#     print(\"No edges were created. Consider increasing `k`.\")\r\n#     edge_index = torch.empty((2, 0), dtype=torch.long)  \r\n\r\n\r\n# graph_data = Data(x=node_features, edge_index=edge_index)\r\n\r\n# #  checking graph before saving\r\n# print(\"\\n Graph Data Summary:\")\r\n# print(graph_data)  # This prints node & edge count\r\n# print(f\"Number of nodes: {graph_data.num_nodes}\")\r\n# print(f\"Number of edges: {graph_data.num_edges}\")\r\n# print(f\"Node feature shape: {graph_data.x.shape}\")\r\n# print(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n\r\n# # Sample a few edges and nodes\r\n# print(\"\\n Sample of Node Features:\")\r\n# print(graph_data.x[:5])  # Show first 5 node features\r\n\r\n# print(\"\\n Sample of Edge Index:\")\r\n# print(graph_data.edge_index[:, :10])  # Show first 10 edges\r\n\r\n# deg = degree(edge_index[0], num_nodes=graph_data.num_nodes)  # Compute degrees for each node\r\n# print(f\" Degree stats:\\n\")\r\n# print(f\" - Min degree: {deg.min().item()}\")\r\n# print(f\" - Max degree: {deg.max().item()}\")\r\n# print(f\" - Mean degree: {deg.float().mean().item()}\")\r\n# print(f\" - Standard deviation: {deg.float().std().item()}\")\r\n",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 13 columns.\n Found 2177342 duplicate rows.\n",
  "history_begin_time" : 1740711710760,
  "history_end_time" : 1740711743977,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "PLxHElRrO2df",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\nfrom sklearn.decomposition import PCA\r\nfrom torch_geometric.utils import degree \r\n\r\n\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\nchunksize = 500000\r\n\r\nprint(\"Loading dataset...\")\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]  \r\n\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    df_list.append(chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\nprint(\" Normalizing features using StandardScaler...\")\r\nscaler = StandardScaler()\r\ndata[useful_columns] = scaler.fit_transform(data[useful_columns])  # Normalize features\r\nprint(\"Normalization complete.\")\r\n\r\n#to check how many components to use\r\npca = PCA()\r\npca.fit(data[useful_columns])\r\n\r\n# Print explained variance\r\nexplained_variance = np.cumsum(pca.explained_variance_ratio_)\r\nprint(\"Explained variance by each component:\\n\", explained_variance)\r\n\r\n# Find the number of components that retain 95% variance\r\nn_components_95 = np.argmax(explained_variance >= 0.95) + 1\r\nprint(f\"To retain 95% of variance, we need {n_components_95} components.\")\r\n\r\nprint(\"Applying PCA to reduce dimensionality...\")\r\npca = PCA(n_components=11)  # Reduce to 11 main features\r\nnode_features = pca.fit_transform(data[useful_columns])\r\nnode_features = torch.tensor(node_features, dtype=torch.float)\r\nprint(f\"PCA applied. Node feature shape: {node_features.shape}\")\r\n\r\n#checking for unique rows\r\nprint(\"Unique rows in node features:\", np.unique(node_features.numpy(), axis=0).shape[0])\r\n\r\nprint(\"Building KDTree and constructing edges (k-NN approach)...\")\r\n\r\ncoordinates = data[['lat', 'lon']].values\r\ntree = KDTree(coordinates)\r\n\r\nk = 5  # Each node connects to its 5 nearest neighbors\r\ndistances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\r\n\r\n# Convert to edge index format\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, k+1):  # Skip first entry (self-node)\r\n        edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T  # Convert list to NumPy array and transpose\r\n\r\nif edges.shape[1] > 0:\r\n    edge_index = torch.tensor(edges, dtype=torch.long)  # Convert to PyTorch tensor\r\n    print(f\"Number of edges created: {edge_index.shape[1]}\")\r\nelse:\r\n    print(\"No edges were created. Consider increasing `k`.\")\r\n    edge_index = torch.empty((2, 0), dtype=torch.long)  \r\n\r\n\r\ngraph_data = Data(x=node_features, edge_index=edge_index)\r\n\r\n#  checking graph before saving\r\nprint(\"\\n Graph Data Summary:\")\r\nprint(graph_data)  # This prints node & edge count\r\nprint(f\"Number of nodes: {graph_data.num_nodes}\")\r\nprint(f\"Number of edges: {graph_data.num_edges}\")\r\nprint(f\"Node feature shape: {graph_data.x.shape}\")\r\nprint(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n\r\n# Sample a few edges and nodes\r\nprint(\"\\n Sample of Node Features:\")\r\nprint(graph_data.x[:5])  # Show first 5 node features\r\n\r\nprint(\"\\n Sample of Edge Index:\")\r\nprint(graph_data.edge_index[:, :10])  # Show first 10 edges\r\n\r\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)  # Compute degrees for each node\r\nprint(f\" Degree stats:\\n\")\r\nprint(f\" - Min degree: {deg.min().item()}\")\r\nprint(f\" - Max degree: {deg.max().item()}\")\r\nprint(f\" - Mean degree: {deg.float().mean().item()}\")\r\nprint(f\" - Standard deviation: {deg.float().std().item()}\")\r\n",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 13 columns.\n Normalizing features using StandardScaler...\nNormalization complete.\nExplained variance by each component:\n [0.33380011 0.45763892 0.56118322 0.63908558 0.71537133 0.78364688\n 0.8437696  0.88881181 0.91960394 0.94710491 0.96973549 0.98674311\n 1.        ]\nTo retain 95% of variance, we need 11 components.\nApplying PCA to reduce dimensionality...\nPCA applied. Node feature shape: torch.Size([6155644, 11])\nUnique rows in node features: 3978302\nBuilding KDTree and constructing edges (k-NN approach)...\nNumber of edges created: 30778220\n Graph Data Summary:\nData(x=[6155644, 11], edge_index=[2, 30778220])\nNumber of nodes: 6155644\nNumber of edges: 30778220\nNode feature shape: torch.Size([6155644, 11])\nEdge index shape: torch.Size([2, 30778220])\n Sample of Node Features:\ntensor([[-2.8888, -1.7301, -1.8027, -0.2331,  0.1632,  0.4564,  0.3739,  0.9708,\n          0.5142, -0.3787,  0.2449],\n        [-2.8888, -1.7301, -1.8027, -0.2331,  0.1632,  0.4564,  0.3739,  0.9708,\n          0.5142, -0.3787,  0.2449],\n        [-2.8888, -1.7301, -1.8027, -0.2331,  0.1632,  0.4564,  0.3739,  0.9708,\n          0.5142, -0.3787,  0.2449],\n        [-2.8888, -1.7301, -1.8027, -0.2331,  0.1632,  0.4564,  0.3739,  0.9708,\n          0.5142, -0.3787,  0.2449],\n        [-2.8888, -1.7301, -1.8027, -0.2331,  0.1632,  0.4564,  0.3739,  0.9708,\n          0.5142, -0.3787,  0.2449]])\n Sample of Edge Index:\ntensor([[  0,   0,   0,   0,   0,   1,   1,   1,   1,   1],\n        [357, 358, 359, 360, 355, 357, 358, 359, 360, 355]])\n Degree stats:\n - Min degree: 5.0\n - Max degree: 5.0\n - Mean degree: 4.999999523162842\n - Standard deviation: 4.768372150465439e-07\n",
  "history_begin_time" : 1740711225873,
  "history_end_time" : 1740711352660,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "c9XJWwetI4tJ",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\nfrom sklearn.decomposition import PCA\r\nfrom torch_geometric.utils import degree \r\n\r\n\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\nchunksize = 500000\r\n\r\nprint(\"Loading dataset...\")\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]  \r\n\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    df_list.append(chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\nprint(\" Normalizing features using StandardScaler...\")\r\nscaler = StandardScaler()\r\ndata[useful_columns] = scaler.fit_transform(data[useful_columns])  # Normalize features\r\nprint(\"Normalization complete.\")\r\n\r\n#to check how many components to use\r\npca = PCA()\r\npca.fit(data[useful_columns])\r\n\r\n# Print explained variance\r\nexplained_variance = np.cumsum(pca.explained_variance_ratio_)\r\nprint(\"Explained variance by each component:\\n\", explained_variance)\r\n\r\n# Find the number of components that retain 95% variance\r\nn_components_95 = np.argmax(explained_variance >= 0.95) + 1\r\nprint(f\"To retain 95% of variance, we need {n_components_95} components.\")\r\n\r\nprint(\"Applying PCA to reduce dimensionality...\")\r\npca = PCA(n_components=11)  # Reduce to 11 main features\r\nnode_features = pca.fit_transform(data[useful_columns])\r\nnode_features = torch.tensor(node_features, dtype=torch.float)\r\nprint(f\"PCA applied. Node feature shape: {node_features.shape}\")\r\n\r\n#checking for unique rows\r\nprint(\"Unique rows in node features:\", np.unique(node_features.numpy(), axis=0).shape[0])\r\n\r\nprint(\"Building KDTree and constructing edges (k-NN approach)...\")\r\n\r\ncoordinates = data[['lat', 'lon']].values\r\ntree = KDTree(coordinates)\r\n\r\nk = 5  # Each node connects to its 5 nearest neighbors\r\ndistances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\r\n\r\n# Convert to edge index format\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, k+1):  # Skip first entry (self-node)\r\n        edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T  # Convert list to NumPy array and transpose\r\n\r\nif edges.shape[1] > 0:\r\n    edge_index = torch.tensor(edges, dtype=torch.long)  # Convert to PyTorch tensor\r\n    print(f\"Number of edges created: {edge_index.shape[1]}\")\r\nelse:\r\n    print(\"No edges were created. Consider increasing `k`.\")\r\n    edge_index = torch.empty((2, 0), dtype=torch.long)  \r\n\r\n\r\ngraph_data = Data(x=node_features, edge_index=edge_index)\r\n\r\n#  checking graph before saving\r\nprint(\"\\n Graph Data Summary:\")\r\nprint(graph_data)  # This prints node & edge count\r\nprint(f\"Number of nodes: {graph_data.num_nodes}\")\r\nprint(f\"Number of edges: {graph_data.num_edges}\")\r\nprint(f\"Node feature shape: {graph_data.x.shape}\")\r\nprint(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n\r\n# Sample a few edges and nodes\r\nprint(\"\\n Sample of Node Features:\")\r\nprint(graph_data.x[:5])  # Show first 5 node features\r\n\r\nprint(\"\\n Sample of Edge Index:\")\r\nprint(graph_data.edge_index[:, :10])  # Show first 10 edges\r\n\r\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)  # Compute degrees for each node\r\nprint(\"Degree stats:\\n\", deg.describe()) \r\n",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 13 columns.\n Normalizing features using StandardScaler...\nNormalization complete.\nExplained variance by each component:\n [0.33380011 0.45763892 0.56118322 0.63908558 0.71537133 0.78364688\n 0.8437696  0.88881181 0.91960394 0.94710491 0.96973549 0.98674311\n 1.        ]\nTo retain 95% of variance, we need 11 components.\nApplying PCA to reduce dimensionality...\nPCA applied. Node feature shape: torch.Size([6155644, 11])\nUnique rows in node features: 3978302\nBuilding KDTree and constructing edges (k-NN approach)...\nNumber of edges created: 30778220\n Graph Data Summary:\nData(x=[6155644, 11], edge_index=[2, 30778220])\nNumber of nodes: 6155644\nNumber of edges: 30778220\nNode feature shape: torch.Size([6155644, 11])\nEdge index shape: torch.Size([2, 30778220])\n Sample of Node Features:\ntensor([[-2.8888, -1.7301, -1.8027, -0.2331,  0.1632,  0.4564,  0.3739,  0.9708,\n          0.5142, -0.3787,  0.2449],\n        [-2.8888, -1.7301, -1.8027, -0.2331,  0.1632,  0.4564,  0.3739,  0.9708,\n          0.5142, -0.3787,  0.2449],\n        [-2.8888, -1.7301, -1.8027, -0.2331,  0.1632,  0.4564,  0.3739,  0.9708,\n          0.5142, -0.3787,  0.2449],\n        [-2.8888, -1.7301, -1.8027, -0.2331,  0.1632,  0.4564,  0.3739,  0.9708,\n          0.5142, -0.3787,  0.2449],\n        [-2.8888, -1.7301, -1.8027, -0.2331,  0.1632,  0.4564,  0.3739,  0.9708,\n          0.5142, -0.3787,  0.2449]])\n Sample of Edge Index:\ntensor([[  0,   0,   0,   0,   0,   1,   1,   1,   1,   1],\n        [357, 358, 359, 360, 355, 357, 358, 359, 360, 355]])\nTraceback (most recent call last):\n  File \"/media/volume1/gw-workspace/c9XJWwetI4tJ/graph_creation.py\", line 100, in <module>\n    print(\"Degree stats:\\n\", deg.describe()) \n                             ^^^^^^^^^^^^\nAttributeError: 'Tensor' object has no attribute 'describe'\n",
  "history_begin_time" : 1740711019094,
  "history_end_time" : 1740711144907,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "4LCJZugdmRE9",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\nfrom sklearn.decomposition import PCA\r\nfrom torch_geometric.utils import degree \r\n\r\n\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\nchunksize = 500000\r\n\r\nprint(\"Loading dataset...\")\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]  \r\n\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    df_list.append(chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\nprint(\" Normalizing features using StandardScaler...\")\r\nscaler = StandardScaler()\r\ndata[useful_columns] = scaler.fit_transform(data[useful_columns])  # Normalize features\r\nprint(\"Normalization complete.\")\r\n\r\n#to check how many components to use\r\npca = PCA()\r\npca.fit(data[useful_columns])\r\n\r\n# Print explained variance\r\nexplained_variance = np.cumsum(pca.explained_variance_ratio_)\r\nprint(\"Explained variance by each component:\\n\", explained_variance)\r\n\r\n# Find the number of components that retain 95% variance\r\nn_components_95 = np.argmax(explained_variance >= 0.95) + 1\r\nprint(f\"To retain 95% of variance, we need {n_components_95} components.\")\r\n\r\nprint(\"Applying PCA to reduce dimensionality...\")\r\npca = PCA(n_components=11)  # Reduce to 11 main features\r\nnode_features = pca.fit_transform(data[useful_columns])\r\nnode_features = torch.tensor(node_features, dtype=torch.float)\r\nprint(f\"PCA applied. Node feature shape: {node_features.shape}\")\r\n\r\n#checking for unique rows\r\nprint(\"Unique rows in node features:\", np.unique(node_features.numpy(), axis=0).shape[0])\r\n\r\nprint(\"Building KDTree and constructing edges (k-NN approach)...\")\r\n\r\ncoordinates = data[['lat', 'lon']].values\r\ntree = KDTree(coordinates)\r\n\r\nk = 5  # Each node connects to its 5 nearest neighbors\r\ndistances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\r\n\r\n# Convert to edge index format\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, k+1):  # Skip first entry (self-node)\r\n        edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T  # Convert list to NumPy array and transpose\r\n\r\nif edges.shape[1] > 0:\r\n    edge_index = torch.tensor(edges, dtype=torch.long)  # Convert to PyTorch tensor\r\n    print(f\"Number of edges created: {edge_index.shape[1]}\")\r\nelse:\r\n    print(\"No edges were created. Consider increasing `k`.\")\r\n    edge_index = torch.empty((2, 0), dtype=torch.long)  \r\n\r\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)  # Compute degrees for each node\r\nprint(\"Degree stats:\\n\", deg.describe()) \r\n\r\ngraph_data = Data(x=node_features, edge_index=edge_index)\r\n\r\n#  checking graph before saving\r\nprint(\"\\n Graph Data Summary:\")\r\nprint(graph_data)  # This prints node & edge count\r\nprint(f\"Number of nodes: {graph_data.num_nodes}\")\r\nprint(f\"Number of edges: {graph_data.num_edges}\")\r\nprint(f\"Node feature shape: {graph_data.x.shape}\")\r\nprint(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n\r\n# Sample a few edges and nodes\r\nprint(\"\\n Sample of Node Features:\")\r\nprint(graph_data.x[:5])  # Show first 5 node features\r\n\r\nprint(\"\\n Sample of Edge Index:\")\r\nprint(graph_data.edge_index[:, :10])  # Show first 10 edges",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 13 columns.\n Normalizing features using StandardScaler...\nNormalization complete.\nExplained variance by each component:\n [0.33380011 0.45763892 0.56118322 0.63908558 0.71537133 0.78364688\n 0.8437696  0.88881181 0.91960394 0.94710491 0.96973549 0.98674311\n 1.        ]\nTo retain 95% of variance, we need 11 components.\nApplying PCA to reduce dimensionality...\nPCA applied. Node feature shape: torch.Size([6155644, 11])\nUnique rows in node features: 3978302\nBuilding KDTree and constructing edges (k-NN approach)...\nNumber of edges created: 30778220\nTraceback (most recent call last):\n  File \"/media/volume1/gw-workspace/4LCJZugdmRE9/graph_creation.py\", line 81, in <module>\n    deg = degree(edge_index[0], num_nodes=graph_data.num_nodes)  # Compute degrees for each node\n                                          ^^^^^^^^^^\nNameError: name 'graph_data' is not defined\n",
  "history_begin_time" : 1740710847727,
  "history_end_time" : 1740710974900,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "X64lc8yZAMwp",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\nfrom sklearn.decomposition import PCA\r\nfrom torch_geometric.utils import degree \r\n\r\n\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\nchunksize = 500000\r\n\r\nprint(\"Loading dataset...\")\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]  \r\n\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    df_list.append(chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\nprint(\" Normalizing features using StandardScaler...\")\r\nscaler = StandardScaler()\r\ndata[useful_columns] = scaler.fit_transform(data[useful_columns])  # Normalize features\r\nprint(\"Normalization complete.\")\r\n\r\n#checking for unique rows\r\nprint(\"Unique rows in node features:\", np.unique(node_features.numpy(), axis=0).shape[0])\r\n\r\n\r\n#to check how many components to use\r\npca = PCA()\r\npca.fit(data[useful_columns])\r\n\r\n# Print explained variance\r\nexplained_variance = np.cumsum(pca.explained_variance_ratio_)\r\nprint(\"Explained variance by each component:\\n\", explained_variance)\r\n\r\n# Find the number of components that retain 95% variance\r\nn_components_95 = np.argmax(explained_variance >= 0.95) + 1\r\nprint(f\"To retain 95% of variance, we need {n_components_95} components.\")\r\n\r\nprint(\"Applying PCA to reduce dimensionality...\")\r\npca = PCA(n_components=11)  # Reduce to 11 main features\r\nnode_features = pca.fit_transform(data[useful_columns])\r\nnode_features = torch.tensor(node_features, dtype=torch.float)\r\nprint(f\"PCA applied. Node feature shape: {node_features.shape}\")\r\n\r\nprint(\"Building KDTree and constructing edges (k-NN approach)...\")\r\n\r\ncoordinates = data[['lat', 'lon']].values\r\ntree = KDTree(coordinates)\r\n\r\nk = 5  # Each node connects to its 5 nearest neighbors\r\ndistances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\r\n\r\n# Convert to edge index format\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, k+1):  # Skip first entry (self-node)\r\n        edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T  # Convert list to NumPy array and transpose\r\n\r\nif edges.shape[1] > 0:\r\n    edge_index = torch.tensor(edges, dtype=torch.long)  # Convert to PyTorch tensor\r\n    print(f\"Number of edges created: {edge_index.shape[1]}\")\r\nelse:\r\n    print(\"No edges were created. Consider increasing `k`.\")\r\n    edge_index = torch.empty((2, 0), dtype=torch.long)  \r\n\r\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)  # Compute degrees for each node\r\nprint(\"Degree stats:\\n\", deg.describe()) \r\n\r\ngraph_data = Data(x=node_features, edge_index=edge_index)\r\n\r\n#  checking graph before saving\r\nprint(\"\\n Graph Data Summary:\")\r\nprint(graph_data)  # This prints node & edge count\r\nprint(f\"Number of nodes: {graph_data.num_nodes}\")\r\nprint(f\"Number of edges: {graph_data.num_edges}\")\r\nprint(f\"Node feature shape: {graph_data.x.shape}\")\r\nprint(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n\r\n# Sample a few edges and nodes\r\nprint(\"\\n Sample of Node Features:\")\r\nprint(graph_data.x[:5])  # Show first 5 node features\r\n\r\nprint(\"\\n Sample of Edge Index:\")\r\nprint(graph_data.edge_index[:, :10])  # Show first 10 edges",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 13 columns.\n Normalizing features using StandardScaler...\nNormalization complete.\nTraceback (most recent call last):\n  File \"/media/volume1/gw-workspace/X64lc8yZAMwp/graph_creation.py\", line 38, in <module>\n    print(\"Unique rows in node features:\", np.unique(node_features.numpy(), axis=0).shape[0])\n                                                     ^^^^^^^^^^^^^\nNameError: name 'node_features' is not defined\n",
  "history_begin_time" : 1740710577427,
  "history_end_time" : 1740710607703,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "AzJEr1b4Ijhq",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\nfrom sklearn.decomposition import PCA\r\n\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\nchunksize = 500000\r\n\r\nprint(\"Loading dataset...\")\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]  \r\n\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    df_list.append(chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\nprint(\" Normalizing features using StandardScaler...\")\r\nscaler = StandardScaler()\r\ndata[useful_columns] = scaler.fit_transform(data[useful_columns])  # Normalize features\r\nprint(\"Normalization complete.\")\r\n\r\n#to check how many components to use\r\npca = PCA()\r\npca.fit(data[useful_columns])\r\n\r\n# Print explained variance\r\nexplained_variance = np.cumsum(pca.explained_variance_ratio_)\r\nprint(\"Explained variance by each component:\\n\", explained_variance)\r\n\r\n# Find the number of components that retain 95% variance\r\nn_components_95 = np.argmax(explained_variance >= 0.95) + 1\r\nprint(f\"To retain 95% of variance, we need {n_components_95} components.\")\r\n\r\nprint(\"Applying PCA to reduce dimensionality...\")\r\npca = PCA(n_components=11)  # Reduce to 11 main features\r\nnode_features = pca.fit_transform(data[useful_columns])\r\nnode_features = torch.tensor(node_features, dtype=torch.float)\r\nprint(f\"PCA applied. Node feature shape: {node_features.shape}\")\r\n\r\nprint(\"Building KDTree and constructing edges (k-NN approach)...\")\r\n\r\ncoordinates = data[['lat', 'lon']].values\r\ntree = KDTree(coordinates)\r\n\r\nk = 5  # Each node connects to its 5 nearest neighbors\r\ndistances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\r\n\r\n# Convert to edge index format\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, k+1):  # Skip first entry (self-node)\r\n        edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T  # Convert list to NumPy array and transpose\r\n\r\nif edges.shape[1] > 0:\r\n    edge_index = torch.tensor(edges, dtype=torch.long)  # Convert to PyTorch tensor\r\n    print(f\"Number of edges created: {edge_index.shape[1]}\")\r\nelse:\r\n    print(\"No edges were created. Consider increasing `k`.\")\r\n    edge_index = torch.empty((2, 0), dtype=torch.long)  \r\n\r\n\r\ngraph_data = Data(x=node_features, edge_index=edge_index)\r\n\r\n#  checking graph before saving\r\nprint(\"\\n Graph Data Summary:\")\r\nprint(graph_data)  # This prints node & edge count\r\nprint(f\"Number of nodes: {graph_data.num_nodes}\")\r\nprint(f\"Number of edges: {graph_data.num_edges}\")\r\nprint(f\"Node feature shape: {graph_data.x.shape}\")\r\nprint(f\"Edge index shape: {graph_data.edge_index.shape}\")\r\n\r\n# Sample a few edges and nodes\r\nprint(\"\\n Sample of Node Features:\")\r\nprint(graph_data.x[:5])  # Show first 5 node features\r\n\r\nprint(\"\\n Sample of Edge Index:\")\r\nprint(graph_data.edge_index[:, :10])  # Show first 10 edges",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 13 columns.\n Normalizing features using StandardScaler...\nNormalization complete.\nExplained variance by each component:\n [0.33380011 0.45763892 0.56118322 0.63908558 0.71537133 0.78364688\n 0.8437696  0.88881181 0.91960394 0.94710491 0.96973549 0.98674311\n 1.        ]\nTo retain 95% of variance, we need 11 components.\nApplying PCA to reduce dimensionality...\nPCA applied. Node feature shape: torch.Size([6155644, 11])\nBuilding KDTree and constructing edges (k-NN approach)...\nNumber of edges created: 30778220\n Graph Data Summary:\nData(x=[6155644, 11], edge_index=[2, 30778220])\nNumber of nodes: 6155644\nNumber of edges: 30778220\nNode feature shape: torch.Size([6155644, 11])\nEdge index shape: torch.Size([2, 30778220])\n Sample of Node Features:\ntensor([[-2.8888, -1.7301, -1.8027, -0.2331,  0.1632,  0.4564,  0.3739,  0.9708,\n          0.5142, -0.3787,  0.2449],\n        [-2.8888, -1.7301, -1.8027, -0.2331,  0.1632,  0.4564,  0.3739,  0.9708,\n          0.5142, -0.3787,  0.2449],\n        [-2.8888, -1.7301, -1.8027, -0.2331,  0.1632,  0.4564,  0.3739,  0.9708,\n          0.5142, -0.3787,  0.2449],\n        [-2.8888, -1.7301, -1.8027, -0.2331,  0.1632,  0.4564,  0.3739,  0.9708,\n          0.5142, -0.3787,  0.2449],\n        [-2.8888, -1.7301, -1.8027, -0.2331,  0.1632,  0.4564,  0.3739,  0.9708,\n          0.5142, -0.3787,  0.2449]])\n Sample of Edge Index:\ntensor([[  0,   0,   0,   0,   0,   1,   1,   1,   1,   1],\n        [357, 358, 359, 360, 355, 357, 358, 359, 360, 355]])\n",
  "history_begin_time" : 1740710128225,
  "history_end_time" : 1740710242197,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "BXKTmSIg94Ak",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\nfrom sklearn.decomposition import PCA\r\n\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\nchunksize = 500000\r\n\r\nprint(\"Loading dataset...\")\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]  \r\n\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    df_list.append(chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\nprint(\" Normalizing features using StandardScaler...\")\r\nscaler = StandardScaler()\r\ndata[useful_columns] = scaler.fit_transform(data[useful_columns])  # Normalize features\r\nprint(\"Normalization complete.\")\r\n\r\n#to check how many components to use\r\npca = PCA()\r\npca.fit(data[useful_columns])\r\n\r\n# Print explained variance\r\nexplained_variance = np.cumsum(pca.explained_variance_ratio_)\r\nprint(\"Explained variance by each component:\\n\", explained_variance)\r\n\r\n# Find the number of components that retain 95% variance\r\nn_components_95 = np.argmax(explained_variance >= 0.95) + 1\r\nprint(f\"To retain 95% of variance, we need {n_components_95} components.\")\r\n\r\nprint(\"Applying PCA to reduce dimensionality...\")\r\npca = PCA(n_components=11)  # Reduce to 11 main features\r\nnode_features = pca.fit_transform(data[useful_columns])\r\nnode_features = torch.tensor(node_features, dtype=torch.float)\r\nprint(f\"PCA applied. Node feature shape: {node_features.shape}\")\r\n\r\nprint(\"Building KDTree and constructing edges (k-NN approach)...\")\r\n\r\ncoordinates = data[['lat', 'lon']].values\r\ntree = KDTree(coordinates)\r\n\r\nk = 5  # Each node connects to its 5 nearest neighbors\r\ndistances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\r\n\r\n# Convert to edge index format\r\nedge_list = []\r\nfor i in range(len(neighbors)):\r\n    for j in range(1, k+1):  # Skip first entry (self-node)\r\n        edge_list.append((i, neighbors[i][j]))\r\n\r\nedges = np.array(edge_list).T  # Convert list to NumPy array and transpose\r\n\r\nif edges.shape[1] > 0:\r\n    edge_index = torch.tensor(edges, dtype=torch.long)  # Convert to PyTorch tensor\r\n    print(f\"Number of edges created: {edge_index.shape[1]}\")\r\nelse:\r\n    print(\"No edges were created. Consider increasing `k`.\")\r\n    edge_index = torch.empty((2, 0), dtype=torch.long)  # Empty tensor",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 13 columns.\n Normalizing features using StandardScaler...\nNormalization complete.\nExplained variance by each component:\n [0.33380011 0.45763892 0.56118322 0.63908558 0.71537133 0.78364688\n 0.8437696  0.88881181 0.91960394 0.94710491 0.96973549 0.98674311\n 1.        ]\nTo retain 95% of variance, we need 11 components.\nApplying PCA to reduce dimensionality...\nPCA applied. Node feature shape: torch.Size([6155644, 11])\nBuilding KDTree and constructing edges (k-NN approach)...\nNumber of edges created: 30778220\n",
  "history_begin_time" : 1740709764183,
  "history_end_time" : 1740709896389,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "hYvZC0zgXASD",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\nfrom sklearn.decomposition import PCA\r\n\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\nchunksize = 500000\r\n\r\nprint(\"Loading dataset...\")\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]  \r\n\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    df_list.append(chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\nprint(\" Normalizing features using StandardScaler...\")\r\nscaler = StandardScaler()\r\ndata[useful_columns] = scaler.fit_transform(data[useful_columns])  # Normalize features\r\nprint(\"Normalization complete.\")\r\n\r\n#to check how many components to use\r\npca = PCA()\r\npca.fit(data[useful_columns])\r\n\r\n# Print explained variance\r\nexplained_variance = np.cumsum(pca.explained_variance_ratio_)\r\nprint(\"Explained variance by each component:\\n\", explained_variance)\r\n\r\n# Find the number of components that retain 95% variance\r\nn_components_95 = np.argmax(explained_variance >= 0.95) + 1\r\nprint(f\"To retain 95% of variance, we need {n_components_95} components.\")\r\n\r\nprint(\"Applying PCA to reduce dimensionality...\")\r\npca = PCA(n_components=11)  # Reduce to 11 main features\r\nnode_features = pca.fit_transform(data[useful_columns])\r\nnode_features = torch.tensor(node_features, dtype=torch.float)\r\nprint(f\"PCA applied. Node feature shape: {node_features.shape}\")\r\n\r\nprint(\"Building KDTree and constructing edges...\")\r\ncoordinates = data[['lat', 'lon']].values\r\ntree = KDTree(coordinates)\r\n\r\nthreshold = 0.005  # Adjust as needed\r\nedges = tree.query_pairs(r=threshold)  # Find pairs of nodes within threshold distance\r\nedges = np.array(list(edges))  # Convert to NumPy array\r\n\r\nif edges.shape[0] > 0:\r\n    edge_index = torch.tensor(edges.T, dtype=torch.long)  # Convert to PyTorch tensor\r\n    print(f\"Number of edges created: {edge_index.shape[1]}\")\r\nelse:\r\n    print(\"No edges were created. Consider increasing the threshold value.\")\r\n    edge_index = torch.empty((2, 0), dtype=torch.long)",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 13 columns.\n Normalizing features using StandardScaler...\nNormalization complete.\nExplained variance by each component:\n [0.33380011 0.45763892 0.56118322 0.63908558 0.71537133 0.78364688\n 0.8437696  0.88881181 0.91960394 0.94710491 0.96973549 0.98674311\n 1.        ]\nTo retain 95% of variance, we need 11 components.\nApplying PCA to reduce dimensionality...\nPCA applied. Node feature shape: torch.Size([6155644, 11])\nBuilding KDTree and constructing edges...\nTraceback (most recent call last):\n  File \"/media/volume1/gw-workspace/hYvZC0zgXASD/graph_creation.py\", line 58, in <module>\n    edges = tree.query_pairs(r=threshold)  # Find pairs of nodes within threshold distance\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/scipy/spatial/_kdtree.py\", line 654, in query_pairs\n    return super().query_pairs(r, p, eps, output_type)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"_ckdtree.pyx\", line 1149, in scipy.spatial._ckdtree.cKDTree.query_pairs\nMemoryError: std::bad_alloc\n",
  "history_begin_time" : 1740709392434,
  "history_end_time" : 1740709516651,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "jGTicbr3mqJG",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\nfrom sklearn.decomposition import PCA\r\n\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\nchunksize = 500000\r\n\r\nprint(\"Loading dataset...\")\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]  \r\n\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    df_list.append(chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\nprint(\" Normalizing features using StandardScaler...\")\r\nscaler = StandardScaler()\r\ndata[useful_columns] = scaler.fit_transform(data[useful_columns])  # Normalize features\r\nprint(\"Normalization complete.\")\r\n\r\n#to check how many components to use\r\npca = PCA()\r\npca.fit(data[useful_columns])\r\n\r\n# Print explained variance\r\nexplained_variance = np.cumsum(pca.explained_variance_ratio_)\r\nprint(\"Explained variance by each component:\\n\", explained_variance)\r\n\r\n# Find the number of components that retain 95% variance\r\nn_components_95 = np.argmax(explained_variance >= 0.95) + 1\r\nprint(f\"To retain 95% of variance, we need {n_components_95} components.\")\r\n\r\nprint(\"Applying PCA to reduce dimensionality...\")\r\npca = PCA(n_components=11)  # Reduce to 11 main features\r\nnode_features = pca.fit_transform(data[useful_columns])\r\nnode_features = torch.tensor(node_features, dtype=torch.float)\r\nprint(f\"PCA applied. Node feature shape: {node_features.shape}\")",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 13 columns.\n Normalizing features using StandardScaler...\nNormalization complete.\nExplained variance by each component:\n [0.33380011 0.45763892 0.56118322 0.63908558 0.71537133 0.78364688\n 0.8437696  0.88881181 0.91960394 0.94710491 0.96973549 0.98674311\n 1.        ]\nTo retain 95% of variance, we need 11 components.\nApplying PCA to reduce dimensionality...\nPCA applied. Node feature shape: torch.Size([6155644, 11])\n",
  "history_begin_time" : 1740709210509,
  "history_end_time" : 1740709249843,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "jqEC7wOEcJEO",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\nfrom sklearn.decomposition import PCA\r\n\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\nchunksize = 500000\r\n\r\nprint(\"Loading dataset...\")\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]  \r\n\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    df_list.append(chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\nprint(\" Normalizing features using StandardScaler...\")\r\nscaler = StandardScaler()\r\ndata[useful_columns] = scaler.fit_transform(data[useful_columns])  # Normalize features\r\nprint(\"Normalization complete.\")\r\n\r\n#to check how many components to use\r\npca = PCA()\r\npca.fit(data[useful_columns])\r\n\r\n# Print explained variance\r\nexplained_variance = np.cumsum(pca.explained_variance_ratio_)\r\nprint(\"Explained variance by each component:\\n\", explained_variance)\r\n\r\n# Find the number of components that retain 95% variance\r\nn_components_95 = np.argmax(explained_variance >= 0.95) + 1\r\nprint(f\"To retain 95% of variance, we need {n_components_95} components.\")\r\n\r\nprint(\"Applying PCA to reduce dimensionality...\")\r\npca = PCA(n_components=10)  # Reduce to 10 main features\r\nnode_features = pca.fit_transform(data[useful_columns])\r\nnode_features = torch.tensor(node_features, dtype=torch.float)\r\nprint(f\"PCA applied. Node feature shape: {node_features.shape}\")",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 13 columns.\n Normalizing features using StandardScaler...\nNormalization complete.\nExplained variance by each component:\n [0.33380011 0.45763892 0.56118322 0.63908558 0.71537133 0.78364688\n 0.8437696  0.88881181 0.91960394 0.94710491 0.96973549 0.98674311\n 1.        ]\nTo retain 95% of variance, we need 11 components.\nApplying PCA to reduce dimensionality...\nPCA applied. Node feature shape: torch.Size([6155644, 10])\n",
  "history_begin_time" : 1740709172093,
  "history_end_time" : 1740709220707,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "0BHNnY5xFSkg",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\nfrom sklearn.decomposition import PCA\r\n\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\nchunksize = 500000\r\n\r\nprint(\"Loading dataset...\")\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]  \r\n\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    df_list.append(chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\nprint(\" Normalizing features using StandardScaler...\")\r\nscaler = StandardScaler()\r\ndata[useful_columns] = scaler.fit_transform(data[useful_columns])  # Normalize features\r\nprint(\"Normalization complete.\")\r\n\r\npca = PCA()\r\npca.fit(data[useful_columns])\r\n\r\n# Print explained variance\r\nexplained_variance = np.cumsum(pca.explained_variance_ratio_)\r\nprint(\"Explained variance by each component:\\n\", explained_variance)\r\n\r\n# Find the number of components that retain 95% variance\r\nn_components_95 = np.argmax(explained_variance >= 0.95) + 1\r\nprint(f\"To retain 95% of variance, we need {n_components_95} components.\")",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 13 columns.\n Normalizing features using StandardScaler...\nNormalization complete.\nExplained variance by each component:\n [0.33380011 0.45763892 0.56118322 0.63908558 0.71537133 0.78364688\n 0.8437696  0.88881181 0.91960394 0.94710491 0.96973549 0.98674311\n 1.        ]\nTo retain 95% of variance, we need 11 components.\n",
  "history_begin_time" : 1740708992843,
  "history_end_time" : 1740709027385,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "WhOnQgYi0Mhm",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\nchunksize = 500000\r\n\r\nprint(\"Loading dataset...\")\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]  \r\n\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    df_list.append(chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\nprint(\" Normalizing features using StandardScaler...\")\r\nscaler = StandardScaler()\r\ndata[useful_columns] = scaler.fit_transform(data[useful_columns])  # Normalize features\r\nprint(\"Normalization complete.\")",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 13 columns.\n Normalizing features using StandardScaler...\nNormalization complete.\n",
  "history_begin_time" : 1740708798132,
  "history_end_time" : 1740708828963,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "1hTQM17gSKXr",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\nchunksize = 500000\r\n\r\nprint(\"Loading dataset...\")\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]  \r\n\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    df_list.append(chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\nprint(\" Normalizing features using StandardScaler...\")\r\nscaler = StandardScaler()\r\ndata[useful_columns] = scaler.fit_transform(data[useful_columns])  # Normalize features\r\nprint(\"✅ Normalization complete.\")",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 13 columns.\n Normalizing features using StandardScaler...\n✅ Normalization complete.\n",
  "history_begin_time" : 1740708761406,
  "history_end_time" : 1740708793199,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "hGObhgWRHTvF",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\nchunksize = 500000\r\n\r\nprint(\"Loading dataset...\")\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]  \r\n\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    df_list.append(chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 13 columns.\n",
  "history_begin_time" : 1740708577303,
  "history_end_time" : 1740708607477,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "RWhdlassLFCl",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\nchunksize = 500000\r\n\r\nprint(\"Loading dataset...\")\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]  \r\n\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    df_list.append(chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\nprint(\"Removing outliers using LOF...\")\r\nlof = LocalOutlierFactor(n_neighbors=20)\r\noutlier_mask = lof.fit_predict(data[useful_columns])  # Uses only feature columns\r\ndata = data[outlier_mask == 1]  # Keep only non-outliers\r\n\r\nprint(f\"Outliers removed. Remaining data: {data.shape[0]} rows.\")",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 13 columns.\nRemoving outliers using LOF...\nOutliers removed. Remaining data: 6062075 rows.\n",
  "history_begin_time" : 1740707776943,
  "history_end_time" : 1740708701454,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "dgY4rFUDJoYX",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\nchunksize = 500000\r\n\r\nprint(\"Loading dataset...\")\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]  \r\n\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    df_list.append(chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\nprint(\"Removing outliers using LOF...\")\r\nlof = LocalOutlierFactor(n_neighbors=20)\r\noutlier_mask = lof.fit_predict(data[useful_columns])  # Uses only feature columns\r\ndata = data[outlier_mask == 1]  # Keep only non-outliers\r\n\r\nprint(f\"Outliers removed. Remaining data: {data.shape[0]} rows.\")",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 13 columns.\nRemoving outliers using LOF...\nOutliers removed. Remaining data: 6062075 rows.\n",
  "history_begin_time" : 1740707243912,
  "history_end_time" : 1740708145225,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "HTMi96LDdeRZ",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\nchunksize = 500000\r\n\r\nprint(\"Loading dataset...\")\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]  \r\n\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    df_list.append(chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\nprint(\"Checking for missing values...\")\r\nprint(data.isnull().sum())\r\n\r\n# View a few rows\r\nprint(\"First few rows of the dataset:\")\r\nprint(data.head())",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 13 columns.\nChecking for missing values...\nlat                                0\nlon                                0\nSWE                                0\nsnow_depth                         0\nair_temperature_observed_f         0\nprecipitation_amount               0\nrelative_humidity_rmin             0\nrelative_humidity_rmax             0\nwind_speed                         0\nElevation                          0\nSlope                              0\ncumulative_SWE                     0\ncumulative_precipitation_amount    0\ndtype: int64\nFirst few rows of the dataset:\n        lat        lon  ...  cumulative_SWE  cumulative_precipitation_amount\n0  29.13346 -103.51433  ...            -1.0                             -1.0\n1  29.13346 -103.51433  ...            -1.0                             -1.0\n2  29.13346 -103.51433  ...            -1.0                             -1.0\n3  29.13346 -103.51433  ...            -1.0                             -1.0\n4  29.13346 -103.51433  ...            -1.0                             -1.0\n[5 rows x 13 columns]\n",
  "history_begin_time" : 1740707151644,
  "history_end_time" : 1740707180990,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "z6OIbRGvJMBf",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\nchunksize = 500000\r\n\r\nprint(\"Loading dataset...\")\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]  \r\n\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    df_list.append(chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n\r\nprint(f\"Dataset contains {data.shape[0]} rows and {data.shape[1]} columns.\")\r\n",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 13 columns.\nDataset contains 6155644 rows and 13 columns.\n",
  "history_begin_time" : 1740707064539,
  "history_end_time" : 1740707093258,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "uF7DO5Pp7tRQ",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\nchunksize = 500000\r\n\r\nprint(\"Loading dataset...\")\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]  \r\n\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    df_list.append(chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 13 columns.\n",
  "history_begin_time" : 1740706850506,
  "history_end_time" : 1740706879889,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "dTW8DZnPd9dL",
  "history_input" : "import pandas as pd\r\nimport torch\r\nimport numpy as np\r\nfrom torch_geometric.data import Data\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy.spatial import KDTree\r\nfrom sklearn.neighbors import LocalOutlierFactor\r\n\r\n# 1: Load Data\r\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\r\nchunksize = 500  # Process 500,000 rows at a time\r\n\r\nprint(\"Loading dataset...\")\r\nuseful_columns = [\r\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\r\n    'air_temperature_observed_f', 'precipitation_amount',\r\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\r\n    'cumulative_SWE', 'cumulative_precipitation_amount'\r\n]  \r\n\r\ndf_list = []\r\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\r\n    df_list.append(chunk)\r\n\r\ndata = pd.concat(df_list, ignore_index=True)\r\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 13 columns.\n",
  "history_begin_time" : 1740706775931,
  "history_end_time" : 1740706822296,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "62dnev7ydu1",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1741409321090,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "wjad30royzq",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1741409322433,
  "history_notes" : null,
  "history_process" : "0ocdgc",
  "host_id" : "100001",
  "indicator" : "Stopped"
}]