[{
  "history_id" : "afsirhm26nl",
  "history_input" : "import pandas as pd\nimport torch\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nimport numpy as np\n\n# Load dataset\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\nprint(\"Loading dataset...\")\ndata = pd.read_csv(file_path)\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\n\n# Define Bounding Box (Western US region)\nLAT_MAX, LON_MAX = 48.864715, -120.190430\nLAT_MIN, LON_MIN = 45.798170, -124.145508\n\n# Step 1: Filter Data Within the Bounding Box\nprint(\"Filtering data within the bounding box...\")\ndata = data[\n    (data['lat'] <= LAT_MAX) & (data['lat'] >= LAT_MIN) & \n    (data['lon'] <= LON_MAX) & (data['lon'] >= LON_MIN)\n]\nprint(f\"Filtered dataset with {data.shape[0]} rows.\")\n\n# Step 2: Apply Grid-Based Sampling\ngrid_size = 0.05  # Reduced grid size to increase node density\nlat_bins = np.arange(LAT_MIN, LAT_MAX, grid_size)\nlon_bins = np.arange(LON_MIN, LON_MAX, grid_size)\n\nsampled_data = []\nfor lat_bin in lat_bins:\n    for lon_bin in lon_bins:\n        subset = data[\n            (data['lat'] >= lat_bin) & (data['lat'] < lat_bin + grid_size) &\n            (data['lon'] >= lon_bin) & (data['lon'] < lon_bin + grid_size)\n        ]\n        if not subset.empty:\n            sampled_data.append(subset.sample(1, random_state=42))  \n\nsampled_data = pd.concat(sampled_data)\nprint(f\"Grid-based sampled dataset with {sampled_data.shape[0]} rows.\")\n\n# Step 3: Normalize Features\nselected_features = [\n    'swe_value', 'SWE', 'fsca', 'air_temperature_tmmx', \n    'air_temperature_tmmn', 'potential_evapotranspiration', \n    'relative_humidity_rmax', 'Elevation', 'Slope', 'Curvature', \n    'Aspect', 'Eastness', 'Northness'\n]\n\nprint(\"Normalizing features...\")\nscaler = StandardScaler()\nsampled_data[selected_features] = scaler.fit_transform(sampled_data[selected_features])\n\n# Step 4: Normalize Labels\nprint(\"Normalizing target variable (swe_value)...\")\nlabel_scaler = StandardScaler()\nsampled_data['swe_value'] = label_scaler.fit_transform(sampled_data['swe_value'].values.reshape(-1, 1))\n\n# Step 5: Save Normalized Dataset\noutput_file_path = 'normalized_grid_sampled_snotel.csv'\nsampled_data.to_csv(output_file_path, index=False)\nprint(f\"Normalized sampled dataset saved as '{output_file_path}'.\")\n\n# Step 6: Create Node Features Tensor\nnode_features = torch.tensor(sampled_data[selected_features].values, dtype=torch.float)\nprint(f\"Node features tensor created with shape: {node_features.shape}\")\n\n# Step 7: Create Labels Tensor\nlabels = torch.tensor(sampled_data['swe_value'].values, dtype=torch.float)\nprint(f\"Labels tensor created with shape: {labels.shape}\")\n\n# Step 8: Build KDTree and Construct Edges Using Distance Threshold\nprint(\"Building KDTree and constructing edges using distance threshold...\")\ncoordinates = sampled_data[['lat', 'lon']].values\ntree = KDTree(coordinates)\n\nk = 30  \nradius = 0.2\nedges = []\nfor i in range(len(coordinates)):\n    knn_neighbors = tree.query(coordinates[i], k=k+1)[1][1:]  \n    radius_neighbors = tree.query_ball_point(coordinates[i], r=radius)  \n    \n    for neighbor in set(knn_neighbors).union(radius_neighbors): \n        if i != neighbor:\n            edges.append((i, neighbor))\n\n\nprint(f\"Number of edges created: {len(edges)}\")\n\n# Convert to Edge Index Tensor\nedge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\nprint(f\"Edge index tensor created with shape: {edge_index.shape}\")\n\n# Step 9: Create PyTorch Geometric Graph Data Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# Save the PyTorch Geometric Data Object\ngraph_data_output_path = '/media/volume1/gat_spatial_training_data_sampled_radius_final_01.pt'\ntorch.save(graph_data, graph_data_output_path)\nprint(f\"PyTorch Geometric Data object saved as '{graph_data_output_path}'.\")",
  "history_output" : "Loading dataset...\n",
  "history_begin_time" : 1741212657858,
  "history_end_time" : 1741212819882,
  "history_notes" : null,
  "history_process" : "3lobvp",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "7YXuuIwo9Wdf",
  "history_input" : "import pandas as pd\nimport torch\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nimport numpy as np\n\n# Load dataset\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\nprint(\"Loading dataset...\")\ndata = pd.read_csv(file_path)\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\n\n# Define Bounding Box (Western US region)\nLAT_MAX, LON_MAX = 48.864715, -120.190430\nLAT_MIN, LON_MIN = 45.798170, -124.145508\n\n# Step 1: Filter Data Within the Bounding Box\nprint(\"Filtering data within the bounding box...\")\ndata = data[\n    (data['lat'] <= LAT_MAX) & (data['lat'] >= LAT_MIN) & \n    (data['lon'] <= LON_MAX) & (data['lon'] >= LON_MIN)\n]\nprint(f\"Filtered dataset with {data.shape[0]} rows.\")\n\n# Step 2: Apply Grid-Based Sampling\ngrid_size = 0.05  # Reduced grid size to increase node density\nlat_bins = np.arange(LAT_MIN, LAT_MAX, grid_size)\nlon_bins = np.arange(LON_MIN, LON_MAX, grid_size)\n\nsampled_data = []\nfor lat_bin in lat_bins:\n    for lon_bin in lon_bins:\n        subset = data[\n            (data['lat'] >= lat_bin) & (data['lat'] < lat_bin + grid_size) &\n            (data['lon'] >= lon_bin) & (data['lon'] < lon_bin + grid_size)\n        ]\n        if not subset.empty:\n            sampled_data.append(subset.sample(1, random_state=42))  \n\nsampled_data = pd.concat(sampled_data)\nprint(f\"Grid-based sampled dataset with {sampled_data.shape[0]} rows.\")\n\n# Step 3: Normalize Features\nselected_features = [\n    'swe_value', 'SWE', 'fsca', 'air_temperature_tmmx', \n    'air_temperature_tmmn', 'potential_evapotranspiration', \n    'relative_humidity_rmax', 'Elevation', 'Slope', 'Curvature', \n    'Aspect', 'Eastness', 'Northness'\n]\n\nprint(\"Normalizing features...\")\nscaler = StandardScaler()\nsampled_data[selected_features] = scaler.fit_transform(sampled_data[selected_features])\n\n# Step 4: Normalize Labels\nprint(\"Normalizing target variable (swe_value)...\")\nlabel_scaler = StandardScaler()\nsampled_data['swe_value'] = label_scaler.fit_transform(sampled_data['swe_value'].values.reshape(-1, 1))\n\n# Step 5: Save Normalized Dataset\noutput_file_path = 'normalized_grid_sampled_snotel.csv'\nsampled_data.to_csv(output_file_path, index=False)\nprint(f\"Normalized sampled dataset saved as '{output_file_path}'.\")\n\n# Step 6: Create Node Features Tensor\nnode_features = torch.tensor(sampled_data[selected_features].values, dtype=torch.float)\nprint(f\"Node features tensor created with shape: {node_features.shape}\")\n\n# Step 7: Create Labels Tensor\nlabels = torch.tensor(sampled_data['swe_value'].values, dtype=torch.float)\nprint(f\"Labels tensor created with shape: {labels.shape}\")\n\n# Step 8: Build KDTree and Construct Edges Using Distance Threshold\nprint(\"Building KDTree and constructing edges using distance threshold...\")\ncoordinates = sampled_data[['lat', 'lon']].values\ntree = KDTree(coordinates)\n\nk = 30  \nradius = 0.2\nedges = []\nfor i in range(len(coordinates)):\n    knn_neighbors = tree.query(coordinates[i], k=k+1)[1][1:]  \n    radius_neighbors = tree.query_ball_point(coordinates[i], r=radius)  \n    \n    for neighbor in set(knn_neighbors).union(radius_neighbors): \n        if i != neighbor:\n            edges.append((i, neighbor))\n\n\nprint(f\"Number of edges created: {len(edges)}\")\n\n# Convert to Edge Index Tensor\nedge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\nprint(f\"Edge index tensor created with shape: {edge_index.shape}\")\n\n# Step 9: Create PyTorch Geometric Graph Data Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# Save the PyTorch Geometric Data Object\ngraph_data_output_path = '/media/volume1/gat_spatial_training_data_sampled_radius_final_01.pt'\ntorch.save(graph_data, graph_data_output_path)\nprint(f\"PyTorch Geometric Data object saved as '{graph_data_output_path}'.\")",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 105 columns.\nFiltering data within the bounding box...\nFiltered dataset with 360859 rows.\nGrid-based sampled dataset with 304 rows.\nNormalizing features...\nNormalizing target variable (swe_value)...\nNormalized sampled dataset saved as 'normalized_grid_sampled_snotel.csv'.\nNode features tensor created with shape: torch.Size([304, 13])\nLabels tensor created with shape: torch.Size([304])\nBuilding KDTree and constructing edges using distance threshold...\nNumber of edges created: 9120\nEdge index tensor created with shape: torch.Size([2, 9120])\nPyTorch Geometric Data object saved as '/media/volume1/gat_spatial_training_data_sampled_radius_final_01.pt'.\n",
  "history_begin_time" : 1741029912785,
  "history_end_time" : 1741029980236,
  "history_notes" : null,
  "history_process" : "3lobvp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "pHPgX5BdwoRp",
  "history_input" : "import pandas as pd\nimport torch\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nimport numpy as np\n\n# Load dataset\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\nprint(\"Loading dataset...\")\ndata = pd.read_csv(file_path)\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\n\n# Define Bounding Box (Western US region)\nLAT_MAX, LON_MAX = 48.864715, -120.190430\nLAT_MIN, LON_MIN = 45.798170, -124.145508\n\n# Step 1: Filter Data Within the Bounding Box\nprint(\"Filtering data within the bounding box...\")\ndata = data[\n    (data['lat'] <= LAT_MAX) & (data['lat'] >= LAT_MIN) & \n    (data['lon'] <= LON_MAX) & (data['lon'] >= LON_MIN)\n]\nprint(f\"Filtered dataset with {data.shape[0]} rows.\")\n\n# Step 2: Apply Grid-Based Sampling\ngrid_size = 0.05  # Reduced grid size to increase node density\nlat_bins = np.arange(LAT_MIN, LAT_MAX, grid_size)\nlon_bins = np.arange(LON_MIN, LON_MAX, grid_size)\n\nsampled_data = []\nfor lat_bin in lat_bins:\n    for lon_bin in lon_bins:\n        subset = data[\n            (data['lat'] >= lat_bin) & (data['lat'] < lat_bin + grid_size) &\n            (data['lon'] >= lon_bin) & (data['lon'] < lon_bin + grid_size)\n        ]\n        if not subset.empty:\n            sampled_data.append(subset.sample(1, random_state=42))  \n\nsampled_data = pd.concat(sampled_data)\nprint(f\"Grid-based sampled dataset with {sampled_data.shape[0]} rows.\")\n\n# Step 3: Normalize Features\nselected_features = [\n    'swe_value', 'SWE', 'fsca', 'air_temperature_tmmx', \n    'air_temperature_tmmn', 'potential_evapotranspiration', \n    'relative_humidity_rmax', 'Elevation', 'Slope', 'Curvature', \n    'Aspect', 'Eastness', 'Northness'\n]\n\nprint(\"Normalizing features...\")\nscaler = StandardScaler()\nsampled_data[selected_features] = scaler.fit_transform(sampled_data[selected_features])\n\n# Step 4: Normalize Labels\nprint(\"Normalizing target variable (swe_value)...\")\nlabel_scaler = StandardScaler()\nsampled_data['swe_value'] = label_scaler.fit_transform(sampled_data['swe_value'].values.reshape(-1, 1))\n\n# Step 5: Save Normalized Dataset\noutput_file_path = 'normalized_grid_sampled_snotel.csv'\nsampled_data.to_csv(output_file_path, index=False)\nprint(f\"Normalized sampled dataset saved as '{output_file_path}'.\")\n\n# Step 6: Create Node Features Tensor\nnode_features = torch.tensor(sampled_data[selected_features].values, dtype=torch.float)\nprint(f\"Node features tensor created with shape: {node_features.shape}\")\n\n# Step 7: Create Labels Tensor\nlabels = torch.tensor(sampled_data['swe_value'].values, dtype=torch.float)\nprint(f\"Labels tensor created with shape: {labels.shape}\")\n\n# Step 8: Build KDTree and Construct Edges Using Distance Threshold\nprint(\"Building KDTree and constructing edges using distance threshold...\")\ncoordinates = sampled_data[['lat', 'lon']].values\ntree = KDTree(coordinates)\n\nk = 30  \nradius = 0.2\nedges = []\nfor i in range(len(coordinates)):\n    knn_neighbors = tree.query(coordinates[i], k=k+1)[1][1:]  \n    radius_neighbors = tree.query_ball_point(coordinates[i], r=radius)  \n    \n    for neighbor in set(knn_neighbors).union(radius_neighbors): \n        if i != neighbor:\n            edges.append((i, neighbor))\n\n\nprint(f\"Number of edges created: {len(edges)}\")\n\n# Convert to Edge Index Tensor\nedge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\nprint(f\"Edge index tensor created with shape: {edge_index.shape}\")\n\n# Step 9: Create PyTorch Geometric Graph Data Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# Save the PyTorch Geometric Data Object\ngraph_data_output_path = '/media/volume1/gat_spatial_training_data_sampled_radius_final_01.pt'\n# torch.save(graph_data, graph_data_output_path)\n# print(f\"PyTorch Geometric Data object saved as '{graph_data_output_path}'.\")",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 105 columns.\nFiltering data within the bounding box...\nFiltered dataset with 360859 rows.\nGrid-based sampled dataset with 304 rows.\nNormalizing features...\nNormalizing target variable (swe_value)...\nNormalized sampled dataset saved as 'normalized_grid_sampled_snotel.csv'.\nNode features tensor created with shape: torch.Size([304, 13])\nLabels tensor created with shape: torch.Size([304])\nBuilding KDTree and constructing edges using distance threshold...\nNumber of edges created: 9120\nEdge index tensor created with shape: torch.Size([2, 9120])\n",
  "history_begin_time" : 1741029829808,
  "history_end_time" : 1741029897928,
  "history_notes" : null,
  "history_process" : "3lobvp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "OTvSNz7DmJA5",
  "history_input" : "import pandas as pd\nimport torch\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nimport numpy as np\n\n# Load dataset\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\nprint(\"Loading dataset...\")\ndata = pd.read_csv(file_path)\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\n\n# Define Bounding Box (Western US region)\nLAT_MAX, LON_MAX = 48.864715, -120.190430\nLAT_MIN, LON_MIN = 45.798170, -124.145508\n\n# Step 1: Filter Data Within the Bounding Box\nprint(\"Filtering data within the bounding box...\")\ndata = data[\n    (data['lat'] <= LAT_MAX) & (data['lat'] >= LAT_MIN) & \n    (data['lon'] <= LON_MAX) & (data['lon'] >= LON_MIN)\n]\nprint(f\"Filtered dataset with {data.shape[0]} rows.\")\n\n# Step 2: Apply Grid-Based Sampling\ngrid_size = 0.05  # Reduced grid size to increase node density\nlat_bins = np.arange(LAT_MIN, LAT_MAX, grid_size)\nlon_bins = np.arange(LON_MIN, LON_MAX, grid_size)\n\nsampled_data = []\nfor lat_bin in lat_bins:\n    for lon_bin in lon_bins:\n        subset = data[\n            (data['lat'] >= lat_bin) & (data['lat'] < lat_bin + grid_size) &\n            (data['lon'] >= lon_bin) & (data['lon'] < lon_bin + grid_size)\n        ]\n        if not subset.empty:\n            sampled_data.append(subset.sample(1, random_state=42))  \n\nsampled_data = pd.concat(sampled_data)\nprint(f\"Grid-based sampled dataset with {sampled_data.shape[0]} rows.\")\n\n# Step 3: Normalize Features\nselected_features = [\n    'swe_value', 'SWE', 'fsca', 'air_temperature_tmmx', \n    'air_temperature_tmmn', 'potential_evapotranspiration', \n    'relative_humidity_rmax', 'Elevation', 'Slope', 'Curvature', \n    'Aspect', 'Eastness', 'Northness'\n]\n\nprint(\"Normalizing features...\")\nscaler = StandardScaler()\nsampled_data[selected_features] = scaler.fit_transform(sampled_data[selected_features])\n\n# Step 4: Normalize Labels\nprint(\"Normalizing target variable (swe_value)...\")\nlabel_scaler = StandardScaler()\nsampled_data['swe_value'] = label_scaler.fit_transform(sampled_data['swe_value'].values.reshape(-1, 1))\n\n# Step 5: Save Normalized Dataset\noutput_file_path = 'normalized_grid_sampled_snotel.csv'\nsampled_data.to_csv(output_file_path, index=False)\nprint(f\"Normalized sampled dataset saved as '{output_file_path}'.\")\n\n# Step 6: Create Node Features Tensor\nnode_features = torch.tensor(sampled_data[selected_features].values, dtype=torch.float)\nprint(f\"Node features tensor created with shape: {node_features.shape}\")\n\n# Step 7: Create Labels Tensor\nlabels = torch.tensor(sampled_data['swe_value'].values, dtype=torch.float)\nprint(f\"Labels tensor created with shape: {labels.shape}\")\n\n# Step 8: Build KDTree and Construct Edges Using Distance Threshold\nprint(\"Building KDTree and constructing edges using distance threshold...\")\ncoordinates = sampled_data[['lat', 'lon']].values\ntree = KDTree(coordinates)\n\nk = 30  \nradius = 0.2\nedges = []\nfor i in range(len(coordinates)):\n    knn_neighbors = tree.query(coordinates[i], k=k+1)[1][1:]  \n    radius_neighbors = tree.query_ball_point(coordinates[i], r=radius)  \n    \n    for neighbor in set(knn_neighbors).union(radius_neighbors): \n        if i != neighbor:\n            edges.append((i, neighbor))\n\n\nprint(f\"Number of edges created: {len(edges)}\")\n\n# Convert to Edge Index Tensor\nedge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\nprint(f\"Edge index tensor created with shape: {edge_index.shape}\")\n\n# Step 9: Create PyTorch Geometric Graph Data Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# Save the PyTorch Geometric Data Object\n# graph_data_output_path = '/media/volume1/gat_spatial_training_data_sampled_radius_0.1_20.pt'\n# torch.save(graph_data, graph_data_output_path)\n# print(f\"PyTorch Geometric Data object saved as '{graph_data_output_path}'.\")",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 105 columns.\nFiltering data within the bounding box...\nFiltered dataset with 360859 rows.\nGrid-based sampled dataset with 304 rows.\nNormalizing features...\nNormalizing target variable (swe_value)...\nNormalized sampled dataset saved as 'normalized_grid_sampled_snotel.csv'.\nNode features tensor created with shape: torch.Size([304, 13])\nLabels tensor created with shape: torch.Size([304])\nBuilding KDTree and constructing edges using distance threshold...\nNumber of edges created: 9120\nEdge index tensor created with shape: torch.Size([2, 9120])\n",
  "history_begin_time" : 1741029664072,
  "history_end_time" : 1741029731300,
  "history_notes" : null,
  "history_process" : "3lobvp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "psG7c0CQxOEA",
  "history_input" : "import pandas as pd\nimport torch\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nimport numpy as np\n\n# Load dataset\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\nprint(\"Loading dataset...\")\ndata = pd.read_csv(file_path)\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\n\n# Define Bounding Box (Western US region)\nLAT_MAX, LON_MAX = 48.864715, -120.190430\nLAT_MIN, LON_MIN = 45.798170, -124.145508\n\n# Step 1: Filter Data Within the Bounding Box\nprint(\"Filtering data within the bounding box...\")\ndata = data[\n    (data['lat'] <= LAT_MAX) & (data['lat'] >= LAT_MIN) & \n    (data['lon'] <= LON_MAX) & (data['lon'] >= LON_MIN)\n]\nprint(f\"Filtered dataset with {data.shape[0]} rows.\")\n\n# Step 2: Apply Grid-Based Sampling\ngrid_size = 0.05  # Reduced grid size to increase node density\nlat_bins = np.arange(LAT_MIN, LAT_MAX, grid_size)\nlon_bins = np.arange(LON_MIN, LON_MAX, grid_size)\n\nsampled_data = []\nfor lat_bin in lat_bins:\n    for lon_bin in lon_bins:\n        subset = data[\n            (data['lat'] >= lat_bin) & (data['lat'] < lat_bin + grid_size) &\n            (data['lon'] >= lon_bin) & (data['lon'] < lon_bin + grid_size)\n        ]\n        if not subset.empty:\n            sampled_data.append(subset.sample(1, random_state=42))  \n\nsampled_data = pd.concat(sampled_data)\nprint(f\"Grid-based sampled dataset with {sampled_data.shape[0]} rows.\")\n\n# Step 3: Normalize Features\nselected_features = [\n    'swe_value', 'SWE', 'fsca', 'air_temperature_tmmx', \n    'air_temperature_tmmn', 'potential_evapotranspiration', \n    'relative_humidity_rmax', 'Elevation', 'Slope', 'Curvature', \n    'Aspect', 'Eastness', 'Northness'\n]\n\nprint(\"Normalizing features...\")\nscaler = StandardScaler()\nsampled_data[selected_features] = scaler.fit_transform(sampled_data[selected_features])\n\n# Step 4: Normalize Labels\nprint(\"Normalizing target variable (swe_value)...\")\nlabel_scaler = StandardScaler()\nsampled_data['swe_value'] = label_scaler.fit_transform(sampled_data['swe_value'].values.reshape(-1, 1))\n\n# Step 5: Save Normalized Dataset\noutput_file_path = 'normalized_grid_sampled_snotel.csv'\nsampled_data.to_csv(output_file_path, index=False)\nprint(f\"Normalized sampled dataset saved as '{output_file_path}'.\")\n\n# Step 6: Create Node Features Tensor\nnode_features = torch.tensor(sampled_data[selected_features].values, dtype=torch.float)\nprint(f\"Node features tensor created with shape: {node_features.shape}\")\n\n# Step 7: Create Labels Tensor\nlabels = torch.tensor(sampled_data['swe_value'].values, dtype=torch.float)\nprint(f\"Labels tensor created with shape: {labels.shape}\")\n\n# Step 8: Build KDTree and Construct Edges Using Distance Threshold\nprint(\"Building KDTree and constructing edges using distance threshold...\")\ncoordinates = sampled_data[['lat', 'lon']].values\ntree = KDTree(coordinates)\n\nk = 20  \nradius = 0.1  \nedges = []\nfor i in range(len(coordinates)):\n    knn_neighbors = tree.query(coordinates[i], k=k+1)[1][1:]  \n    radius_neighbors = tree.query_ball_point(coordinates[i], r=radius)  \n    \n    for neighbor in set(knn_neighbors).union(radius_neighbors): \n        if i != neighbor:\n            edges.append((i, neighbor))\n\n\nprint(f\"Number of edges created: {len(edges)}\")\n\n# Convert to Edge Index Tensor\nedge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\nprint(f\"Edge index tensor created with shape: {edge_index.shape}\")\n\n# Step 9: Create PyTorch Geometric Graph Data Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# Save the PyTorch Geometric Data Object\n# graph_data_output_path = '/media/volume1/gat_spatial_training_data_sampled_radius_0.1_20.pt'\n# torch.save(graph_data, graph_data_output_path)\n# print(f\"PyTorch Geometric Data object saved as '{graph_data_output_path}'.\")",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 105 columns.\nFiltering data within the bounding box...\nFiltered dataset with 360859 rows.\nGrid-based sampled dataset with 304 rows.\nNormalizing features...\nNormalizing target variable (swe_value)...\nNormalized sampled dataset saved as 'normalized_grid_sampled_snotel.csv'.\nNode features tensor created with shape: torch.Size([304, 13])\nLabels tensor created with shape: torch.Size([304])\nBuilding KDTree and constructing edges using distance threshold...\nNumber of edges created: 6080\nEdge index tensor created with shape: torch.Size([2, 6080])\n",
  "history_begin_time" : 1741029449965,
  "history_end_time" : 1741029518122,
  "history_notes" : null,
  "history_process" : "3lobvp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "kkescycsgvNj",
  "history_input" : "import pandas as pd\nimport torch\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nimport numpy as np\n\n# Load dataset\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\nprint(\"Loading dataset...\")\ndata = pd.read_csv(file_path)\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\n\n# Define Bounding Box (Western US region)\nLAT_MAX, LON_MAX = 48.864715, -120.190430\nLAT_MIN, LON_MIN = 45.798170, -124.145508\n\n# Step 1: Filter Data Within the Bounding Box\nprint(\"Filtering data within the bounding box...\")\ndata = data[\n    (data['lat'] <= LAT_MAX) & (data['lat'] >= LAT_MIN) & \n    (data['lon'] <= LON_MAX) & (data['lon'] >= LON_MIN)\n]\nprint(f\"Filtered dataset with {data.shape[0]} rows.\")\n\n# Step 2: Apply Grid-Based Sampling\ngrid_size = 0.1  # Reduced grid size to increase node density\nlat_bins = np.arange(LAT_MIN, LAT_MAX, grid_size)\nlon_bins = np.arange(LON_MIN, LON_MAX, grid_size)\n\nsampled_data = []\nfor lat_bin in lat_bins:\n    for lon_bin in lon_bins:\n        subset = data[\n            (data['lat'] >= lat_bin) & (data['lat'] < lat_bin + grid_size) &\n            (data['lon'] >= lon_bin) & (data['lon'] < lon_bin + grid_size)\n        ]\n        if not subset.empty:\n            sampled_data.append(subset.sample(1, random_state=42))  \n\nsampled_data = pd.concat(sampled_data)\nprint(f\"Grid-based sampled dataset with {sampled_data.shape[0]} rows.\")\n\n# Step 3: Normalize Features\nselected_features = [\n    'swe_value', 'SWE', 'fsca', 'air_temperature_tmmx', \n    'air_temperature_tmmn', 'potential_evapotranspiration', \n    'relative_humidity_rmax', 'Elevation', 'Slope', 'Curvature', \n    'Aspect', 'Eastness', 'Northness'\n]\n\nprint(\"Normalizing features...\")\nscaler = StandardScaler()\nsampled_data[selected_features] = scaler.fit_transform(sampled_data[selected_features])\n\n# Step 4: Normalize Labels\nprint(\"Normalizing target variable (swe_value)...\")\nlabel_scaler = StandardScaler()\nsampled_data['swe_value'] = label_scaler.fit_transform(sampled_data['swe_value'].values.reshape(-1, 1))\n\n# Step 5: Save Normalized Dataset\noutput_file_path = 'normalized_grid_sampled_snotel.csv'\nsampled_data.to_csv(output_file_path, index=False)\nprint(f\"Normalized sampled dataset saved as '{output_file_path}'.\")\n\n# Step 6: Create Node Features Tensor\nnode_features = torch.tensor(sampled_data[selected_features].values, dtype=torch.float)\nprint(f\"Node features tensor created with shape: {node_features.shape}\")\n\n# Step 7: Create Labels Tensor\nlabels = torch.tensor(sampled_data['swe_value'].values, dtype=torch.float)\nprint(f\"Labels tensor created with shape: {labels.shape}\")\n\n# Step 8: Build KDTree and Construct Edges Using Distance Threshold\nprint(\"Building KDTree and constructing edges using distance threshold...\")\ncoordinates = sampled_data[['lat', 'lon']].values\ntree = KDTree(coordinates)\n\nk = 20  \nradius = 0.1  \nedges = []\nfor i in range(len(coordinates)):\n    knn_neighbors = tree.query(coordinates[i], k=k+1)[1][1:]  \n    radius_neighbors = tree.query_ball_point(coordinates[i], r=radius)  \n    \n    for neighbor in set(knn_neighbors).union(radius_neighbors): \n        if i != neighbor:\n            edges.append((i, neighbor))\n\n\nprint(f\"Number of edges created: {len(edges)}\")\n\n# Convert to Edge Index Tensor\nedge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\nprint(f\"Edge index tensor created with shape: {edge_index.shape}\")\n\n# Step 9: Create PyTorch Geometric Graph Data Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# Save the PyTorch Geometric Data Object\n# graph_data_output_path = '/media/volume1/gat_spatial_training_data_sampled_radius_0.1_20.pt'\n# torch.save(graph_data, graph_data_output_path)\n# print(f\"PyTorch Geometric Data object saved as '{graph_data_output_path}'.\")",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 105 columns.\nFiltering data within the bounding box...\nFiltered dataset with 360859 rows.\nGrid-based sampled dataset with 247 rows.\nNormalizing features...\nNormalizing target variable (swe_value)...\nNormalized sampled dataset saved as 'normalized_grid_sampled_snotel.csv'.\nNode features tensor created with shape: torch.Size([247, 13])\nLabels tensor created with shape: torch.Size([247])\nBuilding KDTree and constructing edges using distance threshold...\nNumber of edges created: 4940\nEdge index tensor created with shape: torch.Size([2, 4940])\n",
  "history_begin_time" : 1741028340890,
  "history_end_time" : 1741028403095,
  "history_notes" : null,
  "history_process" : "3lobvp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "IuPyBkDwonWX",
  "history_input" : "import pandas as pd\nimport torch\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nimport numpy as np\n\n# Load dataset\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\nprint(\"Loading dataset...\")\ndata = pd.read_csv(file_path)\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\n\n# Define Bounding Box (Western US region)\nLAT_MAX, LON_MAX = 48.864715, -120.190430\nLAT_MIN, LON_MIN = 45.798170, -124.145508\n\n# Step 1: Filter Data Within the Bounding Box\nprint(\"Filtering data within the bounding box...\")\ndata = data[\n    (data['lat'] <= LAT_MAX) & (data['lat'] >= LAT_MIN) & \n    (data['lon'] <= LON_MAX) & (data['lon'] >= LON_MIN)\n]\nprint(f\"Filtered dataset with {data.shape[0]} rows.\")\n\n# Step 2: Apply Grid-Based Sampling\ngrid_size = 0.1  # Reduced grid size to increase node density\nlat_bins = np.arange(LAT_MIN, LAT_MAX, grid_size)\nlon_bins = np.arange(LON_MIN, LON_MAX, grid_size)\n\nsampled_data = []\nfor lat_bin in lat_bins:\n    for lon_bin in lon_bins:\n        subset = data[\n            (data['lat'] >= lat_bin) & (data['lat'] < lat_bin + grid_size) &\n            (data['lon'] >= lon_bin) & (data['lon'] < lon_bin + grid_size)\n        ]\n        if not subset.empty:\n            sampled_data.append(subset.sample(1, random_state=42))  \n\nsampled_data = pd.concat(sampled_data)\nprint(f\"Grid-based sampled dataset with {sampled_data.shape[0]} rows.\")\n\n# Step 3: Normalize Features\nselected_features = [\n    'swe_value', 'SWE', 'fsca', 'air_temperature_tmmx', \n    'air_temperature_tmmn', 'potential_evapotranspiration', \n    'relative_humidity_rmax', 'Elevation', 'Slope', 'Curvature', \n    'Aspect', 'Eastness', 'Northness'\n]\n\nprint(\"Normalizing features...\")\nscaler = StandardScaler()\nsampled_data[selected_features] = scaler.fit_transform(sampled_data[selected_features])\n\n# Step 4: Normalize Labels\nprint(\"Normalizing target variable (swe_value)...\")\nlabel_scaler = StandardScaler()\nsampled_data['swe_value'] = label_scaler.fit_transform(sampled_data['swe_value'].values.reshape(-1, 1))\n\n# Step 5: Save Normalized Dataset\noutput_file_path = 'normalized_grid_sampled_snotel.csv'\nsampled_data.to_csv(output_file_path, index=False)\nprint(f\"Normalized sampled dataset saved as '{output_file_path}'.\")\n\n# Step 6: Create Node Features Tensor\nnode_features = torch.tensor(sampled_data[selected_features].values, dtype=torch.float)\nprint(f\"Node features tensor created with shape: {node_features.shape}\")\n\n# Step 7: Create Labels Tensor\nlabels = torch.tensor(sampled_data['swe_value'].values, dtype=torch.float)\nprint(f\"Labels tensor created with shape: {labels.shape}\")\n\n# Step 8: Build KDTree and Construct Edges Using Distance Threshold\nprint(\"Building KDTree and constructing edges using distance threshold...\")\ncoordinates = sampled_data[['lat', 'lon']].values\ntree = KDTree(coordinates)\n\nradius = 0.05  \nedges = []\nfor i in range(len(coordinates)):\n    neighbors = tree.query_ball_point(coordinates[i], r=radius)\n    for neighbor in neighbors:\n        if i != neighbor:\n            edges.append((i, neighbor))\n\nprint(f\"Number of edges created: {len(edges)}\")\n\n# Convert to Edge Index Tensor\nedge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\nprint(f\"Edge index tensor created with shape: {edge_index.shape}\")\n\n# Step 9: Create PyTorch Geometric Graph Data Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# Save the PyTorch Geometric Data Object\ngraph_data_output_path = '/media/volume1/gat_spatial_training_data_sampled_radius_0.05.pt'\ntorch.save(graph_data, graph_data_output_path)\nprint(f\"PyTorch Geometric Data object saved as '{graph_data_output_path}'.\")",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 105 columns.\nFiltering data within the bounding box...\nFiltered dataset with 360859 rows.\nGrid-based sampled dataset with 247 rows.\nNormalizing features...\nNormalizing target variable (swe_value)...\nNormalized sampled dataset saved as 'normalized_grid_sampled_snotel.csv'.\nNode features tensor created with shape: torch.Size([247, 13])\nLabels tensor created with shape: torch.Size([247])\nBuilding KDTree and constructing edges using distance threshold...\nNumber of edges created: 40\nEdge index tensor created with shape: torch.Size([2, 40])\nPyTorch Geometric Data object saved as '/media/volume1/gat_spatial_training_data_sampled_radius_0.05.pt'.\n",
  "history_begin_time" : 1741028042108,
  "history_end_time" : 1741028104369,
  "history_notes" : null,
  "history_process" : "3lobvp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "yfUYolN5Glte",
  "history_input" : "import pandas as pd\nimport torch\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nimport numpy as np\n\n# Load dataset\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\nprint(\"Loading dataset...\")\ndata = pd.read_csv(file_path)\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\n\n# Define Bounding Box (Western US region)\nLAT_MAX, LON_MAX = 48.864715, -120.190430\nLAT_MIN, LON_MIN = 45.798170, -124.145508\n\n# Step 1: Filter Data Within the Bounding Box\nprint(\"Filtering data within the bounding box...\")\ndata = data[\n    (data['lat'] <= LAT_MAX) & (data['lat'] >= LAT_MIN) & \n    (data['lon'] <= LON_MAX) & (data['lon'] >= LON_MIN)\n]\nprint(f\"Filtered dataset with {data.shape[0]} rows.\")\n\n# Step 2: Apply Grid-Based Sampling\ngrid_size = 0.2\nlat_bins = np.arange(LAT_MIN, LAT_MAX, grid_size)\nlon_bins = np.arange(LON_MIN, LON_MAX, grid_size)\n\nsampled_data = []\nfor lat_bin in lat_bins:\n    for lon_bin in lon_bins:\n        subset = data[\n            (data['lat'] >= lat_bin) & (data['lat'] < lat_bin + grid_size) &\n            (data['lon'] >= lon_bin) & (data['lon'] < lon_bin + grid_size)\n        ]\n        if not subset.empty:\n            sampled_data.append(subset.sample(1, random_state=42))  \n\nsampled_data = pd.concat(sampled_data)\nprint(f\"Grid-based sampled dataset with {sampled_data.shape[0]} rows.\")\n\n# Step 3: Normalize Features\nselected_features = [\n    'swe_value', 'SWE', 'fsca', 'air_temperature_tmmx', \n    'air_temperature_tmmn', 'potential_evapotranspiration', \n    'relative_humidity_rmax', 'Elevation', 'Slope', 'Curvature', \n    'Aspect', 'Eastness', 'Northness'\n]\n\nprint(\"Normalizing features...\")\nscaler = StandardScaler()\nsampled_data[selected_features] = scaler.fit_transform(sampled_data[selected_features])\n\n# Step 4: Normalize Labels\nprint(\"Normalizing target variable (swe_value)...\")\nlabel_scaler = StandardScaler()\nsampled_data['swe_value'] = label_scaler.fit_transform(sampled_data['swe_value'].values.reshape(-1, 1))\n\n# Step 5: Save Normalized Dataset\noutput_file_path = 'normalized_grid_sampled_snotel.csv'\nsampled_data.to_csv(output_file_path, index=False)\nprint(f\"Normalized sampled dataset saved as '{output_file_path}'.\")\n\n# Step 6: Create Node Features Tensor\nnode_features = torch.tensor(sampled_data[selected_features].values, dtype=torch.float)\nprint(f\"Node features tensor created with shape: {node_features.shape}\")\n\n# Step 7: Create Labels Tensor\nlabels = torch.tensor(sampled_data['swe_value'].values, dtype=torch.float)\nprint(f\"Labels tensor created with shape: {labels.shape}\")\n\n# Step 8: Build KDTree and Construct Edges Using KNN\nprint(\"Building KDTree and constructing edges using KNN...\")\ncoordinates = sampled_data[['lat', 'lon']].values\ntree = KDTree(coordinates)\n\nk = 20  # Each node connects to 5 nearest neighbors\nedges = []\nfor i in range(len(coordinates)):\n    neighbors = tree.query(coordinates[i], k=k+1)[1][1:]  \n    for neighbor in neighbors:\n        edges.append((i, neighbor))\n\nprint(f\"Number of edges created: {len(edges)}\")\n\n# Convert to Edge Index Tensor\nedge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\nprint(f\"Edge index tensor created with shape: {edge_index.shape}\")\n\n# Step 9: Create PyTorch Geometric Graph Data Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# Save the PyTorch Geometric Data Object\ngraph_data_output_path = '/media/volume1/gat_spatial_training_data_sampled_k20_g02.pt'\ntorch.save(graph_data, graph_data_output_path)\nprint(f\"PyTorch Geometric Data object saved as '{graph_data_output_path}'.\")",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 105 columns.\nFiltering data within the bounding box...\nFiltered dataset with 360859 rows.\nGrid-based sampled dataset with 163 rows.\nNormalizing features...\nNormalizing target variable (swe_value)...\nNormalized sampled dataset saved as 'normalized_grid_sampled_snotel.csv'.\nNode features tensor created with shape: torch.Size([163, 13])\nLabels tensor created with shape: torch.Size([163])\nBuilding KDTree and constructing edges using KNN...\nNumber of edges created: 3260\nEdge index tensor created with shape: torch.Size([2, 3260])\nPyTorch Geometric Data object saved as '/media/volume1/gat_spatial_training_data_sampled_k20_g02.pt'.\n",
  "history_begin_time" : 1741026910521,
  "history_end_time" : 1741026972182,
  "history_notes" : null,
  "history_process" : "3lobvp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "D25jt38wD53X",
  "history_input" : "import pandas as pd\nimport torch\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nimport numpy as np\n\n# Load dataset\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\nprint(\"Loading dataset...\")\ndata = pd.read_csv(file_path)\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\n\n# Define Bounding Box (Western US region)\nLAT_MAX, LON_MAX = 48.864715, -120.190430\nLAT_MIN, LON_MIN = 45.798170, -124.145508\n\n# Step 1: Filter Data Within the Bounding Box\nprint(\"Filtering data within the bounding box...\")\ndata = data[\n    (data['lat'] <= LAT_MAX) & (data['lat'] >= LAT_MIN) & \n    (data['lon'] <= LON_MAX) & (data['lon'] >= LON_MIN)\n]\nprint(f\"Filtered dataset with {data.shape[0]} rows.\")\n\n# Step 2: Apply Grid-Based Sampling\ngrid_size = 0.5 \nlat_bins = np.arange(LAT_MIN, LAT_MAX, grid_size)\nlon_bins = np.arange(LON_MIN, LON_MAX, grid_size)\n\nsampled_data = []\nfor lat_bin in lat_bins:\n    for lon_bin in lon_bins:\n        subset = data[\n            (data['lat'] >= lat_bin) & (data['lat'] < lat_bin + grid_size) &\n            (data['lon'] >= lon_bin) & (data['lon'] < lon_bin + grid_size)\n        ]\n        if not subset.empty:\n            sampled_data.append(subset.sample(1, random_state=42))  \n\nsampled_data = pd.concat(sampled_data)\nprint(f\"Grid-based sampled dataset with {sampled_data.shape[0]} rows.\")\n\n# Step 3: Normalize Features\nselected_features = [\n    'swe_value', 'SWE', 'fsca', 'air_temperature_tmmx', \n    'air_temperature_tmmn', 'potential_evapotranspiration', \n    'relative_humidity_rmax', 'Elevation', 'Slope', 'Curvature', \n    'Aspect', 'Eastness', 'Northness'\n]\n\nprint(\"Normalizing features...\")\nscaler = StandardScaler()\nsampled_data[selected_features] = scaler.fit_transform(sampled_data[selected_features])\n\n# Step 4: Normalize Labels\nprint(\"Normalizing target variable (swe_value)...\")\nlabel_scaler = StandardScaler()\nsampled_data['swe_value'] = label_scaler.fit_transform(sampled_data['swe_value'].values.reshape(-1, 1))\n\n# Step 5: Save Normalized Dataset\noutput_file_path = 'normalized_grid_sampled_snotel.csv'\nsampled_data.to_csv(output_file_path, index=False)\nprint(f\"Normalized sampled dataset saved as '{output_file_path}'.\")\n\n# Step 6: Create Node Features Tensor\nnode_features = torch.tensor(sampled_data[selected_features].values, dtype=torch.float)\nprint(f\"Node features tensor created with shape: {node_features.shape}\")\n\n# Step 7: Create Labels Tensor\nlabels = torch.tensor(sampled_data['swe_value'].values, dtype=torch.float)\nprint(f\"Labels tensor created with shape: {labels.shape}\")\n\n# Step 8: Build KDTree and Construct Edges Using KNN\nprint(\"Building KDTree and constructing edges using KNN...\")\ncoordinates = sampled_data[['lat', 'lon']].values\ntree = KDTree(coordinates)\n\nk = 20  # Each node connects to 5 nearest neighbors\nedges = []\nfor i in range(len(coordinates)):\n    neighbors = tree.query(coordinates[i], k=k+1)[1][1:]  \n    for neighbor in neighbors:\n        edges.append((i, neighbor))\n\nprint(f\"Number of edges created: {len(edges)}\")\n\n# Convert to Edge Index Tensor\nedge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\nprint(f\"Edge index tensor created with shape: {edge_index.shape}\")\n\n# Step 9: Create PyTorch Geometric Graph Data Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# Save the PyTorch Geometric Data Object\ngraph_data_output_path = '/media/volume1/gat_spatial_training_data_sampled_k20_g05.pt'\ntorch.save(graph_data, graph_data_output_path)\nprint(f\"PyTorch Geometric Data object saved as '{graph_data_output_path}'.\")",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 105 columns.\nFiltering data within the bounding box...\nFiltered dataset with 360859 rows.\nGrid-based sampled dataset with 50 rows.\nNormalizing features...\nNormalizing target variable (swe_value)...\nNormalized sampled dataset saved as 'normalized_grid_sampled_snotel.csv'.\nNode features tensor created with shape: torch.Size([50, 13])\nLabels tensor created with shape: torch.Size([50])\nBuilding KDTree and constructing edges using KNN...\nNumber of edges created: 1000\nEdge index tensor created with shape: torch.Size([2, 1000])\nPyTorch Geometric Data object saved as '/media/volume1/gat_spatial_training_data_sampled_k20_g05.pt'.\n",
  "history_begin_time" : 1741026821161,
  "history_end_time" : 1741026880969,
  "history_notes" : null,
  "history_process" : "3lobvp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "85Xp6LTrbW4Q",
  "history_input" : "import pandas as pd\nimport torch\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nimport numpy as np\n\n# Load dataset\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\nprint(\"Loading dataset...\")\ndata = pd.read_csv(file_path)\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\n\n# Define Bounding Box (Western US region)\nLAT_MAX, LON_MAX = 48.864715, -120.190430\nLAT_MIN, LON_MIN = 45.798170, -124.145508\n\n# Step 1: Filter Data Within the Bounding Box\nprint(\"Filtering data within the bounding box...\")\ndata = data[\n    (data['lat'] <= LAT_MAX) & (data['lat'] >= LAT_MIN) & \n    (data['lon'] <= LON_MAX) & (data['lon'] >= LON_MIN)\n]\nprint(f\"Filtered dataset with {data.shape[0]} rows.\")\n\n# Step 2: Apply Grid-Based Sampling\ngrid_size = 0.2  \nlat_bins = np.arange(LAT_MIN, LAT_MAX, grid_size)\nlon_bins = np.arange(LON_MIN, LON_MAX, grid_size)\n\nsampled_data = []\nfor lat_bin in lat_bins:\n    for lon_bin in lon_bins:\n        subset = data[\n            (data['lat'] >= lat_bin) & (data['lat'] < lat_bin + grid_size) &\n            (data['lon'] >= lon_bin) & (data['lon'] < lon_bin + grid_size)\n        ]\n        if not subset.empty:\n            sampled_data.append(subset.sample(1, random_state=42))  \n\nsampled_data = pd.concat(sampled_data)\nprint(f\"Grid-based sampled dataset with {sampled_data.shape[0]} rows.\")\n\n# Step 3: Normalize Features\nselected_features = [\n    'swe_value', 'SWE', 'fsca', 'air_temperature_tmmx', \n    'air_temperature_tmmn', 'potential_evapotranspiration', \n    'relative_humidity_rmax', 'Elevation', 'Slope', 'Curvature', \n    'Aspect', 'Eastness', 'Northness'\n]\n\nprint(\"Normalizing features...\")\nscaler = StandardScaler()\nsampled_data[selected_features] = scaler.fit_transform(sampled_data[selected_features])\n\n# Step 4: Normalize Labels\nprint(\"Normalizing target variable (swe_value)...\")\nlabel_scaler = StandardScaler()\nsampled_data['swe_value'] = label_scaler.fit_transform(sampled_data['swe_value'].values.reshape(-1, 1))\n\n# Step 5: Save Normalized Dataset\noutput_file_path = 'normalized_grid_sampled_snotel.csv'\nsampled_data.to_csv(output_file_path, index=False)\nprint(f\"Normalized sampled dataset saved as '{output_file_path}'.\")\n\n# Step 6: Create Node Features Tensor\nnode_features = torch.tensor(sampled_data[selected_features].values, dtype=torch.float)\nprint(f\"Node features tensor created with shape: {node_features.shape}\")\n\n# Step 7: Create Labels Tensor\nlabels = torch.tensor(sampled_data['swe_value'].values, dtype=torch.float)\nprint(f\"Labels tensor created with shape: {labels.shape}\")\n\n# Step 8: Build KDTree and Construct Edges Using KNN\nprint(\"Building KDTree and constructing edges using KNN...\")\ncoordinates = sampled_data[['lat', 'lon']].values\ntree = KDTree(coordinates)\n\nk = 20  # Each node connects to 5 nearest neighbors\nedges = []\nfor i in range(len(coordinates)):\n    neighbors = tree.query(coordinates[i], k=k+1)[1][1:]  \n    for neighbor in neighbors:\n        edges.append((i, neighbor))\n\nprint(f\"Number of edges created: {len(edges)}\")\n\n# Convert to Edge Index Tensor\nedge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\nprint(f\"Edge index tensor created with shape: {edge_index.shape}\")\n\n# Step 9: Create PyTorch Geometric Graph Data Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# Save the PyTorch Geometric Data Object\ngraph_data_output_path = '/media/volume1/gat_spatial_training_data_sampled_knn_30km.pt'\ntorch.save(graph_data, graph_data_output_path)\nprint(f\"PyTorch Geometric Data object saved as '{graph_data_output_path}'.\")",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 105 columns.\nFiltering data within the bounding box...\nFiltered dataset with 360859 rows.\nGrid-based sampled dataset with 163 rows.\nNormalizing features...\nNormalizing target variable (swe_value)...\nNormalized sampled dataset saved as 'normalized_grid_sampled_snotel.csv'.\nNode features tensor created with shape: torch.Size([163, 13])\nLabels tensor created with shape: torch.Size([163])\nBuilding KDTree and constructing edges using KNN...\nNumber of edges created: 3260\nEdge index tensor created with shape: torch.Size([2, 3260])\nPyTorch Geometric Data object saved as '/media/volume1/gat_spatial_training_data_sampled_knn_30km.pt'.\n",
  "history_begin_time" : 1741026651639,
  "history_end_time" : 1741026713038,
  "history_notes" : "k=2-",
  "history_process" : "3lobvp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "BANbEm3GhRLC",
  "history_input" : "import pandas as pd\nimport torch\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nimport numpy as np\n\n# Load dataset\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\nprint(\"Loading dataset...\")\ndata = pd.read_csv(file_path)\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\n\n# Define Bounding Box (Western US region)\nLAT_MAX, LON_MAX = 48.864715, -120.190430\nLAT_MIN, LON_MIN = 45.798170, -124.145508\n\n# Step 1: Filter Data Within the Bounding Box\nprint(\"Filtering data within the bounding box...\")\ndata = data[\n    (data['lat'] <= LAT_MAX) & (data['lat'] >= LAT_MIN) & \n    (data['lon'] <= LON_MAX) & (data['lon'] >= LON_MIN)\n]\nprint(f\"Filtered dataset with {data.shape[0]} rows.\")\n\n# Step 2: Apply Grid-Based Sampling\ngrid_size = 0.2  \nlat_bins = np.arange(LAT_MIN, LAT_MAX, grid_size)\nlon_bins = np.arange(LON_MIN, LON_MAX, grid_size)\n\nsampled_data = []\nfor lat_bin in lat_bins:\n    for lon_bin in lon_bins:\n        subset = data[\n            (data['lat'] >= lat_bin) & (data['lat'] < lat_bin + grid_size) &\n            (data['lon'] >= lon_bin) & (data['lon'] < lon_bin + grid_size)\n        ]\n        if not subset.empty:\n            sampled_data.append(subset.sample(1, random_state=42))  \n\nsampled_data = pd.concat(sampled_data)\nprint(f\"Grid-based sampled dataset with {sampled_data.shape[0]} rows.\")\n\n# Step 3: Normalize Features\nselected_features = [\n    'swe_value', 'SWE', 'fsca', 'air_temperature_tmmx', \n    'air_temperature_tmmn', 'potential_evapotranspiration', \n    'relative_humidity_rmax', 'Elevation', 'Slope', 'Curvature', \n    'Aspect', 'Eastness', 'Northness'\n]\n\nprint(\"Normalizing features...\")\nscaler = StandardScaler()\nsampled_data[selected_features] = scaler.fit_transform(sampled_data[selected_features])\n\n# Step 4: Normalize Labels\nprint(\"Normalizing target variable (swe_value)...\")\nlabel_scaler = StandardScaler()\nsampled_data['swe_value'] = label_scaler.fit_transform(sampled_data['swe_value'].values.reshape(-1, 1))\n\n# Step 5: Save Normalized Dataset\noutput_file_path = 'normalized_grid_sampled_snotel.csv'\nsampled_data.to_csv(output_file_path, index=False)\nprint(f\"Normalized sampled dataset saved as '{output_file_path}'.\")\n\n# Step 6: Create Node Features Tensor\nnode_features = torch.tensor(sampled_data[selected_features].values, dtype=torch.float)\nprint(f\"Node features tensor created with shape: {node_features.shape}\")\n\n# Step 7: Create Labels Tensor\nlabels = torch.tensor(sampled_data['swe_value'].values, dtype=torch.float)\nprint(f\"Labels tensor created with shape: {labels.shape}\")\n\n# Step 8: Build KDTree and Construct Edges Using KNN\nprint(\"Building KDTree and constructing edges using KNN...\")\ncoordinates = sampled_data[['lat', 'lon']].values\ntree = KDTree(coordinates)\n\nk = 10  # Each node connects to 5 nearest neighbors\nedges = []\nfor i in range(len(coordinates)):\n    neighbors = tree.query(coordinates[i], k=k+1)[1][1:]  \n    for neighbor in neighbors:\n        edges.append((i, neighbor))\n\nprint(f\"Number of edges created: {len(edges)}\")\n\n# Convert to Edge Index Tensor\nedge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\nprint(f\"Edge index tensor created with shape: {edge_index.shape}\")\n\n# Step 9: Create PyTorch Geometric Graph Data Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# Save the PyTorch Geometric Data Object\ngraph_data_output_path = '/media/volume1/gat_spatial_training_data_sampled_knn_20km.pt'\ntorch.save(graph_data, graph_data_output_path)\nprint(f\"PyTorch Geometric Data object saved as '{graph_data_output_path}'.\")",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 105 columns.\nFiltering data within the bounding box...\nFiltered dataset with 360859 rows.\nGrid-based sampled dataset with 163 rows.\nNormalizing features...\nNormalizing target variable (swe_value)...\nNormalized sampled dataset saved as 'normalized_grid_sampled_snotel.csv'.\nNode features tensor created with shape: torch.Size([163, 13])\nLabels tensor created with shape: torch.Size([163])\nBuilding KDTree and constructing edges using KNN...\nNumber of edges created: 1630\nEdge index tensor created with shape: torch.Size([2, 1630])\nPyTorch Geometric Data object saved as '/media/volume1/gat_spatial_training_data_sampled_knn_20km.pt'.\n",
  "history_begin_time" : 1741026562266,
  "history_end_time" : 1741026623411,
  "history_notes" : "k=10 e=1630",
  "history_process" : "3lobvp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "ADQO3KQQT7zo",
  "history_input" : "import pandas as pd\nimport torch\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nimport numpy as np\n\n# Load dataset\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\nprint(\"Loading dataset...\")\ndata = pd.read_csv(file_path)\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\n\n# Define Bounding Box (Western US region)\nLAT_MAX, LON_MAX = 48.864715, -120.190430\nLAT_MIN, LON_MIN = 45.798170, -124.145508\n\n# Step 1: Filter Data Within the Bounding Box\nprint(\"Filtering data within the bounding box...\")\ndata = data[\n    (data['lat'] <= LAT_MAX) & (data['lat'] >= LAT_MIN) & \n    (data['lon'] <= LON_MAX) & (data['lon'] >= LON_MIN)\n]\nprint(f\"Filtered dataset with {data.shape[0]} rows.\")\n\n# Step 2: Apply Grid-Based Sampling\ngrid_size = 0.1  # Grid step size for lat/lon\nlat_bins = np.arange(LAT_MIN, LAT_MAX, grid_size)\nlon_bins = np.arange(LON_MIN, LON_MAX, grid_size)\n\nsampled_data = []\nfor lat_bin in lat_bins:\n    for lon_bin in lon_bins:\n        subset = data[\n            (data['lat'] >= lat_bin) & (data['lat'] < lat_bin + grid_size) &\n            (data['lon'] >= lon_bin) & (data['lon'] < lon_bin + grid_size)\n        ]\n        if not subset.empty:\n            sampled_data.append(subset.sample(1, random_state=42))  # Pick one representative point\n\nsampled_data = pd.concat(sampled_data)\nprint(f\"Grid-based sampled dataset with {sampled_data.shape[0]} rows.\")\n\n# Step 3: Normalize Features\nselected_features = [\n    'swe_value', 'SWE', 'fsca', 'air_temperature_tmmx', \n    'air_temperature_tmmn', 'potential_evapotranspiration', \n    'relative_humidity_rmax', 'Elevation', 'Slope', 'Curvature', \n    'Aspect', 'Eastness', 'Northness'\n]\n\nprint(\"Normalizing features...\")\nscaler = StandardScaler()\nsampled_data[selected_features] = scaler.fit_transform(sampled_data[selected_features])\n\n# Step 4: Normalize Labels\nprint(\"Normalizing target variable (swe_value)...\")\nlabel_scaler = StandardScaler()\nsampled_data['swe_value'] = label_scaler.fit_transform(sampled_data['swe_value'].values.reshape(-1, 1))\n\n# Step 5: Save Normalized Dataset\noutput_file_path = 'normalized_grid_sampled_snotel.csv'\nsampled_data.to_csv(output_file_path, index=False)\nprint(f\"Normalized sampled dataset saved as '{output_file_path}'.\")\n\n# Step 6: Create Node Features Tensor\nnode_features = torch.tensor(sampled_data[selected_features].values, dtype=torch.float)\nprint(f\"Node features tensor created with shape: {node_features.shape}\")\n\n# Step 7: Create Labels Tensor\nlabels = torch.tensor(sampled_data['swe_value'].values, dtype=torch.float)\nprint(f\"Labels tensor created with shape: {labels.shape}\")\n\n# Step 8: Build KDTree and Construct Edges Using KNN\nprint(\"Building KDTree and constructing edges using KNN...\")\ncoordinates = sampled_data[['lat', 'lon']].values\ntree = KDTree(coordinates)\n\nk = 5  # Each node connects to 5 nearest neighbors\nedges = []\nfor i in range(len(coordinates)):\n    neighbors = tree.query(coordinates[i], k=k+1)[1][1:]  # Skip self (first index)\n    for neighbor in neighbors:\n        edges.append((i, neighbor))\n\nprint(f\"Number of edges created: {len(edges)}\")\n\n# Convert to Edge Index Tensor\nedge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\nprint(f\"Edge index tensor created with shape: {edge_index.shape}\")\n\n# Step 9: Create PyTorch Geometric Graph Data Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# Save the PyTorch Geometric Data Object\ngraph_data_output_path = '/media/volume1/gat_spatial_training_data_sampled_knn_10km.pt'\ntorch.save(graph_data, graph_data_output_path)\nprint(f\"PyTorch Geometric Data object saved as '{graph_data_output_path}'.\")",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 105 columns.\nFiltering data within the bounding box...\nFiltered dataset with 360859 rows.\nGrid-based sampled dataset with 247 rows.\nNormalizing features...\nNormalizing target variable (swe_value)...\nNormalized sampled dataset saved as 'normalized_grid_sampled_snotel.csv'.\nNode features tensor created with shape: torch.Size([247, 13])\nLabels tensor created with shape: torch.Size([247])\nBuilding KDTree and constructing edges using KNN...\nNumber of edges created: 1235\nEdge index tensor created with shape: torch.Size([2, 1235])\nPyTorch Geometric Data object saved as '/media/volume1/gat_spatial_training_data_sampled_knn_10km.pt'.\n",
  "history_begin_time" : 1741023912006,
  "history_end_time" : 1741023973675,
  "history_notes" : "k=5  e =1235",
  "history_process" : "3lobvp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "XXl35fiDsXTI",
  "history_input" : "import pandas as pd\nimport torch\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nimport numpy as np\n\n# Load dataset\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\nprint(\"Loading dataset...\")\ndata = pd.read_csv(file_path)\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\n\n# Define Bounding Box (Western US region)\nLAT_MAX, LON_MAX = 48.864715, -120.190430\nLAT_MIN, LON_MIN = 45.798170, -124.145508\n\n# Step 1: Filter Data Within the Bounding Box\nprint(\"Filtering data within the bounding box...\")\ndata = data[\n    (data['lat'] <= LAT_MAX) & (data['lat'] >= LAT_MIN) & \n    (data['lon'] <= LON_MAX) & (data['lon'] >= LON_MIN)\n]\nprint(f\"Filtered dataset with {data.shape[0]} rows.\")\n\n# Step 2: Apply Grid-Based Sampling\ngrid_size = 0.1 \nlat_bins = np.arange(LAT_MIN, LAT_MAX, grid_size)\nlon_bins = np.arange(LON_MIN, LON_MAX, grid_size)\n\nsampled_data = []\nfor lat_bin in lat_bins:\n    for lon_bin in lon_bins:\n        subset = data[\n            (data['lat'] >= lat_bin) & (data['lat'] < lat_bin + grid_size) &\n            (data['lon'] >= lon_bin) & (data['lon'] < lon_bin + grid_size)\n        ]\n        if not subset.empty:\n            sampled_data.append(subset.sample(1, random_state=42))  # Pick one representative point\n\nsampled_data = pd.concat(sampled_data)\nprint(f\"Grid-based sampled dataset with {sampled_data.shape[0]} rows.\")\n\n# Step 3: Normalize Features\nselected_features = [\n    'swe_value', 'SWE', 'fsca', 'air_temperature_tmmx', \n    'air_temperature_tmmn', 'potential_evapotranspiration', \n    'relative_humidity_rmax', 'Elevation', 'Slope', 'Curvature', \n    'Aspect', 'Eastness', 'Northness'\n]\n\nprint(\"Normalizing features...\")\nscaler = StandardScaler()\nsampled_data[selected_features] = scaler.fit_transform(sampled_data[selected_features])\n\n# Step 4: Normalize Labels\nprint(\"Normalizing target variable (swe_value)...\")\nlabel_scaler = StandardScaler()\nsampled_data['swe_value'] = label_scaler.fit_transform(sampled_data['swe_value'].values.reshape(-1, 1))\n\n# Step 5: Save Normalized Dataset\noutput_file_path = 'normalized_grid_sampled_snotel.csv'\nsampled_data.to_csv(output_file_path, index=False)\nprint(f\"Normalized sampled dataset saved as '{output_file_path}'.\")\n\n# Step 6: Create Node Features Tensor\nnode_features = torch.tensor(sampled_data[selected_features].values, dtype=torch.float)\nprint(f\"Node features tensor created with shape: {node_features.shape}\")\n\n# Step 7: Create Labels Tensor\nlabels = torch.tensor(sampled_data['swe_value'].values, dtype=torch.float)\nprint(f\"Labels tensor created with shape: {labels.shape}\")\n\n# Step 8: Build KDTree and Construct Edges\nprint(\"Building KDTree and constructing edges...\")\ncoordinates = sampled_data[['lat', 'lon']].values\ntree = KDTree(coordinates)\nthreshold = 0.1\n\nbatch_edges = tree.query_pairs(r=threshold)\nedges = list(batch_edges)\nprint(f\"Number of edges created: {len(edges)}\")\n\n# Convert to Edge Index Tensor\nedge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\nprint(f\"Edge index tensor created with shape: {edge_index.shape}\")\n\n# Step 9: Create PyTorch Geometric Graph Data Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# Save the PyTorch Geometric Data Object\ngraph_data_output_path = '/media/volume1/gat_spatial_training_data_sampled_10km.pt'\ntorch.save(graph_data, graph_data_output_path)\nprint(f\"PyTorch Geometric Data object saved as '{graph_data_output_path}'.\")\n",
  "history_output" : "Loading dataset...\n",
  "history_begin_time" : 1741023318208,
  "history_end_time" : 1741023338218,
  "history_notes" : null,
  "history_process" : "3lobvp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "4TOoyMJZLaaw",
  "history_input" : "import pandas as pd\nimport torch\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nimport numpy as np\n\n# Load dataset\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\nprint(\"Loading dataset...\")\ndata = pd.read_csv(file_path)\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\n\n# Define Bounding Box (Western US region)\nLAT_MAX, LON_MAX = 48.864715, -120.190430\nLAT_MIN, LON_MIN = 45.798170, -124.145508\n\n# Step 1: Filter Data Within the Bounding Box\nprint(\"Filtering data within the bounding box...\")\ndata = data[\n    (data['lat'] <= LAT_MAX) & (data['lat'] >= LAT_MIN) & \n    (data['lon'] <= LON_MAX) & (data['lon'] >= LON_MIN)\n]\nprint(f\"Filtered dataset with {data.shape[0]} rows.\")\n\n# Step 2: Apply Grid-Based Sampling\ngrid_size = 0.1 \nlat_bins = np.arange(LAT_MIN, LAT_MAX, grid_size)\nlon_bins = np.arange(LON_MIN, LON_MAX, grid_size)\n\nsampled_data = []\nfor lat_bin in lat_bins:\n    for lon_bin in lon_bins:\n        subset = data[\n            (data['lat'] >= lat_bin) & (data['lat'] < lat_bin + grid_size) &\n            (data['lon'] >= lon_bin) & (data['lon'] < lon_bin + grid_size)\n        ]\n        if not subset.empty:\n            sampled_data.append(subset.sample(1, random_state=42))  # Pick one representative point\n\nsampled_data = pd.concat(sampled_data)\nprint(f\"Grid-based sampled dataset with {sampled_data.shape[0]} rows.\")\n\n# Step 3: Normalize Features\nselected_features = [\n    'swe_value', 'SWE', 'fsca', 'air_temperature_tmmx', \n    'air_temperature_tmmn', 'potential_evapotranspiration', \n    'relative_humidity_rmax', 'Elevation', 'Slope', 'Curvature', \n    'Aspect', 'Eastness', 'Northness'\n]\n\nprint(\"Normalizing features...\")\nscaler = StandardScaler()\nsampled_data[selected_features] = scaler.fit_transform(sampled_data[selected_features])\n\n# Step 4: Normalize Labels\nprint(\"Normalizing target variable (swe_value)...\")\nlabel_scaler = StandardScaler()\nsampled_data['swe_value'] = label_scaler.fit_transform(sampled_data['swe_value'].values.reshape(-1, 1))\n\n# Step 5: Save Normalized Dataset\noutput_file_path = 'normalized_grid_sampled_snotel.csv'\nsampled_data.to_csv(output_file_path, index=False)\nprint(f\"Normalized sampled dataset saved as '{output_file_path}'.\")\n\n# Step 6: Create Node Features Tensor\nnode_features = torch.tensor(sampled_data[selected_features].values, dtype=torch.float)\nprint(f\"Node features tensor created with shape: {node_features.shape}\")\n\n# Step 7: Create Labels Tensor\nlabels = torch.tensor(sampled_data['swe_value'].values, dtype=torch.float)\nprint(f\"Labels tensor created with shape: {labels.shape}\")\n\n# Step 8: Build KDTree and Construct Edges\nprint(\"Building KDTree and constructing edges...\")\ncoordinates = sampled_data[['lat', 'lon']].values\ntree = KDTree(coordinates)\nthreshold = 0.05\n\nbatch_edges = tree.query_pairs(r=threshold)\nedges = list(batch_edges)\nprint(f\"Number of edges created: {len(edges)}\")\n\n# Convert to Edge Index Tensor\nedge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\nprint(f\"Edge index tensor created with shape: {edge_index.shape}\")\n\n# Step 9: Create PyTorch Geometric Graph Data Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# Save the PyTorch Geometric Data Object\ngraph_data_output_path = '/media/volume1/gat_spatial_training_data_sampled_1.pt'\ntorch.save(graph_data, graph_data_output_path)\nprint(f\"PyTorch Geometric Data object saved as '{graph_data_output_path}'.\")\n",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 105 columns.\nFiltering data within the bounding box...\nFiltered dataset with 360859 rows.\nGrid-based sampled dataset with 247 rows.\nNormalizing features...\nNormalizing target variable (swe_value)...\nNormalized sampled dataset saved as 'normalized_grid_sampled_snotel.csv'.\nNode features tensor created with shape: torch.Size([247, 13])\nLabels tensor created with shape: torch.Size([247])\nBuilding KDTree and constructing edges...\nNumber of edges created: 20\nEdge index tensor created with shape: torch.Size([2, 20])\nPyTorch Geometric Data object saved as '/media/volume1/gat_spatial_training_data_sampled_1.pt'.\n",
  "history_begin_time" : 1741023024702,
  "history_end_time" : 1741023086622,
  "history_notes" : null,
  "history_process" : "3lobvp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "6FhMGLPg9wd7",
  "history_input" : "import pandas as pd\nimport torch\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nimport numpy as np\n\n# Load dataset\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\nprint(\"Loading dataset...\")\ndata = pd.read_csv(file_path)\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\n\n# Define Bounding Box (Western US region)\nLAT_MAX, LON_MAX = 48.864715, -120.190430\nLAT_MIN, LON_MIN = 45.798170, -124.145508\n\n# Step 1: Filter Data Within the Bounding Box\nprint(\"Filtering data within the bounding box...\")\ndata = data[\n    (data['lat'] <= LAT_MAX) & (data['lat'] >= LAT_MIN) & \n    (data['lon'] <= LON_MAX) & (data['lon'] >= LON_MIN)\n]\nprint(f\"Filtered dataset with {data.shape[0]} rows.\")\n\n# Step 2: Apply Grid-Based Sampling\ngrid_size = 0.1 \nlat_bins = np.arange(LAT_MIN, LAT_MAX, grid_size)\nlon_bins = np.arange(LON_MIN, LON_MAX, grid_size)\n\nsampled_data = []\nfor lat_bin in lat_bins:\n    for lon_bin in lon_bins:\n        subset = data[\n            (data['lat'] >= lat_bin) & (data['lat'] < lat_bin + grid_size) &\n            (data['lon'] >= lon_bin) & (data['lon'] < lon_bin + grid_size)\n        ]\n        if not subset.empty:\n            sampled_data.append(subset.sample(1, random_state=42))  # Pick one representative point\n\nsampled_data = pd.concat(sampled_data)\nprint(f\"Grid-based sampled dataset with {sampled_data.shape[0]} rows.\")\n\n# Step 3: Normalize Features\nselected_features = [\n    'swe_value', 'SWE', 'fsca', 'air_temperature_tmmx', \n    'air_temperature_tmmn', 'potential_evapotranspiration', \n    'relative_humidity_rmax', 'Elevation', 'Slope', 'Curvature', \n    'Aspect', 'Eastness', 'Northness'\n]\n\nprint(\"Normalizing features...\")\nscaler = StandardScaler()\nsampled_data[selected_features] = scaler.fit_transform(sampled_data[selected_features])\n\n# Step 4: Normalize Labels\nprint(\"Normalizing target variable (swe_value)...\")\nlabel_scaler = StandardScaler()\nsampled_data['swe_value'] = label_scaler.fit_transform(sampled_data['swe_value'].values.reshape(-1, 1))\n\n# Step 5: Save Normalized Dataset\noutput_file_path = 'normalized_grid_sampled_snotel.csv'\nsampled_data.to_csv(output_file_path, index=False)\nprint(f\"Normalized sampled dataset saved as '{output_file_path}'.\")\n\n# Step 6: Create Node Features Tensor\nnode_features = torch.tensor(sampled_data[selected_features].values, dtype=torch.float)\nprint(f\"Node features tensor created with shape: {node_features.shape}\")\n\n# Step 7: Create Labels Tensor\nlabels = torch.tensor(sampled_data['swe_value'].values, dtype=torch.float)\nprint(f\"Labels tensor created with shape: {labels.shape}\")\n\n# Step 8: Build KDTree and Construct Edges\nprint(\"Building KDTree and constructing edges...\")\ncoordinates = sampled_data[['lat', 'lon']].values\ntree = KDTree(coordinates)\nthreshold = 0.01\n\nbatch_edges = tree.query_pairs(r=threshold)\nedges = list(batch_edges)\nprint(f\"Number of edges created: {len(edges)}\")\n\n# Convert to Edge Index Tensor\nedge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\nprint(f\"Edge index tensor created with shape: {edge_index.shape}\")\n\n# Step 9: Create PyTorch Geometric Graph Data Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# Save the PyTorch Geometric Data Object\ngraph_data_output_path = '/media/volume1/gat_spatial_training_data_sampled_1.pt'\ntorch.save(graph_data, graph_data_output_path)\nprint(f\"PyTorch Geometric Data object saved as '{graph_data_output_path}'.\")\n",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 105 columns.\nFiltering data within the bounding box...\nFiltered dataset with 360859 rows.\nGrid-based sampled dataset with 247 rows.\nNormalizing features...\nNormalizing target variable (swe_value)...\nNormalized sampled dataset saved as 'normalized_grid_sampled_snotel.csv'.\nNode features tensor created with shape: torch.Size([247, 13])\nLabels tensor created with shape: torch.Size([247])\nBuilding KDTree and constructing edges...\nNumber of edges created: 2\nEdge index tensor created with shape: torch.Size([2, 2])\nPyTorch Geometric Data object saved as '/media/volume1/gat_spatial_training_data_sampled_1.pt'.\n",
  "history_begin_time" : 1741022862694,
  "history_end_time" : 1741022930213,
  "history_notes" : null,
  "history_process" : "3lobvp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "2a1No4IZeZrV",
  "history_input" : "import pandas as pd\nimport torch\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nimport numpy as np\n\n# Load dataset\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\nprint(\"Loading dataset...\")\ndata = pd.read_csv(file_path)\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\n\n# State boundary coordinates\nlat_min, lat_max = min(45.798170, 47.294134), max(48.864715, 47.561701)\nlon_min, lon_max = min(-124.145508, -120.673828), max(-120.190430, -117.158203)\n\n# Step 1: Filter data within boundary\nprint(\"Filtering data within state boundaries...\")\nfiltered_data = data[\n    (data['lat'] >= lat_min) & (data['lat'] <= lat_max) &\n    (data['lon'] >= lon_min) & (data['lon'] <= lon_max)\n]\n\n# Reduce the sample size to prevent OOM issues\nsample_size = 50000\nfiltered_data = filtered_data.sample(n=min(sample_size, len(filtered_data)), random_state=42)\nprint(f\"Filtered dataset with {filtered_data.shape[0]} rows.\")\n\n# Step 2: Normalize selected features\nselected_features = [\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount'\n]\n\nprint(\"Normalizing features...\")\nfiltered_data = filtered_data.copy()  # Avoid SettingWithCopyWarning\nscaler = StandardScaler()\nfiltered_data[selected_features] = scaler.fit_transform(filtered_data[selected_features])\n\n# Save normalized filtered data\noutput_file_path = 'normalized_filtered_snotel.csv'\nfiltered_data.to_csv(output_file_path, index=False)\nprint(f\"Normalized filtered dataset saved as '{output_file_path}'.\")\n\n# Step 3: Create node features tensor\nnode_features = torch.tensor(filtered_data[selected_features].values, dtype=torch.float)\nprint(f\"Node features tensor created with shape: {node_features.shape}\")\n\n# Step 4: Create labels tensor\nlabels = torch.tensor(filtered_data['swe_value'].values, dtype=torch.float)\nprint(f\"Labels tensor created with shape: {labels.shape}\")\n\n# Step 5: Build KDTree and construct edges efficiently\nprint(\"Building KDTree and constructing edges...\")\ncoordinates = filtered_data[['lat', 'lon']].values\ntree = KDTree(coordinates)\nthreshold = 0.005  # Adjust as needed\n\n# Process edges in batches to prevent memory overload\nedges = []\nfor i in range(len(coordinates)):\n    neighbors = tree.query_ball_point(coordinates[i], r=threshold)\n    for neighbor in neighbors:\n        if i != neighbor:  # Avoid self-loops\n            edges.append((i, neighbor))\n\nprint(f\"Number of edges created: {len(edges)}\")\nedge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n\n# Step 6: Create PyTorch Geometric graph data object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# Save the PyTorch Geometric Data object\ngraph_data_output_path = '/media/volume1/gat_spatial_training_data_filtered.pt'\ntorch.save(graph_data, graph_data_output_path)\nprint(f\"PyTorch Geometric Data object saved as '{graph_data_output_path}'.\")\n",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 105 columns.\nFiltering data within state boundaries...\nFiltered dataset with 50000 rows.\nNormalizing features...\nNormalized filtered dataset saved as 'normalized_filtered_snotel.csv'.\nNode features tensor created with shape: torch.Size([50000, 13])\nLabels tensor created with shape: torch.Size([50000])\nBuilding KDTree and constructing edges...\nNumber of edges created: 7064598\nPyTorch Geometric Data object saved as '/media/volume1/gat_spatial_training_data_filtered.pt'.\n",
  "history_begin_time" : 1740417787817,
  "history_end_time" : 1740417883014,
  "history_notes" : null,
  "history_process" : "3lobvp",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "hef5rAk1tgDP",
  "history_input" : "import pandas as pd\nimport torch\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nimport numpy as np\n\n# Load dataset\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\nprint(\"Loading dataset...\")\ndata = pd.read_csv(file_path)\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\n\n# State boundary coordinates\nlat_min, lat_max = min(45.798170, 47.294134), max(48.864715, 47.561701)\nlon_min, lon_max = min(-124.145508, -120.673828), max(-120.190430, -117.158203)\n\n# Step 1: Filter data within boundary\nprint(\"Filtering data within state boundaries...\")\nfiltered_data = data[\n    (data['lat'] >= lat_min) & (data['lat'] <= lat_max) &\n    (data['lon'] >= lon_min) & (data['lon'] <= lon_max)\n]\nprint(f\"Filtered dataset with {filtered_data.shape[0]} rows.\")\n\n# Step 2: Normalize selected features\nselected_features = [\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount'\n]\n\nprint(\"Normalizing features...\")\nscaler = StandardScaler()\nfiltered_data[selected_features] = scaler.fit_transform(filtered_data[selected_features])\n\n# Save normalized filtered data\noutput_file_path = 'normalized_filtered_snotel.csv'\nfiltered_data.to_csv(output_file_path, index=False)\nprint(f\"Normalized filtered dataset saved as '{output_file_path}'.\")\n\n# Step 3: Create node features tensor\nnode_features = torch.tensor(filtered_data[selected_features].values, dtype=torch.float)\nprint(f\"Node features tensor created with shape: {node_features.shape}\")\n\n# Step 4: Create labels tensor\nlabels = torch.tensor(filtered_data['swe_value'].values, dtype=torch.float)\nprint(f\"Labels tensor created with shape: {labels.shape}\")\n\n# Step 5: Build KDTree and construct edges\nprint(\"Building KDTree and constructing edges...\")\ncoordinates = filtered_data[['lat', 'lon']].values\ntree = KDTree(coordinates)\nthreshold = 0.005  # Adjust as needed\n\nbatch_edges = tree.query_pairs(r=threshold)\ntedges = list(batch_edges)\nprint(f\"Number of edges created: {len(tedges)}\")\n\n# Convert to edge index tensor\nedge_index = torch.tensor(tedges, dtype=torch.long).t().contiguous()\nprint(f\"Edge index tensor created with shape: {edge_index.shape}\")\n\n# Step 6: Create PyTorch Geometric graph data object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# Save the PyTorch Geometric Data object\ngraph_data_output_path = '/media/volume1/gat_spatial_training_data_filtered.pt'\ntorch.save(graph_data, graph_data_output_path)\nprint(f\"PyTorch Geometric Data object saved as '{graph_data_output_path}'.\")\n",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows and 105 columns.\nFiltering data within state boundaries...\nFiltered dataset with 477000 rows.\nNormalizing features...\n/home/geo2021/gw-workspace/hef5rAk1tgDP/state_data_creation.py:36: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  filtered_data[selected_features] = scaler.fit_transform(filtered_data[selected_features])\nNormalized filtered dataset saved as 'normalized_filtered_snotel.csv'.\nNode features tensor created with shape: torch.Size([477000, 13])\nLabels tensor created with shape: torch.Size([477000])\nBuilding KDTree and constructing edges...\n",
  "history_begin_time" : 1740416180028,
  "history_end_time" : 1740416416219,
  "history_notes" : null,
  "history_process" : "3lobvp",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "aqt5orim78o",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1741409321079,
  "history_notes" : null,
  "history_process" : "3lobvp",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "2eru6jee1ph",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1741409322413,
  "history_notes" : null,
  "history_process" : "3lobvp",
  "host_id" : "100001",
  "indicator" : "Stopped"
}]