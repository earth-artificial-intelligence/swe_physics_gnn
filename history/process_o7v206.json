[{
  "history_id" : "NOwO2fyvWpzv",
  "history_input" : "import numpy as np\r\nimport torch\r\nimport warnings\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv, BatchNorm\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\n\r\n# Load and verify graph\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data = torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\"Number of Edges: {g_data.num_edges}\")\r\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\"Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n    except Exception as e:\r\n        print(f\"Error loading graph: {e}\")\r\n        exit()\r\n\r\n# Define GraphSAGE model with deeper layers & mean aggregation\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv5 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")  # Added extra layer\r\n\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n        self.bn4 = BatchNorm(hidden_dim)\r\n        self.bn5 = BatchNorm(hidden_dim)\r\n\r\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\r\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\r\n\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn5(self.conv5(x, edge_index)))\r\n\r\n        x = F.leaky_relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n\r\n    loss.backward()\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n# Main function\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph_final_reduced_wa_4.pt\"  \r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n    # Normalize node features\r\n    scaler_x = StandardScaler()\r\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\r\n\r\n    # Normalize target variable\r\n    y_scaler = StandardScaler()\r\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\r\n\r\n    # Training split\r\n    indices= torch.randperm(g_data.num_nodes)\r\n    train_size=int(0.8*g_data.num_nodes)\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[indices[:train_size]] = True  \r\n    test_mask = ~train_mask  \r\n\r\n    print(f\"Training on {train_size} nodes & Testing on {g_data.num_nodes - train_size} nodes\")\r\n\r\n    # Define model, optimizer, and loss function\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=512, output_dim=1, dropout=0.4).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-3)\r\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=50)\r\n    criterion = nn.SmoothL1Loss(beta=1.0)\r\n\r\n    best_r2 = -float(\"inf\")\r\n\r\n    print(\"Starting training for 1000 epochs...\")\r\n\r\n    for epoch in range(1, 5001):  \r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch % 100 == 0 or epoch == 5:  \r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\"Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n            if r2 > best_r2:\r\n                best_r2 = r2\r\n\r\n    print(f\"Best R² score: {best_r2:.4f}\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph_final_reduced_wa_4.pt\nNumber of Nodes: 1791\nNumber of Edges: 3582\nNode Feature Shape: torch.Size([1791, 8])\nEdge Index Shape: torch.Size([2, 3582])\nTarget Variable Shape: torch.Size([1791])\nTraining on 1432 nodes & Testing on 359 nodes\nStarting training for 1000 epochs...\nEpoch 5: Train Loss=0.3940 | R²=0.0366 | RMSE=0.9667 | MAE=0.8346\nEpoch 100: Train Loss=0.2397 | R²=0.3263 | RMSE=0.8084 | MAE=0.5781\nEpoch 200: Train Loss=0.2089 | R²=0.3770 | RMSE=0.7774 | MAE=0.5197\nEpoch 300: Train Loss=0.1905 | R²=0.4418 | RMSE=0.7358 | MAE=0.4759\nEpoch 400: Train Loss=0.1769 | R²=0.4773 | RMSE=0.7121 | MAE=0.4645\nEpoch 500: Train Loss=0.1760 | R²=0.5083 | RMSE=0.6906 | MAE=0.4505\nEpoch 600: Train Loss=0.1639 | R²=0.5364 | RMSE=0.6706 | MAE=0.4320\nEpoch 700: Train Loss=0.1621 | R²=0.5218 | RMSE=0.6811 | MAE=0.4451\nEpoch 800: Train Loss=0.1435 | R²=0.5418 | RMSE=0.6667 | MAE=0.4254\nEpoch 900: Train Loss=0.1401 | R²=0.5483 | RMSE=0.6620 | MAE=0.4272\nEpoch 1000: Train Loss=0.1397 | R²=0.5878 | RMSE=0.6323 | MAE=0.3999\nEpoch 1100: Train Loss=0.1304 | R²=0.5737 | RMSE=0.6431 | MAE=0.3958\nEpoch 1200: Train Loss=0.1220 | R²=0.5648 | RMSE=0.6497 | MAE=0.3874\nEpoch 1300: Train Loss=0.1252 | R²=0.5797 | RMSE=0.6385 | MAE=0.3801\nEpoch 1400: Train Loss=0.1195 | R²=0.5729 | RMSE=0.6437 | MAE=0.3732\nEpoch 1500: Train Loss=0.1246 | R²=0.5588 | RMSE=0.6542 | MAE=0.3745\nEpoch 1600: Train Loss=0.1149 | R²=0.5533 | RMSE=0.6583 | MAE=0.3692\nEpoch 1700: Train Loss=0.1136 | R²=0.5660 | RMSE=0.6488 | MAE=0.3639\nEpoch 1800: Train Loss=0.1117 | R²=0.5677 | RMSE=0.6475 | MAE=0.3648\nEpoch 1900: Train Loss=0.1096 | R²=0.5599 | RMSE=0.6534 | MAE=0.3612\nEpoch 2000: Train Loss=0.1069 | R²=0.5808 | RMSE=0.6377 | MAE=0.3554\nEpoch 2100: Train Loss=0.1093 | R²=0.5504 | RMSE=0.6604 | MAE=0.3670\nEpoch 2200: Train Loss=0.1033 | R²=0.5784 | RMSE=0.6395 | MAE=0.3424\nEpoch 2300: Train Loss=0.1051 | R²=0.5941 | RMSE=0.6275 | MAE=0.3405\nEpoch 2400: Train Loss=0.0990 | R²=0.5563 | RMSE=0.6561 | MAE=0.3515\nEpoch 2500: Train Loss=0.0996 | R²=0.5786 | RMSE=0.6394 | MAE=0.3404\nEpoch 2600: Train Loss=0.0956 | R²=0.5867 | RMSE=0.6332 | MAE=0.3367\nEpoch 2700: Train Loss=0.0991 | R²=0.5548 | RMSE=0.6572 | MAE=0.3525\nEpoch 2800: Train Loss=0.0969 | R²=0.5716 | RMSE=0.6446 | MAE=0.3383\nEpoch 2900: Train Loss=0.0939 | R²=0.5584 | RMSE=0.6545 | MAE=0.3385\nEpoch 3000: Train Loss=0.0994 | R²=0.5794 | RMSE=0.6387 | MAE=0.3362\nEpoch 3100: Train Loss=0.0974 | R²=0.5779 | RMSE=0.6399 | MAE=0.3465\nEpoch 3200: Train Loss=0.0959 | R²=0.6032 | RMSE=0.6205 | MAE=0.3371\nEpoch 3300: Train Loss=0.0896 | R²=0.5681 | RMSE=0.6472 | MAE=0.3436\nEpoch 3400: Train Loss=0.0918 | R²=0.5764 | RMSE=0.6411 | MAE=0.3358\nEpoch 3500: Train Loss=0.0936 | R²=0.5906 | RMSE=0.6302 | MAE=0.3290\nEpoch 3600: Train Loss=0.0890 | R²=0.5980 | RMSE=0.6245 | MAE=0.3362\nEpoch 3700: Train Loss=0.0860 | R²=0.5979 | RMSE=0.6245 | MAE=0.3249\nEpoch 3800: Train Loss=0.0898 | R²=0.5851 | RMSE=0.6344 | MAE=0.3336\nEpoch 3900: Train Loss=0.0914 | R²=0.5838 | RMSE=0.6354 | MAE=0.3324\nEpoch 4000: Train Loss=0.0923 | R²=0.5893 | RMSE=0.6312 | MAE=0.3300\nEpoch 4100: Train Loss=0.0933 | R²=0.5925 | RMSE=0.6287 | MAE=0.3269\nEpoch 4200: Train Loss=0.0881 | R²=0.5750 | RMSE=0.6421 | MAE=0.3330\nEpoch 4300: Train Loss=0.0850 | R²=0.5547 | RMSE=0.6572 | MAE=0.3324\nEpoch 4400: Train Loss=0.0868 | R²=0.5676 | RMSE=0.6477 | MAE=0.3337\nEpoch 4500: Train Loss=0.0886 | R²=0.5809 | RMSE=0.6376 | MAE=0.3334\nEpoch 4600: Train Loss=0.0874 | R²=0.6026 | RMSE=0.6209 | MAE=0.3204\nEpoch 4700: Train Loss=0.0870 | R²=0.5845 | RMSE=0.6349 | MAE=0.3253\nEpoch 4800: Train Loss=0.0888 | R²=0.5794 | RMSE=0.6387 | MAE=0.3282\nEpoch 4900: Train Loss=0.0882 | R²=0.5780 | RMSE=0.6398 | MAE=0.3359\nEpoch 5000: Train Loss=0.0847 | R²=0.5724 | RMSE=0.6440 | MAE=0.3243\nBest R² score: 0.6032\n",
  "history_begin_time" : 1742609789344,
  "history_end_time" : 1742610411920,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "rGeq9RYziVLa",
  "history_input" : "import numpy as np\r\nimport torch\r\nimport warnings\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv, BatchNorm\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\n\r\n# Load and verify graph\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data = torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\"Number of Edges: {g_data.num_edges}\")\r\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\"Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n    except Exception as e:\r\n        print(f\"Error loading graph: {e}\")\r\n        exit()\r\n\r\n# Define GraphSAGE model with deeper layers & mean aggregation\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv5 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")  # Added extra layer\r\n\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n        self.bn4 = BatchNorm(hidden_dim)\r\n        self.bn5 = BatchNorm(hidden_dim)\r\n\r\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\r\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\r\n\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn5(self.conv5(x, edge_index)))\r\n\r\n        x = F.leaky_relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n\r\n    loss.backward()\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n# Main function\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph_final_reduced_wa_4.pt\"  \r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n    # Normalize node features\r\n    scaler_x = StandardScaler()\r\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\r\n\r\n    # Normalize target variable\r\n    y_scaler = StandardScaler()\r\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\r\n\r\n    # Training split\r\n    indices= torch.randperm(g_data.num_nodes)\r\n    train_size=int(0.8*g_data.num_nodes)\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[indices[:train_size]] = True  \r\n    test_mask = ~train_mask  \r\n\r\n    print(f\"Training on {train_size} nodes & Testing on {g_data.num_nodes - train_size} nodes\")\r\n\r\n    # Define model, optimizer, and loss function\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=512, output_dim=1, dropout=0.4).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-3)\r\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=50)\r\n    criterion = nn.SmoothL1Loss(beta=1.0)\r\n\r\n    best_r2 = -float(\"inf\")\r\n\r\n    print(\"Starting training for 1000 epochs...\")\r\n\r\n    for epoch in range(1, 5001):  \r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch % 100 == 0 or epoch == 5:  \r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\"Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n            if r2 > best_r2:\r\n                best_r2 = r2\r\n\r\n    print(f\"Best R² score: {best_r2:.4f}\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph_final_reduced_wa_4.pt\nNumber of Nodes: 1791\nNumber of Edges: 3582\nNode Feature Shape: torch.Size([1791, 8])\nEdge Index Shape: torch.Size([2, 3582])\nTarget Variable Shape: torch.Size([1791])\nTraining on 1432 nodes & Testing on 359 nodes\nStarting training for 1000 epochs...\nEpoch 5: Train Loss=0.3884 | R²=0.0412 | RMSE=0.9593 | MAE=0.8138\nEpoch 100: Train Loss=0.2402 | R²=0.3297 | RMSE=0.8021 | MAE=0.5663\nEpoch 200: Train Loss=0.2057 | R²=0.4019 | RMSE=0.7576 | MAE=0.5066\nEpoch 300: Train Loss=0.1952 | R²=0.4453 | RMSE=0.7296 | MAE=0.4726\nEpoch 400: Train Loss=0.1875 | R²=0.4354 | RMSE=0.7361 | MAE=0.4673\nEpoch 500: Train Loss=0.1734 | R²=0.4391 | RMSE=0.7337 | MAE=0.4558\nEpoch 600: Train Loss=0.1677 | R²=0.4317 | RMSE=0.7385 | MAE=0.4619\nEpoch 700: Train Loss=0.1625 | R²=0.4536 | RMSE=0.7241 | MAE=0.4563\nEpoch 800: Train Loss=0.1498 | R²=0.4520 | RMSE=0.7252 | MAE=0.4502\nEpoch 900: Train Loss=0.1321 | R²=0.4773 | RMSE=0.7083 | MAE=0.4304\nEpoch 1000: Train Loss=0.1365 | R²=0.4912 | RMSE=0.6988 | MAE=0.4280\nEpoch 1100: Train Loss=0.1299 | R²=0.5048 | RMSE=0.6894 | MAE=0.4130\nEpoch 1200: Train Loss=0.1207 | R²=0.4960 | RMSE=0.6955 | MAE=0.4124\nEpoch 1300: Train Loss=0.1173 | R²=0.5046 | RMSE=0.6896 | MAE=0.4043\nEpoch 1400: Train Loss=0.1201 | R²=0.5095 | RMSE=0.6861 | MAE=0.3980\nEpoch 1500: Train Loss=0.1181 | R²=0.5308 | RMSE=0.6710 | MAE=0.3833\nEpoch 1600: Train Loss=0.1175 | R²=0.5301 | RMSE=0.6716 | MAE=0.3740\nEpoch 1700: Train Loss=0.1133 | R²=0.5356 | RMSE=0.6676 | MAE=0.3701\nEpoch 1800: Train Loss=0.1093 | R²=0.5388 | RMSE=0.6653 | MAE=0.3717\nEpoch 1900: Train Loss=0.1071 | R²=0.5436 | RMSE=0.6618 | MAE=0.3692\nEpoch 2000: Train Loss=0.1083 | R²=0.5162 | RMSE=0.6814 | MAE=0.3731\nEpoch 2100: Train Loss=0.1016 | R²=0.5576 | RMSE=0.6516 | MAE=0.3554\nEpoch 2200: Train Loss=0.0968 | R²=0.5588 | RMSE=0.6507 | MAE=0.3584\nEpoch 2300: Train Loss=0.0998 | R²=0.5344 | RMSE=0.6685 | MAE=0.3581\nEpoch 2400: Train Loss=0.0972 | R²=0.5456 | RMSE=0.6604 | MAE=0.3566\nEpoch 2500: Train Loss=0.0964 | R²=0.5348 | RMSE=0.6682 | MAE=0.3551\nEpoch 2600: Train Loss=0.0945 | R²=0.5555 | RMSE=0.6531 | MAE=0.3499\nEpoch 2700: Train Loss=0.0974 | R²=0.5400 | RMSE=0.6645 | MAE=0.3470\nEpoch 2800: Train Loss=0.0939 | R²=0.5569 | RMSE=0.6522 | MAE=0.3481\nEpoch 2900: Train Loss=0.0974 | R²=0.5627 | RMSE=0.6478 | MAE=0.3474\nEpoch 3000: Train Loss=0.0926 | R²=0.5757 | RMSE=0.6381 | MAE=0.3378\nEpoch 3100: Train Loss=0.0902 | R²=0.5406 | RMSE=0.6640 | MAE=0.3485\nEpoch 3200: Train Loss=0.0948 | R²=0.5888 | RMSE=0.6282 | MAE=0.3399\nEpoch 3300: Train Loss=0.0943 | R²=0.5653 | RMSE=0.6459 | MAE=0.3379\nEpoch 3400: Train Loss=0.0921 | R²=0.5750 | RMSE=0.6387 | MAE=0.3417\nEpoch 3500: Train Loss=0.0929 | R²=0.5674 | RMSE=0.6444 | MAE=0.3363\nEpoch 3600: Train Loss=0.0899 | R²=0.5697 | RMSE=0.6426 | MAE=0.3349\nEpoch 3700: Train Loss=0.0855 | R²=0.5829 | RMSE=0.6327 | MAE=0.3380\nEpoch 3800: Train Loss=0.0975 | R²=0.6003 | RMSE=0.6194 | MAE=0.3280\nEpoch 3900: Train Loss=0.0847 | R²=0.5570 | RMSE=0.6520 | MAE=0.3394\nEpoch 4000: Train Loss=0.0858 | R²=0.5727 | RMSE=0.6404 | MAE=0.3337\nEpoch 4100: Train Loss=0.0917 | R²=0.5737 | RMSE=0.6396 | MAE=0.3345\nEpoch 4200: Train Loss=0.0887 | R²=0.5973 | RMSE=0.6217 | MAE=0.3276\nEpoch 4300: Train Loss=0.0876 | R²=0.5804 | RMSE=0.6346 | MAE=0.3332\nEpoch 4400: Train Loss=0.0834 | R²=0.5800 | RMSE=0.6349 | MAE=0.3322\nEpoch 4500: Train Loss=0.0869 | R²=0.5488 | RMSE=0.6581 | MAE=0.3355\nEpoch 4600: Train Loss=0.0847 | R²=0.5856 | RMSE=0.6306 | MAE=0.3296\nEpoch 4700: Train Loss=0.0880 | R²=0.5804 | RMSE=0.6346 | MAE=0.3302\nEpoch 4800: Train Loss=0.0879 | R²=0.5876 | RMSE=0.6291 | MAE=0.3302\nEpoch 4900: Train Loss=0.0851 | R²=0.6170 | RMSE=0.6063 | MAE=0.3111\nEpoch 5000: Train Loss=0.0830 | R²=0.6065 | RMSE=0.6145 | MAE=0.3159\nBest R² score: 0.6170\n",
  "history_begin_time" : 1741792780427,
  "history_end_time" : 1741793676545,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "bEhswdXJX8AB",
  "history_input" : "import numpy as np\r\nimport torch\r\nimport warnings\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv, BatchNorm\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\n\r\n# Load and verify graph\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data = torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\"Number of Edges: {g_data.num_edges}\")\r\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\"Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n    except Exception as e:\r\n        print(f\"Error loading graph: {e}\")\r\n        exit()\r\n\r\n# Define GraphSAGE model with deeper layers & mean aggregation\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv5 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")  # Added extra layer\r\n\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n        self.bn4 = BatchNorm(hidden_dim)\r\n        self.bn5 = BatchNorm(hidden_dim)\r\n\r\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\r\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\r\n\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn5(self.conv5(x, edge_index)))\r\n\r\n        x = F.leaky_relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n\r\n    loss.backward()\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n# Main function\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph_final_reduced_wa_4.pt\"  \r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n    # Normalize node features\r\n    scaler_x = StandardScaler()\r\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\r\n\r\n    # Normalize target variable\r\n    y_scaler = StandardScaler()\r\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\r\n\r\n    # Training split\r\n    indices= torch.randperm(g_data.num_nodes)\r\n    train_size=int(0.8*g_data.num_nodes)\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[indices[:train_size]] = True  \r\n    test_mask = ~train_mask  \r\n\r\n    print(f\"Training on {train_size} nodes & Testing on {g_data.num_nodes - train_size} nodes\")\r\n\r\n    # Define model, optimizer, and loss function\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=512, output_dim=1, dropout=0.4).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-3)\r\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=50)\r\n    criterion = nn.SmoothL1Loss(beta=1.0)\r\n\r\n    best_r2 = -float(\"inf\")\r\n\r\n    print(\"Starting training for 1000 epochs...\")\r\n\r\n    for epoch in range(1, 5001):  \r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch % 100 == 0 or epoch == 5:  \r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\"Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n            if r2 > best_r2:\r\n                best_r2 = r2\r\n\r\n    print(f\"Best R² score: {best_r2:.4f}\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph_final_reduced_wa_4.pt\nNumber of Nodes: 1791\nNumber of Edges: 3582\nNode Feature Shape: torch.Size([1791, 8])\nEdge Index Shape: torch.Size([2, 3582])\nTarget Variable Shape: torch.Size([1791])\nTraining on 1432 nodes & Testing on 359 nodes\nStarting training for 1000 epochs...\nEpoch 5: Train Loss=0.3958 | R²=0.0376 | RMSE=0.9364 | MAE=0.7922\nEpoch 100: Train Loss=0.2393 | R²=0.2938 | RMSE=0.8022 | MAE=0.5952\nEpoch 200: Train Loss=0.2092 | R²=0.2832 | RMSE=0.8081 | MAE=0.5411\nEpoch 300: Train Loss=0.1949 | R²=0.3606 | RMSE=0.7633 | MAE=0.4921\nEpoch 400: Train Loss=0.1849 | R²=0.3403 | RMSE=0.7752 | MAE=0.4837\nEpoch 500: Train Loss=0.1821 | R²=0.3488 | RMSE=0.7703 | MAE=0.4747\nEpoch 600: Train Loss=0.1701 | R²=0.3610 | RMSE=0.7630 | MAE=0.4702\nEpoch 700: Train Loss=0.1574 | R²=0.4053 | RMSE=0.7361 | MAE=0.4405\nEpoch 800: Train Loss=0.1520 | R²=0.4191 | RMSE=0.7275 | MAE=0.4318\nEpoch 900: Train Loss=0.1344 | R²=0.4049 | RMSE=0.7363 | MAE=0.4308\nEpoch 1000: Train Loss=0.1319 | R²=0.4403 | RMSE=0.7141 | MAE=0.4026\nEpoch 1100: Train Loss=0.1290 | R²=0.4394 | RMSE=0.7147 | MAE=0.4016\nEpoch 1200: Train Loss=0.1246 | R²=0.4651 | RMSE=0.6981 | MAE=0.3855\nEpoch 1300: Train Loss=0.1175 | R²=0.4861 | RMSE=0.6842 | MAE=0.3716\nEpoch 1400: Train Loss=0.1198 | R²=0.4864 | RMSE=0.6841 | MAE=0.3634\nEpoch 1500: Train Loss=0.1147 | R²=0.4965 | RMSE=0.6773 | MAE=0.3569\nEpoch 1600: Train Loss=0.1162 | R²=0.5213 | RMSE=0.6604 | MAE=0.3464\nEpoch 1700: Train Loss=0.1065 | R²=0.5100 | RMSE=0.6681 | MAE=0.3461\nEpoch 1800: Train Loss=0.1069 | R²=0.5215 | RMSE=0.6603 | MAE=0.3492\nEpoch 1900: Train Loss=0.1065 | R²=0.5233 | RMSE=0.6590 | MAE=0.3395\nEpoch 2000: Train Loss=0.1018 | R²=0.5343 | RMSE=0.6514 | MAE=0.3377\nEpoch 2100: Train Loss=0.1034 | R²=0.5368 | RMSE=0.6496 | MAE=0.3360\nEpoch 2200: Train Loss=0.1062 | R²=0.5396 | RMSE=0.6477 | MAE=0.3376\nEpoch 2300: Train Loss=0.0996 | R²=0.5265 | RMSE=0.6568 | MAE=0.3351\nEpoch 2400: Train Loss=0.0939 | R²=0.5395 | RMSE=0.6477 | MAE=0.3365\nEpoch 2500: Train Loss=0.0989 | R²=0.5415 | RMSE=0.6464 | MAE=0.3268\nEpoch 2600: Train Loss=0.1007 | R²=0.5266 | RMSE=0.6568 | MAE=0.3352\nEpoch 2700: Train Loss=0.0950 | R²=0.5420 | RMSE=0.6460 | MAE=0.3242\nEpoch 2800: Train Loss=0.0959 | R²=0.5294 | RMSE=0.6548 | MAE=0.3341\nEpoch 2900: Train Loss=0.0982 | R²=0.5363 | RMSE=0.6499 | MAE=0.3340\nEpoch 3000: Train Loss=0.0899 | R²=0.5331 | RMSE=0.6522 | MAE=0.3269\nEpoch 3100: Train Loss=0.0929 | R²=0.5173 | RMSE=0.6631 | MAE=0.3357\nEpoch 3200: Train Loss=0.0904 | R²=0.5381 | RMSE=0.6487 | MAE=0.3324\nEpoch 3300: Train Loss=0.0926 | R²=0.5402 | RMSE=0.6472 | MAE=0.3251\nEpoch 3400: Train Loss=0.0945 | R²=0.5344 | RMSE=0.6513 | MAE=0.3266\nEpoch 3500: Train Loss=0.0913 | R²=0.5244 | RMSE=0.6583 | MAE=0.3354\nEpoch 3600: Train Loss=0.0916 | R²=0.5041 | RMSE=0.6722 | MAE=0.3349\nEpoch 3700: Train Loss=0.0999 | R²=0.5343 | RMSE=0.6514 | MAE=0.3299\nEpoch 3800: Train Loss=0.0892 | R²=0.5152 | RMSE=0.6646 | MAE=0.3340\nEpoch 3900: Train Loss=0.0899 | R²=0.5217 | RMSE=0.6601 | MAE=0.3239\nEpoch 4000: Train Loss=0.0883 | R²=0.5200 | RMSE=0.6613 | MAE=0.3267\nEpoch 4100: Train Loss=0.0903 | R²=0.5285 | RMSE=0.6555 | MAE=0.3268\nEpoch 4200: Train Loss=0.0924 | R²=0.5199 | RMSE=0.6614 | MAE=0.3309\nEpoch 4300: Train Loss=0.0887 | R²=0.5379 | RMSE=0.6488 | MAE=0.3261\nEpoch 4400: Train Loss=0.0858 | R²=0.5401 | RMSE=0.6473 | MAE=0.3223\nEpoch 4500: Train Loss=0.0868 | R²=0.5431 | RMSE=0.6452 | MAE=0.3239\nEpoch 4600: Train Loss=0.0861 | R²=0.5188 | RMSE=0.6621 | MAE=0.3244\nEpoch 4700: Train Loss=0.0883 | R²=0.5249 | RMSE=0.6579 | MAE=0.3272\nEpoch 4800: Train Loss=0.0857 | R²=0.5208 | RMSE=0.6608 | MAE=0.3256\nEpoch 4900: Train Loss=0.0868 | R²=0.5242 | RMSE=0.6584 | MAE=0.3263\nEpoch 5000: Train Loss=0.0891 | R²=0.5315 | RMSE=0.6533 | MAE=0.3233\nBest R² score: 0.5431\n",
  "history_begin_time" : 1741792433502,
  "history_end_time" : 1741793326154,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "N5yUMUvqhYwK",
  "history_input" : "import numpy as np\r\nimport torch\r\nimport warnings\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv, BatchNorm\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\n\r\n# Load and verify graph\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data = torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\"Number of Edges: {g_data.num_edges}\")\r\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\"Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n    except Exception as e:\r\n        print(f\"Error loading graph: {e}\")\r\n        exit()\r\n\r\n# Define GraphSAGE model with deeper layers & mean aggregation\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv5 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")  # Added extra layer\r\n\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n        self.bn4 = BatchNorm(hidden_dim)\r\n        self.bn5 = BatchNorm(hidden_dim)\r\n\r\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\r\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\r\n\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn5(self.conv5(x, edge_index)))\r\n\r\n        x = F.leaky_relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n\r\n    loss.backward()\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n# Main function\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph_final_reduced_wa_4.pt\"  \r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n    # Normalize node features\r\n    scaler_x = StandardScaler()\r\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\r\n\r\n    # Normalize target variable\r\n    y_scaler = StandardScaler()\r\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\r\n\r\n    # Training split\r\n    indices= torch.randperm(g_data.num_nodes)\r\n    train_size=int(0.8*g_data.num_nodes)\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[indices[:train_size]] = True  \r\n    test_mask = ~train_mask  \r\n\r\n    print(f\"Training on {train_size} nodes & Testing on {g_data.num_nodes - train_size} nodes\")\r\n\r\n    # Define model, optimizer, and loss function\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=512, output_dim=1, dropout=0.4).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-3)\r\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=50)\r\n    criterion = nn.SmoothL1Loss(beta=1.0)\r\n\r\n    best_r2 = -float(\"inf\")\r\n\r\n    print(\"Starting training for 1000 epochs...\")\r\n\r\n    for epoch in range(1, 1001):  \r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch % 100 == 0 or epoch == 5:  \r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\"Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n            if r2 > best_r2:\r\n                best_r2 = r2\r\n\r\n    print(f\"Best R² score: {best_r2:.4f}\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph_final_reduced_wa_4.pt\nNumber of Nodes: 1791\nNumber of Edges: 3582\nNode Feature Shape: torch.Size([1791, 8])\nEdge Index Shape: torch.Size([2, 3582])\nTarget Variable Shape: torch.Size([1791])\nTraining on 1432 nodes & Testing on 359 nodes\nStarting training for 1000 epochs...\nEpoch 5: Train Loss=0.3879 | R²=-0.0071 | RMSE=1.0147 | MAE=0.8419\nEpoch 100: Train Loss=0.2356 | R²=0.4132 | RMSE=0.7745 | MAE=0.5725\nEpoch 200: Train Loss=0.2185 | R²=0.4427 | RMSE=0.7548 | MAE=0.5305\nEpoch 300: Train Loss=0.1930 | R²=0.4537 | RMSE=0.7473 | MAE=0.4937\nEpoch 400: Train Loss=0.1822 | R²=0.5396 | RMSE=0.6860 | MAE=0.4509\nEpoch 500: Train Loss=0.1764 | R²=0.5485 | RMSE=0.6794 | MAE=0.4344\nEpoch 600: Train Loss=0.1722 | R²=0.5898 | RMSE=0.6476 | MAE=0.4219\nEpoch 700: Train Loss=0.1556 | R²=0.5945 | RMSE=0.6438 | MAE=0.4142\nEpoch 800: Train Loss=0.1522 | R²=0.6003 | RMSE=0.6392 | MAE=0.4085\nEpoch 900: Train Loss=0.1432 | R²=0.6178 | RMSE=0.6250 | MAE=0.3976\nEpoch 1000: Train Loss=0.1423 | R²=0.6392 | RMSE=0.6073 | MAE=0.3813\nBest R² score: 0.6392\n",
  "history_begin_time" : 1741792241029,
  "history_end_time" : 1741792371304,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "f7b6YzK0AYMM",
  "history_input" : "import numpy as np\r\nimport torch\r\nimport warnings\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv, BatchNorm\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\n\r\n# Load and verify graph\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data = torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\"Number of Edges: {g_data.num_edges}\")\r\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\"Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n    except Exception as e:\r\n        print(f\"Error loading graph: {e}\")\r\n        exit()\r\n\r\n# Define GraphSAGE model with deeper layers & mean aggregation\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv5 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")  # Added extra layer\r\n\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n        self.bn4 = BatchNorm(hidden_dim)\r\n        self.bn5 = BatchNorm(hidden_dim)\r\n\r\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\r\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\r\n\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn5(self.conv5(x, edge_index)))\r\n\r\n        x = F.leaky_relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n\r\n    loss.backward()\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n# Main function\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph_final_1_1.pt\"  \r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n    # Normalize node features\r\n    scaler_x = StandardScaler()\r\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\r\n\r\n    # Normalize target variable\r\n    y_scaler = StandardScaler()\r\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\r\n\r\n    # Training split\r\n    indices= torch.randperm(g_data.num_nodes)\r\n    train_size=int(0.8*g_data.num_nodes)\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[indices[:train_size]] = True  \r\n    test_mask = ~train_mask  \r\n\r\n    print(f\"Training on {train_size} nodes & Testing on {g_data.num_nodes - train_size} nodes\")\r\n\r\n    # Define model, optimizer, and loss function\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=512, output_dim=1, dropout=0.4).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-3)\r\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=50)\r\n    criterion = nn.SmoothL1Loss(beta=1.0)\r\n\r\n    best_r2 = -float(\"inf\")\r\n\r\n    print(\"Starting training for 1000 epochs...\")\r\n\r\n    for epoch in range(1, 1001):  \r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch % 100 == 0 or epoch == 5:  \r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\"Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n            if r2 > best_r2:\r\n                best_r2 = r2\r\n\r\n    print(f\"Best R² score: {best_r2:.4f}\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph_final_1_1.pt\nNumber of Nodes: 703\nNumber of Edges: 348\nNode Feature Shape: torch.Size([703, 11])\nEdge Index Shape: torch.Size([2, 348])\nTarget Variable Shape: torch.Size([703])\nTraining on 562 nodes & Testing on 141 nodes\nStarting training for 1000 epochs...\nEpoch 5: Train Loss=0.4580 | R²=-0.0097 | RMSE=0.9311 | MAE=0.8705\nEpoch 100: Train Loss=0.1526 | R²=0.7399 | RMSE=0.4725 | MAE=0.2913\nEpoch 200: Train Loss=0.0998 | R²=0.8179 | RMSE=0.3954 | MAE=0.2305\nEpoch 300: Train Loss=0.0918 | R²=0.8496 | RMSE=0.3594 | MAE=0.2007\nEpoch 400: Train Loss=0.0742 | R²=0.8408 | RMSE=0.3697 | MAE=0.2012\nEpoch 500: Train Loss=0.0720 | R²=0.8586 | RMSE=0.3484 | MAE=0.1915\nEpoch 600: Train Loss=0.0732 | R²=0.8315 | RMSE=0.3803 | MAE=0.1948\nEpoch 700: Train Loss=0.0719 | R²=0.8715 | RMSE=0.3322 | MAE=0.1707\nEpoch 800: Train Loss=0.0567 | R²=0.8743 | RMSE=0.3285 | MAE=0.1669\nEpoch 900: Train Loss=0.0572 | R²=0.8652 | RMSE=0.3401 | MAE=0.1711\nEpoch 1000: Train Loss=0.0614 | R²=0.8662 | RMSE=0.3389 | MAE=0.1679\nBest R² score: 0.8743\n",
  "history_begin_time" : 1741031960386,
  "history_end_time" : 1741032006943,
  "history_notes" : "Best R² score: 0.8743",
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "kSin8Hqdr0NU",
  "history_input" : "import numpy as np\r\nimport torch\r\nimport warnings\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv, BatchNorm\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom torch.optim.lr_scheduler import StepLR\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\n\r\n# Load and verify graph\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data = torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\"Number of Edges: {g_data.num_edges}\")\r\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\"Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n    except Exception as e:\r\n        print(f\"Error loading graph: {e}\")\r\n        exit()\r\n\r\n# Define GraphSAGE model\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"max\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\r\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\r\n\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n        self.bn4 = BatchNorm(hidden_dim)\r\n\r\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\r\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\r\n\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\r\n\r\n        x = F.leaky_relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n\r\n    loss.backward()\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n# Main function\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph_final_1_1.pt\"  \r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n    # Normalize node features\r\n    scaler_x = StandardScaler()\r\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\r\n\r\n    # Normalize target variable\r\n    y_scaler = StandardScaler()\r\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\r\n\r\n    # training\r\n    indices= torch.randperm(g_data.num_nodes)\r\n    train_size=int(0.8*g_data.num_nodes)\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[indices[:train_size]] = True  \r\n    test_mask = ~train_mask  \r\n\r\n    print(f\"Training on {train_size} nodes & Testing on {g_data.num_nodes - train_size}nodes\")\r\n\r\n    # Define model, optimizer, and loss function\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=256, output_dim=1, dropout=0.4).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=3e-5, weight_decay=5e-4)\r\n    scheduler = StepLR(optimizer, step_size=50, gamma=0.8)\r\n    criterion = nn.SmoothL1Loss(beta=1.0)\r\n\r\n\r\n    # best_r2 = -float(\"inf\")\r\n    # early_stop_count = 0\r\n    # patience = 100  # Stop if no improvement after 50 epochs\r\n\r\n    print(\"Starting training for 1000 epochs...\")\r\n\r\n    for epoch in range(1, 1001):  \r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch == 5:\r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\"✅ Epoch 5: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f} - Training is running!\")\r\n\r\n        \r\n\r\n        if epoch % 100 == 0:  \r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\" Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n            # # Early stopping check\r\n            # if r2 > best_r2:\r\n            #     best_r2 = r2\r\n            #     early_stop_count = 0\r\n            # else:\r\n            #     early_stop_count += 1\r\n            #     if early_stop_count >= patience:\r\n            #         print(f\"Early stopping at epoch {epoch}. Best R²={best_r2:.4f}\")\r\n            #         break\r\n\r\n    # print(f\"Final R² score after 300 epochs: {best_r2:.4f}\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph_final_1_1.pt\nNumber of Nodes: 703\nNumber of Edges: 348\nNode Feature Shape: torch.Size([703, 11])\nEdge Index Shape: torch.Size([2, 348])\nTarget Variable Shape: torch.Size([703])\nTraining on 562 nodes & Testing on 141nodes\nStarting training for 1000 epochs...\n✅ Epoch 5: Train Loss=0.4624 | R²=0.0002 | RMSE=0.9909 | MAE=0.9093 - Training is running!\n Epoch 100: Train Loss=0.2351 | R²=0.5028 | RMSE=0.6988 | MAE=0.5592\n Epoch 200: Train Loss=0.1551 | R²=0.5923 | RMSE=0.6327 | MAE=0.3777\n Epoch 300: Train Loss=0.1570 | R²=0.6276 | RMSE=0.6048 | MAE=0.3599\n Epoch 400: Train Loss=0.1404 | R²=0.6580 | RMSE=0.5795 | MAE=0.3467\n Epoch 500: Train Loss=0.1327 | R²=0.6801 | RMSE=0.5605 | MAE=0.3367\n Epoch 600: Train Loss=0.1227 | R²=0.7008 | RMSE=0.5421 | MAE=0.3233\n Epoch 700: Train Loss=0.1113 | R²=0.7116 | RMSE=0.5322 | MAE=0.3146\n Epoch 800: Train Loss=0.1089 | R²=0.7181 | RMSE=0.5261 | MAE=0.3060\n Epoch 900: Train Loss=0.1073 | R²=0.7237 | RMSE=0.5209 | MAE=0.2960\n Epoch 1000: Train Loss=0.0985 | R²=0.7281 | RMSE=0.5167 | MAE=0.2911\n",
  "history_begin_time" : 1741031532574,
  "history_end_time" : 1741031557260,
  "history_notes" : "Epoch 1000: Train Loss=0.0985 | R²=0.7281 | RMSE=0.5167 | MAE=0.2911",
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "pZxOGRNbq8sZ",
  "history_input" : "import numpy as np\r\nimport torch\r\nimport warnings\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv, BatchNorm\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom torch.optim.lr_scheduler import StepLR\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\n\r\n# Load and verify graph\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data = torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\"Number of Edges: {g_data.num_edges}\")\r\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\"Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n    except Exception as e:\r\n        print(f\"Error loading graph: {e}\")\r\n        exit()\r\n\r\n# Define GraphSAGE model\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"max\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\r\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\r\n\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n        self.bn4 = BatchNorm(hidden_dim)\r\n\r\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\r\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\r\n\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\r\n\r\n        x = F.leaky_relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n\r\n    loss.backward()\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n# Main function\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n    # Normalize node features\r\n    scaler_x = StandardScaler()\r\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\r\n\r\n    # Normalize target variable\r\n    y_scaler = StandardScaler()\r\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\r\n\r\n    # training\r\n    indices= torch.randperm(g_data.num_nodes)\r\n    train_size=int(0.8*g_data.num_nodes)\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[indices[:train_size]] = True  \r\n    test_mask = ~train_mask  \r\n\r\n    print(f\"Training on {train_size} nodes & Testing on {g_data.num_nodes - train_size}nodes\")\r\n\r\n    # Define model, optimizer, and loss function\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=256, output_dim=1, dropout=0.4).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=3e-5, weight_decay=5e-4)\r\n    scheduler = StepLR(optimizer, step_size=50, gamma=0.8)\r\n    criterion = nn.SmoothL1Loss(beta=1.0)\r\n\r\n\r\n    # best_r2 = -float(\"inf\")\r\n    # early_stop_count = 0\r\n    # patience = 100  # Stop if no improvement after 50 epochs\r\n\r\n    print(\"Starting training for 1000 epochs...\")\r\n\r\n    for epoch in range(1, 1001):  \r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch == 5:\r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\"✅ Epoch 5: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f} - Training is running!\")\r\n\r\n        \r\n\r\n        if epoch % 100 == 0:  \r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\" Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n            # # Early stopping check\r\n            # if r2 > best_r2:\r\n            #     best_r2 = r2\r\n            #     early_stop_count = 0\r\n            # else:\r\n            #     early_stop_count += 1\r\n            #     if early_stop_count >= patience:\r\n            #         print(f\"Early stopping at epoch {epoch}. Best R²={best_r2:.4f}\")\r\n            #         break\r\n\r\n    # print(f\"Final R² score after 300 epochs: {best_r2:.4f}\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 434795\nNumber of Edges: 2173386\nNode Feature Shape: torch.Size([434795, 11])\nEdge Index Shape: torch.Size([2, 2173386])\nTarget Variable Shape: torch.Size([434795])\nTraining on 347836 nodes & Testing on 86959nodes\nStarting training for 1000 epochs...\n✅ Epoch 5: Train Loss=0.4127 | R²=0.0019 | RMSE=0.9964 | MAE=0.8420 - Training is running!\n Epoch 100: Train Loss=0.3051 | R²=0.2925 | RMSE=0.8389 | MAE=0.5173\n Epoch 200: Train Loss=0.2651 | R²=0.4292 | RMSE=0.7535 | MAE=0.4459\n Epoch 300: Train Loss=0.2464 | R²=0.4777 | RMSE=0.7208 | MAE=0.4147\n Epoch 400: Train Loss=0.2341 | R²=0.4884 | RMSE=0.7134 | MAE=0.4004\n Epoch 500: Train Loss=0.2253 | R²=0.4896 | RMSE=0.7125 | MAE=0.3901\n Epoch 600: Train Loss=0.2184 | R²=0.4925 | RMSE=0.7105 | MAE=0.3793\n Epoch 700: Train Loss=0.2143 | R²=0.4973 | RMSE=0.7071 | MAE=0.3695\n Epoch 800: Train Loss=0.2101 | R²=0.5032 | RMSE=0.7030 | MAE=0.3604\n Epoch 900: Train Loss=0.2080 | R²=0.5078 | RMSE=0.6997 | MAE=0.3536\n Epoch 1000: Train Loss=0.2051 | R²=0.5116 | RMSE=0.6970 | MAE=0.3478\n",
  "history_begin_time" : 1740939744055,
  "history_end_time" : 1740968532358,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "mswqLNMVEmAv",
  "history_input" : "import numpy as np\r\nimport torch\r\nimport warnings\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv, BatchNorm\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom torch.optim.lr_scheduler import StepLR\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\n\r\n# Load and verify graph\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data = torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\"Number of Edges: {g_data.num_edges}\")\r\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\"Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n    except Exception as e:\r\n        print(f\"Error loading graph: {e}\")\r\n        exit()\r\n\r\n# Define GraphSAGE model\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"max\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\r\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\r\n\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n        self.bn4 = BatchNorm(hidden_dim)\r\n\r\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\r\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\r\n\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\r\n\r\n        x = F.leaky_relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n\r\n    loss.backward()\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n# Main function\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n    # Normalize node features\r\n    scaler_x = StandardScaler()\r\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\r\n\r\n    # Normalize target variable\r\n    y_scaler = StandardScaler()\r\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\r\n\r\n    # training\r\n    indices= torch.randperm(g_data.num_nodes)\r\n    train_size=int(0.8*g_data.num_nodes)\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[indices[:train_size]] = True  \r\n    test_mask = ~train_mask  \r\n\r\n    print(f\"Training on {train_size} nodes & Testing on {g_data.num_nodes - train_size}nodes\")\r\n\r\n    # Define model, optimizer, and loss function\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=256, output_dim=1, dropout=0.4).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=3e-5, weight_decay=5e-4)\r\n    scheduler = StepLR(optimizer, step_size=50, gamma=0.8)\r\n    criterion = nn.SmoothL1Loss(beta=1.0)\r\n\r\n\r\n    # best_r2 = -float(\"inf\")\r\n    # early_stop_count = 0\r\n    # patience = 100  # Stop if no improvement after 50 epochs\r\n\r\n    print(\"Starting training for 500 epochs...\")\r\n\r\n    for epoch in range(1, 501):  \r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch == 5:\r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\"✅ Epoch 5: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f} - Training is running!\")\r\n\r\n        \r\n\r\n        if epoch % 100 == 0:  \r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\" Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n            # # Early stopping check\r\n            # if r2 > best_r2:\r\n            #     best_r2 = r2\r\n            #     early_stop_count = 0\r\n            # else:\r\n            #     early_stop_count += 1\r\n            #     if early_stop_count >= patience:\r\n            #         print(f\"Early stopping at epoch {epoch}. Best R²={best_r2:.4f}\")\r\n            #         break\r\n\r\n    # print(f\"Final R² score after 300 epochs: {best_r2:.4f}\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 434795\nNumber of Edges: 2173386\nNode Feature Shape: torch.Size([434795, 11])\nEdge Index Shape: torch.Size([2, 2173386])\nTarget Variable Shape: torch.Size([434795])\nTraining on 347836 nodes & Testing on 86959nodes\nStarting training for 500 epochs...\n✅ Epoch 5: Train Loss=0.4130 | R²=0.0020 | RMSE=0.9958 | MAE=0.8224 - Training is running!\n Epoch 100: Train Loss=0.3016 | R²=0.3208 | RMSE=0.8215 | MAE=0.5200\n Epoch 200: Train Loss=0.2619 | R²=0.4487 | RMSE=0.7402 | MAE=0.4486\n Epoch 300: Train Loss=0.2442 | R²=0.4749 | RMSE=0.7223 | MAE=0.4206\n Epoch 400: Train Loss=0.2312 | R²=0.4759 | RMSE=0.7217 | MAE=0.4004\n Epoch 500: Train Loss=0.2233 | R²=0.4735 | RMSE=0.7233 | MAE=0.3884\n",
  "history_begin_time" : 1740859547536,
  "history_end_time" : 1740873992647,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "HQmKqR5JL1AK",
  "history_input" : "import numpy as np\r\nimport torch\r\nimport warnings\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv, BatchNorm\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom torch.optim.lr_scheduler import StepLR\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\n\r\n# Load and verify graph\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data = torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\"Number of Edges: {g_data.num_edges}\")\r\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\"Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n    except Exception as e:\r\n        print(f\"Error loading graph: {e}\")\r\n        exit()\r\n\r\n# Define GraphSAGE model\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"max\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\r\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\r\n\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n        self.bn4 = BatchNorm(hidden_dim)\r\n\r\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\r\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\r\n\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\r\n\r\n        x = F.leaky_relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n\r\n    loss.backward()\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n# Main function\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n    # Normalize node features\r\n    scaler_x = StandardScaler()\r\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\r\n\r\n    # Normalize target variable\r\n    y_scaler = StandardScaler()\r\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\r\n\r\n    # training\r\n    indices= torch.randperm(g_data.num_nodes)\r\n    train_size=int(0.8*g_data.num_nodes)\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[indices[:train_size]] = True  \r\n    test_mask = ~train_mask  \r\n\r\n    print(f\"Training on {train_size} nodes & Testing on {g_data.num_nodes - train_size}nodes\")\r\n\r\n    # Define model, optimizer, and loss function\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=512, output_dim=1, dropout=0.4).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=3e-5, weight_decay=5e-4)\r\n    scheduler = StepLR(optimizer, step_size=50, gamma=0.8)\r\n    criterion = nn.SmoothL1Loss(beta=1.0)\r\n\r\n\r\n    # best_r2 = -float(\"inf\")\r\n    # early_stop_count = 0\r\n    # patience = 100  # Stop if no improvement after 50 epochs\r\n\r\n    print(\"Starting training for 500 epochs...\")\r\n\r\n    for epoch in range(1, 501):  \r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch == 5:\r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\"✅ Epoch 5: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f} - Training is running!\")\r\n\r\n        \r\n\r\n        if epoch % 100 == 0:  \r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\" Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n            # # Early stopping check\r\n            # if r2 > best_r2:\r\n            #     best_r2 = r2\r\n            #     early_stop_count = 0\r\n            # else:\r\n            #     early_stop_count += 1\r\n            #     if early_stop_count >= patience:\r\n            #         print(f\"Early stopping at epoch {epoch}. Best R²={best_r2:.4f}\")\r\n            #         break\r\n\r\n    # print(f\"Final R² score after 300 epochs: {best_r2:.4f}\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 434795\nNumber of Edges: 2173386\nNode Feature Shape: torch.Size([434795, 11])\nEdge Index Shape: torch.Size([2, 2173386])\nTarget Variable Shape: torch.Size([434795])\nTraining on 347836 nodes & Testing on 86959nodes\nStarting training for 500 epochs...\n",
  "history_begin_time" : 1740859334810,
  "history_end_time" : 1740859447260,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "zLZpvjI6jliX",
  "history_input" : "import numpy as np\r\nimport torch\r\nimport warnings\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv, BatchNorm\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom torch.optim.lr_scheduler import StepLR\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\n\r\n# Load and verify graph\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data = torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\"Number of Edges: {g_data.num_edges}\")\r\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\"Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n    except Exception as e:\r\n        print(f\"Error loading graph: {e}\")\r\n        exit()\r\n\r\n# Define GraphSAGE model\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"max\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\r\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\r\n\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n        self.bn4 = BatchNorm(hidden_dim)\r\n\r\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\r\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\r\n\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\r\n\r\n        x = F.leaky_relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n\r\n    loss.backward()\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n# Main function\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n    # Normalize node features\r\n    scaler_x = StandardScaler()\r\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\r\n\r\n    # Normalize target variable\r\n    y_scaler = StandardScaler()\r\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\r\n\r\n    # training\r\n    indices= torch.randperm(g_data.num_nodes)\r\n    train_size=int(0.8*g_data.num_nodes)\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[indices[:train_size]] = True  \r\n    test_mask = ~train_mask  \r\n\r\n    print(f\"Training on {train_size} nodes & Testing on {g_data.num_nodes - train_size}nodes\")\r\n\r\n    # Define model, optimizer, and loss function\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=512, output_dim=1, dropout=0.4).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=3e-5, weight_decay=5e-4)\r\n    scheduler = StepLR(optimizer, step_size=50, gamma=0.8)\r\n\r\n\r\n    # best_r2 = -float(\"inf\")\r\n    # early_stop_count = 0\r\n    # patience = 100  # Stop if no improvement after 50 epochs\r\n\r\n    print(\"Starting training for 500 epochs...\")\r\n\r\n    for epoch in range(1, 501):  \r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch == 5:\r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\"✅ Epoch 5: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f} - Training is running!\")\r\n\r\n        \r\n\r\n        if epoch % 100 == 0:  \r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\" Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n            # # Early stopping check\r\n            # if r2 > best_r2:\r\n            #     best_r2 = r2\r\n            #     early_stop_count = 0\r\n            # else:\r\n            #     early_stop_count += 1\r\n            #     if early_stop_count >= patience:\r\n            #         print(f\"Early stopping at epoch {epoch}. Best R²={best_r2:.4f}\")\r\n            #         break\r\n\r\n    # print(f\"Final R² score after 300 epochs: {best_r2:.4f}\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 434795\nNumber of Edges: 2173386\nNode Feature Shape: torch.Size([434795, 11])\nEdge Index Shape: torch.Size([2, 2173386])\nTarget Variable Shape: torch.Size([434795])\nTraining on 347836 nodes & Testing on 86959nodes\nStarting training for 500 epochs...\nTraceback (most recent call last):\n  File \"/media/volume1/gw-workspace/zLZpvjI6jliX/graph_model.py\", line 162, in <module>\n    main()\n  File \"/media/volume1/gw-workspace/zLZpvjI6jliX/graph_model.py\", line 137, in main\n    train_loss = train(model, optimizer, criterion, g_data, train_mask)\n                                         ^^^^^^^^^\nNameError: name 'criterion' is not defined\n",
  "history_begin_time" : 1740858980367,
  "history_end_time" : 1740859008432,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "LfBysBUxIgyo",
  "history_input" : "import numpy as np\r\nimport torch\r\nimport warnings\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv, BatchNorm\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\n\r\n# Load and verify graph\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data = torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\"Number of Edges: {g_data.num_edges}\")\r\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\"Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n    except Exception as e:\r\n        print(f\"Error loading graph: {e}\")\r\n        exit()\r\n\r\n# Define GraphSAGE model\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"max\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\r\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"max\")\r\n\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n        self.bn4 = BatchNorm(hidden_dim)\r\n\r\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\r\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\r\n\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\r\n\r\n        x = F.leaky_relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n\r\n    loss.backward()\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n# Main function\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n    # Normalize node features\r\n    scaler_x = StandardScaler()\r\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\r\n\r\n    # Normalize target variable\r\n    y_scaler = StandardScaler()\r\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\r\n\r\n    # training\r\n    indices= torch.randperm(g_data.num_nodes)\r\n    train_size=int(0.8*g_data.num_nodes)\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[:train_size] = True  \r\n    test_mask = ~train_mask  \r\n\r\n    print(f\"Training on {train_size} nodes & Testing on {g_data.num_nodes - train_size}nodes\")\r\n\r\n    # Define model, optimizer, and loss function\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=512, output_dim=1, dropout=0.4).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=3e-5, weight_decay=1e-4)\r\n    scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\r\n    criterion = nn.SmoothL1Loss(beta=1.0)  # Huber Loss\r\n\r\n    # best_r2 = -float(\"inf\")\r\n    # early_stop_count = 0\r\n    # patience = 100  # Stop if no improvement after 50 epochs\r\n\r\n    print(\"Starting training for 500 epochs...\")\r\n\r\n    for epoch in range(1, 501):  \r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch == 5:\r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\"✅ Epoch 5: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f} - Training is running!\")\r\n\r\n        \r\n\r\n        if epoch % 100 == 0:  \r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\" Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n            # # Early stopping check\r\n            # if r2 > best_r2:\r\n            #     best_r2 = r2\r\n            #     early_stop_count = 0\r\n            # else:\r\n            #     early_stop_count += 1\r\n            #     if early_stop_count >= patience:\r\n            #         print(f\"Early stopping at epoch {epoch}. Best R²={best_r2:.4f}\")\r\n            #         break\r\n\r\n    # print(f\"Final R² score after 300 epochs: {best_r2:.4f}\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 434795\nNumber of Edges: 2173386\nNode Feature Shape: torch.Size([434795, 11])\nEdge Index Shape: torch.Size([2, 2173386])\nTarget Variable Shape: torch.Size([434795])\nTraining on 347836 nodes & Testing on 86959nodes\nStarting training for 500 epochs...\n",
  "history_begin_time" : 1740858303033,
  "history_end_time" : 1740858394765,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "drJw6dJ458v4",
  "history_input" : "import numpy as np\r\nimport torch\r\nimport warnings\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv, BatchNorm\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\n\r\n# Load and verify graph\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data = torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\"Number of Edges: {g_data.num_edges}\")\r\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\"Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n    except Exception as e:\r\n        print(f\"Error loading graph: {e}\")\r\n        exit()\r\n\r\n# Define GraphSAGE model\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"aggr\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"aggr\")\r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"aggr\")\r\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"aggr\")\r\n\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n        self.bn4 = BatchNorm(hidden_dim)\r\n\r\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\r\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\r\n\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\r\n\r\n        x = F.leaky_relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n\r\n    loss.backward()\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n# Main function\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n    # Normalize node features\r\n    scaler_x = StandardScaler()\r\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\r\n\r\n    # Normalize target variable\r\n    y_scaler = StandardScaler()\r\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\r\n\r\n    # training\r\n    indices= torch.randperm(g_data.num_nodes)\r\n    train_size=int(0.8*g_data.num_nodes)\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[:train_size] = True  \r\n    test_mask = ~train_mask  \r\n\r\n    print(f\"Training on {train_size} nodes & Testing on {g_data.num_nodes - train_size}nodes\")\r\n\r\n    # Define model, optimizer, and loss function\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=512, output_dim=1, dropout=0.4, aggr=\"max\").to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=3e-5, weight_decay=1e-4)\r\n    scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\r\n    criterion = nn.SmoothL1Loss(beta=1.0)  # Huber Loss\r\n\r\n    # best_r2 = -float(\"inf\")\r\n    # early_stop_count = 0\r\n    # patience = 100  # Stop if no improvement after 50 epochs\r\n\r\n    print(\"Starting training for 500 epochs...\")\r\n\r\n    for epoch in range(1, 501):  \r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch == 5:\r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\"✅ Epoch 5: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f} - Training is running!\")\r\n\r\n        \r\n\r\n        if epoch % 100 == 0:  \r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\" Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n            # # Early stopping check\r\n            # if r2 > best_r2:\r\n            #     best_r2 = r2\r\n            #     early_stop_count = 0\r\n            # else:\r\n            #     early_stop_count += 1\r\n            #     if early_stop_count >= patience:\r\n            #         print(f\"Early stopping at epoch {epoch}. Best R²={best_r2:.4f}\")\r\n            #         break\r\n\r\n    # print(f\"Final R² score after 300 epochs: {best_r2:.4f}\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 434795\nNumber of Edges: 2173386\nNode Feature Shape: torch.Size([434795, 11])\nEdge Index Shape: torch.Size([2, 2173386])\nTarget Variable Shape: torch.Size([434795])\nTraining on 347836 nodes & Testing on 86959nodes\nTraceback (most recent call last):\n  File \"/media/volume1/gw-workspace/drJw6dJ458v4/graph_model.py\", line 162, in <module>\n    main()\n  File \"/media/volume1/gw-workspace/drJw6dJ458v4/graph_model.py\", line 125, in main\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=512, output_dim=1, dropout=0.4, aggr=\"max\").to(device)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: GraphSAGE.__init__() got an unexpected keyword argument 'aggr'\n",
  "history_begin_time" : 1740858204801,
  "history_end_time" : 1740858209586,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "JAwP5n9v22lI",
  "history_input" : "import numpy as np\r\nimport torch\r\nimport warnings\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv, BatchNorm\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\n\r\n# Load and verify graph\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data = torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\"Number of Edges: {g_data.num_edges}\")\r\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\"Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n    except Exception as e:\r\n        print(f\"Error loading graph: {e}\")\r\n        exit()\r\n\r\n# Define GraphSAGE model\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n        self.bn4 = BatchNorm(hidden_dim)\r\n\r\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\r\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\r\n\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\r\n\r\n        x = F.leaky_relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n\r\n    loss.backward()\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n# Main function\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n    # Normalize node features\r\n    scaler_x = StandardScaler()\r\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\r\n\r\n    # Normalize target variable\r\n    y_scaler = StandardScaler()\r\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\r\n\r\n    # training\r\n    indices= torch.randperm(g_data.num_nodes)\r\n    train_size=int(0.8*g_data.num_nodes)\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[:train_size] = True  \r\n    test_mask = ~train_mask  \r\n\r\n    print(f\"Training on {train_size} nodes & Testing on {g_data.num_nodes - train_size}nodes\")\r\n\r\n    # Define model, optimizer, and loss function\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=512, output_dim=1, dropout=0.35).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=3e-5, weight_decay=1e-4)\r\n    scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\r\n    criterion = nn.SmoothL1Loss(beta=1.0)  # Huber Loss\r\n\r\n    # best_r2 = -float(\"inf\")\r\n    # early_stop_count = 0\r\n    # patience = 100  # Stop if no improvement after 50 epochs\r\n\r\n    print(\"Starting training for 500 epochs...\")\r\n\r\n    for epoch in range(1, 501):  \r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch == 5:\r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\"✅ Epoch 5: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f} - Training is running!\")\r\n\r\n        \r\n\r\n        if epoch % 100 == 0:  \r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\" Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n            # # Early stopping check\r\n            # if r2 > best_r2:\r\n            #     best_r2 = r2\r\n            #     early_stop_count = 0\r\n            # else:\r\n            #     early_stop_count += 1\r\n            #     if early_stop_count >= patience:\r\n            #         print(f\"Early stopping at epoch {epoch}. Best R²={best_r2:.4f}\")\r\n            #         break\r\n\r\n    # print(f\"Final R² score after 300 epochs: {best_r2:.4f}\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 434795\nNumber of Edges: 2173386\nNode Feature Shape: torch.Size([434795, 11])\nEdge Index Shape: torch.Size([2, 2173386])\nTarget Variable Shape: torch.Size([434795])\nTraining on 347836 nodes & Testing on 86959nodes\nStarting training for 500 epochs...\n✅ Epoch 5: Train Loss=0.4081 | R²=-0.0052 | RMSE=1.0909 | MAE=0.9300 - Training is running!\n Epoch 100: Train Loss=0.2295 | R²=0.3343 | RMSE=0.8878 | MAE=0.5984\n Epoch 200: Train Loss=0.2014 | R²=0.3511 | RMSE=0.8765 | MAE=0.5660\n Epoch 300: Train Loss=0.1880 | R²=0.3336 | RMSE=0.8883 | MAE=0.5528\n Epoch 400: Train Loss=0.1808 | R²=0.3305 | RMSE=0.8903 | MAE=0.5425\n Epoch 500: Train Loss=0.1762 | R²=0.3315 | RMSE=0.8897 | MAE=0.5349\n",
  "history_begin_time" : 1740840636968,
  "history_end_time" : 1740856410672,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "JDpt4aLWrnBV",
  "history_input" : "import numpy as np\r\nimport torch\r\nimport warnings\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv, BatchNorm\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\n\r\n# Load and verify graph\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data = torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\"Number of Edges: {g_data.num_edges}\")\r\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\"Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n    except Exception as e:\r\n        print(f\"Error loading graph: {e}\")\r\n        exit()\r\n\r\n# Define GraphSAGE model\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n        self.bn4 = BatchNorm(hidden_dim)\r\n\r\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\r\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\r\n\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\r\n\r\n        x = F.leaky_relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n\r\n    loss.backward()\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n# Main function\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n    # Normalize node features\r\n    scaler_x = StandardScaler()\r\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\r\n\r\n    # Normalize target variable\r\n    y_scaler = StandardScaler()\r\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\r\n\r\n    # training\r\n    indices= torch.randperm(g_data.num_nodes)\r\n    train_size=int(0.8*g_data.num_nodes)\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[:train_size] = True  \r\n    test_mask = ~train_mask  \r\n\r\n    print(f\"Training on {train_size} nodes & Testing on {g_data.num_nodes - train_size}nodes\")\r\n\r\n    # Define model, optimizer, and loss function\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=512, output_dim=1, dropout=0.35).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=3e-5, weight_decay=1e-4)\r\n    scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\r\n    criterion = nn.SmoothL1Loss(beta=1.0)  # Huber Loss\r\n\r\n    # best_r2 = -float(\"inf\")\r\n    # early_stop_count = 0\r\n    # patience = 100  # Stop if no improvement after 50 epochs\r\n\r\n    print(\"Starting training for 300 epochs...\")\r\n\r\n    for epoch in range(1, 301):  \r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch == 5:\r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\"✅ Epoch 5: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f} - Training is running!\")\r\n\r\n        \r\n\r\n        if epoch % 100 == 0:  \r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\" Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n            # # Early stopping check\r\n            # if r2 > best_r2:\r\n            #     best_r2 = r2\r\n            #     early_stop_count = 0\r\n            # else:\r\n            #     early_stop_count += 1\r\n            #     if early_stop_count >= patience:\r\n            #         print(f\"Early stopping at epoch {epoch}. Best R²={best_r2:.4f}\")\r\n            #         break\r\n\r\n    # print(f\"Final R² score after 300 epochs: {best_r2:.4f}\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 434795\nNumber of Edges: 2173386\nNode Feature Shape: torch.Size([434795, 11])\nEdge Index Shape: torch.Size([2, 2173386])\nTarget Variable Shape: torch.Size([434795])\nTraining on 347836 nodes & Testing on 86959nodes\nStarting training for 300 epochs...\n",
  "history_begin_time" : 1740840586333,
  "history_end_time" : 1740840612742,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "cdXWqsTI4CYq",
  "history_input" : "import numpy as np\r\nimport torch\r\nimport warnings\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv, BatchNorm\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\n\r\n# Load and verify graph\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data = torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\"Number of Edges: {g_data.num_edges}\")\r\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\"Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n    except Exception as e:\r\n        print(f\"Error loading graph: {e}\")\r\n        exit()\r\n\r\n# Define GraphSAGE model\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n        self.bn4 = BatchNorm(hidden_dim)\r\n\r\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\r\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\r\n\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\r\n\r\n        x = F.leaky_relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n\r\n    loss.backward()\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n# Main function\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n    # Normalize node features\r\n    scaler_x = StandardScaler()\r\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\r\n\r\n    # Normalize target variable\r\n    y_scaler = StandardScaler()\r\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\r\n\r\n    # training\r\n    indices= torch.randperm(g_data.num_nodes)\r\n    train_size=int(0.8*g_data.num_nodes)\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[:train_size] = True  \r\n    test_mask = ~train_mask  \r\n\r\n    print(f\"Training on {train_size} nodes & Testing on {g_data.num_nodes - train_size}nodes\")\r\n\r\n    # Define model, optimizer, and loss function\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=512, output_dim=1, dropout=0.35).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=3e-5, weight_decay=1e-4)\r\n    scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\r\n    criterion = nn.SmoothL1Loss(beta=1.0)  # Huber Loss\r\n\r\n    best_r2 = -float(\"inf\")\r\n    early_stop_count = 0\r\n    patience = 60  # Stop if no improvement after 50 epochs\r\n\r\n    print(\"Starting training for 300 epochs...\")\r\n\r\n    for epoch in range(1, 301):  \r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch == 5:\r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\"✅ Epoch 5: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f} - Training is running!\")\r\n\r\n        \r\n\r\n        if epoch % 10 == 0:  \r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\" Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n            # Early stopping check\r\n            if r2 > best_r2:\r\n                best_r2 = r2\r\n                early_stop_count = 0\r\n            else:\r\n                early_stop_count += 1\r\n                if early_stop_count >= patience:\r\n                    print(f\"Early stopping at epoch {epoch}. Best R²={best_r2:.4f}\")\r\n                    break\r\n\r\n    print(f\"Final R² score after 300 epochs: {best_r2:.4f}\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 434795\nNumber of Edges: 2173386\nNode Feature Shape: torch.Size([434795, 11])\nEdge Index Shape: torch.Size([2, 2173386])\nTarget Variable Shape: torch.Size([434795])\nTraining on 347836 nodes & Testing on 86959nodes\nStarting training for 300 epochs...\n✅ Epoch 5: Train Loss=0.3781 | R²=-0.0099 | RMSE=1.0935 | MAE=0.9178 - Training is running!\n Epoch 10: Train Loss=0.3523 | R²=0.0001 | RMSE=1.0881 | MAE=0.9003\n Epoch 20: Train Loss=0.3232 | R²=0.0582 | RMSE=1.0560 | MAE=0.8348\n Epoch 30: Train Loss=0.2951 | R²=0.1442 | RMSE=1.0066 | MAE=0.7479\n Epoch 40: Train Loss=0.2757 | R²=0.1772 | RMSE=0.9870 | MAE=0.6972\n Epoch 50: Train Loss=0.2647 | R²=0.2055 | RMSE=0.9699 | MAE=0.6779\n Epoch 60: Train Loss=0.2535 | R²=0.2494 | RMSE=0.9427 | MAE=0.6513\n Epoch 70: Train Loss=0.2440 | R²=0.2820 | RMSE=0.9220 | MAE=0.6216\n Epoch 80: Train Loss=0.2366 | R²=0.3107 | RMSE=0.9034 | MAE=0.6000\n Epoch 90: Train Loss=0.2310 | R²=0.3299 | RMSE=0.8907 | MAE=0.5931\n Epoch 100: Train Loss=0.2272 | R²=0.3432 | RMSE=0.8818 | MAE=0.5920\n Epoch 110: Train Loss=0.2238 | R²=0.3513 | RMSE=0.8764 | MAE=0.5889\n Epoch 120: Train Loss=0.2201 | R²=0.3565 | RMSE=0.8729 | MAE=0.5852\n Epoch 130: Train Loss=0.2172 | R²=0.3616 | RMSE=0.8694 | MAE=0.5815\n Epoch 140: Train Loss=0.2141 | R²=0.3642 | RMSE=0.8676 | MAE=0.5781\n Epoch 150: Train Loss=0.2118 | R²=0.3657 | RMSE=0.8666 | MAE=0.5749\n Epoch 160: Train Loss=0.2094 | R²=0.3655 | RMSE=0.8667 | MAE=0.5716\n Epoch 170: Train Loss=0.2062 | R²=0.3642 | RMSE=0.8676 | MAE=0.5681\n Epoch 180: Train Loss=0.2041 | R²=0.3618 | RMSE=0.8692 | MAE=0.5648\n Epoch 190: Train Loss=0.2020 | R²=0.3593 | RMSE=0.8710 | MAE=0.5616\n Epoch 200: Train Loss=0.1999 | R²=0.3564 | RMSE=0.8729 | MAE=0.5586\n Epoch 210: Train Loss=0.1978 | R²=0.3537 | RMSE=0.8747 | MAE=0.5565\n Epoch 220: Train Loss=0.1964 | R²=0.3510 | RMSE=0.8766 | MAE=0.5548\n Epoch 230: Train Loss=0.1946 | R²=0.3479 | RMSE=0.8786 | MAE=0.5534\n Epoch 240: Train Loss=0.1933 | R²=0.3455 | RMSE=0.8803 | MAE=0.5522\n Epoch 250: Train Loss=0.1921 | R²=0.3426 | RMSE=0.8822 | MAE=0.5513\n Epoch 260: Train Loss=0.1905 | R²=0.3408 | RMSE=0.8834 | MAE=0.5500\n Epoch 270: Train Loss=0.1894 | R²=0.3393 | RMSE=0.8844 | MAE=0.5487\n Epoch 280: Train Loss=0.1877 | R²=0.3371 | RMSE=0.8859 | MAE=0.5475\n Epoch 290: Train Loss=0.1873 | R²=0.3366 | RMSE=0.8863 | MAE=0.5461\n Epoch 300: Train Loss=0.1862 | R²=0.3350 | RMSE=0.8873 | MAE=0.5448\nFinal R² score after 300 epochs: 0.3657\n",
  "history_begin_time" : 1740811398047,
  "history_end_time" : 1740821100472,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "3zh7p3xVk9nG",
  "history_input" : "import numpy as np\r\nimport torch\r\nimport warnings\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv, BatchNorm\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\n\r\n# Load and verify graph\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data = torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\"Number of Edges: {g_data.num_edges}\")\r\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\"Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n    except Exception as e:\r\n        print(f\"Error loading graph: {e}\")\r\n        exit()\r\n\r\n# Define GraphSAGE model\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n        self.bn4 = BatchNorm(hidden_dim)\r\n\r\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\r\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\r\n\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\r\n\r\n        x = F.leaky_relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n\r\n    loss.backward()\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n# Main function\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n    # Normalize node features\r\n    scaler_x = StandardScaler()\r\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\r\n\r\n    # Normalize target variable\r\n    y_scaler = StandardScaler()\r\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\r\n\r\n    # training\r\n    indices= torch.randperm(g_data.num_nodes)\r\n    train_size=int(0.8*g_data.num_nodes)\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[:sample_size] = True  \r\n    test_mask = ~train_mask  \r\n\r\n    print(f\"Training on {train_size} nodes & Testing on {g_data.num_nodes - train_size}nodes\")\r\n\r\n    # Define model, optimizer, and loss function\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=512, output_dim=1, dropout=0.35).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=3e-5, weight_decay=1e-4)\r\n    scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\r\n    criterion = nn.SmoothL1Loss(beta=1.0)  # Huber Loss\r\n\r\n    best_r2 = -float(\"inf\")\r\n    early_stop_count = 0\r\n    patience = 60  # Stop if no improvement after 50 epochs\r\n\r\n    print(\"Starting training for 300 epochs...\")\r\n\r\n    for epoch in range(1, 301):  \r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch == 5:\r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\"✅ Epoch 5: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f} - Training is running!\")\r\n\r\n        \r\n\r\n        if epoch % 10 == 0:  \r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\" Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n            # Early stopping check\r\n            if r2 > best_r2:\r\n                best_r2 = r2\r\n                early_stop_count = 0\r\n            else:\r\n                early_stop_count += 1\r\n                if early_stop_count >= patience:\r\n                    print(f\"Early stopping at epoch {epoch}. Best R²={best_r2:.4f}\")\r\n                    break\r\n\r\n    print(f\"Final R² score after 300 epochs: {best_r2:.4f}\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 434795\nNumber of Edges: 2173386\nNode Feature Shape: torch.Size([434795, 11])\nEdge Index Shape: torch.Size([2, 2173386])\nTarget Variable Shape: torch.Size([434795])\nTraceback (most recent call last):\n  File \"/media/volume1/gw-workspace/3zh7p3xVk9nG/graph_model.py\", line 162, in <module>\n    main()\n  File \"/media/volume1/gw-workspace/3zh7p3xVk9nG/graph_model.py\", line 119, in main\n    train_mask[:sample_size] = True  \n                ^^^^^^^^^^^\nNameError: name 'sample_size' is not defined\n",
  "history_begin_time" : 1740811319203,
  "history_end_time" : 1740811324283,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "zv09OvlIaOCo",
  "history_input" : "import numpy as np\r\nimport torch\r\nimport warnings\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv, BatchNorm\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\n\r\n# Load and verify graph\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data = torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\"Number of Edges: {g_data.num_edges}\")\r\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\"Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n    except Exception as e:\r\n        print(f\"Error loading graph: {e}\")\r\n        exit()\r\n\r\n# Define GraphSAGE model\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n        self.bn4 = BatchNorm(hidden_dim)\r\n\r\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\r\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\r\n\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\r\n\r\n        x = F.leaky_relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n\r\n    loss.backward()\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n# Main function\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n    # Normalize node features\r\n    scaler_x = StandardScaler()\r\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\r\n\r\n    # Normalize target variable\r\n    y_scaler = StandardScaler()\r\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\r\n\r\n    # Define training and testing masks\r\n    sample_size = min(300000, g_data.num_nodes)  # Increase training size if possible\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[:sample_size] = True  \r\n    test_mask = ~train_mask  \r\n\r\n    print(f\"Training on {sample_size} nodes...\")\r\n\r\n    # Define model, optimizer, and loss function\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=256, output_dim=1, dropout=0.4).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=5e-5)\r\n    scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\r\n    criterion = nn.SmoothL1Loss(beta=1.0)  # Huber Loss\r\n\r\n    best_r2 = -float(\"inf\")\r\n    early_stop_count = 0\r\n    patience = 20  # Stop if no improvement after 50 epochs\r\n\r\n    print(\"Starting training for 300 epochs...\")\r\n\r\n    for epoch in range(1, 301):  \r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch == 5:\r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\"✅ Epoch 5: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f} - Training is running!\")\r\n\r\n        \r\n\r\n        if epoch % 10 == 0:  \r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\" Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n            # Early stopping check\r\n            if r2 > best_r2:\r\n                best_r2 = r2\r\n                early_stop_count = 0\r\n            else:\r\n                early_stop_count += 1\r\n                if early_stop_count >= patience:\r\n                    print(f\"Early stopping at epoch {epoch}. Best R²={best_r2:.4f}\")\r\n                    break\r\n\r\n    print(\"Training complete!\")\r\n    print(f\"Final R² score after 300 epochs: {best_r2:.4f}\")\r\n\r\n    if best_r2 > 0.4:  # If R² is good, recommend training longer\r\n        print(\"🚀 R² improved significantly! You can proceed with longer training.\")\r\n    else:\r\n        print(\"⚠️ R² is still low. Consider tuning hyperparameters or increasing training data.\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 434795\nNumber of Edges: 2173386\nNode Feature Shape: torch.Size([434795, 11])\nEdge Index Shape: torch.Size([2, 2173386])\nTarget Variable Shape: torch.Size([434795])\nTraining on 300000 nodes...\nStarting training for 300 epochs...\n✅ Epoch 5: Train Loss=0.3694 | R²=-0.0040 | RMSE=1.0332 | MAE=0.8131 - Training is running!\n Epoch 10: Train Loss=0.3522 | R²=0.0178 | RMSE=1.0220 | MAE=0.7841\n Epoch 20: Train Loss=0.3135 | R²=0.1422 | RMSE=0.9551 | MAE=0.6916\n Epoch 30: Train Loss=0.2927 | R²=0.2341 | RMSE=0.9025 | MAE=0.6096\n Epoch 40: Train Loss=0.2768 | R²=0.2879 | RMSE=0.8702 | MAE=0.6000\n Epoch 50: Train Loss=0.2633 | R²=0.3379 | RMSE=0.8391 | MAE=0.5763\n Epoch 60: Train Loss=0.2521 | R²=0.3741 | RMSE=0.8158 | MAE=0.5359\n Epoch 70: Train Loss=0.2432 | R²=0.3750 | RMSE=0.8152 | MAE=0.5228\n Epoch 80: Train Loss=0.2363 | R²=0.3580 | RMSE=0.8262 | MAE=0.5238\n Epoch 90: Train Loss=0.2307 | R²=0.3348 | RMSE=0.8411 | MAE=0.5227\n Epoch 100: Train Loss=0.2249 | R²=0.3151 | RMSE=0.8534 | MAE=0.5237\n Epoch 110: Train Loss=0.2204 | R²=0.2956 | RMSE=0.8655 | MAE=0.5254\n Epoch 120: Train Loss=0.2162 | R²=0.2769 | RMSE=0.8769 | MAE=0.5278\n Epoch 130: Train Loss=0.2119 | R²=0.2639 | RMSE=0.8847 | MAE=0.5281\n Epoch 140: Train Loss=0.2092 | R²=0.2551 | RMSE=0.8900 | MAE=0.5276\n Epoch 150: Train Loss=0.2054 | R²=0.2482 | RMSE=0.8941 | MAE=0.5269\n Epoch 160: Train Loss=0.2030 | R²=0.2462 | RMSE=0.8953 | MAE=0.5250\n Epoch 170: Train Loss=0.2010 | R²=0.2436 | RMSE=0.8969 | MAE=0.5227\n Epoch 180: Train Loss=0.1991 | R²=0.2459 | RMSE=0.8955 | MAE=0.5196\n Epoch 190: Train Loss=0.1974 | R²=0.2491 | RMSE=0.8936 | MAE=0.5163\n Epoch 200: Train Loss=0.1952 | R²=0.2519 | RMSE=0.8919 | MAE=0.5137\n Epoch 210: Train Loss=0.1938 | R²=0.2559 | RMSE=0.8895 | MAE=0.5108\n Epoch 220: Train Loss=0.1923 | R²=0.2595 | RMSE=0.8873 | MAE=0.5083\n Epoch 230: Train Loss=0.1916 | R²=0.2630 | RMSE=0.8853 | MAE=0.5053\n Epoch 240: Train Loss=0.1903 | R²=0.2698 | RMSE=0.8812 | MAE=0.5020\n Epoch 250: Train Loss=0.1888 | R²=0.2728 | RMSE=0.8794 | MAE=0.5000\n Epoch 260: Train Loss=0.1873 | R²=0.2749 | RMSE=0.8781 | MAE=0.4986\n Epoch 270: Train Loss=0.1873 | R²=0.2819 | RMSE=0.8738 | MAE=0.4951\nEarly stopping at epoch 270. Best R²=0.3750\nTraining complete!\nFinal R² score after 300 epochs: 0.3750\n⚠️ R² is still low. Consider tuning hyperparameters or increasing training data.\n",
  "history_begin_time" : 1740805836473,
  "history_end_time" : 1740809882850,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "6I5G2rMjMdvi",
  "history_input" : "import numpy as np\r\nimport torch\r\nimport warnings\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv, BatchNorm\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\n\r\n# Load and verify graph\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data = torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\"Number of Edges: {g_data.num_edges}\")\r\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\"Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n    except Exception as e:\r\n        print(f\"Error loading graph: {e}\")\r\n        exit()\r\n\r\n# Define GraphSAGE model\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n        self.bn4 = BatchNorm(hidden_dim)\r\n\r\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\r\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\r\n\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\r\n\r\n        x = F.leaky_relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n\r\n    loss.backward()\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n# Main function\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n    # Normalize node features\r\n    scaler_x = StandardScaler()\r\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\r\n\r\n    # Normalize target variable\r\n    y_scaler = StandardScaler()\r\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\r\n\r\n    # Define training and testing masks\r\n    sample_size = min(300000, g_data.num_nodes)  # Increase training size if possible\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[:sample_size] = True  \r\n    test_mask = ~train_mask  \r\n\r\n    print(f\"Training on {sample_size} nodes...\")\r\n\r\n    # Define model, optimizer, and loss function\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=256, output_dim=1, dropout=0.4).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=5e-5)\r\n    scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\r\n    criterion = nn.SmoothL1Loss(beta=1.0)  # Huber Loss\r\n\r\n    best_r2 = -float(\"inf\")\r\n    early_stop_count = 0\r\n    patience = 20  # Stop if no improvement after 50 epochs\r\n\r\n    print(\"Starting training for 300 epochs...\")\r\n\r\n    for epoch in range(1, 301):  \r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch == 5:\r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\"✅ Epoch 5: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f} - Training is running!\")\r\n\r\n        \r\n\r\n        if epoch % 10 == 0:  \r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\" Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n            # Early stopping check\r\n            if r2 > best_r2:\r\n                best_r2 = r2\r\n                early_stop_count = 0\r\n            else:\r\n                early_stop_count += 1\r\n                if early_stop_count >= patience:\r\n                    print(f\"Early stopping at epoch {epoch}. Best R²={best_r2:.4f}\")\r\n                    break\r\n\r\n    print(\"Training complete!\")\r\n    print(f\"Final R² score after 300 epochs: {best_r2:.4f}\")\r\n\r\n    if best_r2 > 0.4:  # If R² is good, recommend training longer\r\n        print(\"🚀 R² improved significantly! You can proceed with longer training.\")\r\n    else:\r\n        print(\"⚠️ R² is still low. Consider tuning hyperparameters or increasing training data.\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 434795\nNumber of Edges: 2173386\nNode Feature Shape: torch.Size([434795, 11])\nEdge Index Shape: torch.Size([2, 2173386])\nTarget Variable Shape: torch.Size([434795])\nTraining on 300000 nodes...\nStarting training for 300 epochs...\n",
  "history_begin_time" : 1740805806764,
  "history_end_time" : 1740805818259,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "CQSQa3e5FvcA",
  "history_input" : "import numpy as np\r\nimport torch\r\nimport warnings\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv, BatchNorm\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\n\r\n# Load and verify graph\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data = torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\"Number of Edges: {g_data.num_edges}\")\r\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\"Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n    except Exception as e:\r\n        print(f\"Error loading graph: {e}\")\r\n        exit()\r\n\r\n# Define GraphSAGE model\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n        self.bn4 = BatchNorm(hidden_dim)\r\n\r\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\r\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\r\n\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\r\n\r\n        x = F.leaky_relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n\r\n    loss.backward()\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n# Main function\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n    # Normalize node features\r\n    scaler_x = StandardScaler()\r\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\r\n\r\n    # Normalize target variable\r\n    y_scaler = StandardScaler()\r\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\r\n\r\n    # Define training and testing masks\r\n    sample_size = min(200000, g_data.num_nodes)  # Increase training size if possible\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[:sample_size] = True  \r\n    test_mask = ~train_mask  \r\n\r\n    print(f\"Training on {sample_size} nodes...\")\r\n\r\n    # Define model, optimizer, and loss function\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=256, output_dim=1, dropout=0.4).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=5e-5)\r\n    scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\r\n    criterion = nn.SmoothL1Loss(beta=1.0)  # Huber Loss\r\n\r\n    best_r2 = -float(\"inf\")\r\n    early_stop_count = 0\r\n    patience = 20  # Stop if no improvement after 50 epochs\r\n\r\n    print(\"Starting training for 300 epochs...\")\r\n\r\n    for epoch in range(1, 301):  \r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch == 5:\r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\"✅ Epoch 5: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f} - Training is running!\")\r\n\r\n        \r\n\r\n        if epoch % 10 == 0:  \r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\" Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n            # Early stopping check\r\n            if r2 > best_r2:\r\n                best_r2 = r2\r\n                early_stop_count = 0\r\n            else:\r\n                early_stop_count += 1\r\n                if early_stop_count >= patience:\r\n                    print(f\"Early stopping at epoch {epoch}. Best R²={best_r2:.4f}\")\r\n                    break\r\n\r\n    print(\"Training complete!\")\r\n    print(f\"Final R² score after 300 epochs: {best_r2:.4f}\")\r\n\r\n    if best_r2 > 0.4:  # If R² is good, recommend training longer\r\n        print(\"🚀 R² improved significantly! You can proceed with longer training.\")\r\n    else:\r\n        print(\"⚠️ R² is still low. Consider tuning hyperparameters or increasing training data.\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 434795\nNumber of Edges: 2173386\nNode Feature Shape: torch.Size([434795, 11])\nEdge Index Shape: torch.Size([2, 2173386])\nTarget Variable Shape: torch.Size([434795])\nTraining on 200000 nodes...\nStarting training for 300 epochs...\n✅ Epoch 5: Train Loss=0.3203 | R²=-0.0153 | RMSE=1.0595 | MAE=0.8334 - Training is running!\n Epoch 10: Train Loss=0.3070 | R²=-0.0027 | RMSE=1.0530 | MAE=0.8118\n Epoch 20: Train Loss=0.2787 | R²=0.0807 | RMSE=1.0082 | MAE=0.7414\n Epoch 30: Train Loss=0.2505 | R²=0.1875 | RMSE=0.9478 | MAE=0.6390\n Epoch 40: Train Loss=0.2356 | R²=0.2446 | RMSE=0.9139 | MAE=0.6141\n Epoch 50: Train Loss=0.2203 | R²=0.2900 | RMSE=0.8860 | MAE=0.5963\n Epoch 60: Train Loss=0.2078 | R²=0.3194 | RMSE=0.8675 | MAE=0.5703\n Epoch 70: Train Loss=0.1979 | R²=0.3036 | RMSE=0.8775 | MAE=0.5584\n Epoch 80: Train Loss=0.1904 | R²=0.2544 | RMSE=0.9080 | MAE=0.5623\n Epoch 90: Train Loss=0.1833 | R²=0.2051 | RMSE=0.9375 | MAE=0.5606\n Epoch 100: Train Loss=0.1786 | R²=0.1771 | RMSE=0.9539 | MAE=0.5530\n Epoch 110: Train Loss=0.1731 | R²=0.1577 | RMSE=0.9651 | MAE=0.5488\n Epoch 120: Train Loss=0.1690 | R²=0.1371 | RMSE=0.9768 | MAE=0.5498\n Epoch 130: Train Loss=0.1654 | R²=0.1229 | RMSE=0.9848 | MAE=0.5513\n Epoch 140: Train Loss=0.1624 | R²=0.1103 | RMSE=0.9918 | MAE=0.5530\n Epoch 150: Train Loss=0.1591 | R²=0.1051 | RMSE=0.9948 | MAE=0.5527\n Epoch 160: Train Loss=0.1561 | R²=0.1049 | RMSE=0.9948 | MAE=0.5502\n Epoch 170: Train Loss=0.1544 | R²=0.1076 | RMSE=0.9933 | MAE=0.5475\n Epoch 180: Train Loss=0.1512 | R²=0.1108 | RMSE=0.9915 | MAE=0.5449\n Epoch 190: Train Loss=0.1491 | R²=0.1143 | RMSE=0.9896 | MAE=0.5421\n Epoch 200: Train Loss=0.1478 | R²=0.1224 | RMSE=0.9850 | MAE=0.5374\n Epoch 210: Train Loss=0.1456 | R²=0.1244 | RMSE=0.9839 | MAE=0.5347\n Epoch 220: Train Loss=0.1444 | R²=0.1315 | RMSE=0.9799 | MAE=0.5304\n Epoch 230: Train Loss=0.1431 | R²=0.1327 | RMSE=0.9793 | MAE=0.5279\n Epoch 240: Train Loss=0.1414 | R²=0.1360 | RMSE=0.9774 | MAE=0.5253\n Epoch 250: Train Loss=0.1415 | R²=0.1415 | RMSE=0.9743 | MAE=0.5221\n Epoch 260: Train Loss=0.1397 | R²=0.1447 | RMSE=0.9725 | MAE=0.5194\nEarly stopping at epoch 260. Best R²=0.3194\nTraining complete!\nFinal R² score after 300 epochs: 0.3194\n⚠️ R² is still low. Consider tuning hyperparameters or increasing training data.\n",
  "history_begin_time" : 1740795901181,
  "history_end_time" : 1740803581441,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "9wsGsKwG5IhC",
  "history_input" : "import numpy as np\r\nimport torch\r\nimport warnings\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv, BatchNorm\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\n\r\n# Load and verify graph\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data = torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\"Number of Edges: {g_data.num_edges}\")\r\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\"Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n    except Exception as e:\r\n        print(f\"Error loading graph: {e}\")\r\n        exit()\r\n\r\n# Define GraphSAGE model\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n        self.bn4 = BatchNorm(hidden_dim)\r\n\r\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\r\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\r\n\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\r\n\r\n        x = F.leaky_relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n\r\n    loss.backward()\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n# Main function\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n    # Normalize node features\r\n    scaler_x = StandardScaler()\r\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\r\n\r\n    # Normalize target variable\r\n    y_scaler = StandardScaler()\r\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\r\n\r\n    # Define training and testing masks\r\n    sample_size = min(100000, g_data.num_nodes)  # Increase training size if possible\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[:sample_size] = True  \r\n    test_mask = ~train_mask  \r\n\r\n    print(f\"Training on {sample_size} nodes...\")\r\n\r\n    # Define model, optimizer, and loss function\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=256, output_dim=1, dropout=0.3).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\r\n    scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\r\n    criterion = nn.SmoothL1Loss(beta=1.0)  # Huber Loss\r\n\r\n    best_r2 = -float(\"inf\")\r\n    early_stop_count = 0\r\n    patience = 50  # Stop if no improvement after 50 epochs\r\n\r\n    print(\"Starting training for 300 epochs...\")\r\n\r\n    for epoch in range(1, 301):  \r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch == 5:\r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\"✅ Epoch 5: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f} - Training is running!\")\r\n\r\n        \r\n\r\n        if epoch % 10 == 0:  \r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\" Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n            # Early stopping check\r\n            if r2 > best_r2:\r\n                best_r2 = r2\r\n                early_stop_count = 0\r\n            else:\r\n                early_stop_count += 1\r\n                if early_stop_count >= patience:\r\n                    print(f\"Early stopping at epoch {epoch}. Best R²={best_r2:.4f}\")\r\n                    break\r\n\r\n    print(\"Training complete!\")\r\n    print(f\"Final R² score after 300 epochs: {best_r2:.4f}\")\r\n\r\n    if best_r2 > 0.4:  # If R² is good, recommend training longer\r\n        print(\"🚀 R² improved significantly! You can proceed with longer training.\")\r\n    else:\r\n        print(\"⚠️ R² is still low. Consider tuning hyperparameters or increasing training data.\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 434795\nNumber of Edges: 2173386\nNode Feature Shape: torch.Size([434795, 11])\nEdge Index Shape: torch.Size([2, 2173386])\nTarget Variable Shape: torch.Size([434795])\nTraining on 100000 nodes...\nStarting training for 300 epochs...\n✅ Epoch 5: Train Loss=0.3142 | R²=-0.0034 | RMSE=1.0217 | MAE=0.8067 - Training is running!\n Epoch 10: Train Loss=0.2966 | R²=0.0132 | RMSE=1.0132 | MAE=0.7845\n Epoch 20: Train Loss=0.2574 | R²=0.1234 | RMSE=0.9550 | MAE=0.7117\n Epoch 30: Train Loss=0.2195 | R²=0.2598 | RMSE=0.8775 | MAE=0.6404\n Epoch 40: Train Loss=0.1951 | R²=0.2866 | RMSE=0.8615 | MAE=0.6126\n Epoch 50: Train Loss=0.1689 | R²=0.2202 | RMSE=0.9007 | MAE=0.6150\n Epoch 60: Train Loss=0.1527 | R²=0.0329 | RMSE=1.0031 | MAE=0.6195\n Epoch 70: Train Loss=0.1440 | R²=-0.0850 | RMSE=1.0624 | MAE=0.6270\n Epoch 80: Train Loss=0.1374 | R²=-0.0892 | RMSE=1.0645 | MAE=0.6155\n Epoch 90: Train Loss=0.1314 | R²=-0.0870 | RMSE=1.0634 | MAE=0.6062\n Epoch 100: Train Loss=0.1274 | R²=-0.0833 | RMSE=1.0616 | MAE=0.6016\n Epoch 110: Train Loss=0.1243 | R²=-0.0804 | RMSE=1.0602 | MAE=0.5980\n Epoch 120: Train Loss=0.1216 | R²=-0.0645 | RMSE=1.0524 | MAE=0.5901\n",
  "history_begin_time" : 1740790402126,
  "history_end_time" : 1740795687164,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "paOUfSpdKWrO",
  "history_input" : "import numpy as np\r\nimport torch\r\nimport warnings\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv, BatchNorm\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\n\r\n# Load and verify graph\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data = torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\"Number of Edges: {g_data.num_edges}\")\r\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\"Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n    except Exception as e:\r\n        print(f\"Error loading graph: {e}\")\r\n        exit()\r\n\r\n# Define GraphSAGE model\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n        self.bn4 = BatchNorm(hidden_dim)\r\n\r\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\r\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\r\n\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\r\n\r\n        x = F.leaky_relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n\r\n    loss.backward()\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n# Main function\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n    # Normalize node features\r\n    scaler_x = StandardScaler()\r\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\r\n\r\n    # Normalize target variable\r\n    y_scaler = StandardScaler()\r\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\r\n\r\n    # Define training and testing masks\r\n    sample_size = min(100000, g_data.num_nodes)  # Increase training size if possible\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[:sample_size] = True  \r\n    test_mask = ~train_mask  \r\n\r\n    print(f\"Training on {sample_size} nodes...\")\r\n\r\n    # Define model, optimizer, and loss function\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=256, output_dim=1, dropout=0.3).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\r\n    scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\r\n    criterion = nn.SmoothL1Loss(beta=1.0)  # Huber Loss\r\n\r\n    best_r2 = -float(\"inf\")\r\n    early_stop_count = 0\r\n    patience = 50  # Stop if no improvement after 50 epochs\r\n\r\n    print(\"Starting training for 300 epochs...\")\r\n\r\n    for epoch in range(1, 301):  \r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch == 5:\r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\"✅ Epoch 5: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f} - Training is running!\")\r\n\r\n        \r\n\r\n        if epoch % 10 == 0:  \r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\" Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n            # Early stopping check\r\n            if r2 > best_r2:\r\n                best_r2 = r2\r\n                early_stop_count = 0\r\n            else:\r\n                early_stop_count += 1\r\n                if early_stop_count >= patience:\r\n                    print(f\"Early stopping at epoch {epoch}. Best R²={best_r2:.4f}\")\r\n                    break\r\n\r\n    print(\"Training complete!\")\r\n    print(f\"Final R² score after 300 epochs: {best_r2:.4f}\")\r\n\r\n    if best_r2 > 0.4:  # If R² is good, recommend training longer\r\n        print(\"🚀 R² improved significantly! You can proceed with longer training.\")\r\n    else:\r\n        print(\"⚠️ R² is still low. Consider tuning hyperparameters or increasing training data.\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 434795\nNumber of Edges: 2173386\nNode Feature Shape: torch.Size([434795, 11])\nEdge Index Shape: torch.Size([2, 2173386])\nTarget Variable Shape: torch.Size([434795])\nTraining on 100000 nodes...\nStarting training for 300 epochs...\n",
  "history_begin_time" : 1740789996823,
  "history_end_time" : 1740790099455,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "qUaWTtDY2JjA",
  "history_input" : "import numpy as np\r\nimport torch\r\nimport warnings\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv, BatchNorm\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\n\r\n# Load and verify graph\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data = torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\"Number of Edges: {g_data.num_edges}\")\r\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\"Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n    except Exception as e:\r\n        print(f\"Error loading graph: {e}\")\r\n        exit()\r\n\r\n# Define GraphSAGE model\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n        self.bn4 = BatchNorm(hidden_dim)\r\n\r\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\r\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\r\n\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\r\n\r\n        x = F.leaky_relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n\r\n    loss.backward()\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n# Main function\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n    # Normalize node features\r\n    scaler_x = StandardScaler()\r\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\r\n\r\n    # Normalize target variable\r\n    y_scaler = StandardScaler()\r\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\r\n\r\n    # Define training and testing masks\r\n    sample_size = min(100000, g_data.num_nodes)  # Increase training size if possible\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[:sample_size] = True  \r\n    test_mask = ~train_mask  \r\n\r\n    print(f\"Training on {sample_size} nodes...\")\r\n\r\n    # Define model, optimizer, and loss function\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=256, output_dim=1, dropout=0.3).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\r\n    scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\r\n    criterion = nn.SmoothL1Loss(beta=1.0)  # Huber Loss\r\n\r\n    best_r2 = -float(\"inf\")\r\n    early_stop_count = 0\r\n    patience = 50  # Stop if no improvement after 50 epochs\r\n\r\n    print(\"Starting training for 300 epochs...\")\r\n\r\n    for epoch in range(1, 301):  \r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch == 5:\r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\"✅ Epoch 5: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f} - Training is running!\")\r\n\r\n        \r\n\r\n        if epoch % 10 == 0:  \r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\" Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n            # Early stopping check\r\n            if r2 > best_r2:\r\n                best_r2 = r2\r\n                early_stop_count = 0\r\n            else:\r\n                early_stop_count += 1\r\n                if early_stop_count >= patience:\r\n                    print(f\"Early stopping at epoch {epoch}. Best R²={best_r2:.4f}\")\r\n                    break\r\n\r\n    print(\"Training complete!\")\r\n    print(f\"Final R² score after 300 epochs: {best_r2:.4f}\")\r\n\r\n    if best_r2 > 0.4:  # If R² is good, recommend training longer\r\n        print(\"🚀 R² improved significantly! You can proceed with longer training.\")\r\n    else:\r\n        print(\"⚠️ R² is still low. Consider tuning hyperparameters or increasing training data.\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 434795\nNumber of Edges: 2173386\nNode Feature Shape: torch.Size([434795, 11])\nEdge Index Shape: torch.Size([2, 2173386])\nTarget Variable Shape: torch.Size([434795])\nTraining on 100000 nodes...\nStarting training for 300 epochs...\n✅ Epoch 5: Train Loss=0.3164 | R²=0.0042 | RMSE=1.0178 | MAE=0.8234 - Training is running!\n Epoch 10: Train Loss=0.2938 | R²=0.0213 | RMSE=1.0091 | MAE=0.7996\n Epoch 20: Train Loss=0.2565 | R²=0.1267 | RMSE=0.9532 | MAE=0.7277\n",
  "history_begin_time" : 1740788257735,
  "history_end_time" : 1740790352639,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "ZmXF4sOB4iwN",
  "history_input" : "import numpy as np\r\nimport torch\r\nimport warnings\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv, BatchNorm\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\n\r\n# Load and verify graph\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data = torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\"Number of Edges: {g_data.num_edges}\")\r\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\"Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n    except Exception as e:\r\n        print(f\"Error loading graph: {e}\")\r\n        exit()\r\n\r\n# Define GraphSAGE model\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n        self.bn4 = BatchNorm(hidden_dim)\r\n\r\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\r\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\r\n\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\r\n\r\n        x = F.leaky_relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n\r\n    loss.backward()\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n# Main function\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n    # Normalize node features\r\n    scaler_x = StandardScaler()\r\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\r\n\r\n    # Normalize target variable\r\n    y_scaler = StandardScaler()\r\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\r\n\r\n    # Define training and testing masks\r\n    sample_size = min(100000, g_data.num_nodes)  # Increase training size if possible\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[:sample_size] = True  \r\n    test_mask = ~train_mask  \r\n\r\n    print(f\"Training on {sample_size} nodes...\")\r\n\r\n    # Define model, optimizer, and loss function\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=256, output_dim=1, dropout=0.3).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\r\n    scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\r\n    criterion = nn.SmoothL1Loss(beta=1.0)  # Huber Loss\r\n\r\n    best_r2 = -float(\"inf\")\r\n    early_stop_count = 0\r\n    patience = 50  # Stop if no improvement after 50 epochs\r\n\r\n    print(\"Starting training for 300 epochs...\")\r\n\r\n    for epoch in range(1, 301):  \r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch == 5:\r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\"✅ Epoch 5: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f} - Training is running!\")\r\n\r\n        \r\n\r\n        if epoch % 10 == 0:  \r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\" Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n            # Early stopping check\r\n            if r2 > best_r2:\r\n                best_r2 = r2\r\n                early_stop_count = 0\r\n            else:\r\n                early_stop_count += 1\r\n                if early_stop_count >= patience:\r\n                    print(f\"Early stopping at epoch {epoch}. Best R²={best_r2:.4f}\")\r\n                    break\r\n\r\n    print(\"Training complete!\")\r\n    print(f\"Final R² score after 300 epochs: {best_r2:.4f}\")\r\n\r\n    if best_r2 > 0.4:  # If R² is good, recommend training longer\r\n        print(\"🚀 R² improved significantly! You can proceed with longer training.\")\r\n    else:\r\n        print(\"⚠️ R² is still low. Consider tuning hyperparameters or increasing training data.\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 434795\nNumber of Edges: 2173386\nNode Feature Shape: torch.Size([434795, 11])\nEdge Index Shape: torch.Size([2, 2173386])\nTarget Variable Shape: torch.Size([434795])\nTraining on 100000 nodes...\nStarting training for 300 epochs...\n✅ Epoch 5: Train Loss=0.3134 | R²=0.0019 | RMSE=1.0190 | MAE=0.8141 - Training is running!\n Epoch 10: Train Loss=0.2942 | R²=0.0261 | RMSE=1.0066 | MAE=0.7930\n Epoch 20: Train Loss=0.2549 | R²=0.1462 | RMSE=0.9425 | MAE=0.7077\n Epoch 30: Train Loss=0.2162 | R²=0.2785 | RMSE=0.8664 | MAE=0.6009\n",
  "history_begin_time" : 1740787652828,
  "history_end_time" : 1740790348173,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "9o23omBlZQz7",
  "history_input" : "import numpy as np\r\nimport torch\r\nimport warnings\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv, BatchNorm\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\n\r\n# Load and verify graph\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data = torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\"Number of Edges: {g_data.num_edges}\")\r\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\"Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n    except Exception as e:\r\n        print(f\"Error loading graph: {e}\")\r\n        exit()\r\n\r\n# Define GraphSAGE model\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n        self.bn4 = BatchNorm(hidden_dim)\r\n\r\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\r\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\r\n\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\r\n\r\n        x = F.leaky_relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n\r\n    loss.backward()\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n# Main function\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n    # Normalize node features\r\n    scaler_x = StandardScaler()\r\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\r\n\r\n    # Normalize target variable\r\n    y_scaler = StandardScaler()\r\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\r\n\r\n    # Define training and testing masks\r\n    sample_size = min(100000, g_data.num_nodes)  # Increase training size if possible\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[:sample_size] = True  \r\n    test_mask = ~train_mask  \r\n\r\n    print(f\"Training on {sample_size} nodes...\")\r\n\r\n    # Define model, optimizer, and loss function\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=256, output_dim=1, dropout=0.3).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\r\n    scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\r\n    criterion = nn.SmoothL1Loss(beta=1.0)  # Huber Loss\r\n\r\n    best_r2 = -float(\"inf\")\r\n    early_stop_count = 0\r\n    patience = 50  # Stop if no improvement after 50 epochs\r\n\r\n    print(\"Starting training for 300 epochs...\")\r\n\r\n    for epoch in range(1, 301):  \r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch == 5:\r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\"✅ Epoch 5: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f} - Training is running!\")\r\n\r\n        \r\n\r\n        if epoch % 10 == 0:  \r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\" Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n            # Early stopping check\r\n            if r2 > best_r2:\r\n                best_r2 = r2\r\n                early_stop_count = 0\r\n            else:\r\n                early_stop_count += 1\r\n                if early_stop_count >= patience:\r\n                    print(f\"Early stopping at epoch {epoch}. Best R²={best_r2:.4f}\")\r\n                    break\r\n\r\n    print(\"Training complete!\")\r\n    print(f\"Final R² score after 300 epochs: {best_r2:.4f}\")\r\n\r\n    if best_r2 > 0.4:  # If R² is good, recommend training longer\r\n        print(\"🚀 R² improved significantly! You can proceed with longer training.\")\r\n    else:\r\n        print(\"⚠️ R² is still low. Consider tuning hyperparameters or increasing training data.\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 434795\nNumber of Edges: 2173386\nNode Feature Shape: torch.Size([434795, 11])\nEdge Index Shape: torch.Size([2, 2173386])\nTarget Variable Shape: torch.Size([434795])\nTraining on 100000 nodes...\nStarting training for 300 epochs...\n✅ Epoch 5: Train Loss=0.3559 | R²=0.0081 | RMSE=1.0159 | MAE=0.8768 - Training is running!\n Epoch 10: Train Loss=0.3135 | R²=0.0304 | RMSE=1.0044 | MAE=0.8608\n Epoch 20: Train Loss=0.2759 | R²=0.1240 | RMSE=0.9547 | MAE=0.7858\n",
  "history_begin_time" : 1740787558690,
  "history_end_time" : 1740790336652,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "EeRLFIkC5lGt",
  "history_input" : "import numpy as np\r\nimport torch\r\nimport warnings\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv, BatchNorm\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\n\r\n# Load and verify graph\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data = torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\"Number of Edges: {g_data.num_edges}\")\r\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\"Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n    except Exception as e:\r\n        print(f\"Error loading graph: {e}\")\r\n        exit()\r\n\r\n# Define GraphSAGE model\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n        self.bn4 = BatchNorm(hidden_dim)\r\n\r\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\r\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\r\n\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\r\n\r\n        x = F.leaky_relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n\r\n    loss.backward()\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n# Main function\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n    # Normalize node features\r\n    scaler_x = StandardScaler()\r\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\r\n\r\n    # Normalize target variable\r\n    y_scaler = StandardScaler()\r\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\r\n\r\n    # Define training and testing masks\r\n    sample_size = min(100000, g_data.num_nodes)  # Increase training size if possible\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[:sample_size] = True  \r\n    test_mask = ~train_mask  \r\n\r\n    print(f\"Training on {sample_size} nodes...\")\r\n\r\n    # Define model, optimizer, and loss function\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=256, output_dim=1, dropout=0.3).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\r\n    scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\r\n    criterion = nn.SmoothL1Loss(beta=1.0)  # Huber Loss\r\n\r\n    best_r2 = -float(\"inf\")\r\n    early_stop_count = 0\r\n    patience = 50  # Stop if no improvement after 50 epochs\r\n\r\n    print(\"Starting training for 300 epochs...\")\r\n\r\n    for epoch in range(1, 301):  \r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch == 5:\r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\"✅ Epoch 5: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f} - Training is running!\")\r\n\r\n        \r\n\r\n        if epoch % 10 == 0:  \r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\" Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n            # Early stopping check\r\n            if r2 > best_r2:\r\n                best_r2 = r2\r\n                early_stop_count = 0\r\n            else:\r\n                early_stop_count += 1\r\n                if early_stop_count >= patience:\r\n                    print(f\"Early stopping at epoch {epoch}. Best R²={best_r2:.4f}\")\r\n                    break\r\n\r\n    print(\"Training complete!\")\r\n    print(f\"Final R² score after 300 epochs: {best_r2:.4f}\")\r\n\r\n    if best_r2 > 0.4:  # If R² is good, recommend training longer\r\n        print(\"🚀 R² improved significantly! You can proceed with longer training.\")\r\n    else:\r\n        print(\"⚠️ R² is still low. Consider tuning hyperparameters or increasing training data.\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 434795\nNumber of Edges: 2173386\nNode Feature Shape: torch.Size([434795, 11])\nEdge Index Shape: torch.Size([2, 2173386])\nTarget Variable Shape: torch.Size([434795])\nTraining on 100000 nodes...\nStarting training for 300 epochs...\n",
  "history_begin_time" : 1740787217418,
  "history_end_time" : 1740787405173,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "jK52synMvYtv",
  "history_input" : "import numpy as np\r\nimport torch\r\nimport warnings\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv, BatchNorm\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\n\r\n# Load and verify graph\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data = torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\"Number of Edges: {g_data.num_edges}\")\r\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\"Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n    except Exception as e:\r\n        print(f\"Error loading graph: {e}\")\r\n        exit()\r\n\r\n# Define GraphSAGE model\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n        self.bn4 = BatchNorm(hidden_dim)\r\n\r\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\r\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\r\n\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\r\n\r\n        x = F.leaky_relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n\r\n    loss.backward()\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n# Main function\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n    # Normalize node features\r\n    scaler_x = StandardScaler()\r\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\r\n\r\n    # Normalize target variable\r\n    y_scaler = StandardScaler()\r\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\r\n\r\n    # Define training and testing masks\r\n    sample_size = min(100000, g_data.num_nodes)  # Increase training size if possible\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[:sample_size] = True  \r\n    test_mask = ~train_mask  \r\n\r\n    print(f\"Training on {sample_size} nodes...\")\r\n\r\n    # Define model, optimizer, and loss function\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=256, output_dim=1, dropout=0.3).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\r\n    scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\r\n    criterion = nn.SmoothL1Loss(beta=1.0)  # Huber Loss\r\n\r\n    best_r2 = -float(\"inf\")\r\n    early_stop_count = 0\r\n    patience = 50  # Stop if no improvement after 50 epochs\r\n\r\n    print(\"Starting training for 300 epochs...\")\r\n\r\n    for epoch in range(1, 301):  \r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch == 5:\r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\"✅ Epoch 5: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f} - Training is running!\")\r\n\r\n        \r\n\r\n        if epoch % 10 == 0:  \r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\" Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n            # Early stopping check\r\n            if r2 > best_r2:\r\n                best_r2 = r2\r\n                early_stop_count = 0\r\n            else:\r\n                early_stop_count += 1\r\n                if early_stop_count >= patience:\r\n                    print(f\"Early stopping at epoch {epoch}. Best R²={best_r2:.4f}\")\r\n                    break\r\n\r\n    print(\"Training complete!\")\r\n    print(f\"Final R² score after 300 epochs: {best_r2:.4f}\")\r\n\r\n    if best_r2 > 0.4:  # If R² is good, recommend training longer\r\n        print(\"🚀 R² improved significantly! You can proceed with longer training.\")\r\n    else:\r\n        print(\"⚠️ R² is still low. Consider tuning hyperparameters or increasing training data.\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 434795\nNumber of Edges: 2173386\nNode Feature Shape: torch.Size([434795, 11])\nEdge Index Shape: torch.Size([2, 2173386])\nTarget Variable Shape: torch.Size([434795])\nTraining on 100000 nodes...\nStarting training for 300 epochs...\n✅ Epoch 5: Train Loss=0.3241 | R²=0.0041 | RMSE=1.0179 | MAE=0.8298 - Training is running!\n",
  "history_begin_time" : 1740787217330,
  "history_end_time" : 1740787741389,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "Ygk3CfHydbaR",
  "history_input" : "import numpy as np\r\nimport torch\r\nimport warnings\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv, BatchNorm\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\n\r\n# Load and verify graph\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data = torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\"Number of Edges: {g_data.num_edges}\")\r\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\"Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n    except Exception as e:\r\n        print(f\"Error loading graph: {e}\")\r\n        exit()\r\n\r\n# Define GraphSAGE model\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n        self.bn4 = BatchNorm(hidden_dim)\r\n\r\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\r\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\r\n\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\r\n\r\n        x = F.leaky_relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n\r\n    loss.backward()\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n# Main function\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n    # Normalize node features\r\n    scaler_x = StandardScaler()\r\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\r\n\r\n    # Normalize target variable\r\n    y_scaler = StandardScaler()\r\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\r\n\r\n    # Define training and testing masks\r\n    sample_size = min(100000, g_data.num_nodes)  # Increase training size if possible\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[:sample_size] = True  \r\n    test_mask = ~train_mask  \r\n\r\n    print(f\"Training on {sample_size} nodes...\")\r\n\r\n    # Define model, optimizer, and loss function\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=256, output_dim=1, dropout=0.3).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\r\n    scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\r\n    criterion = nn.SmoothL1Loss(beta=1.0)  # Huber Loss\r\n\r\n    best_r2 = -float(\"inf\")\r\n    early_stop_count = 0\r\n    patience = 50  # Stop if no improvement after 50 epochs\r\n\r\n    print(\"Starting training for 300 epochs...\")\r\n\r\n    for epoch in range(1, 301):  \r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch == 5:\r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\"✅ Epoch 5: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f} - Training is running!\")\r\n\r\n        \r\n\r\n        if epoch % 10 == 0:  \r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\" Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n            # Early stopping check\r\n            if r2 > best_r2:\r\n                best_r2 = r2\r\n                early_stop_count = 0\r\n            else:\r\n                early_stop_count += 1\r\n                if early_stop_count >= patience:\r\n                    print(f\"Early stopping at epoch {epoch}. Best R²={best_r2:.4f}\")\r\n                    break\r\n\r\n    print(\"Training complete!\")\r\n    print(f\"Final R² score after 300 epochs: {best_r2:.4f}\")\r\n\r\n    if best_r2 > 0.4:  # If R² is good, recommend training longer\r\n        print(\"🚀 R² improved significantly! You can proceed with longer training.\")\r\n    else:\r\n        print(\"⚠️ R² is still low. Consider tuning hyperparameters or increasing training data.\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 434795\nNumber of Edges: 2173386\nNode Feature Shape: torch.Size([434795, 11])\nEdge Index Shape: torch.Size([2, 2173386])\nTarget Variable Shape: torch.Size([434795])\nTraining on 100000 nodes...\nStarting training for 300 epochs...\n",
  "history_begin_time" : 1740787217313,
  "history_end_time" : 1740787405991,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "8WhZl9tMz28n",
  "history_input" : "import numpy as np\r\nimport torch\r\nimport warnings\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv, BatchNorm\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\n\r\n# Load and verify graph\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data = torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\"Number of Edges: {g_data.num_edges}\")\r\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\"Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n    except Exception as e:\r\n        print(f\"Error loading graph: {e}\")\r\n        exit()\r\n\r\n# Define GraphSAGE model\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n        self.bn4 = BatchNorm(hidden_dim)\r\n\r\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\r\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\r\n\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\r\n\r\n        x = F.leaky_relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n\r\n    loss.backward()\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n# Main function\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n    # Normalize node features\r\n    scaler_x = StandardScaler()\r\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\r\n\r\n    # Normalize target variable\r\n    y_scaler = StandardScaler()\r\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\r\n\r\n    # Define training and testing masks\r\n    sample_size = min(100000, g_data.num_nodes)  # Increase training size if possible\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[:sample_size] = True  \r\n    test_mask = ~train_mask  \r\n\r\n    print(f\"Training on {sample_size} nodes...\")\r\n\r\n    # Define model, optimizer, and loss function\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=256, output_dim=1, dropout=0.3).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\r\n    scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\r\n    criterion = nn.SmoothL1Loss(beta=1.0)  # Huber Loss\r\n\r\n    best_r2 = -float(\"inf\")\r\n    early_stop_count = 0\r\n    patience = 50  # Stop if no improvement after 50 epochs\r\n\r\n    print(\"Starting training for 300 epochs...\")\r\n\r\n    for epoch in range(1, 301):  \r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch == 5:\r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\"✅ Epoch 5: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f} - Training is running!\")\r\n\r\n        \r\n\r\n        if epoch % 10 == 0:  \r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\" Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n            # Early stopping check\r\n            if r2 > best_r2:\r\n                best_r2 = r2\r\n                early_stop_count = 0\r\n            else:\r\n                early_stop_count += 1\r\n                if early_stop_count >= patience:\r\n                    print(f\"Early stopping at epoch {epoch}. Best R²={best_r2:.4f}\")\r\n                    break\r\n\r\n    print(\"Training complete!\")\r\n    print(f\"Final R² score after 300 epochs: {best_r2:.4f}\")\r\n\r\n    if best_r2 > 0.4:  # If R² is good, recommend training longer\r\n        print(\"🚀 R² improved significantly! You can proceed with longer training.\")\r\n    else:\r\n        print(\"⚠️ R² is still low. Consider tuning hyperparameters or increasing training data.\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 434795\nNumber of Edges: 2173386\nNode Feature Shape: torch.Size([434795, 11])\nEdge Index Shape: torch.Size([2, 2173386])\nTarget Variable Shape: torch.Size([434795])\nTraining on 100000 nodes...\nStarting training for 300 epochs...\n✅ Epoch 5: Train Loss=0.3119 | R²=0.0037 | RMSE=1.0181 | MAE=0.8183 - Training is running!\n Epoch 10: Train Loss=0.2922 | R²=0.0219 | RMSE=1.0088 | MAE=0.7890\n",
  "history_begin_time" : 1740787216902,
  "history_end_time" : 1740788612863,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "FWkZSH9QNxxX",
  "history_input" : "import numpy as np\r\nimport torch\r\nimport warnings\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv, BatchNorm\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\n\r\n# Load and verify graph\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data = torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\"Number of Edges: {g_data.num_edges}\")\r\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\"Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n    except Exception as e:\r\n        print(f\"Error loading graph: {e}\")\r\n        exit()\r\n\r\n# Define GraphSAGE model\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n        self.bn4 = BatchNorm(hidden_dim)\r\n\r\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\r\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\r\n\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\r\n\r\n        x = F.leaky_relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n\r\n    loss.backward()\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n# Main function\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n    # Normalize node features\r\n    scaler_x = StandardScaler()\r\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\r\n\r\n    # Normalize target variable\r\n    y_scaler = StandardScaler()\r\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\r\n\r\n    # Define training and testing masks\r\n    sample_size = min(100000, g_data.num_nodes)  # Increase training size if possible\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[:sample_size] = True  \r\n    test_mask = ~train_mask  \r\n\r\n    print(f\"Training on {sample_size} nodes...\")\r\n\r\n    # Define model, optimizer, and loss function\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=256, output_dim=1, dropout=0.3).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\r\n    scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\r\n    criterion = nn.SmoothL1Loss(beta=1.0)  # Huber Loss\r\n\r\n    best_r2 = -float(\"inf\")\r\n    early_stop_count = 0\r\n    patience = 50  # Stop if no improvement after 50 epochs\r\n\r\n    print(\"Starting training for 300 epochs...\")\r\n\r\n    for epoch in range(1, 301):  \r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch == 5:\r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\"✅ Epoch 5: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f} - Training is running!\")\r\n\r\n        \r\n\r\n        if epoch % 10 == 0:  \r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\" Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n            # Early stopping check\r\n            if r2 > best_r2:\r\n                best_r2 = r2\r\n                early_stop_count = 0\r\n            else:\r\n                early_stop_count += 1\r\n                if early_stop_count >= patience:\r\n                    print(f\"Early stopping at epoch {epoch}. Best R²={best_r2:.4f}\")\r\n                    break\r\n\r\n    print(\"Training complete!\")\r\n    print(f\"Final R² score after 300 epochs: {best_r2:.4f}\")\r\n\r\n    if best_r2 > 0.4:  # If R² is good, recommend training longer\r\n        print(\"🚀 R² improved significantly! You can proceed with longer training.\")\r\n    else:\r\n        print(\"⚠️ R² is still low. Consider tuning hyperparameters or increasing training data.\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 434795\nNumber of Edges: 2173386\nNode Feature Shape: torch.Size([434795, 11])\nEdge Index Shape: torch.Size([2, 2173386])\nTarget Variable Shape: torch.Size([434795])\nTraining on 100000 nodes...\nStarting training for 300 epochs...\n",
  "history_begin_time" : 1740787144742,
  "history_end_time" : 1740787406216,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "owxMCgmIu62k",
  "history_input" : "import numpy as np\r\nimport torch\r\nimport warnings\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv, BatchNorm\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\n\r\n# Load and verify graph\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data = torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\"Number of Edges: {g_data.num_edges}\")\r\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\"Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n    except Exception as e:\r\n        print(f\"Error loading graph: {e}\")\r\n        exit()\r\n\r\n# Define GraphSAGE model\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n        self.bn4 = BatchNorm(hidden_dim)\r\n\r\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\r\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\r\n\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\r\n\r\n        x = F.leaky_relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n\r\n    loss.backward()\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n# Main function\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n    # Normalize node features\r\n    scaler_x = StandardScaler()\r\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\r\n\r\n    # Normalize target variable\r\n    y_scaler = StandardScaler()\r\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\r\n\r\n    # Define training and testing masks\r\n    sample_size = min(100000, g_data.num_nodes)  # Increase training size if possible\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[:sample_size] = True  \r\n    test_mask = ~train_mask  \r\n\r\n    print(f\"Training on {sample_size} nodes...\")\r\n\r\n    # Define model, optimizer, and loss function\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=256, output_dim=1, dropout=0.3).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\r\n    scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\r\n    criterion = nn.SmoothL1Loss(beta=1.0)  # Huber Loss\r\n\r\n    best_r2 = -float(\"inf\")\r\n    early_stop_count = 0\r\n    patience = 50  # Stop if no improvement after 50 epochs\r\n\r\n    print(\"Starting training for 300 epochs...\")\r\n\r\n    for epoch in range(1, 301):  \r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch == 5:\r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\"✅ Epoch 5: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f} - Training is running!\")\r\n\r\n        \r\n\r\n        if epoch % 10 == 0:  \r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\" Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n            # Early stopping check\r\n            if r2 > best_r2:\r\n                best_r2 = r2\r\n                early_stop_count = 0\r\n            else:\r\n                early_stop_count += 1\r\n                if early_stop_count >= patience:\r\n                    print(f\"Early stopping at epoch {epoch}. Best R²={best_r2:.4f}\")\r\n                    break\r\n\r\n    print(\"Training complete!\")\r\n    print(f\"Final R² score after 300 epochs: {best_r2:.4f}\")\r\n\r\n    if best_r2 > 0.4:  # If R² is good, recommend training longer\r\n        print(\"🚀 R² improved significantly! You can proceed with longer training.\")\r\n    else:\r\n        print(\"⚠️ R² is still low. Consider tuning hyperparameters or increasing training data.\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 434795\nNumber of Edges: 2173386\nNode Feature Shape: torch.Size([434795, 11])\nEdge Index Shape: torch.Size([2, 2173386])\nTarget Variable Shape: torch.Size([434795])\nTraining on 100000 nodes...\nStarting training for 300 epochs...\n✅ Epoch 5: Train Loss=0.3152 | R²=0.0020 | RMSE=1.0190 | MAE=0.8276 - Training is running!\n Epoch 10: Train Loss=0.2959 | R²=0.0173 | RMSE=1.0111 | MAE=0.8016\n",
  "history_begin_time" : 1740787138053,
  "history_end_time" : 1740788147535,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "9gZQLUTkB5mN",
  "history_input" : "import numpy as np\r\nimport torch\r\nimport warnings\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv, BatchNorm\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\n\r\n# Load and verify graph\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data = torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\"Number of Edges: {g_data.num_edges}\")\r\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\"Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n    except Exception as e:\r\n        print(f\"Error loading graph: {e}\")\r\n        exit()\r\n\r\n# Define GraphSAGE model\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n        self.bn4 = BatchNorm(hidden_dim)\r\n\r\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\r\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\r\n\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\r\n\r\n        x = F.leaky_relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n\r\n    loss.backward()\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n# Main function\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n    # Normalize node features\r\n    scaler_x = StandardScaler()\r\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\r\n\r\n    # Normalize target variable\r\n    y_scaler = StandardScaler()\r\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\r\n\r\n    # Define training and testing masks\r\n    sample_size = min(100000, g_data.num_nodes)  # Increase training size if possible\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[:sample_size] = True  \r\n    test_mask = ~train_mask  \r\n\r\n    print(f\"Training on {sample_size} nodes...\")\r\n\r\n    # Define model, optimizer, and loss function\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=256, output_dim=1, dropout=0.3).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\r\n    scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\r\n    criterion = nn.SmoothL1Loss(beta=1.0)  # Huber Loss\r\n\r\n    best_r2 = -float(\"inf\")\r\n    early_stop_count = 0\r\n    patience = 50  # Stop if no improvement after 50 epochs\r\n\r\n    print(\"Starting training for 300 epochs...\")\r\n\r\n    for epoch in range(1, 301):  \r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch == 5:\r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\"✅ Epoch 5: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f} - Training is running!\")\r\n\r\n        \r\n\r\n        if epoch % 10 == 0:  \r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\" Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n            # Early stopping check\r\n            if r2 > best_r2:\r\n                best_r2 = r2\r\n                early_stop_count = 0\r\n            else:\r\n                early_stop_count += 1\r\n                if early_stop_count >= patience:\r\n                    print(f\"Early stopping at epoch {epoch}. Best R²={best_r2:.4f}\")\r\n                    break\r\n\r\n    print(\"Training complete!\")\r\n    print(f\"Final R² score after 300 epochs: {best_r2:.4f}\")\r\n\r\n    if best_r2 > 0.4:  # If R² is good, recommend training longer\r\n        print(\"🚀 R² improved significantly! You can proceed with longer training.\")\r\n    else:\r\n        print(\"⚠️ R² is still low. Consider tuning hyperparameters or increasing training data.\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 434795\nNumber of Edges: 2173386\nNode Feature Shape: torch.Size([434795, 11])\nEdge Index Shape: torch.Size([2, 2173386])\nTarget Variable Shape: torch.Size([434795])\nTraining on 100000 nodes...\nStarting training for 300 epochs...\n✅ Epoch 5: Train Loss=0.3305 | R²=0.0055 | RMSE=1.0172 | MAE=0.8342 - Training is running!\n",
  "history_begin_time" : 1740787137877,
  "history_end_time" : 1740787722167,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "rReIDKD5yIQo",
  "history_input" : "import numpy as np\r\nimport torch\r\nimport warnings\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv, BatchNorm\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\n\r\n# Load and verify graph\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data = torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\"Number of Edges: {g_data.num_edges}\")\r\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\"Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n    except Exception as e:\r\n        print(f\"Error loading graph: {e}\")\r\n        exit()\r\n\r\n# Define GraphSAGE model\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n        self.bn4 = BatchNorm(hidden_dim)\r\n\r\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\r\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\r\n\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\r\n\r\n        x = F.leaky_relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n\r\n    loss.backward()\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n# Main function\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n    # Normalize node features\r\n    scaler_x = StandardScaler()\r\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\r\n\r\n    # Normalize target variable\r\n    y_scaler = StandardScaler()\r\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\r\n\r\n    # Define training and testing masks\r\n    sample_size = min(100000, g_data.num_nodes)  # Increase training size if possible\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[:sample_size] = True  \r\n    test_mask = ~train_mask  \r\n\r\n    print(f\"Training on {sample_size} nodes...\")\r\n\r\n    # Define model, optimizer, and loss function\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=256, output_dim=1, dropout=0.3).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\r\n    scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\r\n    criterion = nn.SmoothL1Loss(beta=1.0)  # Huber Loss\r\n\r\n    best_r2 = -float(\"inf\")\r\n    early_stop_count = 0\r\n    patience = 50  # Stop if no improvement after 50 epochs\r\n\r\n    print(\"Starting training for 300 epochs...\")\r\n\r\n    for epoch in range(1, 301):  \r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch == 5:\r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\"✅ Epoch 5: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f} - Training is running!\")\r\n\r\n        \r\n\r\n        if epoch % 10 == 0:  \r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\" Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n            # Early stopping check\r\n            if r2 > best_r2:\r\n                best_r2 = r2\r\n                early_stop_count = 0\r\n            else:\r\n                early_stop_count += 1\r\n                if early_stop_count >= patience:\r\n                    print(f\"Early stopping at epoch {epoch}. Best R²={best_r2:.4f}\")\r\n                    break\r\n\r\n    print(\"Training complete!\")\r\n    print(f\"Final R² score after 300 epochs: {best_r2:.4f}\")\r\n\r\n    if best_r2 > 0.4:  # If R² is good, recommend training longer\r\n        print(\"🚀 R² improved significantly! You can proceed with longer training.\")\r\n    else:\r\n        print(\"⚠️ R² is still low. Consider tuning hyperparameters or increasing training data.\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 434795\nNumber of Edges: 2173386\nNode Feature Shape: torch.Size([434795, 11])\nEdge Index Shape: torch.Size([2, 2173386])\nTarget Variable Shape: torch.Size([434795])\nTraining on 100000 nodes...\nStarting training for 300 epochs...\n",
  "history_begin_time" : 1740787137452,
  "history_end_time" : 1740787406267,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "RhRYOgRaTCbS",
  "history_input" : "import numpy as np\r\nimport torch\r\nimport warnings\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv, BatchNorm\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\n\r\n# Load and verify graph\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data = torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\"Number of Edges: {g_data.num_edges}\")\r\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\"Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n    except Exception as e:\r\n        print(f\"Error loading graph: {e}\")\r\n        exit()\r\n\r\n# Define GraphSAGE model\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n        self.bn4 = BatchNorm(hidden_dim)\r\n\r\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\r\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\r\n\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\r\n\r\n        x = F.leaky_relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n\r\n    loss.backward()\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n# Main function\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n    # Normalize node features\r\n    scaler_x = StandardScaler()\r\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\r\n\r\n    # Normalize target variable\r\n    y_scaler = StandardScaler()\r\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\r\n\r\n    # Define training and testing masks\r\n    sample_size = min(100000, g_data.num_nodes)  # Increase training size if possible\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[:sample_size] = True  \r\n    test_mask = ~train_mask  \r\n\r\n    print(f\"Training on {sample_size} nodes...\")\r\n\r\n    # Define model, optimizer, and loss function\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=256, output_dim=1, dropout=0.3).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\r\n    scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\r\n    criterion = nn.SmoothL1Loss(beta=1.0)  # Huber Loss\r\n\r\n    best_r2 = -float(\"inf\")\r\n    early_stop_count = 0\r\n    patience = 50  # Stop if no improvement after 50 epochs\r\n\r\n    print(\"Starting training for 300 epochs...\")\r\n\r\n    for epoch in range(1, 301):  \r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch == 5:\r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\"✅ Epoch 5: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f} - Training is running!\")\r\n\r\n        \r\n\r\n        if epoch % 10 == 0:  \r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\" Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n            # Early stopping check\r\n            if r2 > best_r2:\r\n                best_r2 = r2\r\n                early_stop_count = 0\r\n            else:\r\n                early_stop_count += 1\r\n                if early_stop_count >= patience:\r\n                    print(f\"Early stopping at epoch {epoch}. Best R²={best_r2:.4f}\")\r\n                    break\r\n\r\n    print(\"Training complete!\")\r\n    print(f\"Final R² score after 300 epochs: {best_r2:.4f}\")\r\n\r\n    if best_r2 > 0.4:  # If R² is good, recommend training longer\r\n        print(\"🚀 R² improved significantly! You can proceed with longer training.\")\r\n    else:\r\n        print(\"⚠️ R² is still low. Consider tuning hyperparameters or increasing training data.\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 434795\nNumber of Edges: 2173386\nNode Feature Shape: torch.Size([434795, 11])\nEdge Index Shape: torch.Size([2, 2173386])\nTarget Variable Shape: torch.Size([434795])\nTraining on 100000 nodes...\nStarting training for 300 epochs...\n✅ Epoch 5: Train Loss=0.3188 | R²=0.0061 | RMSE=1.0169 | MAE=0.8374 - Training is running!\n Epoch 10: Train Loss=0.3000 | R²=0.0224 | RMSE=1.0085 | MAE=0.8115\n Epoch 20: Train Loss=0.2645 | R²=0.1170 | RMSE=0.9585 | MAE=0.7345\n Epoch 30: Train Loss=0.2270 | R²=0.2452 | RMSE=0.8861 | MAE=0.6298\n Epoch 40: Train Loss=0.2035 | R²=0.2764 | RMSE=0.8676 | MAE=0.6092\n Epoch 50: Train Loss=0.1784 | R²=0.2376 | RMSE=0.8906 | MAE=0.6122\n Epoch 60: Train Loss=0.1603 | R²=0.0823 | RMSE=0.9771 | MAE=0.6169\n Epoch 70: Train Loss=0.1499 | R²=-0.0848 | RMSE=1.0624 | MAE=0.6350\n Epoch 80: Train Loss=0.1421 | R²=-0.1121 | RMSE=1.0757 | MAE=0.6323\n Epoch 90: Train Loss=0.1362 | R²=-0.0821 | RMSE=1.0610 | MAE=0.6126\n Epoch 100: Train Loss=0.1318 | R²=-0.0606 | RMSE=1.0504 | MAE=0.6006\n Epoch 110: Train Loss=0.1279 | R²=-0.0557 | RMSE=1.0480 | MAE=0.5949\n Epoch 120: Train Loss=0.1243 | R²=-0.0448 | RMSE=1.0426 | MAE=0.5882\n Epoch 130: Train Loss=0.1216 | R²=-0.0332 | RMSE=1.0368 | MAE=0.5822\n Epoch 140: Train Loss=0.1192 | R²=-0.0255 | RMSE=1.0329 | MAE=0.5769\n Epoch 150: Train Loss=0.1170 | R²=-0.0177 | RMSE=1.0290 | MAE=0.5728\n Epoch 160: Train Loss=0.1146 | R²=-0.0085 | RMSE=1.0243 | MAE=0.5676\n Epoch 170: Train Loss=0.1130 | R²=0.0030 | RMSE=1.0184 | MAE=0.5620\n Epoch 180: Train Loss=0.1109 | R²=0.0104 | RMSE=1.0146 | MAE=0.5578\n Epoch 190: Train Loss=0.1092 | R²=0.0190 | RMSE=1.0103 | MAE=0.5536\n Epoch 200: Train Loss=0.1088 | R²=0.0234 | RMSE=1.0080 | MAE=0.5507\n Epoch 210: Train Loss=0.1069 | R²=0.0305 | RMSE=1.0043 | MAE=0.5469\n Epoch 220: Train Loss=0.1056 | R²=0.0383 | RMSE=1.0003 | MAE=0.5428\n Epoch 230: Train Loss=0.1045 | R²=0.0395 | RMSE=0.9996 | MAE=0.5408\n Epoch 240: Train Loss=0.1037 | R²=0.0489 | RMSE=0.9948 | MAE=0.5360\n Epoch 250: Train Loss=0.1030 | R²=0.0563 | RMSE=0.9909 | MAE=0.5329\n Epoch 260: Train Loss=0.1018 | R²=0.0579 | RMSE=0.9900 | MAE=0.5310\n Epoch 270: Train Loss=0.1011 | R²=0.0631 | RMSE=0.9873 | MAE=0.5285\n Epoch 280: Train Loss=0.1007 | R²=0.0627 | RMSE=0.9875 | MAE=0.5277\n Epoch 290: Train Loss=0.0995 | R²=0.0659 | RMSE=0.9858 | MAE=0.5242\n Epoch 300: Train Loss=0.0994 | R²=0.0676 | RMSE=0.9849 | MAE=0.5234\nTraining complete!\nFinal R² score after 300 epochs: 0.2764\n⚠️ R² is still low. Consider tuning hyperparameters or increasing training data.\n",
  "history_begin_time" : 1740787103309,
  "history_end_time" : 1740801758363,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "zd5WdixeQAUv",
  "history_input" : "import numpy as np\r\nimport torch\r\nimport warnings\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv, BatchNorm\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\n\r\n# Load and verify graph\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data = torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\"Number of Edges: {g_data.num_edges}\")\r\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\"Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n    except Exception as e:\r\n        print(f\"Error loading graph: {e}\")\r\n        exit()\r\n\r\n# Define GraphSAGE model\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n        self.bn4 = BatchNorm(hidden_dim)\r\n\r\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\r\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\r\n\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\r\n\r\n        x = F.leaky_relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n\r\n    loss.backward()\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n# Main function\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n    # Normalize node features\r\n    scaler_x = StandardScaler()\r\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\r\n\r\n    # Normalize target variable\r\n    y_scaler = StandardScaler()\r\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\r\n\r\n    # Define training and testing masks\r\n    sample_size = min(100000, g_data.num_nodes)  # Increase training size if possible\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[:sample_size] = True  \r\n    test_mask = ~train_mask  \r\n\r\n    print(f\"Training on {sample_size} nodes...\")\r\n\r\n    # Define model, optimizer, and loss function\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=256, output_dim=1, dropout=0.3).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\r\n    scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\r\n    criterion = nn.SmoothL1Loss(beta=1.0)  # Huber Loss\r\n\r\n    best_r2 = -float(\"inf\")\r\n    early_stop_count = 0\r\n    patience = 50  # Stop if no improvement after 50 epochs\r\n\r\n    print(\"Starting training for 300 epochs...\")\r\n\r\n    for epoch in range(1, 301):  \r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch == 5:\r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\"✅ Epoch 5: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f} - Training is running!\")\r\n\r\n        \r\n\r\n        if epoch % 10 == 0:  \r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\" Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n            # Early stopping check\r\n            if r2 > best_r2:\r\n                best_r2 = r2\r\n                early_stop_count = 0\r\n            else:\r\n                early_stop_count += 1\r\n                if early_stop_count >= patience:\r\n                    print(f\"Early stopping at epoch {epoch}. Best R²={best_r2:.4f}\")\r\n                    break\r\n\r\n    print(\"Training complete!\")\r\n    print(f\"Final R² score after 300 epochs: {best_r2:.4f}\")\r\n\r\n    if best_r2 > 0.4:  # If R² is good, recommend training longer\r\n        print(\"🚀 R² improved significantly! You can proceed with longer training.\")\r\n    else:\r\n        print(\"⚠️ R² is still low. Consider tuning hyperparameters or increasing training data.\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 434795\nNumber of Edges: 2173386\nNode Feature Shape: torch.Size([434795, 11])\nEdge Index Shape: torch.Size([2, 2173386])\nTarget Variable Shape: torch.Size([434795])\nTraining on 100000 nodes...\nStarting training for 300 epochs...\n✅ Epoch 5: Train Loss=0.3376 | R²=0.0030 | RMSE=1.0184 | MAE=0.8486 - Training is running!\n Epoch 10: Train Loss=0.3030 | R²=0.0214 | RMSE=1.0090 | MAE=0.8287\n Epoch 20: Train Loss=0.2718 | R²=0.1111 | RMSE=0.9616 | MAE=0.7478\n Epoch 30: Train Loss=0.2339 | R²=0.2403 | RMSE=0.8890 | MAE=0.6420\n Epoch 40: Train Loss=0.2068 | R²=0.2869 | RMSE=0.8613 | MAE=0.5934\n Epoch 50: Train Loss=0.1847 | R²=0.2894 | RMSE=0.8598 | MAE=0.5916\n Epoch 60: Train Loss=0.1631 | R²=0.2038 | RMSE=0.9102 | MAE=0.6005\n Epoch 70: Train Loss=0.1492 | R²=0.0434 | RMSE=0.9976 | MAE=0.6070\n Epoch 80: Train Loss=0.1416 | R²=-0.0470 | RMSE=1.0437 | MAE=0.6125\n Epoch 90: Train Loss=0.1356 | R²=-0.0526 | RMSE=1.0465 | MAE=0.6059\n Epoch 100: Train Loss=0.1305 | R²=-0.0469 | RMSE=1.0437 | MAE=0.5967\n Epoch 110: Train Loss=0.1272 | R²=-0.0420 | RMSE=1.0412 | MAE=0.5900\n Epoch 120: Train Loss=0.1240 | R²=-0.0413 | RMSE=1.0409 | MAE=0.5876\n Epoch 130: Train Loss=0.1201 | R²=-0.0357 | RMSE=1.0380 | MAE=0.5839\n Epoch 140: Train Loss=0.1175 | R²=-0.0304 | RMSE=1.0354 | MAE=0.5812\n Epoch 150: Train Loss=0.1142 | R²=-0.0220 | RMSE=1.0311 | MAE=0.5769\n Epoch 160: Train Loss=0.1126 | R²=-0.0156 | RMSE=1.0279 | MAE=0.5738\n Epoch 170: Train Loss=0.1105 | R²=-0.0059 | RMSE=1.0230 | MAE=0.5693\n Epoch 180: Train Loss=0.1096 | R²=0.0031 | RMSE=1.0184 | MAE=0.5651\n Epoch 190: Train Loss=0.1080 | R²=0.0120 | RMSE=1.0138 | MAE=0.5605\n Epoch 200: Train Loss=0.1062 | R²=0.0189 | RMSE=1.0103 | MAE=0.5575\n Epoch 210: Train Loss=0.1050 | R²=0.0279 | RMSE=1.0057 | MAE=0.5530\n Epoch 220: Train Loss=0.1043 | R²=0.0375 | RMSE=1.0007 | MAE=0.5485\n Epoch 230: Train Loss=0.1031 | R²=0.0450 | RMSE=0.9968 | MAE=0.5447\n Epoch 240: Train Loss=0.1018 | R²=0.0504 | RMSE=0.9940 | MAE=0.5417\n Epoch 250: Train Loss=0.1018 | R²=0.0541 | RMSE=0.9920 | MAE=0.5398\n Epoch 260: Train Loss=0.1002 | R²=0.0577 | RMSE=0.9901 | MAE=0.5372\n Epoch 270: Train Loss=0.0999 | R²=0.0635 | RMSE=0.9871 | MAE=0.5340\n Epoch 280: Train Loss=0.0989 | R²=0.0675 | RMSE=0.9850 | MAE=0.5316\n Epoch 290: Train Loss=0.0978 | R²=0.0728 | RMSE=0.9822 | MAE=0.5291\n Epoch 300: Train Loss=0.0982 | R²=0.0750 | RMSE=0.9810 | MAE=0.5271\nTraining complete!\nFinal R² score after 300 epochs: 0.2894\n⚠️ R² is still low. Consider tuning hyperparameters or increasing training data.\n",
  "history_begin_time" : 1740787089278,
  "history_end_time" : 1740801710021,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "Vntv63j1fvAZ",
  "history_input" : "import numpy as np\r\nimport torch\r\nimport warnings\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv, BatchNorm\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\n\r\n# Load and verify graph\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data = torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\"Number of Edges: {g_data.num_edges}\")\r\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\"Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n    except Exception as e:\r\n        print(f\"Error loading graph: {e}\")\r\n        exit()\r\n\r\n# Define GraphSAGE model\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n        self.bn4 = BatchNorm(hidden_dim)\r\n\r\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\r\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\r\n\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\r\n\r\n        x = F.leaky_relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n\r\n    loss.backward()\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n# Main function\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n    # Normalize node features\r\n    scaler_x = StandardScaler()\r\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\r\n\r\n    # Normalize target variable\r\n    y_scaler = StandardScaler()\r\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\r\n\r\n    # Define training and testing masks\r\n    sample_size = min(100000, g_data.num_nodes)  # Increase training size if possible\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[:sample_size] = True  \r\n    test_mask = ~train_mask  \r\n\r\n    print(f\"Training on {sample_size} nodes...\")\r\n\r\n    # Define model, optimizer, and loss function\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=256, output_dim=1, dropout=0.3).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\r\n    scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\r\n    criterion = nn.SmoothL1Loss(beta=1.0)  # Huber Loss\r\n\r\n    best_r2 = -float(\"inf\")\r\n    early_stop_count = 0\r\n    patience = 50  # Stop if no improvement after 50 epochs\r\n\r\n    print(\"Starting training for 300 epochs...\")\r\n\r\n    for epoch in range(1, 301):  \r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch == 5:\r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\"✅ Epoch 5: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f} - Training is running!\")\r\n\r\n        \r\n\r\n        if epoch % 10 == 0:  \r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\" Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n            # Early stopping check\r\n            if r2 > best_r2:\r\n                best_r2 = r2\r\n                early_stop_count = 0\r\n            else:\r\n                early_stop_count += 1\r\n                if early_stop_count >= patience:\r\n                    print(f\"Early stopping at epoch {epoch}. Best R²={best_r2:.4f}\")\r\n                    break\r\n\r\n    print(\"Training complete!\")\r\n    print(f\"Final R² score after 300 epochs: {best_r2:.4f}\")\r\n\r\n    if best_r2 > 0.4:  # If R² is good, recommend training longer\r\n        print(\"🚀 R² improved significantly! You can proceed with longer training.\")\r\n    else:\r\n        print(\"⚠️ R² is still low. Consider tuning hyperparameters or increasing training data.\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 434795\nNumber of Edges: 2173386\nNode Feature Shape: torch.Size([434795, 11])\nEdge Index Shape: torch.Size([2, 2173386])\nTarget Variable Shape: torch.Size([434795])\nTraining on 100000 nodes...\nStarting training for 300 epochs...\n✅ Epoch 5: Train Loss=0.3203 | R²=0.0045 | RMSE=1.0177 | MAE=0.8484 - Training is running!\n Epoch 10: Train Loss=0.3017 | R²=0.0185 | RMSE=1.0105 | MAE=0.8306\n Epoch 20: Train Loss=0.2738 | R²=0.1001 | RMSE=0.9676 | MAE=0.7657\n Epoch 30: Train Loss=0.2370 | R²=0.2320 | RMSE=0.8939 | MAE=0.6622\n Epoch 40: Train Loss=0.2066 | R²=0.2814 | RMSE=0.8646 | MAE=0.6186\n Epoch 50: Train Loss=0.1826 | R²=0.2672 | RMSE=0.8732 | MAE=0.6178\n Epoch 60: Train Loss=0.1608 | R²=0.1668 | RMSE=0.9310 | MAE=0.6260\n Epoch 70: Train Loss=0.1471 | R²=-0.0118 | RMSE=1.0260 | MAE=0.6343\n Epoch 80: Train Loss=0.1390 | R²=-0.1112 | RMSE=1.0752 | MAE=0.6391\n Epoch 90: Train Loss=0.1340 | R²=-0.1053 | RMSE=1.0723 | MAE=0.6252\n Epoch 100: Train Loss=0.1295 | R²=-0.0872 | RMSE=1.0635 | MAE=0.6089\n Epoch 110: Train Loss=0.1252 | R²=-0.0766 | RMSE=1.0583 | MAE=0.6004\n Epoch 120: Train Loss=0.1225 | R²=-0.0746 | RMSE=1.0573 | MAE=0.5960\n Epoch 130: Train Loss=0.1200 | R²=-0.0688 | RMSE=1.0545 | MAE=0.5911\n Epoch 140: Train Loss=0.1173 | R²=-0.0644 | RMSE=1.0523 | MAE=0.5872\n Epoch 150: Train Loss=0.1153 | R²=-0.0569 | RMSE=1.0486 | MAE=0.5827\n Epoch 160: Train Loss=0.1136 | R²=-0.0478 | RMSE=1.0441 | MAE=0.5782\n Epoch 170: Train Loss=0.1126 | R²=-0.0401 | RMSE=1.0402 | MAE=0.5745\n Epoch 180: Train Loss=0.1104 | R²=-0.0279 | RMSE=1.0341 | MAE=0.5689\n Epoch 190: Train Loss=0.1087 | R²=-0.0194 | RMSE=1.0298 | MAE=0.5652\n Epoch 200: Train Loss=0.1068 | R²=-0.0114 | RMSE=1.0258 | MAE=0.5616\n Epoch 210: Train Loss=0.1063 | R²=-0.0021 | RMSE=1.0211 | MAE=0.5581\n Epoch 220: Train Loss=0.1050 | R²=0.0077 | RMSE=1.0161 | MAE=0.5536\n Epoch 230: Train Loss=0.1040 | R²=0.0146 | RMSE=1.0125 | MAE=0.5504\n Epoch 240: Train Loss=0.1024 | R²=0.0203 | RMSE=1.0096 | MAE=0.5484\n Epoch 250: Train Loss=0.1019 | R²=0.0259 | RMSE=1.0067 | MAE=0.5450\n Epoch 260: Train Loss=0.1018 | R²=0.0314 | RMSE=1.0039 | MAE=0.5421\n Epoch 270: Train Loss=0.1002 | R²=0.0369 | RMSE=1.0010 | MAE=0.5391\n Epoch 280: Train Loss=0.0997 | R²=0.0428 | RMSE=0.9979 | MAE=0.5353\n Epoch 290: Train Loss=0.0993 | R²=0.0481 | RMSE=0.9952 | MAE=0.5335\n Epoch 300: Train Loss=0.0978 | R²=0.0487 | RMSE=0.9948 | MAE=0.5319\nTraining complete!\nFinal R² score after 300 epochs: 0.2814\n⚠️ R² is still low. Consider tuning hyperparameters or increasing training data.\n",
  "history_begin_time" : 1740787088780,
  "history_end_time" : 1740801724580,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "A9gZYGWwzDuP",
  "history_input" : "import numpy as np\r\nimport torch\r\nimport warnings\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv, BatchNorm\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\n\r\n# Load and verify graph\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data = torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\"Number of Edges: {g_data.num_edges}\")\r\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\"Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n    except Exception as e:\r\n        print(f\"Error loading graph: {e}\")\r\n        exit()\r\n\r\n# Define GraphSAGE model\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n        self.bn4 = BatchNorm(hidden_dim)\r\n\r\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\r\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\r\n\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\r\n\r\n        x = F.leaky_relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n\r\n    loss.backward()\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n# Main function\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n    # Normalize node features\r\n    scaler_x = StandardScaler()\r\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\r\n\r\n    # Normalize target variable\r\n    y_scaler = StandardScaler()\r\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\r\n\r\n    # Define training and testing masks\r\n    sample_size = min(100000, g_data.num_nodes)  # Increase training size if possible\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[:sample_size] = True  \r\n    test_mask = ~train_mask  \r\n\r\n    print(f\"Training on {sample_size} nodes...\")\r\n\r\n    # Define model, optimizer, and loss function\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=256, output_dim=1, dropout=0.3).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\r\n    scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\r\n    criterion = nn.SmoothL1Loss(beta=1.0)  # Huber Loss\r\n\r\n    best_r2 = -float(\"inf\")\r\n    early_stop_count = 0\r\n    patience = 50  # Stop if no improvement after 50 epochs\r\n\r\n    print(\"Starting training for 300 epochs...\")\r\n\r\n    for epoch in range(1, 301):  \r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch == 5:\r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\"✅ Epoch 5: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f} - Training is running!\")\r\n\r\n        \r\n\r\n        if epoch % 10 == 0:  \r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\" Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n            # Early stopping check\r\n            if r2 > best_r2:\r\n                best_r2 = r2\r\n                early_stop_count = 0\r\n            else:\r\n                early_stop_count += 1\r\n                if early_stop_count >= patience:\r\n                    print(f\"Early stopping at epoch {epoch}. Best R²={best_r2:.4f}\")\r\n                    break\r\n\r\n    print(\"Training complete!\")\r\n    print(f\"Final R² score after 300 epochs: {best_r2:.4f}\")\r\n\r\n    if best_r2 > 0.4:  # If R² is good, recommend training longer\r\n        print(\"🚀 R² improved significantly! You can proceed with longer training.\")\r\n    else:\r\n        print(\"⚠️ R² is still low. Consider tuning hyperparameters or increasing training data.\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 434795\nNumber of Edges: 2173386\nNode Feature Shape: torch.Size([434795, 11])\nEdge Index Shape: torch.Size([2, 2173386])\nTarget Variable Shape: torch.Size([434795])\nTraining on 100000 nodes...\nStarting training for 300 epochs...\n",
  "history_begin_time" : 1740787079615,
  "history_end_time" : 1740787318841,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "dyo1AKDNjCcl",
  "history_input" : "import numpy as np\r\nimport torch\r\nimport warnings\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv, BatchNorm\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\n\r\n# Load and verify graph\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data = torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\"Number of Edges: {g_data.num_edges}\")\r\n        print(f\"Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\"Edge Index Shape: {g_data.edge_index.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\"Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\"Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n    except Exception as e:\r\n        print(f\"Error loading graph: {e}\")\r\n        exit()\r\n\r\n# Define GraphSAGE model\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n        self.bn4 = BatchNorm(hidden_dim)\r\n\r\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\r\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\r\n\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = F.leaky_relu(self.bn1(self.conv1(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn2(self.conv2(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn3(self.conv3(x, edge_index)))\r\n        x = self.dropout(x)\r\n\r\n        x = F.leaky_relu(self.bn4(self.conv4(x, edge_index)))\r\n\r\n        x = F.leaky_relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n\r\n    loss.backward()\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevent exploding gradients\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n# Main function\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n    # Normalize node features\r\n    scaler_x = StandardScaler()\r\n    g_data.x = torch.tensor(scaler_x.fit_transform(g_data.x.cpu()), dtype=torch.float).to(device)\r\n\r\n    # Normalize target variable\r\n    y_scaler = StandardScaler()\r\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\r\n\r\n    # Define training and testing masks\r\n    sample_size = min(100000, g_data.num_nodes)  # Increase training size if possible\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[:sample_size] = True  \r\n    test_mask = ~train_mask  \r\n\r\n    print(f\"Training on {sample_size} nodes...\")\r\n\r\n    # Define model, optimizer, and loss function\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=256, output_dim=1, dropout=0.3).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\r\n    scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\r\n    criterion = nn.SmoothL1Loss(beta=1.0)  # Huber Loss\r\n\r\n    best_r2 = -float(\"inf\")\r\n    early_stop_count = 0\r\n    patience = 50  # Stop if no improvement after 50 epochs\r\n\r\n    print(\"Starting training for 300 epochs...\")\r\n\r\n    for epoch in range(1, 301):  \r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch == 5:\r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\"✅ Epoch 5: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f} - Training is running!\")\r\n\r\n        \r\n\r\n        if epoch % 50 == 0:  \r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\" Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n            # Early stopping check\r\n            if r2 > best_r2:\r\n                best_r2 = r2\r\n                early_stop_count = 0\r\n            else:\r\n                early_stop_count += 1\r\n                if early_stop_count >= patience:\r\n                    print(f\"Early stopping at epoch {epoch}. Best R²={best_r2:.4f}\")\r\n                    break\r\n\r\n    print(\"Training complete!\")\r\n    print(f\"Final R² score after 300 epochs: {best_r2:.4f}\")\r\n\r\n    if best_r2 > 0.4:  # If R² is good, recommend training longer\r\n        print(\"🚀 R² improved significantly! You can proceed with longer training.\")\r\n    else:\r\n        print(\"⚠️ R² is still low. Consider tuning hyperparameters or increasing training data.\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 434795\nNumber of Edges: 2173386\nNode Feature Shape: torch.Size([434795, 11])\nEdge Index Shape: torch.Size([2, 2173386])\nTarget Variable Shape: torch.Size([434795])\nTraining on 100000 nodes...\nStarting training for 300 epochs...\n✅ Epoch 5: Train Loss=0.3193 | R²=0.0050 | RMSE=1.0175 | MAE=0.8253 - Training is running!\n",
  "history_begin_time" : 1740786460160,
  "history_end_time" : 1740787720847,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "cM0pHGTJPiY0",
  "history_input" : "#GNN model training testing\r\nimport numpy as np\r\nimport torch\r\nimport warnings\r\nimport os\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\nfrom torch_geometric.nn import BatchNorm\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\r\n\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data=torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\" Number of Edges: {g_data.num_edges}\")\r\n        print(f\" Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\" Edge Index Shape: {g_data.edge_index.shape}\")\r\n        # print(f\" Target Variable Shape: {g_data.y.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\" Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\" Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n\r\n    except Exception as e:\r\n        print(f\" Error loading graph: {e}\")\r\n        exit()\r\n\r\n\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.4):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.bn4 = BatchNorm(hidden_dim)\r\n        self.fc = nn.Linear(hidden_dim, output_dim)\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = self.conv1(x, edge_index)\r\n        x = self.bn1(x)\r\n        x = F.leaky_relu(x)\r\n        x = self.dropout(x)\r\n\r\n        x = self.conv2(x, edge_index)\r\n        x = self.bn2(x)\r\n        x = F.leaky_relu(x)\r\n        x = self.dropout(x)\r\n\r\n        x = self.conv3(x, edge_index)\r\n        x = self.bn3(x)\r\n        x = F.leaky_relu(x)\r\n\r\n        x = self.conv4(x, edge_index)\r\n        x = self.bn4(x)\r\n        x = F.leaky_relu(x)\r\n\r\n        x = self.fc(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n\r\n    loss.backward()\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Prevent exploding gradients\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n\r\n    # Load and check\r\n    g_data = load_and_verify_graph(g_path)\r\n    y_scaler = StandardScaler()\r\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\r\n\r\n\r\n    # Quick test on 50,000 nodes\r\n    sample_size = 50000\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[:sample_size] = True  # First 50,000 nodes for training\r\n    test_mask = ~train_mask  # Remaining for testing\r\n\r\n    print(f\" Running on small sample: {sample_size} training nodes\")\r\n\r\n    # ✅ GraphSAGE Model (Improved)\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=256, output_dim=1, dropout=0.4).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-5)  # ⬇️ Reduced learning rate\r\n    criterion = nn.MSELoss()\r\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=50, verbose=True)\r\n\r\n\r\n    print(\" Quick Test: Training GraphSAGE on Small Sample...\")\r\n\r\n    # ✅ Training for 300 epochs, logging every 50 epochs\r\n    for epoch in range(1, 301):\r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n        \r\n        if epoch == 5:  # ✅ Print after 5th epoch to confirm it's running\r\n            print(\"✅ Training is running. Reached Epoch 5!\")\r\n\r\n       \r\n\r\n        if epoch % 50 == 0:  # ✅ Log every 50 epochs\r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\" Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n    print(\"✅ Quick Test Complete!\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 434795\n Number of Edges: 2173386\n Node Feature Shape: torch.Size([434795, 11])\n Edge Index Shape: torch.Size([2, 2173386])\n Target Variable Shape: torch.Size([434795])\n Running on small sample: 50000 training nodes\n Quick Test: Training GraphSAGE on Small Sample...\n✅ Training is running. Reached Epoch 5!\n Epoch 50: Train Loss=0.3245 | R²=0.0436 | RMSE=0.9885 | MAE=0.5931\n",
  "history_begin_time" : 1740785110870,
  "history_end_time" : 1740787322094,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "DQdWX4BhxdNE",
  "history_input" : "#GNN model training testing\r\nimport numpy as np\r\nimport torch\r\nimport warnings\r\nimport os\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\nfrom torch_geometric.nn import BatchNorm\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\r\n\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data=torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\" Number of Edges: {g_data.num_edges}\")\r\n        print(f\" Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\" Edge Index Shape: {g_data.edge_index.shape}\")\r\n        # print(f\" Target Variable Shape: {g_data.y.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\" Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\" Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n\r\n    except Exception as e:\r\n        print(f\" Error loading graph: {e}\")\r\n        exit()\r\n\r\n\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.4):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n        self.conv4 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.bn4 = BatchNorm(hidden_dim)\r\n        self.fc = nn.Linear(hidden_dim, output_dim)\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = self.conv1(x, edge_index)\r\n        x = self.bn1(x)\r\n        x = F.leaky_relu(x)\r\n        x = self.dropout(x)\r\n\r\n        x = self.conv2(x, edge_index)\r\n        x = self.bn2(x)\r\n        x = F.leaky_relu(x)\r\n        x = self.dropout(x)\r\n\r\n        x = self.conv3(x, edge_index)\r\n        x = self.bn3(x)\r\n        x = F.leaky_relu(x)\r\n\r\n        x = self.conv4(x, edge_index)\r\n        x = self.bn4(x)\r\n        x = F.leaky_relu(x)\r\n\r\n        x = self.fc(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n\r\n    loss.backward()\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Prevent exploding gradients\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n\r\n    # Load and check\r\n    g_data = load_and_verify_graph(g_path)\r\n    y_scaler = StandardScaler()\r\n    g_data.y = torch.tensor(y_scaler.fit_transform(g_data.y.view(-1, 1)), dtype=torch.float).view(-1).to(device)\r\n\r\n\r\n    # Quick test on 50,000 nodes\r\n    sample_size = 50000\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[:sample_size] = True  # First 50,000 nodes for training\r\n    test_mask = ~train_mask  # Remaining for testing\r\n\r\n    print(f\" Running on small sample: {sample_size} training nodes\")\r\n\r\n    # ✅ GraphSAGE Model (Improved)\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=256, output_dim=1, dropout=0.4).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-5)  # ⬇️ Reduced learning rate\r\n    criterion = nn.MSELoss()\r\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=50, verbose=True)\r\n\r\n\r\n    print(\" Quick Test: Training GraphSAGE on Small Sample...\")\r\n\r\n    # ✅ Training for 300 epochs, logging every 50 epochs\r\n    for epoch in range(1, 301):\r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch % 50 == 0:  # ✅ Log every 50 epochs\r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\" Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n    print(\"✅ Quick Test Complete!\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 434795\n Number of Edges: 2173386\n Node Feature Shape: torch.Size([434795, 11])\n Edge Index Shape: torch.Size([2, 2173386])\n Target Variable Shape: torch.Size([434795])\n Running on small sample: 50000 training nodes\n Quick Test: Training GraphSAGE on Small Sample...\n Epoch 50: Train Loss=0.3260 | R²=0.0446 | RMSE=0.9880 | MAE=0.5890\n Epoch 100: Train Loss=0.2745 | R²=0.1357 | RMSE=0.9398 | MAE=0.5414\n",
  "history_begin_time" : 1740784434760,
  "history_end_time" : 1740787366099,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "LXHQ0RSLrvJ8",
  "history_input" : "#GNN model training testing\r\nimport numpy as np\r\nimport torch\r\nimport warnings\r\nimport os\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\nfrom torch_geometric.nn import BatchNorm\r\n\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data=torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\" Number of Edges: {g_data.num_edges}\")\r\n        print(f\" Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\" Edge Index Shape: {g_data.edge_index.shape}\")\r\n        # print(f\" Target Variable Shape: {g_data.y.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\" Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\" Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n\r\n    except Exception as e:\r\n        print(f\" Error loading graph: {e}\")\r\n        exit()\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.4):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n        self.fc = nn.Linear(hidden_dim, output_dim)\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = self.conv1(x, edge_index)\r\n        x = self.bn1(x)\r\n        x = F.leaky_relu(x)\r\n        x = self.dropout(x)\r\n\r\n        x = self.conv2(x, edge_index)\r\n        x = self.bn2(x)\r\n        x = F.leaky_relu(x)\r\n        x = self.dropout(x)\r\n\r\n        x = self.conv3(x, edge_index)\r\n        x = self.bn3(x)\r\n        x = F.leaky_relu(x)\r\n\r\n        x = self.fc(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n    loss.backward()\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n\r\n    # Load and check\r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n    # Quick test on 50,000 nodes\r\n    sample_size = 50000\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[:sample_size] = True  # First 50,000 nodes for training\r\n    test_mask = ~train_mask  # Remaining for testing\r\n\r\n    print(f\" Running on small sample: {sample_size} training nodes\")\r\n\r\n    # ✅ GraphSAGE Model (Improved)\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=128, output_dim=1, dropout=0.4).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=5e-4)  # ⬇️ Reduced learning rate\r\n    criterion = nn.MSELoss()\r\n\r\n    print(\" Quick Test: Training GraphSAGE on Small Sample...\")\r\n\r\n    # ✅ Training for 300 epochs, logging every 50 epochs\r\n    for epoch in range(1, 301):\r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch % 50 == 0:  # ✅ Log every 50 epochs\r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\" Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n    print(\"✅ Quick Test Complete!\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 434795\n Number of Edges: 2173386\n Node Feature Shape: torch.Size([434795, 11])\n Edge Index Shape: torch.Size([2, 2173386])\n Target Variable Shape: torch.Size([434795])\n Running on small sample: 50000 training nodes\n Quick Test: Training GraphSAGE on Small Sample...\n Epoch 50: Train Loss=166666.6250 | R²=-0.2795 | RMSE=473.5123 | MAE=227.6588\n Epoch 100: Train Loss=164654.0938 | R²=-0.2721 | RMSE=472.1549 | MAE=227.4412\n Epoch 150: Train Loss=162076.1875 | R²=-0.2614 | RMSE=470.1529 | MAE=227.2623\n Epoch 200: Train Loss=158679.9844 | R²=-0.2523 | RMSE=468.4534 | MAE=226.9490\n Epoch 250: Train Loss=154348.3750 | R²=-0.2433 | RMSE=466.7671 | MAE=226.6120\n Epoch 300: Train Loss=148457.0625 | R²=-0.2372 | RMSE=465.6231 | MAE=227.0607\n✅ Quick Test Complete!\n",
  "history_begin_time" : 1740782887217,
  "history_end_time" : 1740784557621,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "sxHwOipbBxV7",
  "history_input" : "#GNN model training testing\r\nimport numpy as np\r\nimport torch\r\nimport warnings\r\nimport os\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\n\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data=torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\" Number of Edges: {g_data.num_edges}\")\r\n        print(f\" Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\" Edge Index Shape: {g_data.edge_index.shape}\")\r\n        # print(f\" Target Variable Shape: {g_data.y.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\" Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\" Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n\r\n    except Exception as e:\r\n        print(f\" Error loading graph: {e}\")\r\n        exit()\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.4):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\r\n        self.bn1 = BatchNorm(hidden_dim)\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.bn2 = BatchNorm(hidden_dim)\r\n        self.conv3 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.bn3 = BatchNorm(hidden_dim)\r\n        self.fc = nn.Linear(hidden_dim, output_dim)\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = self.conv1(x, edge_index)\r\n        x = self.bn1(x)\r\n        x = F.leaky_relu(x)\r\n        x = self.dropout(x)\r\n\r\n        x = self.conv2(x, edge_index)\r\n        x = self.bn2(x)\r\n        x = F.leaky_relu(x)\r\n        x = self.dropout(x)\r\n\r\n        x = self.conv3(x, edge_index)\r\n        x = self.bn3(x)\r\n        x = F.leaky_relu(x)\r\n\r\n        x = self.fc(x)\r\n        return x\r\n\r\n# Training function\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n    loss.backward()\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n# Testing function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n\r\n    # Load and check\r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n    # Quick test on 50,000 nodes\r\n    sample_size = 50000\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[:sample_size] = True  # First 50,000 nodes for training\r\n    test_mask = ~train_mask  # Remaining for testing\r\n\r\n    print(f\" Running on small sample: {sample_size} training nodes\")\r\n\r\n    # ✅ GraphSAGE Model (Improved)\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=128, output_dim=1, dropout=0.4).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=5e-4)  # ⬇️ Reduced learning rate\r\n    criterion = nn.MSELoss()\r\n\r\n    print(\" Quick Test: Training GraphSAGE on Small Sample...\")\r\n\r\n    # ✅ Training for 300 epochs, logging every 50 epochs\r\n    for epoch in range(1, 301):\r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch % 50 == 0:  # ✅ Log every 50 epochs\r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\" Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n    print(\"✅ Quick Test Complete!\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 434795\n Number of Edges: 2173386\n Node Feature Shape: torch.Size([434795, 11])\n Edge Index Shape: torch.Size([2, 2173386])\n Target Variable Shape: torch.Size([434795])\n Running on small sample: 50000 training nodes\nTraceback (most recent call last):\n  File \"/media/volume1/gw-workspace/sxHwOipbBxV7/graph_model.py\", line 127, in <module>\n    main()\n  File \"/media/volume1/gw-workspace/sxHwOipbBxV7/graph_model.py\", line 110, in main\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=128, output_dim=1, dropout=0.4).to(device)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/media/volume1/gw-workspace/sxHwOipbBxV7/graph_model.py\", line 44, in __init__\n    self.bn1 = BatchNorm(hidden_dim)\n               ^^^^^^^^^\nNameError: name 'BatchNorm' is not defined\n",
  "history_begin_time" : 1740782547533,
  "history_end_time" : 1740782552277,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "Fs5VGQcdtUzM",
  "history_input" : "#GNN model training testing\r\nimport numpy as np\r\nimport torch\r\nimport warnings\r\nimport os\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\n\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data=torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\" Number of Edges: {g_data.num_edges}\")\r\n        print(f\" Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\" Edge Index Shape: {g_data.edge_index.shape}\")\r\n        # print(f\" Target Variable Shape: {g_data.y.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\" Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\" Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n\r\n    except Exception as e:\r\n        print(f\" Error loading graph: {e}\")\r\n        exit()\r\n\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.5):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.fc = nn.Linear(hidden_dim, output_dim)\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = self.conv1(x, edge_index)\r\n        x = F.relu(x)\r\n        x = self.dropout(x)\r\n        x = self.conv2(x, edge_index)\r\n        x = F.relu(x)\r\n        x = self.fc(x)\r\n        return x\r\n\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n    loss.backward()\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n\r\n# Step 4: Testing Function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n\r\n#load and check\r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n#quick test\r\n    sample_size = 50000\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[:sample_size] = True  # First 5,000 nodes for training\r\n    test_mask = ~train_mask  # Remaining for testing\r\n\r\n    print(f\" Running on small sample: {sample_size} training nodes\")\r\n\r\n    # Initialize GraphSAGE Model\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=128, output_dim=1, dropout=0.3).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\r\n    criterion = nn.MSELoss()\r\n\r\n    print(\" Quick Test: Training GraphSAGE on Small Sample...\")\r\n\r\n    # Train for just 50 epochs first\r\n    for epoch in range(1, 51):\r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch % 1 == 0:  # Print every epoch (since it's a quick test)\r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\" Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n    print(\"✅ Quick Test Complete!\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 434795\n Number of Edges: 2173386\n Node Feature Shape: torch.Size([434795, 11])\n Edge Index Shape: torch.Size([2, 2173386])\n Target Variable Shape: torch.Size([434795])\n Running on small sample: 50000 training nodes\n Quick Test: Training GraphSAGE on Small Sample...\n Epoch 1: Train Loss=168204.6562 | R²=-0.2888 | RMSE=475.2440 | MAE=227.1841\n Epoch 2: Train Loss=168176.2656 | R²=-0.2886 | RMSE=475.1970 | MAE=227.2282\n Epoch 3: Train Loss=168148.3125 | R²=-0.2883 | RMSE=475.1499 | MAE=227.2757\n Epoch 4: Train Loss=168120.1719 | R²=-0.2881 | RMSE=475.1027 | MAE=227.3234\n Epoch 5: Train Loss=168091.5469 | R²=-0.2878 | RMSE=475.0548 | MAE=227.3717\n Epoch 6: Train Loss=168062.4531 | R²=-0.2875 | RMSE=475.0056 | MAE=227.4209\n Epoch 7: Train Loss=168033.5312 | R²=-0.2873 | RMSE=474.9549 | MAE=227.4715\n Epoch 8: Train Loss=168002.8906 | R²=-0.2870 | RMSE=474.9020 | MAE=227.5236\n Epoch 9: Train Loss=167971.0469 | R²=-0.2867 | RMSE=474.8466 | MAE=227.5776\n Epoch 10: Train Loss=167938.8438 | R²=-0.2864 | RMSE=474.7884 | MAE=227.6338\n Epoch 11: Train Loss=167904.3281 | R²=-0.2860 | RMSE=474.7272 | MAE=227.6924\n Epoch 12: Train Loss=167868.0625 | R²=-0.2857 | RMSE=474.6627 | MAE=227.7535\n Epoch 13: Train Loss=167830.1562 | R²=-0.2853 | RMSE=474.5945 | MAE=227.8174\n Epoch 14: Train Loss=167790.2812 | R²=-0.2849 | RMSE=474.5226 | MAE=227.8842\n Epoch 15: Train Loss=167746.1250 | R²=-0.2845 | RMSE=474.4465 | MAE=227.9539\n Epoch 16: Train Loss=167700.2344 | R²=-0.2841 | RMSE=474.3661 | MAE=228.0268\n Epoch 17: Train Loss=167653.4844 | R²=-0.2836 | RMSE=474.2814 | MAE=228.1031\n Epoch 18: Train Loss=167602.2344 | R²=-0.2831 | RMSE=474.1918 | MAE=228.1828\n Epoch 19: Train Loss=167548.9688 | R²=-0.2826 | RMSE=474.0973 | MAE=228.2659\n Epoch 20: Train Loss=167492.6250 | R²=-0.2821 | RMSE=473.9977 | MAE=228.3527\n Epoch 21: Train Loss=167432.5156 | R²=-0.2815 | RMSE=473.8927 | MAE=228.4432\n Epoch 22: Train Loss=167369.7969 | R²=-0.2809 | RMSE=473.7823 | MAE=228.5376\n Epoch 23: Train Loss=167302.6094 | R²=-0.2803 | RMSE=473.6660 | MAE=228.6359\n Epoch 24: Train Loss=167232.2656 | R²=-0.2796 | RMSE=473.5438 | MAE=228.7382\n Epoch 25: Train Loss=167159.9844 | R²=-0.2789 | RMSE=473.4154 | MAE=228.8446\n Epoch 26: Train Loss=167079.5000 | R²=-0.2782 | RMSE=473.2807 | MAE=228.9552\n Epoch 27: Train Loss=166998.2969 | R²=-0.2774 | RMSE=473.1393 | MAE=229.0701\n Epoch 28: Train Loss=166912.3438 | R²=-0.2766 | RMSE=472.9911 | MAE=229.1895\n Epoch 29: Train Loss=166821.8281 | R²=-0.2758 | RMSE=472.8358 | MAE=229.3136\n Epoch 30: Train Loss=166727.2500 | R²=-0.2749 | RMSE=472.6729 | MAE=229.4422\n Epoch 31: Train Loss=166627.4688 | R²=-0.2740 | RMSE=472.5025 | MAE=229.5756\n Epoch 32: Train Loss=166523.0000 | R²=-0.2730 | RMSE=472.3242 | MAE=229.7140\n Epoch 33: Train Loss=166416.3125 | R²=-0.2720 | RMSE=472.1377 | MAE=229.8574\n Epoch 34: Train Loss=166296.5938 | R²=-0.2710 | RMSE=471.9427 | MAE=230.0059\n Epoch 35: Train Loss=166175.8125 | R²=-0.2699 | RMSE=471.7389 | MAE=230.1597\n Epoch 36: Train Loss=166052.7344 | R²=-0.2687 | RMSE=471.5262 | MAE=230.3190\n Epoch 37: Train Loss=165917.1094 | R²=-0.2675 | RMSE=471.3040 | MAE=230.4838\n Epoch 38: Train Loss=165779.9844 | R²=-0.2663 | RMSE=471.0722 | MAE=230.6543\n Epoch 39: Train Loss=165639.7812 | R²=-0.2650 | RMSE=470.8305 | MAE=230.8307\n Epoch 40: Train Loss=165486.4219 | R²=-0.2636 | RMSE=470.5785 | MAE=231.0130\n Epoch 41: Train Loss=165329.4531 | R²=-0.2622 | RMSE=470.3158 | MAE=231.2014\n Epoch 42: Train Loss=165169.6406 | R²=-0.2608 | RMSE=470.0424 | MAE=231.3960\n Epoch 43: Train Loss=165002.8125 | R²=-0.2592 | RMSE=469.7580 | MAE=231.5969\n Epoch 44: Train Loss=164822.6094 | R²=-0.2577 | RMSE=469.4620 | MAE=231.8043\n Epoch 45: Train Loss=164632.3281 | R²=-0.2560 | RMSE=469.1542 | MAE=232.0183\n Epoch 46: Train Loss=164446.0312 | R²=-0.2543 | RMSE=468.8345 | MAE=232.2389\n Epoch 47: Train Loss=164245.5781 | R²=-0.2525 | RMSE=468.5024 | MAE=232.4664\n Epoch 48: Train Loss=164042.2812 | R²=-0.2507 | RMSE=468.1576 | MAE=232.7007\n Epoch 49: Train Loss=163828.1719 | R²=-0.2488 | RMSE=467.7999 | MAE=232.9421\n Epoch 50: Train Loss=163593.7031 | R²=-0.2468 | RMSE=467.4290 | MAE=233.1908\n✅ Quick Test Complete!\n",
  "history_begin_time" : 1740782055185,
  "history_end_time" : 1740782254468,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "KfNZ0FDw5XGN",
  "history_input" : "#GNN model training testing\r\nimport numpy as np\r\nimport torch\r\nimport warnings\r\nimport os\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\n\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data=torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\" Number of Edges: {g_data.num_edges}\")\r\n        print(f\" Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\" Edge Index Shape: {g_data.edge_index.shape}\")\r\n        # print(f\" Target Variable Shape: {g_data.y.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\" Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\" Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n\r\n    except Exception as e:\r\n        print(f\" Error loading graph: {e}\")\r\n        exit()\r\n\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.5):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.fc = nn.Linear(hidden_dim, output_dim)\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = self.conv1(x, edge_index)\r\n        x = F.relu(x)\r\n        x = self.dropout(x)\r\n        x = self.conv2(x, edge_index)\r\n        x = F.relu(x)\r\n        x = self.fc(x)\r\n        return x\r\n\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n    loss.backward()\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n\r\n# Step 4: Testing Function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n\r\n#load and check\r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n#quick test\r\n    sample_size = 50,000\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[:sample_size] = True  # First 5,000 nodes for training\r\n    test_mask = ~train_mask  # Remaining for testing\r\n\r\n    print(f\" Running on small sample: {sample_size} training nodes\")\r\n\r\n    # Initialize GraphSAGE Model\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=128, output_dim=1, dropout=0.3).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\r\n    criterion = nn.MSELoss()\r\n\r\n    print(\" Quick Test: Training GraphSAGE on Small Sample...\")\r\n\r\n    # Train for just 50 epochs first\r\n    for epoch in range(1, 51):\r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch % 1 == 0:  # Print every epoch (since it's a quick test)\r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\" Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n    print(\"✅ Quick Test Complete!\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 434795\n Number of Edges: 2173386\n Node Feature Shape: torch.Size([434795, 11])\n Edge Index Shape: torch.Size([2, 2173386])\n Target Variable Shape: torch.Size([434795])\nTraceback (most recent call last):\n  File \"/media/volume1/gw-workspace/KfNZ0FDw5XGN/graph_model.py\", line 117, in <module>\n    main()\n  File \"/media/volume1/gw-workspace/KfNZ0FDw5XGN/graph_model.py\", line 93, in main\n    train_mask[:sample_size] = True  # First 5,000 nodes for training\n    ~~~~~~~~~~^^^^^^^^^^^^^^\nTypeError: slice indices must be integers or None or have an __index__ method\n",
  "history_begin_time" : 1740781969274,
  "history_end_time" : 1740781974083,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "wI0h9wfxvz2Q",
  "history_input" : "#GNN model training testing\r\nimport numpy as np\r\nimport torch\r\nimport warnings\r\nimport os\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\n\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data=torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\" Number of Edges: {g_data.num_edges}\")\r\n        print(f\" Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\" Edge Index Shape: {g_data.edge_index.shape}\")\r\n        # print(f\" Target Variable Shape: {g_data.y.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\" Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\" Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n\r\n    except Exception as e:\r\n        print(f\" Error loading graph: {e}\")\r\n        exit()\r\n\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.5):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.fc = nn.Linear(hidden_dim, output_dim)\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = self.conv1(x, edge_index)\r\n        x = F.relu(x)\r\n        x = self.dropout(x)\r\n        x = self.conv2(x, edge_index)\r\n        x = F.relu(x)\r\n        x = self.fc(x)\r\n        return x\r\n\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n    loss.backward()\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n\r\n# Step 4: Testing Function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n\r\n#load and check\r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n#quick test\r\n    sample_size = 100,000\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[:sample_size] = True  # First 5,000 nodes for training\r\n    test_mask = ~train_mask  # Remaining for testing\r\n\r\n    print(f\" Running on small sample: {sample_size} training nodes\")\r\n\r\n    # Initialize GraphSAGE Model\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=64, output_dim=1, dropout=0.5).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\r\n    criterion = nn.MSELoss()\r\n\r\n    print(\" Quick Test: Training GraphSAGE on Small Sample...\")\r\n\r\n    # Train for just 10 epochs first\r\n    for epoch in range(1, 11):\r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch % 1 == 0:  # Print every epoch (since it's a quick test)\r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\" Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n    print(\"✅ Quick Test Complete!\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 434795\n Number of Edges: 2173386\n Node Feature Shape: torch.Size([434795, 11])\n Edge Index Shape: torch.Size([2, 2173386])\n Target Variable Shape: torch.Size([434795])\nTraceback (most recent call last):\n  File \"/media/volume1/gw-workspace/wI0h9wfxvz2Q/graph_model.py\", line 117, in <module>\n    main()\n  File \"/media/volume1/gw-workspace/wI0h9wfxvz2Q/graph_model.py\", line 93, in main\n    train_mask[:sample_size] = True  # First 5,000 nodes for training\n    ~~~~~~~~~~^^^^^^^^^^^^^^\nTypeError: slice indices must be integers or None or have an __index__ method\n",
  "history_begin_time" : 1740781882155,
  "history_end_time" : 1740781886979,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "cMjVPouJhKLi",
  "history_input" : "#GNN model training testing\r\nimport numpy as np\r\nimport torch\r\nimport warnings\r\nimport os\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\n\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data=torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\" Number of Edges: {g_data.num_edges}\")\r\n        print(f\" Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\" Edge Index Shape: {g_data.edge_index.shape}\")\r\n        # print(f\" Target Variable Shape: {g_data.y.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\" Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\" Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n\r\n    except Exception as e:\r\n        print(f\" Error loading graph: {e}\")\r\n        exit()\r\n\r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.5):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.fc = nn.Linear(hidden_dim, output_dim)\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = self.conv1(x, edge_index)\r\n        x = F.relu(x)\r\n        x = self.dropout(x)\r\n        x = self.conv2(x, edge_index)\r\n        x = F.relu(x)\r\n        x = self.fc(x)\r\n        return x\r\n\r\ndef train(model, optimizer, loss_fn, graph, train_mask):\r\n    model.train()\r\n    optimizer.zero_grad()\r\n    predictions = model(graph)[train_mask].squeeze()\r\n    loss = loss_fn(predictions, graph.y[train_mask])\r\n    loss.backward()\r\n    optimizer.step()\r\n    return loss.item()\r\n\r\n\r\n# Step 4: Testing Function\r\ndef test(model, loss_fn, graph, test_mask):\r\n    model.eval()\r\n    with torch.no_grad():\r\n        predictions = model(graph)[test_mask].squeeze().cpu().numpy()\r\n        actuals = graph.y[test_mask].cpu().numpy()\r\n\r\n        mse = mean_squared_error(actuals, predictions)\r\n        r2 = r2_score(actuals, predictions)\r\n        mae = mean_absolute_error(actuals, predictions)\r\n        rmse = mean_squared_error(actuals, predictions, squared=False)\r\n\r\n    return mse, r2, rmse, mae\r\n\r\n\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n\r\n#load and check\r\n    g_data = load_and_verify_graph(g_path)\r\n\r\n#quick test\r\n    sample_size = 5000\r\n    train_mask = torch.zeros(g_data.num_nodes, dtype=torch.bool)\r\n    train_mask[:sample_size] = True  # First 5,000 nodes for training\r\n    test_mask = ~train_mask  # Remaining for testing\r\n\r\n    print(f\" Running on small sample: {sample_size} training nodes\")\r\n\r\n    # Initialize GraphSAGE Model\r\n    model = GraphSAGE(input_dim=g_data.num_features, hidden_dim=64, output_dim=1, dropout=0.5).to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\r\n    criterion = nn.MSELoss()\r\n\r\n    print(\" Quick Test: Training GraphSAGE on Small Sample...\")\r\n\r\n    # Train for just 5 epochs first\r\n    for epoch in range(1, 6):\r\n        train_loss = train(model, optimizer, criterion, g_data, train_mask)\r\n\r\n        if epoch % 1 == 0:  # Print every epoch (since it's a quick test)\r\n            test_loss, r2, rmse, mae = test(model, criterion, g_data, test_mask)\r\n            print(f\" Epoch {epoch}: Train Loss={train_loss:.4f} | R²={r2:.4f} | RMSE={rmse:.4f} | MAE={mae:.4f}\")\r\n\r\n    print(\"✅ Quick Test Complete!\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 434795\n Number of Edges: 2173386\n Node Feature Shape: torch.Size([434795, 11])\n Edge Index Shape: torch.Size([2, 2173386])\n Target Variable Shape: torch.Size([434795])\n Running on small sample: 5000 training nodes\n Quick Test: Training GraphSAGE on Small Sample...\n Epoch 1: Train Loss=332514.3438 | R²=-0.2760 | RMSE=466.7940 | MAE=219.1498\n Epoch 2: Train Loss=332478.9062 | R²=-0.2758 | RMSE=466.7688 | MAE=219.1669\n Epoch 3: Train Loss=332439.4062 | R²=-0.2757 | RMSE=466.7436 | MAE=219.1875\n Epoch 4: Train Loss=332401.7188 | R²=-0.2756 | RMSE=466.7181 | MAE=219.2097\n Epoch 5: Train Loss=332368.0938 | R²=-0.2754 | RMSE=466.6926 | MAE=219.2326\n✅ Quick Test Complete!\n",
  "history_begin_time" : 1740781737105,
  "history_end_time" : 1740781752430,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "OqGZv9WXSCJK",
  "history_input" : "#GNN model training testing\r\nimport numpy as np\r\nimport torch\r\nimport warnings\r\nimport os\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch_geometric.nn import SAGEConv\r\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\r\n\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data=torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\" Number of Edges: {g_data.num_edges}\")\r\n        print(f\" Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\" Edge Index Shape: {g_data.edge_index.shape}\")\r\n        # print(f\" Target Variable Shape: {g_data.y.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\" Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\" Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n\r\n    except Exception as e:\r\n        print(f\" Error loading graph: {e}\")\r\n        exit()\r\n        \r\nclass GraphSAGE(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.5):\r\n        super(GraphSAGE, self).__init__()\r\n        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr=\"mean\")\r\n        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\")\r\n        self.fc = nn.Linear(hidden_dim, output_dim)\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = self.conv1(x, edge_index)\r\n        x = F.relu(x)\r\n        x = self.dropout(x)\r\n        x = self.conv2(x, edge_index)\r\n        x = F.relu(x)\r\n        x = self.fc(x)\r\n        return x\r\n\r\n\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n\r\n#load and check\r\n    g_data = load_and_verify_graph(g_path)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 434795\n Number of Edges: 2173386\n Node Feature Shape: torch.Size([434795, 11])\n Edge Index Shape: torch.Size([2, 2173386])\n Target Variable Shape: torch.Size([434795])\n",
  "history_begin_time" : 1740781100084,
  "history_end_time" : 1740781104965,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "Hs3mcha3ivRA",
  "history_input" : "#GNN model training testing\r\nimport numpy as np\r\nimport torch\r\nimport warnings\r\nimport os\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data=torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\" Number of Edges: {g_data.num_edges}\")\r\n        print(f\" Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\" Edge Index Shape: {g_data.edge_index.shape}\")\r\n        # print(f\" Target Variable Shape: {g_data.y.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\" Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\" Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n\r\n    except Exception as e:\r\n        print(f\" Error loading graph: {e}\")\r\n        exit()\r\n\r\n\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n\r\n#load and check\r\n    g_data = load_and_verify_graph(g_path)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 434795\n Number of Edges: 2173386\n Node Feature Shape: torch.Size([434795, 11])\n Edge Index Shape: torch.Size([2, 2173386])\n Target Variable Shape: torch.Size([434795])\n",
  "history_begin_time" : 1740780060665,
  "history_end_time" : 1740780065075,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "PF4Y4jKba7sO",
  "history_input" : "#GNN model training testing\r\nimport numpy as np\r\nimport torch\r\nimport warnings\r\nimport os\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data=torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\" Number of Edges: {g_data.num_edges}\")\r\n        print(f\" Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\" Edge Index Shape: {g_data.edge_index.shape}\")\r\n        # print(f\" Target Variable Shape: {g_data.y.shape}\")\r\n\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\" Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\" Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n\r\n    except Exception as e:\r\n        print(f\" Error loading graph: {e}\")\r\n        exit()\r\n\r\n\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n\r\n#load and check\r\n    g_data = load_and_verify_graph(g_path)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 3978302\n Number of Edges: 19888728\n Node Feature Shape: torch.Size([3978302, 12])\n Edge Index Shape: torch.Size([2, 19888728])\n Warning: Target variable `y` is missing or None!\n",
  "history_begin_time" : 1740722300223,
  "history_end_time" : 1740722304803,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "0NaU8IQLIyDR",
  "history_input" : "#GNN model training testing\r\nimport numpy as np\r\nimport torch\r\nimport warnings\r\nimport os\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data=torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\" Number of Edges: {g_data.num_edges}\")\r\n        print(f\" Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\" Edge Index Shape: {g_data.edge_index.shape}\")\r\n        print(f\" Target Variable Shape: {g_data.y.shape}\")\r\n        if hasattr(g_data, \"y\") and g_data.y is not None:\r\n            print(f\" Target Variable Shape: {g_data.y.shape}\")\r\n        else:\r\n            print(f\" Warning: Target variable `y` is missing or None!\")\r\n\r\n        return g_data\r\n\r\n    except Exception as e:\r\n        print(f\" Error loading graph: {e}\")\r\n        exit()\r\n\r\n\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n\r\n#load and check\r\n    g_data = load_and_verify_graph(g_path)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 3978302\n Number of Edges: 19888728\n Node Feature Shape: torch.Size([3978302, 12])\n Edge Index Shape: torch.Size([2, 19888728])\n Error loading graph: 'NoneType' object has no attribute 'shape'\n",
  "history_begin_time" : 1740722260643,
  "history_end_time" : 1740722265276,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "30sgpozAMUZl",
  "history_input" : "#GNN model training testing\r\nimport numpy as np\r\nimport torch\r\nimport warnings\r\nimport os\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data=torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\" Number of Edges: {g_data.num_edges}\")\r\n        print(f\" Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\" Edge Index Shape: {g_data.edge_index.shape}\")\r\n        print(f\" Target Variable Shape: {g_data.y.shape}\")\r\n\r\n        return g_data\r\n\r\n    except Exception as e:\r\n        print(f\" Error loading graph: {e}\")\r\n        exit()\r\n\r\n\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n\r\n#load and check\r\n    g_data = load_and_verify_graph(g_path)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 3978302\n Number of Edges: 19888728\n Node Feature Shape: torch.Size([3978302, 12])\n Edge Index Shape: torch.Size([2, 19888728])\n Error loading graph: 'NoneType' object has no attribute 'shape'\n",
  "history_begin_time" : 1740722121037,
  "history_end_time" : 1740722125522,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "hbz0aaMUse8x",
  "history_input" : "#GNN model training testing\r\nimport numpy as np\r\nimport torch\r\nimport warnings\r\nimport os\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data=torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\" Number of Edges: {g_data.num_edges}\")\r\n        print(f\" Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\" Edge Index Shape: {g_data.edge_index.shape}\")\r\n        print(f\" Target Variable Shape: {g_data.y.shape}\")\r\n\r\n        return g_data\r\n\r\n    except Exception as e:\r\n        print(f\" Error loading graph: {e}\")\r\n        exit()\r\n\r\n\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n\r\n#load and check\r\n    graph_data = load_and_verify_graph(g_path)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 3978302\n Number of Edges: 19888728\n Node Feature Shape: torch.Size([3978302, 12])\n Edge Index Shape: torch.Size([2, 19888728])\n Error loading graph: 'NoneType' object has no attribute 'shape'\n",
  "history_begin_time" : 1740722090339,
  "history_end_time" : 1740722094854,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "22pEajRjvEko",
  "history_input" : "#GNN model training testing\r\nimport numpy as np\r\nimport torch\r\nimport warnings\r\nimport os\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data=torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {g_data.num_nodes}\")\r\n        print(f\" Number of Edges: {g_data.num_edges}\")\r\n        print(f\" Node Feature Shape: {g_data.x.shape}\")\r\n        print(f\" Edge Index Shape: {g_data.edge_index.shape}\")\r\n        print(f\" Target Variable Shape: {g_data.y.shape}\")\r\n\r\n        return graph_data\r\n\r\n    except Exception as e:\r\n        print(f\" Error loading graph: {e}\")\r\n        exit()\r\n\r\n\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n\r\n#load and check\r\n    graph_data = load_and_verify_graph(g_path)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\nNumber of Nodes: 3978302\n Number of Edges: 19888728\n Node Feature Shape: torch.Size([3978302, 12])\n Edge Index Shape: torch.Size([2, 19888728])\n Error loading graph: 'NoneType' object has no attribute 'shape'\n",
  "history_begin_time" : 1740722008691,
  "history_end_time" : 1740722013177,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "O6JVeRRd8AEm",
  "history_input" : "#GNN model training testing\r\nimport numpy as np\r\nimport torch\r\nimport warnings\r\nimport os\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data=torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {g_path}\")\r\n        print(f\"Number of Nodes: {graph_data.num_nodes}\")\r\n        print(f\" Number of Edges: {graph_data.num_edges}\")\r\n        print(f\" Node Feature Shape: {graph_data.x.shape}\")\r\n        print(f\" Edge Index Shape: {graph_data.edge_index.shape}\")\r\n        print(f\" Target Variable Shape: {graph_data.y.shape}\")\r\n\r\n        return graph_data\r\n\r\n    except Exception as e:\r\n        print(f\" Error loading graph: {e}\")\r\n        exit()\r\n\r\n\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n\r\n#load and check\r\n    graph_data = load_and_verify_graph(g_path)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\nGraph successfully loaded from /media/volume1/gnn_graph.pt\n Error loading graph: name 'graph_data' is not defined\n",
  "history_begin_time" : 1740721969360,
  "history_end_time" : 1740721974026,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "uaci9vmitfyN",
  "history_input" : "#GNN model training testing\r\nimport numpy as np\r\nimport torch\r\nimport warnings\r\nimport os\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data=torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {graph_path}\")\r\n        print(f\"Number of Nodes: {graph_data.num_nodes}\")\r\n        print(f\" Number of Edges: {graph_data.num_edges}\")\r\n        print(f\" Node Feature Shape: {graph_data.x.shape}\")\r\n        print(f\" Edge Index Shape: {graph_data.edge_index.shape}\")\r\n        print(f\" Target Variable Shape: {graph_data.y.shape}\")\r\n\r\n        return graph_data\r\n\r\n    except Exception as e:\r\n        print(f\" Error loading graph: {e}\")\r\n        exit()\r\n\r\n\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n\r\n#load and check\r\n    graph_data = load_and_verify_graph(g_path)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Using device: cpu\nLoading graph dataset\n Error loading graph: name 'graph_path' is not defined\n",
  "history_begin_time" : 1740721946022,
  "history_end_time" : 1740721950544,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "J0eahbCyxhoC",
  "history_input" : "#GNN model training testing\r\nimport torch\r\nimport warnings\r\nimport os\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data=torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {graph_path}\")\r\n        print(f\"Number of Nodes: {graph_data.num_nodes}\")\r\n        print(f\" Number of Edges: {graph_data.num_edges}\")\r\n        print(f\" Node Feature Shape: {graph_data.x.shape}\")\r\n        print(f\" Edge Index Shape: {graph_data.edge_index.shape}\")\r\n        print(f\" Target Variable Shape: {graph_data.y.shape}\")\r\n\r\n        return graph_data\r\n\r\n    except Exception as e:\r\n        print(f\" Error loading graph: {e}\")\r\n        exit()\r\n\r\n\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n\r\n#load and check\r\n    graph_data = load_and_verify_graph(g_path)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp-a34b3233.so.1 library.\n\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n",
  "history_begin_time" : 1740721895916,
  "history_end_time" : 1740721896996,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "GrZ8cwwCgkd2",
  "history_input" : "#GNN model training testing\r\nimport torch\r\nimport warnings\r\nimport os\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data=torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {graph_path}\")\r\n        print(f\"Number of Nodes: {graph_data.num_nodes}\")\r\n        print(f\" Number of Edges: {graph_data.num_edges}\")\r\n        print(f\" Node Feature Shape: {graph_data.x.shape}\")\r\n        print(f\" Edge Index Shape: {graph_data.edge_index.shape}\")\r\n        print(f\" Target Variable Shape: {graph_data.y.shape}\")\r\n\r\n        return graph_data\r\n\r\n    except Exception as e:\r\n        print(f\" Error loading graph: {e}\")\r\n        exit()\r\n\r\n\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n\r\n#load and check\r\n    graph_data = load_and_verify_graph(g_path)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp-a34b3233.so.1 library.\n\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n",
  "history_begin_time" : 1740721895539,
  "history_end_time" : 1740721896445,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "2XfOFIFJV0Qx",
  "history_input" : "#GNN model training testing\r\nimport torch\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data=torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {graph_path}\")\r\n        print(f\"Number of Nodes: {graph_data.num_nodes}\")\r\n        print(f\" Number of Edges: {graph_data.num_edges}\")\r\n        print(f\" Node Feature Shape: {graph_data.x.shape}\")\r\n        print(f\" Edge Index Shape: {graph_data.edge_index.shape}\")\r\n        print(f\" Target Variable Shape: {graph_data.y.shape}\")\r\n\r\n        return graph_data\r\n\r\n    except Exception as e:\r\n        print(f\" Error loading graph: {e}\")\r\n        exit()\r\n\r\n\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n\r\n#load and check\r\n    graph_data = load_and_verify_graph(g_path)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp-a34b3233.so.1 library.\n\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n",
  "history_begin_time" : 1740721813927,
  "history_end_time" : 1740721814882,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "qpHIIMtYe53m",
  "history_input" : "#GNN model training testing\r\nimport torch\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n\r\n# Check Device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nprint(f\"Using device: {device}\")\r\ndef load_and_verify_graph(g_path):\r\n    print(\"Loading graph dataset\")\r\n    try:\r\n        g_data=torch.load(g_path)\r\n        print(f\"Graph successfully loaded from {graph_path}\")\r\n        print(f\"Number of Nodes: {graph_data.num_nodes}\")\r\n        print(f\" Number of Edges: {graph_data.num_edges}\")\r\n        print(f\" Node Feature Shape: {graph_data.x.shape}\")\r\n        print(f\" Edge Index Shape: {graph_data.edge_index.shape}\")\r\n        print(f\" Target Variable Shape: {graph_data.y.shape}\")\r\n\r\n        return graph_data\r\n\r\n    except Exception as e:\r\n        print(f\" Error loading graph: {e}\")\r\n        exit()\r\n\r\n\r\ndef main():\r\n    g_path = \"/media/volume1/gnn_graph.pt\"  \r\n\r\n#load and check\r\n    graph_data = load_and_verify_graph(g_path)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
  "history_output" : "Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp-a34b3233.so.1 library.\n\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n",
  "history_begin_time" : 1740721800745,
  "history_end_time" : 1740721801760,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "i32x1rbfw0e",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1741409321092,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "qsd9tx0i6om",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1741409322435,
  "history_notes" : null,
  "history_process" : "o7v206",
  "host_id" : "100001",
  "indicator" : "Stopped"
}]