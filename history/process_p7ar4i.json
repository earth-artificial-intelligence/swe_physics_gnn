[{
  "history_id" : "TSzmzXUkThu9",
  "history_input" : "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\nimport pandas as pd\nimport torch\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nfrom torch_geometric.utils import degree\n\n# 📍 File path\nfile_path = '/media/volume1/all_points_final_merged_training.csv'\nchunksize = 500000\n\n# 📋 Intended columns\nbase_columns = [\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness',\n    'SWE', 'fsca', 'air_temperature_tmmx', 'air_temperature_tmmn',\n    'potential_evapotranspiration', 'relative_humidity_rmax', 'relative_humidity_rmin',\n    'mean_vapor_pressure_deficit', 'wind_speed', 'snodas_mask', 'water_year', 'swe_value'\n]\n\ntime_series_cols = [\n    f\"{col}_{i}\" for col in [\n        \"SWE\", \"air_temperature_tmmx\", \"air_temperature_tmmn\", \"potential_evapotranspiration\",\n        \"relative_humidity_rmax\", \"relative_humidity_rmin\", \"mean_vapor_pressure_deficit\",\n        \"precipitation_amount\", \"wind_speed\", \"fsca\"\n    ] for i in range(1, 8)\n]\n\ncumulative_cols = [\n    \"cumulative_SWE\", \"cumulative_air_temperature_tmmn\", \"cumulative_air_temperature_tmmx\",\n    \"cumulative_potential_evapotranspiration\", \"cumulative_mean_vapor_pressure_deficit\",\n    \"cumulative_precipitation_amount\", \"cumulative_relative_humidity_rmax\",\n    \"cumulative_relative_humidity_rmin\", \"cumulative_wind_speed\", \"cumulative_fsca\"\n]\n\nintended_columns = base_columns + time_series_cols + cumulative_cols\n\n# 🔍 Check available columns\nprint(\"🔍 Checking available columns in CSV...\")\npreview = pd.read_csv(file_path, nrows=1)\navailable_columns = set(preview.columns)\nuseful_columns = [col for col in intended_columns if col in available_columns]\nif 'swe_value' not in useful_columns:\n    useful_columns.append('swe_value')\n\nmissing_columns = [col for col in intended_columns if col not in available_columns]\nif missing_columns:\n    print(f\"⚠️ Missing columns skipped: {missing_columns}\")\n\n# 📥 Load dataset\nprint(\"\\n📥 Loading dataset...\")\ndf_list = []\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\n    if 'date' in chunk.columns:\n        chunk['date'] = pd.to_datetime(chunk['date'], errors='coerce')\n        chunk = chunk.dropna(subset=['date'])\n        chunk['day_of_year'] = chunk['date'].dt.dayofyear\n    else:\n        chunk['day_of_year'] = 1\n\n    if 'swe_value' in chunk.columns:\n        chunk = chunk[(chunk['swe_value'] >= 0) & (chunk['swe_value'] < 3000)]\n\n    df_list.append(chunk)\n\ndata = pd.concat(df_list, ignore_index=True)\ndata = data.drop_duplicates()\nprint(f\"✅ Loaded shape: {data.shape}\")\n\n# 🧮 Binning\ngrid_size = 0.5\nnum_time_bins = 8\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\ndata['dayofyear_bin'] = (data['day_of_year'] // (365 / num_time_bins)).astype(int)\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['dayofyear_bin'].astype(str)\n\n# 📦 Aggregation\nagg_cols = {col: 'mean' for col in useful_columns if col not in ['date', 'lat', 'lon']}\nagg_cols.update({'lat': 'mean', 'lon': 'mean', 'day_of_year': 'mean'})\nif 'date' in useful_columns:\n    agg_cols['date'] = 'first'\nagg_cols['swe_value'] = 'mean'\n\nmerged_nodes = data.groupby('grid_id').agg(agg_cols).reset_index()\n\n# ⏳ Temporal encoding\nmerged_nodes['sin_doy'] = np.sin(2 * np.pi * merged_nodes['day_of_year'] / 365)\nmerged_nodes['cos_doy'] = np.cos(2 * np.pi * merged_nodes['day_of_year'] / 365)\n\n# 🌍 Spatial encoding\nmerged_nodes['lat_rad'] = np.radians(merged_nodes['lat'])\nmerged_nodes['lon_rad'] = np.radians(merged_nodes['lon'])\nmerged_nodes['sin_lat'] = np.sin(merged_nodes['lat_rad'])\nmerged_nodes['cos_lat'] = np.cos(merged_nodes['lat_rad'])\nmerged_nodes['sin_lon'] = np.sin(merged_nodes['lon_rad'])\nmerged_nodes['cos_lon'] = np.cos(merged_nodes['lon_rad'])\n\n# 🧪 Final feature selection\nexclude = ['grid_id', 'lat', 'lon', 'lat_rad', 'lon_rad', 'date', 'day_of_year', 'swe_value']\nfeature_columns = [col for col in merged_nodes.columns if col not in exclude]\nfeature_columns += ['sin_lat', 'cos_lat', 'sin_lon', 'cos_lon']\n\n# ⚙️ Normalize & filter correlations\nscaler = StandardScaler()\nnode_features_scaled = scaler.fit_transform(merged_nodes[feature_columns])\ndf_scaled = pd.DataFrame(node_features_scaled, columns=feature_columns)\n\ncorr = df_scaled.corr().abs()\nupper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\nto_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\nprint(f\"🧹 Dropping highly correlated features: {to_drop}\")\ndf_scaled.drop(columns=to_drop, inplace=True)\n\n# Convert to tensor\nnode_features = torch.tensor(df_scaled.values, dtype=torch.float)\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\n\n# 🌐 Build edges with KDTree\ncoordinates = merged_nodes[['lat', 'lon']].values\ntree = KDTree(coordinates)\nk = 6\nthreshold = 0.5\ndistances, neighbors = tree.query(coordinates, k=k+1)\n\nedge_list = []\nfor i in range(len(neighbors)):\n    for j in range(1, k+1):\n        if distances[i][j] < threshold:\n            edge_list.append((i, neighbors[i][j]))\n\nedges = np.array(edge_list).T\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.size > 0 else torch.empty((2, 0), dtype=torch.long)\n\n# 🧵 Create Graph\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\ngraph_data.dates = merged_nodes['date'].astype(str).tolist() if 'date' in merged_nodes.columns else []\n\n# 📊 Graph Summary\nprint(\"\\n📊 Graph Summary\")\nprint(f\"🔹 Nodes: {graph_data.num_nodes}\")\nprint(f\"🔹 Edges: {graph_data.num_edges}\")\nprint(f\"🔹 Features per node: {graph_data.num_node_features}\")\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\nprint(f\"   • Min Degree : {deg.min().item()}\")\nprint(f\"   • Max Degree : {deg.max().item()}\")\nprint(f\"   • Mean Degree: {deg.float().mean():.2f}\")\nprint(f\"   • Isolated   : {(deg == 0).sum().item()} nodes\")\n\n# 💾 Save graph\nsave_path = \"/media/volume1/gnn_graph_with_time_all_states_2.pt\"\ntorch.save(graph_data, save_path)\nprint(f\"\\n✅ Graph data saved at: {save_path}\")\n",
  "history_output" : "🔍 Checking available columns in CSV...\n⚠️ Missing columns skipped: ['snodas_mask']\n📥 Loading dataset...\n✅ Loaded shape: (4218762, 101)\n🧹 Dropping highly correlated features: ['air_temperature_tmmn', 'relative_humidity_rmax', 'wind_speed', 'SWE_1', 'SWE_2', 'SWE_3', 'SWE_4', 'SWE_5', 'SWE_6', 'SWE_7', 'air_temperature_tmmx_1', 'air_temperature_tmmx_2', 'air_temperature_tmmx_3', 'air_temperature_tmmx_4', 'air_temperature_tmmx_5', 'air_temperature_tmmx_6', 'air_temperature_tmmx_7', 'air_temperature_tmmn_1', 'air_temperature_tmmn_2', 'air_temperature_tmmn_3', 'air_temperature_tmmn_4', 'air_temperature_tmmn_5', 'air_temperature_tmmn_6', 'air_temperature_tmmn_7', 'potential_evapotranspiration_1', 'potential_evapotranspiration_2', 'potential_evapotranspiration_3', 'potential_evapotranspiration_4', 'potential_evapotranspiration_5', 'potential_evapotranspiration_6', 'potential_evapotranspiration_7', 'relative_humidity_rmax_1', 'relative_humidity_rmax_2', 'relative_humidity_rmax_3', 'relative_humidity_rmax_4', 'relative_humidity_rmax_5', 'relative_humidity_rmax_6', 'relative_humidity_rmax_7', 'relative_humidity_rmin_1', 'relative_humidity_rmin_2', 'relative_humidity_rmin_3', 'relative_humidity_rmin_4', 'relative_humidity_rmin_5', 'relative_humidity_rmin_6', 'relative_humidity_rmin_7', 'mean_vapor_pressure_deficit_1', 'mean_vapor_pressure_deficit_2', 'mean_vapor_pressure_deficit_3', 'mean_vapor_pressure_deficit_4', 'mean_vapor_pressure_deficit_5', 'mean_vapor_pressure_deficit_6', 'mean_vapor_pressure_deficit_7', 'wind_speed_1', 'wind_speed_2', 'wind_speed_3', 'wind_speed_4', 'wind_speed_5', 'wind_speed_6', 'wind_speed_7', 'fsca_1', 'fsca_2', 'fsca_3', 'fsca_4', 'fsca_5', 'fsca_6', 'fsca_7', 'cumulative_air_temperature_tmmn', 'cumulative_air_temperature_tmmx', 'cumulative_potential_evapotranspiration', 'cumulative_mean_vapor_pressure_deficit', 'cumulative_relative_humidity_rmax', 'cumulative_relative_humidity_rmin', 'cumulative_wind_speed', 'sin_lat', 'cos_lat', 'sin_lon', 'cos_lon', 'sin_lat', 'cos_lat', 'sin_lon', 'cos_lon']\n📊 Graph Summary\n🔹 Nodes: 10623\n🔹 Edges: 63610\n🔹 Features per node: 25\n   • Min Degree : 0.0\n   • Max Degree : 6.0\n   • Mean Degree: 5.99\n   • Isolated   : 2 nodes\n✅ Graph data saved at: /media/volume1/gnn_graph_with_time_all_states_2.pt\n",
  "history_begin_time" : 1746108002324,
  "history_end_time" : 1746108285366,
  "history_notes" : null,
  "history_process" : "p7ar4i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "zktpItSRg9Do",
  "history_input" : "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\nimport pandas as pd\nimport torch\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nfrom torch_geometric.utils import degree\n\n# 📍 File path\nfile_path = '/media/volume1/all_points_final_merged_training.csv'\nchunksize = 500000\n\n# 📋 Intended columns\nbase_columns = [\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness',\n    'SWE', 'fsca', 'air_temperature_tmmx', 'air_temperature_tmmn',\n    'potential_evapotranspiration', 'relative_humidity_rmax', 'relative_humidity_rmin',\n    'mean_vapor_pressure_deficit', 'wind_speed', 'snodas_mask', 'water_year', 'swe_value'\n]\n\ntime_series_cols = [\n    f\"{col}_{i}\" for col in [\n        \"SWE\", \"air_temperature_tmmx\", \"air_temperature_tmmn\", \"potential_evapotranspiration\",\n        \"relative_humidity_rmax\", \"relative_humidity_rmin\", \"mean_vapor_pressure_deficit\",\n        \"precipitation_amount\", \"wind_speed\", \"fsca\"\n    ] for i in range(1, 8)\n]\n\ncumulative_cols = [\n    \"cumulative_SWE\", \"cumulative_air_temperature_tmmn\", \"cumulative_air_temperature_tmmx\",\n    \"cumulative_potential_evapotranspiration\", \"cumulative_mean_vapor_pressure_deficit\",\n    \"cumulative_precipitation_amount\", \"cumulative_relative_humidity_rmax\",\n    \"cumulative_relative_humidity_rmin\", \"cumulative_wind_speed\", \"cumulative_fsca\"\n]\n\nintended_columns = base_columns + time_series_cols + cumulative_cols\n\n# 🔍 Check available columns\nprint(\"🔍 Checking available columns in CSV...\")\npreview = pd.read_csv(file_path, nrows=1)\navailable_columns = set(preview.columns)\nuseful_columns = [col for col in intended_columns if col in available_columns]\nif 'swe_value' not in useful_columns:\n    useful_columns.append('swe_value')\n\nmissing_columns = [col for col in intended_columns if col not in available_columns]\nif missing_columns:\n    print(f\"⚠️ Missing columns skipped: {missing_columns}\")\n\n# 📥 Load dataset\nprint(\"\\n📥 Loading dataset...\")\ndf_list = []\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\n    if 'date' in chunk.columns:\n        chunk['date'] = pd.to_datetime(chunk['date'], errors='coerce')\n        chunk = chunk.dropna(subset=['date'])\n        chunk['day_of_year'] = chunk['date'].dt.dayofyear\n    else:\n        chunk['day_of_year'] = 1\n\n    if 'swe_value' in chunk.columns:\n        chunk = chunk[(chunk['swe_value'] >= 0) & (chunk['swe_value'] < 3000)]\n\n    df_list.append(chunk)\n\ndata = pd.concat(df_list, ignore_index=True)\ndata = data.drop_duplicates()\nprint(f\"✅ Loaded shape: {data.shape}\")\n\n# 🧮 Binning\ngrid_size = 0.4\nnum_time_bins = 12\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\ndata['dayofyear_bin'] = (data['day_of_year'] // (365 / num_time_bins)).astype(int)\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['dayofyear_bin'].astype(str)\n\n# 📦 Aggregation\nagg_cols = {col: 'mean' for col in useful_columns if col not in ['date', 'lat', 'lon']}\nagg_cols.update({'lat': 'mean', 'lon': 'mean', 'day_of_year': 'mean'})\nif 'date' in useful_columns:\n    agg_cols['date'] = 'first'\nagg_cols['swe_value'] = 'mean'\n\nmerged_nodes = data.groupby('grid_id').agg(agg_cols).reset_index()\n\n# ⏳ Temporal encoding\nmerged_nodes['sin_doy'] = np.sin(2 * np.pi * merged_nodes['day_of_year'] / 365)\nmerged_nodes['cos_doy'] = np.cos(2 * np.pi * merged_nodes['day_of_year'] / 365)\n\n# 🌍 Spatial encoding\nmerged_nodes['lat_rad'] = np.radians(merged_nodes['lat'])\nmerged_nodes['lon_rad'] = np.radians(merged_nodes['lon'])\nmerged_nodes['sin_lat'] = np.sin(merged_nodes['lat_rad'])\nmerged_nodes['cos_lat'] = np.cos(merged_nodes['lat_rad'])\nmerged_nodes['sin_lon'] = np.sin(merged_nodes['lon_rad'])\nmerged_nodes['cos_lon'] = np.cos(merged_nodes['lon_rad'])\n\n# 🧪 Final feature selection\nexclude = ['grid_id', 'lat', 'lon', 'lat_rad', 'lon_rad', 'date', 'day_of_year', 'swe_value']\nfeature_columns = [col for col in merged_nodes.columns if col not in exclude]\nfeature_columns += ['sin_lat', 'cos_lat', 'sin_lon', 'cos_lon']\n\n# ⚙️ Normalize & filter correlations\nscaler = StandardScaler()\nnode_features_scaled = scaler.fit_transform(merged_nodes[feature_columns])\ndf_scaled = pd.DataFrame(node_features_scaled, columns=feature_columns)\n\ncorr = df_scaled.corr().abs()\nupper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\nto_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\nprint(f\"🧹 Dropping highly correlated features: {to_drop}\")\ndf_scaled.drop(columns=to_drop, inplace=True)\n\n# Convert to tensor\nnode_features = torch.tensor(df_scaled.values, dtype=torch.float)\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\n\n# 🌐 Build edges with KDTree\ncoordinates = merged_nodes[['lat', 'lon']].values\ntree = KDTree(coordinates)\nk = 6\nthreshold = 0.5\ndistances, neighbors = tree.query(coordinates, k=k+1)\n\nedge_list = []\nfor i in range(len(neighbors)):\n    for j in range(1, k+1):\n        if distances[i][j] < threshold:\n            edge_list.append((i, neighbors[i][j]))\n\nedges = np.array(edge_list).T\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.size > 0 else torch.empty((2, 0), dtype=torch.long)\n\n# 🧵 Create Graph\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\ngraph_data.dates = merged_nodes['date'].astype(str).tolist() if 'date' in merged_nodes.columns else []\n\n# 📊 Graph Summary\nprint(\"\\n📊 Graph Summary\")\nprint(f\"🔹 Nodes: {graph_data.num_nodes}\")\nprint(f\"🔹 Edges: {graph_data.num_edges}\")\nprint(f\"🔹 Features per node: {graph_data.num_node_features}\")\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\nprint(f\"   • Min Degree : {deg.min().item()}\")\nprint(f\"   • Max Degree : {deg.max().item()}\")\nprint(f\"   • Mean Degree: {deg.float().mean():.2f}\")\nprint(f\"   • Isolated   : {(deg == 0).sum().item()} nodes\")\n\n# 💾 Save graph\nsave_path = \"/media/volume1/gnn_graph_with_time_all_states_2.pt\"\ntorch.save(graph_data, save_path)\nprint(f\"\\n✅ Graph data saved at: {save_path}\")\n",
  "history_output" : "🔍 Checking available columns in CSV...\n⚠️ Missing columns skipped: ['snodas_mask']\n📥 Loading dataset...\n✅ Loaded shape: (4218762, 101)\n🧹 Dropping highly correlated features: ['air_temperature_tmmn', 'relative_humidity_rmax', 'SWE_1', 'SWE_2', 'SWE_3', 'SWE_4', 'SWE_5', 'SWE_6', 'SWE_7', 'air_temperature_tmmx_1', 'air_temperature_tmmx_2', 'air_temperature_tmmx_3', 'air_temperature_tmmx_4', 'air_temperature_tmmx_5', 'air_temperature_tmmx_6', 'air_temperature_tmmx_7', 'air_temperature_tmmn_1', 'air_temperature_tmmn_2', 'air_temperature_tmmn_3', 'air_temperature_tmmn_4', 'air_temperature_tmmn_5', 'air_temperature_tmmn_6', 'air_temperature_tmmn_7', 'potential_evapotranspiration_1', 'potential_evapotranspiration_2', 'potential_evapotranspiration_3', 'potential_evapotranspiration_4', 'potential_evapotranspiration_5', 'potential_evapotranspiration_6', 'potential_evapotranspiration_7', 'relative_humidity_rmax_1', 'relative_humidity_rmax_2', 'relative_humidity_rmax_3', 'relative_humidity_rmax_4', 'relative_humidity_rmax_5', 'relative_humidity_rmax_6', 'relative_humidity_rmax_7', 'relative_humidity_rmin_1', 'relative_humidity_rmin_2', 'relative_humidity_rmin_3', 'relative_humidity_rmin_4', 'relative_humidity_rmin_5', 'relative_humidity_rmin_6', 'relative_humidity_rmin_7', 'mean_vapor_pressure_deficit_1', 'mean_vapor_pressure_deficit_2', 'mean_vapor_pressure_deficit_3', 'mean_vapor_pressure_deficit_4', 'mean_vapor_pressure_deficit_5', 'mean_vapor_pressure_deficit_6', 'mean_vapor_pressure_deficit_7', 'wind_speed_1', 'wind_speed_2', 'wind_speed_3', 'wind_speed_4', 'wind_speed_5', 'wind_speed_6', 'wind_speed_7', 'fsca_1', 'fsca_2', 'fsca_3', 'fsca_4', 'fsca_5', 'fsca_6', 'fsca_7', 'cumulative_air_temperature_tmmn', 'cumulative_air_temperature_tmmx', 'cumulative_potential_evapotranspiration', 'cumulative_mean_vapor_pressure_deficit', 'cumulative_relative_humidity_rmax', 'cumulative_relative_humidity_rmin', 'cumulative_wind_speed', 'sin_lat', 'cos_lat', 'sin_lon', 'cos_lon', 'sin_lat', 'cos_lat', 'sin_lon', 'cos_lon']\n📊 Graph Summary\n🔹 Nodes: 19885\n🔹 Edges: 119232\n🔹 Features per node: 26\n   • Min Degree : 0.0\n   • Max Degree : 6.0\n   • Mean Degree: 6.00\n   • Isolated   : 2 nodes\n✅ Graph data saved at: /media/volume1/gnn_graph_with_time_all_states_2.pt\n",
  "history_begin_time" : 1746107747981,
  "history_end_time" : 1746107974542,
  "history_notes" : null,
  "history_process" : "p7ar4i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "30mXTrzfNFrn",
  "history_input" : "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\nimport pandas as pd\nimport torch\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nfrom torch_geometric.utils import degree\n\n# 📍 File path\nfile_path = '/media/volume1/all_points_final_merged_training.csv'\nchunksize = 500000\n\n# 📋 Intended columns\nbase_columns = [\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness',\n    'SWE', 'fsca', 'air_temperature_tmmx', 'air_temperature_tmmn',\n    'potential_evapotranspiration', 'relative_humidity_rmax', 'relative_humidity_rmin',\n    'mean_vapor_pressure_deficit', 'wind_speed', 'snodas_mask', 'water_year', 'swe_value'\n]\n\ntime_series_cols = [\n    f\"{col}_{i}\" for col in [\n        \"SWE\", \"air_temperature_tmmx\", \"air_temperature_tmmn\", \"potential_evapotranspiration\",\n        \"relative_humidity_rmax\", \"relative_humidity_rmin\", \"mean_vapor_pressure_deficit\",\n        \"precipitation_amount\", \"wind_speed\", \"fsca\"\n    ] for i in range(1, 8)\n]\n\ncumulative_cols = [\n    \"cumulative_SWE\", \"cumulative_air_temperature_tmmn\", \"cumulative_air_temperature_tmmx\",\n    \"cumulative_potential_evapotranspiration\", \"cumulative_mean_vapor_pressure_deficit\",\n    \"cumulative_precipitation_amount\", \"cumulative_relative_humidity_rmax\",\n    \"cumulative_relative_humidity_rmin\", \"cumulative_wind_speed\", \"cumulative_fsca\"\n]\n\nintended_columns = base_columns + time_series_cols + cumulative_cols\n\n# 🔍 Check available columns\nprint(\"🔍 Checking available columns in CSV...\")\npreview = pd.read_csv(file_path, nrows=1)\navailable_columns = set(preview.columns)\nuseful_columns = [col for col in intended_columns if col in available_columns]\nif 'swe_value' not in useful_columns:\n    useful_columns.append('swe_value')\n\nmissing_columns = [col for col in intended_columns if col not in available_columns]\nif missing_columns:\n    print(f\"⚠️ Missing columns skipped: {missing_columns}\")\n\n# 📥 Load dataset\nprint(\"\\n📥 Loading dataset...\")\ndf_list = []\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\n    if 'date' in chunk.columns:\n        chunk['date'] = pd.to_datetime(chunk['date'], errors='coerce')\n        chunk = chunk.dropna(subset=['date'])\n        chunk['day_of_year'] = chunk['date'].dt.dayofyear\n    else:\n        chunk['day_of_year'] = 1\n\n    if 'swe_value' in chunk.columns:\n        chunk = chunk[(chunk['swe_value'] >= 0) & (chunk['swe_value'] < 3000)]\n\n    df_list.append(chunk)\n\ndata = pd.concat(df_list, ignore_index=True)\ndata = data.drop_duplicates()\nprint(f\"✅ Loaded shape: {data.shape}\")\n\n# 🧮 Binning\ngrid_size = 0.2\nnum_time_bins = 24\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\ndata['dayofyear_bin'] = (data['day_of_year'] // (365 / num_time_bins)).astype(int)\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['dayofyear_bin'].astype(str)\n\n# 📦 Aggregation\nagg_cols = {col: 'mean' for col in useful_columns if col not in ['date', 'lat', 'lon']}\nagg_cols.update({'lat': 'mean', 'lon': 'mean', 'day_of_year': 'mean'})\nif 'date' in useful_columns:\n    agg_cols['date'] = 'first'\nagg_cols['swe_value'] = 'mean'\n\nmerged_nodes = data.groupby('grid_id').agg(agg_cols).reset_index()\n\n# ⏳ Temporal encoding\nmerged_nodes['sin_doy'] = np.sin(2 * np.pi * merged_nodes['day_of_year'] / 365)\nmerged_nodes['cos_doy'] = np.cos(2 * np.pi * merged_nodes['day_of_year'] / 365)\n\n# 🌍 Spatial encoding\nmerged_nodes['lat_rad'] = np.radians(merged_nodes['lat'])\nmerged_nodes['lon_rad'] = np.radians(merged_nodes['lon'])\nmerged_nodes['sin_lat'] = np.sin(merged_nodes['lat_rad'])\nmerged_nodes['cos_lat'] = np.cos(merged_nodes['lat_rad'])\nmerged_nodes['sin_lon'] = np.sin(merged_nodes['lon_rad'])\nmerged_nodes['cos_lon'] = np.cos(merged_nodes['lon_rad'])\n\n# 🧪 Final feature selection\nexclude = ['grid_id', 'lat', 'lon', 'lat_rad', 'lon_rad', 'date', 'day_of_year', 'swe_value']\nfeature_columns = [col for col in merged_nodes.columns if col not in exclude]\nfeature_columns += ['sin_lat', 'cos_lat', 'sin_lon', 'cos_lon']\n\n# ⚙️ Normalize & filter correlations\nscaler = StandardScaler()\nnode_features_scaled = scaler.fit_transform(merged_nodes[feature_columns])\ndf_scaled = pd.DataFrame(node_features_scaled, columns=feature_columns)\n\ncorr = df_scaled.corr().abs()\nupper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\nto_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\nprint(f\"🧹 Dropping highly correlated features: {to_drop}\")\ndf_scaled.drop(columns=to_drop, inplace=True)\n\n# Convert to tensor\nnode_features = torch.tensor(df_scaled.values, dtype=torch.float)\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\n\n# 🌐 Build edges with KDTree\ncoordinates = merged_nodes[['lat', 'lon']].values\ntree = KDTree(coordinates)\nk = 6\nthreshold = 0.5\ndistances, neighbors = tree.query(coordinates, k=k+1)\n\nedge_list = []\nfor i in range(len(neighbors)):\n    for j in range(1, k+1):\n        if distances[i][j] < threshold:\n            edge_list.append((i, neighbors[i][j]))\n\nedges = np.array(edge_list).T\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.size > 0 else torch.empty((2, 0), dtype=torch.long)\n\n# 🧵 Create Graph\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\ngraph_data.dates = merged_nodes['date'].astype(str).tolist() if 'date' in merged_nodes.columns else []\n\n# 📊 Graph Summary\nprint(\"\\n📊 Graph Summary\")\nprint(f\"🔹 Nodes: {graph_data.num_nodes}\")\nprint(f\"🔹 Edges: {graph_data.num_edges}\")\nprint(f\"🔹 Features per node: {graph_data.num_node_features}\")\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\nprint(f\"   • Min Degree : {deg.min().item()}\")\nprint(f\"   • Max Degree : {deg.max().item()}\")\nprint(f\"   • Mean Degree: {deg.float().mean():.2f}\")\nprint(f\"   • Isolated   : {(deg == 0).sum().item()} nodes\")\n\n# 💾 Save graph\nsave_path = \"/media/volume1/gnn_graph_with_time_all_states.pt\"\ntorch.save(graph_data, save_path)\nprint(f\"\\n✅ Graph data saved at: {save_path}\")\n",
  "history_output" : "🔍 Checking available columns in CSV...\n⚠️ Missing columns skipped: ['snodas_mask']\n📥 Loading dataset...\n✅ Loaded shape: (4218762, 101)\n🧹 Dropping highly correlated features: ['air_temperature_tmmn', 'relative_humidity_rmax', 'SWE_1', 'SWE_2', 'SWE_3', 'SWE_4', 'SWE_5', 'SWE_6', 'SWE_7', 'air_temperature_tmmx_1', 'air_temperature_tmmx_2', 'air_temperature_tmmx_3', 'air_temperature_tmmx_4', 'air_temperature_tmmx_5', 'air_temperature_tmmx_6', 'air_temperature_tmmx_7', 'air_temperature_tmmn_1', 'air_temperature_tmmn_2', 'air_temperature_tmmn_3', 'air_temperature_tmmn_4', 'air_temperature_tmmn_5', 'air_temperature_tmmn_6', 'air_temperature_tmmn_7', 'potential_evapotranspiration_1', 'potential_evapotranspiration_2', 'potential_evapotranspiration_3', 'potential_evapotranspiration_4', 'potential_evapotranspiration_5', 'potential_evapotranspiration_6', 'potential_evapotranspiration_7', 'relative_humidity_rmax_1', 'relative_humidity_rmax_2', 'relative_humidity_rmax_3', 'relative_humidity_rmax_4', 'relative_humidity_rmax_5', 'relative_humidity_rmax_6', 'relative_humidity_rmax_7', 'relative_humidity_rmin_1', 'relative_humidity_rmin_2', 'relative_humidity_rmin_3', 'relative_humidity_rmin_4', 'relative_humidity_rmin_5', 'relative_humidity_rmin_6', 'relative_humidity_rmin_7', 'mean_vapor_pressure_deficit_1', 'mean_vapor_pressure_deficit_2', 'mean_vapor_pressure_deficit_3', 'mean_vapor_pressure_deficit_4', 'mean_vapor_pressure_deficit_5', 'mean_vapor_pressure_deficit_6', 'mean_vapor_pressure_deficit_7', 'wind_speed_1', 'wind_speed_2', 'wind_speed_3', 'wind_speed_4', 'wind_speed_5', 'wind_speed_6', 'wind_speed_7', 'fsca_1', 'fsca_2', 'fsca_3', 'fsca_4', 'fsca_5', 'fsca_6', 'fsca_7', 'cumulative_air_temperature_tmmn', 'cumulative_air_temperature_tmmx', 'cumulative_potential_evapotranspiration', 'cumulative_mean_vapor_pressure_deficit', 'cumulative_relative_humidity_rmax', 'cumulative_relative_humidity_rmin', 'cumulative_wind_speed', 'sin_lat', 'cos_lat', 'sin_lon', 'cos_lon', 'sin_lat', 'cos_lat', 'sin_lon', 'cos_lon']\n📊 Graph Summary\n🔹 Nodes: 68557\n🔹 Edges: 411292\n🔹 Features per node: 26\n   • Min Degree : 0.0\n   • Max Degree : 6.0\n   • Mean Degree: 6.00\n   • Isolated   : 1 nodes\n✅ Graph data saved at: /media/volume1/gnn_graph_with_time_all_states.pt\n",
  "history_begin_time" : 1746104099667,
  "history_end_time" : 1746104196087,
  "history_notes" : null,
  "history_process" : "p7ar4i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "6dEbDo7736Fx",
  "history_input" : "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\nimport pandas as pd\nimport torch\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nfrom torch_geometric.utils import degree\n\n# 📍 File path\nfile_path = '/media/volume1/all_points_final_merged_training.csv'\nchunksize = 500000\n\n# 📋 Intended columns\nbase_columns = [\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness',\n    'SWE', 'fsca', 'air_temperature_tmmx', 'air_temperature_tmmn',\n    'potential_evapotranspiration', 'relative_humidity_rmax', 'relative_humidity_rmin',\n    'mean_vapor_pressure_deficit', 'wind_speed', 'snodas_mask', 'water_year', 'swe_value'\n]\n\ntime_series_cols = [\n    f\"{col}_{i}\" for col in [\n        \"SWE\", \"air_temperature_tmmx\", \"air_temperature_tmmn\", \"potential_evapotranspiration\",\n        \"relative_humidity_rmax\", \"relative_humidity_rmin\", \"mean_vapor_pressure_deficit\",\n        \"precipitation_amount\", \"wind_speed\", \"fsca\"\n    ] for i in range(1, 8)\n]\n\ncumulative_cols = [\n    \"cumulative_SWE\", \"cumulative_air_temperature_tmmn\", \"cumulative_air_temperature_tmmx\",\n    \"cumulative_potential_evapotranspiration\", \"cumulative_mean_vapor_pressure_deficit\",\n    \"cumulative_precipitation_amount\", \"cumulative_relative_humidity_rmax\",\n    \"cumulative_relative_humidity_rmin\", \"cumulative_wind_speed\", \"cumulative_fsca\"\n]\n\nintended_columns = base_columns + time_series_cols + cumulative_cols\n\n# 🔍 Check available columns\nprint(\"🔍 Checking available columns in CSV...\")\npreview = pd.read_csv(file_path, nrows=1)\navailable_columns = set(preview.columns)\nuseful_columns = [col for col in intended_columns if col in available_columns]\nif 'swe_value' not in useful_columns:\n    useful_columns.append('swe_value')\n\nmissing_columns = [col for col in intended_columns if col not in available_columns]\nif missing_columns:\n    print(f\"⚠️ Missing columns skipped: {missing_columns}\")\n\n# 📥 Load dataset\nprint(\"\\n📥 Loading dataset...\")\ndf_list = []\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\n    if 'date' in chunk.columns:\n        chunk['date'] = pd.to_datetime(chunk['date'], errors='coerce')\n        chunk = chunk.dropna(subset=['date'])\n        chunk['day_of_year'] = chunk['date'].dt.dayofyear\n    else:\n        chunk['day_of_year'] = 1\n\n    if 'swe_value' in chunk.columns:\n        chunk = chunk[(chunk['swe_value'] >= 0) & (chunk['swe_value'] < 3000)]\n\n    df_list.append(chunk)\n\ndata = pd.concat(df_list, ignore_index=True)\ndata = data.drop_duplicates()\nprint(f\"✅ Loaded shape: {data.shape}\")\n\n# 🧮 Binning\ngrid_size = 0.2\nnum_time_bins = 24\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\ndata['dayofyear_bin'] = (data['day_of_year'] // (365 / num_time_bins)).astype(int)\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['dayofyear_bin'].astype(str)\n\n# 📦 Aggregation\nagg_cols = {col: 'mean' for col in useful_columns if col not in ['date', 'lat', 'lon']}\nagg_cols.update({'lat': 'mean', 'lon': 'mean', 'day_of_year': 'mean'})\nif 'date' in useful_columns:\n    agg_cols['date'] = 'first'\nagg_cols['swe_value'] = 'mean'\n\nmerged_nodes = data.groupby('grid_id').agg(agg_cols).reset_index()\n\n# ⏳ Temporal encoding\nmerged_nodes['sin_doy'] = np.sin(2 * np.pi * merged_nodes['day_of_year'] / 365)\nmerged_nodes['cos_doy'] = np.cos(2 * np.pi * merged_nodes['day_of_year'] / 365)\n\n# 🌍 Spatial encoding\nmerged_nodes['lat_rad'] = np.radians(merged_nodes['lat'])\nmerged_nodes['lon_rad'] = np.radians(merged_nodes['lon'])\nmerged_nodes['sin_lat'] = np.sin(merged_nodes['lat_rad'])\nmerged_nodes['cos_lat'] = np.cos(merged_nodes['lat_rad'])\nmerged_nodes['sin_lon'] = np.sin(merged_nodes['lon_rad'])\nmerged_nodes['cos_lon'] = np.cos(merged_nodes['lon_rad'])\n\n# 🧪 Final feature selection\nexclude = ['grid_id', 'lat', 'lon', 'lat_rad', 'lon_rad', 'date', 'day_of_year', 'swe_value']\nfeature_columns = [col for col in merged_nodes.columns if col not in exclude]\nfeature_columns += ['sin_lat', 'cos_lat', 'sin_lon', 'cos_lon']\n\n# ⚙️ Normalize & filter correlations\nscaler = StandardScaler()\nnode_features_scaled = scaler.fit_transform(merged_nodes[feature_columns])\ndf_scaled = pd.DataFrame(node_features_scaled, columns=feature_columns)\n\ncorr = df_scaled.corr().abs()\nupper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\nto_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\nprint(f\"🧹 Dropping highly correlated features: {to_drop}\")\ndf_scaled.drop(columns=to_drop, inplace=True)\n\n# Convert to tensor\nnode_features = torch.tensor(df_scaled.values, dtype=torch.float)\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\n\n# 🌐 Build edges with KDTree\ncoordinates = merged_nodes[['lat', 'lon']].values\ntree = KDTree(coordinates)\nk = 6\nthreshold = 0.5\ndistances, neighbors = tree.query(coordinates, k=k+1)\n\nedge_list = []\nfor i in range(len(neighbors)):\n    for j in range(1, k+1):\n        if distances[i][j] < threshold:\n            edge_list.append((i, neighbors[i][j]))\n\nedges = np.array(edge_list).T\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.size > 0 else torch.empty((2, 0), dtype=torch.long)\n\n# 🧵 Create Graph\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\ngraph_data.dates = merged_nodes['date'].astype(str).tolist() if 'date' in merged_nodes.columns else []\n\n# 📊 Graph Summary\nprint(\"\\n📊 Graph Summary\")\nprint(f\"🔹 Nodes: {graph_data.num_nodes}\")\nprint(f\"🔹 Edges: {graph_data.num_edges}\")\nprint(f\"🔹 Features per node: {graph_data.num_node_features}\")\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\nprint(f\"   • Min Degree : {deg.min().item()}\")\nprint(f\"   • Max Degree : {deg.max().item()}\")\nprint(f\"   • Mean Degree: {deg.float().mean():.2f}\")\nprint(f\"   • Isolated   : {(deg == 0).sum().item()} nodes\")\n\n# 💾 Save graph\n# save_path = \"/media/volume1/gnn_graph_pinn_data_final.pt\"\n# torch.save(graph_data, save_path)\n# print(f\"\\n✅ Graph data saved at: {save_path}\")\n",
  "history_output" : "🔍 Checking available columns in CSV...\n⚠️ Missing columns skipped: ['snodas_mask']\n📥 Loading dataset...\n✅ Loaded shape: (4218762, 101)\n🧹 Dropping highly correlated features: ['air_temperature_tmmn', 'relative_humidity_rmax', 'SWE_1', 'SWE_2', 'SWE_3', 'SWE_4', 'SWE_5', 'SWE_6', 'SWE_7', 'air_temperature_tmmx_1', 'air_temperature_tmmx_2', 'air_temperature_tmmx_3', 'air_temperature_tmmx_4', 'air_temperature_tmmx_5', 'air_temperature_tmmx_6', 'air_temperature_tmmx_7', 'air_temperature_tmmn_1', 'air_temperature_tmmn_2', 'air_temperature_tmmn_3', 'air_temperature_tmmn_4', 'air_temperature_tmmn_5', 'air_temperature_tmmn_6', 'air_temperature_tmmn_7', 'potential_evapotranspiration_1', 'potential_evapotranspiration_2', 'potential_evapotranspiration_3', 'potential_evapotranspiration_4', 'potential_evapotranspiration_5', 'potential_evapotranspiration_6', 'potential_evapotranspiration_7', 'relative_humidity_rmax_1', 'relative_humidity_rmax_2', 'relative_humidity_rmax_3', 'relative_humidity_rmax_4', 'relative_humidity_rmax_5', 'relative_humidity_rmax_6', 'relative_humidity_rmax_7', 'relative_humidity_rmin_1', 'relative_humidity_rmin_2', 'relative_humidity_rmin_3', 'relative_humidity_rmin_4', 'relative_humidity_rmin_5', 'relative_humidity_rmin_6', 'relative_humidity_rmin_7', 'mean_vapor_pressure_deficit_1', 'mean_vapor_pressure_deficit_2', 'mean_vapor_pressure_deficit_3', 'mean_vapor_pressure_deficit_4', 'mean_vapor_pressure_deficit_5', 'mean_vapor_pressure_deficit_6', 'mean_vapor_pressure_deficit_7', 'wind_speed_1', 'wind_speed_2', 'wind_speed_3', 'wind_speed_4', 'wind_speed_5', 'wind_speed_6', 'wind_speed_7', 'fsca_1', 'fsca_2', 'fsca_3', 'fsca_4', 'fsca_5', 'fsca_6', 'fsca_7', 'cumulative_air_temperature_tmmn', 'cumulative_air_temperature_tmmx', 'cumulative_potential_evapotranspiration', 'cumulative_mean_vapor_pressure_deficit', 'cumulative_relative_humidity_rmax', 'cumulative_relative_humidity_rmin', 'cumulative_wind_speed', 'sin_lat', 'cos_lat', 'sin_lon', 'cos_lon', 'sin_lat', 'cos_lat', 'sin_lon', 'cos_lon']\n📊 Graph Summary\n🔹 Nodes: 68557\n🔹 Edges: 411292\n🔹 Features per node: 26\n   • Min Degree : 0.0\n   • Max Degree : 6.0\n   • Mean Degree: 6.00\n   • Isolated   : 1 nodes\n",
  "history_begin_time" : 1746103743002,
  "history_end_time" : 1746103837996,
  "history_notes" : null,
  "history_process" : "p7ar4i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "CjP7fUI65B14",
  "history_input" : "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\nimport pandas as pd\nimport torch\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nfrom torch_geometric.utils import degree\n\n# 📍 File path\nfile_path = '/media/volume1/all_points_final_merged_training.csv'\nchunksize = 500000\n\n# 📋 Intended columns (used if they exist)\nbase_columns = [\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness',\n    'SWE', 'fsca', 'air_temperature_tmmx', 'air_temperature_tmmn',\n    'potential_evapotranspiration', 'relative_humidity_rmax', 'relative_humidity_rmin',\n    'mean_vapor_pressure_deficit', 'wind_speed', 'snodas_mask', 'water_year'\n]\n\ntime_series_cols = [\n    f\"{col}_{i}\" for col in [\n        \"SWE\", \"air_temperature_tmmx\", \"air_temperature_tmmn\", \"potential_evapotranspiration\",\n        \"relative_humidity_rmax\", \"relative_humidity_rmin\", \"mean_vapor_pressure_deficit\",\n        \"precipitation_amount\", \"wind_speed\", \"fsca\"\n    ] for i in range(1, 8)\n]\n\ncumulative_cols = [\n    \"cumulative_SWE\", \"cumulative_air_temperature_tmmn\", \"cumulative_air_temperature_tmmx\",\n    \"cumulative_potential_evapotranspiration\", \"cumulative_mean_vapor_pressure_deficit\",\n    \"cumulative_precipitation_amount\", \"cumulative_relative_humidity_rmax\",\n    \"cumulative_relative_humidity_rmin\", \"cumulative_wind_speed\", \"cumulative_fsca\"\n]\n\nintended_columns = base_columns + time_series_cols + cumulative_cols\n\n# 🔍 Preview to detect available columns\nprint(\"🔍 Checking available columns in CSV...\")\npreview = pd.read_csv(file_path, nrows=1)\navailable_columns = set(preview.columns)\nuseful_columns = [col for col in intended_columns if col in available_columns]\n\nmissing_columns = [col for col in intended_columns if col not in available_columns]\nif missing_columns:\n    print(f\"⚠️ Missing columns skipped: {missing_columns}\")\n\n# 🧠 Load and clean data\nprint(\"\\n📥 Loading dataset...\")\ndf_list = []\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\n    if 'date' in chunk.columns:\n        chunk['date'] = pd.to_datetime(chunk['date'], errors='coerce')\n        chunk = chunk.dropna(subset=['date'])\n        chunk['day_of_year'] = chunk['date'].dt.dayofyear\n    else:\n        chunk['day_of_year'] = 1  # default value to avoid errors\n\n    if 'swe_value' in chunk.columns:\n        chunk = chunk[(chunk['swe_value'] >= 0) & (chunk['swe_value'] < 3000)]\n\n    df_list.append(chunk)\n\ndata = pd.concat(df_list, ignore_index=True)\ndata = data.drop_duplicates()\nprint(f\"✅ Loaded shape: {data.shape}\")\n\n# 🧮 Spatial & temporal binning\ngrid_size = 0.2\nnum_time_bins = 24\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\ndata['dayofyear_bin'] = (data['day_of_year'] // (365 / num_time_bins)).astype(int)\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['dayofyear_bin'].astype(str)\n\n# 📦 Aggregation\nagg_cols = {col: 'mean' for col in useful_columns if col not in ['date', 'lat', 'lon']}\nagg_cols.update({'lat': 'mean', 'lon': 'mean'})\nif 'date' in useful_columns:\n    agg_cols['date'] = 'first'\nagg_cols['day_of_year'] = 'mean'\n\nmerged_nodes = data.groupby('grid_id').agg(agg_cols).reset_index()\n\n# ⏳ Temporal encoding\nif 'day_of_year' in merged_nodes.columns:\n    merged_nodes['sin_doy'] = np.sin(2 * np.pi * merged_nodes['day_of_year'] / 365)\n    merged_nodes['cos_doy'] = np.cos(2 * np.pi * merged_nodes['day_of_year'] / 365)\n\n# 🧪 Feature columns\nfeature_columns = [col for col in merged_nodes.columns if col not in ['grid_id', 'lat', 'lon', 'date', 'day_of_year']]\n\n\n# print(feature_columns)\n\n# ⚙️ Normalize\nscaler = StandardScaler()\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\nnode_features = torch.tensor(node_features, dtype=torch.float)\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float) if 'swe_value' in merged_nodes.columns else torch.zeros(len(merged_nodes))\n\n# 🌍 KDTree for edges\ncoordinates = merged_nodes[['lat', 'lon']].values\ntree = KDTree(coordinates)\nk = 6\nthreshold = 0.5\ndistances, neighbors = tree.query(coordinates, k=k+1)\n\nedge_list = []\nfor i in range(len(neighbors)):\n    for j in range(1, k+1):\n        if distances[i][j] < threshold:\n            edge_list.append((i, neighbors[i][j]))\n\nedges = np.array(edge_list).T\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.size > 0 else torch.empty((2, 0), dtype=torch.long)\n\n# 🧵 Graph Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\ngraph_data.dates = merged_nodes['date'].astype(str).tolist() if 'date' in merged_nodes.columns else []\n\n# 📊 Stats\nprint(\"\\n📊 Graph Summary Report\")\nprint(f\"🔹 Nodes: {graph_data.num_nodes}\")\nprint(f\"🔹 Edges: {graph_data.num_edges}\")\nprint(f\"🔹 Feature Shape: {graph_data.x.shape}\")\nprint(f\"🔹 Edge Shape: {graph_data.edge_index.shape}\")\n\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\nprint(f\"   • Min Degree : {deg.min().item()}\")\nprint(f\"   • Max Degree : {deg.max().item()}\")\nprint(f\"   • Mean Degree: {deg.float().mean():.2f}\")\nprint(f\"   • Std Degree : {deg.float().std():.2f}\")\nprint(f\"   • Isolated   : {(deg == 0).sum().item()} nodes ({(deg == 0).sum().item() / graph_data.num_nodes * 100:.2f}%)\")\n\n# 💾 Save\nsave_path = \"/media/volume1/gnn_graph_pinn_data_new_3.pt\"\ntorch.save(graph_data, save_path)\nprint(f\"\\n✅ Graph data saved at: {save_path}\")\n",
  "history_output" : "🔍 Checking available columns in CSV...\n⚠️ Missing columns skipped: ['snodas_mask']\n📥 Loading dataset...\n✅ Loaded shape: (6219212, 100)\n📊 Graph Summary Report\n🔹 Nodes: 77476\n🔹 Edges: 464856\n🔹 Feature Shape: torch.Size([77476, 98])\n🔹 Edge Shape: torch.Size([2, 464856])\n   • Min Degree : 6.0\n   • Max Degree : 6.0\n   • Mean Degree: 6.00\n   • Std Degree : 0.00\n   • Isolated   : 0 nodes (0.00%)\n✅ Graph data saved at: /media/volume1/gnn_graph_pinn_data_new_3.pt\n",
  "history_begin_time" : 1745506947835,
  "history_end_time" : 1745507055745,
  "history_notes" : null,
  "history_process" : "p7ar4i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "MAHomv0R6GZK",
  "history_input" : "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\nimport pandas as pd\nimport torch\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nfrom torch_geometric.utils import degree\n\n# 📍 File path\nfile_path = '/media/volume1/all_points_final_merged_training.csv'\nchunksize = 500000\n\n# 📋 Intended columns (used if they exist)\nbase_columns = [\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness',\n    'SWE', 'fsca', 'air_temperature_tmmx', 'air_temperature_tmmn',\n    'potential_evapotranspiration', 'relative_humidity_rmax', 'relative_humidity_rmin',\n    'mean_vapor_pressure_deficit', 'wind_speed', 'snodas_mask', 'water_year'\n]\n\ntime_series_cols = [\n    f\"{col}_{i}\" for col in [\n        \"SWE\", \"air_temperature_tmmx\", \"air_temperature_tmmn\", \"potential_evapotranspiration\",\n        \"relative_humidity_rmax\", \"relative_humidity_rmin\", \"mean_vapor_pressure_deficit\",\n        \"precipitation_amount\", \"wind_speed\", \"fsca\"\n    ] for i in range(1, 8)\n]\n\ncumulative_cols = [\n    \"cumulative_SWE\", \"cumulative_air_temperature_tmmn\", \"cumulative_air_temperature_tmmx\",\n    \"cumulative_potential_evapotranspiration\", \"cumulative_mean_vapor_pressure_deficit\",\n    \"cumulative_precipitation_amount\", \"cumulative_relative_humidity_rmax\",\n    \"cumulative_relative_humidity_rmin\", \"cumulative_wind_speed\", \"cumulative_fsca\"\n]\n\nintended_columns = base_columns + time_series_cols + cumulative_cols\n\n# 🔍 Preview to detect available columns\nprint(\"🔍 Checking available columns in CSV...\")\npreview = pd.read_csv(file_path, nrows=1)\navailable_columns = set(preview.columns)\nuseful_columns = [col for col in intended_columns if col in available_columns]\n\nmissing_columns = [col for col in intended_columns if col not in available_columns]\nif missing_columns:\n    print(f\"⚠️ Missing columns skipped: {missing_columns}\")\n\n# 🧠 Load and clean data\nprint(\"\\n📥 Loading dataset...\")\ndf_list = []\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\n    if 'date' in chunk.columns:\n        chunk['date'] = pd.to_datetime(chunk['date'], errors='coerce')\n        chunk = chunk.dropna(subset=['date'])\n        chunk['day_of_year'] = chunk['date'].dt.dayofyear\n    else:\n        chunk['day_of_year'] = 1  # default value to avoid errors\n\n    if 'swe_value' in chunk.columns:\n        chunk = chunk[(chunk['swe_value'] >= 0) & (chunk['swe_value'] < 3000)]\n\n    df_list.append(chunk)\n\ndata = pd.concat(df_list, ignore_index=True)\ndata = data.drop_duplicates()\nprint(f\"✅ Loaded shape: {data.shape}\")\n\n# 🧮 Spatial & temporal binning\ngrid_size = 0.2\nnum_time_bins = 24\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\ndata['dayofyear_bin'] = (data['day_of_year'] // (365 / num_time_bins)).astype(int)\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['dayofyear_bin'].astype(str)\n\n# 📦 Aggregation\nagg_cols = {col: 'mean' for col in useful_columns if col not in ['date', 'lat', 'lon']}\nagg_cols.update({'lat': 'mean', 'lon': 'mean'})\nif 'date' in useful_columns:\n    agg_cols['date'] = 'first'\nagg_cols['day_of_year'] = 'mean'\n\nmerged_nodes = data.groupby('grid_id').agg(agg_cols).reset_index()\n\n# ⏳ Temporal encoding\nif 'day_of_year' in merged_nodes.columns:\n    merged_nodes['sin_doy'] = np.sin(2 * np.pi * merged_nodes['day_of_year'] / 365)\n    merged_nodes['cos_doy'] = np.cos(2 * np.pi * merged_nodes['day_of_year'] / 365)\n\n# 🧪 Feature columns\nfeature_columns = [col for col in merged_nodes.columns if col not in ['grid_id', 'swe_value', 'lat', 'lon', 'date', 'day_of_year']]\n\n# print(feature_columns)\n\n# ⚙️ Normalize\nscaler = StandardScaler()\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\nnode_features = torch.tensor(node_features, dtype=torch.float)\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float) if 'swe_value' in merged_nodes.columns else torch.zeros(len(merged_nodes))\n\n# 🌍 KDTree for edges\ncoordinates = merged_nodes[['lat', 'lon']].values\ntree = KDTree(coordinates)\nk = 6\nthreshold = 0.5\ndistances, neighbors = tree.query(coordinates, k=k+1)\n\nedge_list = []\nfor i in range(len(neighbors)):\n    for j in range(1, k+1):\n        if distances[i][j] < threshold:\n            edge_list.append((i, neighbors[i][j]))\n\nedges = np.array(edge_list).T\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.size > 0 else torch.empty((2, 0), dtype=torch.long)\n\n# 🧵 Graph Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\ngraph_data.dates = merged_nodes['date'].astype(str).tolist() if 'date' in merged_nodes.columns else []\n\n# 📊 Stats\nprint(\"\\n📊 Graph Summary Report\")\nprint(f\"🔹 Nodes: {graph_data.num_nodes}\")\nprint(f\"🔹 Edges: {graph_data.num_edges}\")\nprint(f\"🔹 Feature Shape: {graph_data.x.shape}\")\nprint(f\"🔹 Edge Shape: {graph_data.edge_index.shape}\")\n\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\nprint(f\"   • Min Degree : {deg.min().item()}\")\nprint(f\"   • Max Degree : {deg.max().item()}\")\nprint(f\"   • Mean Degree: {deg.float().mean():.2f}\")\nprint(f\"   • Std Degree : {deg.float().std():.2f}\")\nprint(f\"   • Isolated   : {(deg == 0).sum().item()} nodes ({(deg == 0).sum().item() / graph_data.num_nodes * 100:.2f}%)\")\n\n# 💾 Save\nsave_path = \"/media/volume1/gnn_graph_pinn_data_new_2.pt\"\ntorch.save(graph_data, save_path)\nprint(f\"\\n✅ Graph data saved at: {save_path}\")\n",
  "history_output" : "🔍 Checking available columns in CSV...\n⚠️ Missing columns skipped: ['snodas_mask']\n📥 Loading dataset...\n✅ Loaded shape: (6219212, 100)\n📊 Graph Summary Report\n🔹 Nodes: 77476\n🔹 Edges: 464856\n🔹 Feature Shape: torch.Size([77476, 98])\n🔹 Edge Shape: torch.Size([2, 464856])\n   • Min Degree : 6.0\n   • Max Degree : 6.0\n   • Mean Degree: 6.00\n   • Std Degree : 0.00\n   • Isolated   : 0 nodes (0.00%)\n✅ Graph data saved at: /media/volume1/gnn_graph_pinn_data_new_2.pt\n",
  "history_begin_time" : 1745506380846,
  "history_end_time" : 1745506488799,
  "history_notes" : null,
  "history_process" : "p7ar4i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "owfQ2DkHtzrY",
  "history_input" : "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\nimport pandas as pd\nimport torch\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nfrom torch_geometric.utils import degree\n\n# 📍 File path\nfile_path = '/media/volume1/all_points_final_merged_training.csv'\nchunksize = 500000\n\n# 📋 Intended columns (used if they exist)\nbase_columns = [\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness',\n    'SWE', 'fsca', 'air_temperature_tmmx', 'air_temperature_tmmn',\n    'potential_evapotranspiration', 'relative_humidity_rmax', 'relative_humidity_rmin',\n    'mean_vapor_pressure_deficit', 'wind_speed', 'snodas_mask', 'water_year'\n]\n\ntime_series_cols = [\n    f\"{col}_{i}\" for col in [\n        \"SWE\", \"air_temperature_tmmx\", \"air_temperature_tmmn\", \"potential_evapotranspiration\",\n        \"relative_humidity_rmax\", \"relative_humidity_rmin\", \"mean_vapor_pressure_deficit\",\n        \"precipitation_amount\", \"wind_speed\", \"fsca\"\n    ] for i in range(1, 8)\n]\n\ncumulative_cols = [\n    \"cumulative_SWE\", \"cumulative_air_temperature_tmmn\", \"cumulative_air_temperature_tmmx\",\n    \"cumulative_potential_evapotranspiration\", \"cumulative_mean_vapor_pressure_deficit\",\n    \"cumulative_precipitation_amount\", \"cumulative_relative_humidity_rmax\",\n    \"cumulative_relative_humidity_rmin\", \"cumulative_wind_speed\", \"cumulative_fsca\"\n]\n\nintended_columns = base_columns + time_series_cols + cumulative_cols\n\n# 🔍 Preview to detect available columns\nprint(\"🔍 Checking available columns in CSV...\")\npreview = pd.read_csv(file_path, nrows=1)\navailable_columns = set(preview.columns)\nuseful_columns = [col for col in intended_columns if col in available_columns]\n\nmissing_columns = [col for col in intended_columns if col not in available_columns]\nif missing_columns:\n    print(f\"⚠️ Missing columns skipped: {missing_columns}\")\n\n# 🧠 Load and clean data\nprint(\"\\n📥 Loading dataset...\")\ndf_list = []\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\n    if 'date' in chunk.columns:\n        chunk['date'] = pd.to_datetime(chunk['date'], errors='coerce')\n        chunk = chunk.dropna(subset=['date'])\n        chunk['day_of_year'] = chunk['date'].dt.dayofyear\n    else:\n        chunk['day_of_year'] = 1  # default value to avoid errors\n\n    if 'swe_value' in chunk.columns:\n        chunk = chunk[(chunk['swe_value'] >= 0) & (chunk['swe_value'] < 3000)]\n\n    df_list.append(chunk)\n\ndata = pd.concat(df_list, ignore_index=True)\ndata = data.drop_duplicates()\nprint(f\"✅ Loaded shape: {data.shape}\")\n\n# 🧮 Spatial & temporal binning\ngrid_size = 0.2\nnum_time_bins = 24\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\ndata['dayofyear_bin'] = (data['day_of_year'] // (365 / num_time_bins)).astype(int)\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['dayofyear_bin'].astype(str)\n\n# 📦 Aggregation\nagg_cols = {col: 'mean' for col in useful_columns if col not in ['date', 'lat', 'lon']}\nagg_cols.update({'lat': 'mean', 'lon': 'mean'})\nif 'date' in useful_columns:\n    agg_cols['date'] = 'first'\nagg_cols['day_of_year'] = 'mean'\n\nmerged_nodes = data.groupby('grid_id').agg(agg_cols).reset_index()\n\n# ⏳ Temporal encoding\nif 'day_of_year' in merged_nodes.columns:\n    merged_nodes['sin_doy'] = np.sin(2 * np.pi * merged_nodes['day_of_year'] / 365)\n    merged_nodes['cos_doy'] = np.cos(2 * np.pi * merged_nodes['day_of_year'] / 365)\n\n# 🧪 Feature columns\nfeature_columns = [col for col in merged_nodes.columns if col not in ['grid_id', 'swe_value', 'lat', 'lon', 'date', 'day_of_year']]\n\nprint(feature_columns)\n\n# ⚙️ Normalize\nscaler = StandardScaler()\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\nnode_features = torch.tensor(node_features, dtype=torch.float)\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float) if 'swe_value' in merged_nodes.columns else torch.zeros(len(merged_nodes))\n\n# 🌍 KDTree for edges\ncoordinates = merged_nodes[['lat', 'lon']].values\ntree = KDTree(coordinates)\nk = 6\nthreshold = 0.5\ndistances, neighbors = tree.query(coordinates, k=k+1)\n\nedge_list = []\nfor i in range(len(neighbors)):\n    for j in range(1, k+1):\n        if distances[i][j] < threshold:\n            edge_list.append((i, neighbors[i][j]))\n\nedges = np.array(edge_list).T\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.size > 0 else torch.empty((2, 0), dtype=torch.long)\n\n# 🧵 Graph Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\ngraph_data.dates = merged_nodes['date'].astype(str).tolist() if 'date' in merged_nodes.columns else []\n\n# 📊 Stats\nprint(\"\\n📊 Graph Summary Report\")\nprint(f\"🔹 Nodes: {graph_data.num_nodes}\")\nprint(f\"🔹 Edges: {graph_data.num_edges}\")\nprint(f\"🔹 Feature Shape: {graph_data.x.shape}\")\nprint(f\"🔹 Edge Shape: {graph_data.edge_index.shape}\")\n\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\nprint(f\"   • Min Degree : {deg.min().item()}\")\nprint(f\"   • Max Degree : {deg.max().item()}\")\nprint(f\"   • Mean Degree: {deg.float().mean():.2f}\")\nprint(f\"   • Std Degree : {deg.float().std():.2f}\")\nprint(f\"   • Isolated   : {(deg == 0).sum().item()} nodes ({(deg == 0).sum().item() / graph_data.num_nodes * 100:.2f}%)\")\n\n# 💾 Save\n# save_path = \"/media/volume1/gnn_graph_pinn_data_new.pt\"\n# torch.save(graph_data, save_path)\n# print(f\"\\n✅ Graph data saved at: {save_path}\")\n",
  "history_output" : "🔍 Checking available columns in CSV...\n⚠️ Missing columns skipped: ['snodas_mask']\n📥 Loading dataset...\n✅ Loaded shape: (6219212, 100)\n['Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness', 'SWE', 'fsca', 'air_temperature_tmmx', 'air_temperature_tmmn', 'potential_evapotranspiration', 'relative_humidity_rmax', 'relative_humidity_rmin', 'mean_vapor_pressure_deficit', 'wind_speed', 'water_year', 'SWE_1', 'SWE_2', 'SWE_3', 'SWE_4', 'SWE_5', 'SWE_6', 'SWE_7', 'air_temperature_tmmx_1', 'air_temperature_tmmx_2', 'air_temperature_tmmx_3', 'air_temperature_tmmx_4', 'air_temperature_tmmx_5', 'air_temperature_tmmx_6', 'air_temperature_tmmx_7', 'air_temperature_tmmn_1', 'air_temperature_tmmn_2', 'air_temperature_tmmn_3', 'air_temperature_tmmn_4', 'air_temperature_tmmn_5', 'air_temperature_tmmn_6', 'air_temperature_tmmn_7', 'potential_evapotranspiration_1', 'potential_evapotranspiration_2', 'potential_evapotranspiration_3', 'potential_evapotranspiration_4', 'potential_evapotranspiration_5', 'potential_evapotranspiration_6', 'potential_evapotranspiration_7', 'relative_humidity_rmax_1', 'relative_humidity_rmax_2', 'relative_humidity_rmax_3', 'relative_humidity_rmax_4', 'relative_humidity_rmax_5', 'relative_humidity_rmax_6', 'relative_humidity_rmax_7', 'relative_humidity_rmin_1', 'relative_humidity_rmin_2', 'relative_humidity_rmin_3', 'relative_humidity_rmin_4', 'relative_humidity_rmin_5', 'relative_humidity_rmin_6', 'relative_humidity_rmin_7', 'mean_vapor_pressure_deficit_1', 'mean_vapor_pressure_deficit_2', 'mean_vapor_pressure_deficit_3', 'mean_vapor_pressure_deficit_4', 'mean_vapor_pressure_deficit_5', 'mean_vapor_pressure_deficit_6', 'mean_vapor_pressure_deficit_7', 'precipitation_amount_1', 'precipitation_amount_2', 'precipitation_amount_3', 'precipitation_amount_4', 'precipitation_amount_5', 'precipitation_amount_6', 'precipitation_amount_7', 'wind_speed_1', 'wind_speed_2', 'wind_speed_3', 'wind_speed_4', 'wind_speed_5', 'wind_speed_6', 'wind_speed_7', 'fsca_1', 'fsca_2', 'fsca_3', 'fsca_4', 'fsca_5', 'fsca_6', 'fsca_7', 'cumulative_SWE', 'cumulative_air_temperature_tmmn', 'cumulative_air_temperature_tmmx', 'cumulative_potential_evapotranspiration', 'cumulative_mean_vapor_pressure_deficit', 'cumulative_precipitation_amount', 'cumulative_relative_humidity_rmax', 'cumulative_relative_humidity_rmin', 'cumulative_wind_speed', 'cumulative_fsca', 'sin_doy', 'cos_doy']\n📊 Graph Summary Report\n🔹 Nodes: 77476\n🔹 Edges: 464856\n🔹 Feature Shape: torch.Size([77476, 98])\n🔹 Edge Shape: torch.Size([2, 464856])\n   • Min Degree : 6.0\n   • Max Degree : 6.0\n   • Mean Degree: 6.00\n   • Std Degree : 0.00\n   • Isolated   : 0 nodes (0.00%)\n",
  "history_begin_time" : 1745505123724,
  "history_end_time" : 1745505232066,
  "history_notes" : null,
  "history_process" : "p7ar4i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "qeQ7nW4VjDgY",
  "history_input" : "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\nimport pandas as pd\nimport torch\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nfrom torch_geometric.utils import degree\n\n# 📍 File path\nfile_path = '/media/volume1/all_points_final_merged_training.csv'\nchunksize = 500000\n\n# 📋 Intended columns (used if they exist)\nbase_columns = [\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness',\n    'SWE', 'fsca', 'air_temperature_tmmx', 'air_temperature_tmmn',\n    'potential_evapotranspiration', 'relative_humidity_rmax', 'relative_humidity_rmin',\n    'mean_vapor_pressure_deficit', 'wind_speed', 'snodas_mask', 'water_year'\n]\n\ntime_series_cols = [\n    f\"{col}_{i}\" for col in [\n        \"SWE\", \"air_temperature_tmmx\", \"air_temperature_tmmn\", \"potential_evapotranspiration\",\n        \"relative_humidity_rmax\", \"relative_humidity_rmin\", \"mean_vapor_pressure_deficit\",\n        \"precipitation_amount\", \"wind_speed\", \"fsca\"\n    ] for i in range(1, 8)\n]\n\ncumulative_cols = [\n    \"cumulative_SWE\", \"cumulative_air_temperature_tmmn\", \"cumulative_air_temperature_tmmx\",\n    \"cumulative_potential_evapotranspiration\", \"cumulative_mean_vapor_pressure_deficit\",\n    \"cumulative_precipitation_amount\", \"cumulative_relative_humidity_rmax\",\n    \"cumulative_relative_humidity_rmin\", \"cumulative_wind_speed\", \"cumulative_fsca\"\n]\n\nintended_columns = base_columns + time_series_cols + cumulative_cols\n\n# 🔍 Preview to detect available columns\nprint(\"🔍 Checking available columns in CSV...\")\npreview = pd.read_csv(file_path, nrows=1)\navailable_columns = set(preview.columns)\nuseful_columns = [col for col in intended_columns if col in available_columns]\n\nmissing_columns = [col for col in intended_columns if col not in available_columns]\nif missing_columns:\n    print(f\"⚠️ Missing columns skipped: {missing_columns}\")\n\n# 🧠 Load and clean data\nprint(\"\\n📥 Loading dataset...\")\ndf_list = []\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\n    if 'date' in chunk.columns:\n        chunk['date'] = pd.to_datetime(chunk['date'], errors='coerce')\n        chunk = chunk.dropna(subset=['date'])\n        chunk['day_of_year'] = chunk['date'].dt.dayofyear\n    else:\n        chunk['day_of_year'] = 1  # default value to avoid errors\n\n    if 'swe_value' in chunk.columns:\n        chunk = chunk[(chunk['swe_value'] >= 0) & (chunk['swe_value'] < 3000)]\n\n    df_list.append(chunk)\n\ndata = pd.concat(df_list, ignore_index=True)\ndata = data.drop_duplicates()\nprint(f\"✅ Loaded shape: {data.shape}\")\n\n# 🧮 Spatial & temporal binning\ngrid_size = 0.2\nnum_time_bins = 24\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\ndata['dayofyear_bin'] = (data['day_of_year'] // (365 / num_time_bins)).astype(int)\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['dayofyear_bin'].astype(str)\n\n# 📦 Aggregation\nagg_cols = {col: 'mean' for col in useful_columns if col not in ['date', 'lat', 'lon']}\nagg_cols.update({'lat': 'mean', 'lon': 'mean'})\nif 'date' in useful_columns:\n    agg_cols['date'] = 'first'\nagg_cols['day_of_year'] = 'mean'\n\nmerged_nodes = data.groupby('grid_id').agg(agg_cols).reset_index()\n\n# ⏳ Temporal encoding\nif 'day_of_year' in merged_nodes.columns:\n    merged_nodes['sin_doy'] = np.sin(2 * np.pi * merged_nodes['day_of_year'] / 365)\n    merged_nodes['cos_doy'] = np.cos(2 * np.pi * merged_nodes['day_of_year'] / 365)\n\n# 🧪 Feature columns\nfeature_columns = [col for col in merged_nodes.columns if col not in ['grid_id', 'swe_value', 'lat', 'lon', 'date', 'day_of_year']]\n\n# ⚙️ Normalize\nscaler = StandardScaler()\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\nnode_features = torch.tensor(node_features, dtype=torch.float)\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float) if 'swe_value' in merged_nodes.columns else torch.zeros(len(merged_nodes))\n\n# 🌍 KDTree for edges\ncoordinates = merged_nodes[['lat', 'lon']].values\ntree = KDTree(coordinates)\nk = 6\nthreshold = 0.5\ndistances, neighbors = tree.query(coordinates, k=k+1)\n\nedge_list = []\nfor i in range(len(neighbors)):\n    for j in range(1, k+1):\n        if distances[i][j] < threshold:\n            edge_list.append((i, neighbors[i][j]))\n\nedges = np.array(edge_list).T\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.size > 0 else torch.empty((2, 0), dtype=torch.long)\n\n# 🧵 Graph Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\ngraph_data.dates = merged_nodes['date'].astype(str).tolist() if 'date' in merged_nodes.columns else []\n\n# 📊 Stats\nprint(\"\\n📊 Graph Summary Report\")\nprint(f\"🔹 Nodes: {graph_data.num_nodes}\")\nprint(f\"🔹 Edges: {graph_data.num_edges}\")\nprint(f\"🔹 Feature Shape: {graph_data.x.shape}\")\nprint(f\"🔹 Edge Shape: {graph_data.edge_index.shape}\")\n\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\nprint(f\"   • Min Degree : {deg.min().item()}\")\nprint(f\"   • Max Degree : {deg.max().item()}\")\nprint(f\"   • Mean Degree: {deg.float().mean():.2f}\")\nprint(f\"   • Std Degree : {deg.float().std():.2f}\")\nprint(f\"   • Isolated   : {(deg == 0).sum().item()} nodes ({(deg == 0).sum().item() / graph_data.num_nodes * 100:.2f}%)\")\n\n# 💾 Save\nsave_path = \"/media/volume1/gnn_graph_pinn_data_new.pt\"\ntorch.save(graph_data, save_path)\nprint(f\"\\n✅ Graph data saved at: {save_path}\")\n",
  "history_output" : "🔍 Checking available columns in CSV...\n⚠️ Missing columns skipped: ['snodas_mask']\n📥 Loading dataset...\n✅ Loaded shape: (6219212, 100)\n📊 Graph Summary Report\n🔹 Nodes: 77476\n🔹 Edges: 464856\n🔹 Feature Shape: torch.Size([77476, 98])\n🔹 Edge Shape: torch.Size([2, 464856])\n   • Min Degree : 6.0\n   • Max Degree : 6.0\n   • Mean Degree: 6.00\n   • Std Degree : 0.00\n   • Isolated   : 0 nodes (0.00%)\n✅ Graph data saved at: /media/volume1/gnn_graph_pinn_data_new.pt\n",
  "history_begin_time" : 1745504805392,
  "history_end_time" : 1745504926588,
  "history_notes" : null,
  "history_process" : "p7ar4i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "iJdvkkokB3kz",
  "history_input" : "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\nimport pandas as pd\nimport torch\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nfrom torch_geometric.utils import degree\n\n# 📍 File path\nfile_path = '/media/volume1/all_points_final_merged_training.csv'\nchunksize = 500000\n\n# 📋 Intended columns (used if they exist)\nbase_columns = [\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness',\n    'SWE', 'swe_value', 'fsca', 'air_temperature_tmmx', 'air_temperature_tmmn',\n    'potential_evapotranspiration', 'relative_humidity_rmax', 'relative_humidity_rmin',\n    'mean_vapor_pressure_deficit', 'wind_speed', 'snodas_mask', 'water_year'\n]\n\ntime_series_cols = [\n    f\"{col}_{i}\" for col in [\n        \"SWE\", \"air_temperature_tmmx\", \"air_temperature_tmmn\", \"potential_evapotranspiration\",\n        \"relative_humidity_rmax\", \"relative_humidity_rmin\", \"mean_vapor_pressure_deficit\",\n        \"precipitation_amount\", \"wind_speed\", \"fsca\"\n    ] for i in range(1, 8)\n]\n\ncumulative_cols = [\n    \"cumulative_SWE\", \"cumulative_air_temperature_tmmn\", \"cumulative_air_temperature_tmmx\",\n    \"cumulative_potential_evapotranspiration\", \"cumulative_mean_vapor_pressure_deficit\",\n    \"cumulative_precipitation_amount\", \"cumulative_relative_humidity_rmax\",\n    \"cumulative_relative_humidity_rmin\", \"cumulative_wind_speed\", \"cumulative_fsca\"\n]\n\nintended_columns = base_columns + time_series_cols + cumulative_cols\n\n# 🔍 Preview to detect available columns\nprint(\"🔍 Checking available columns in CSV...\")\npreview = pd.read_csv(file_path, nrows=1)\navailable_columns = set(preview.columns)\nuseful_columns = [col for col in intended_columns if col in available_columns]\n\nmissing_columns = [col for col in intended_columns if col not in available_columns]\nif missing_columns:\n    print(f\"⚠️ Missing columns skipped: {missing_columns}\")\n\n# 🧠 Load and clean data\nprint(\"\\n📥 Loading dataset...\")\ndf_list = []\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\n    if 'date' in chunk.columns:\n        chunk['date'] = pd.to_datetime(chunk['date'], errors='coerce')\n        chunk = chunk.dropna(subset=['date'])\n        chunk['day_of_year'] = chunk['date'].dt.dayofyear\n    else:\n        chunk['day_of_year'] = 1  # default value to avoid errors\n\n    if 'swe_value' in chunk.columns:\n        chunk = chunk[(chunk['swe_value'] >= 0) & (chunk['swe_value'] < 3000)]\n\n    df_list.append(chunk)\n\ndata = pd.concat(df_list, ignore_index=True)\ndata = data.drop_duplicates()\nprint(f\"✅ Loaded shape: {data.shape}\")\n\n# 🧮 Spatial & temporal binning\ngrid_size = 0.2\nnum_time_bins = 24\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\ndata['dayofyear_bin'] = (data['day_of_year'] // (365 / num_time_bins)).astype(int)\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['dayofyear_bin'].astype(str)\n\n# 📦 Aggregation\nagg_cols = {col: 'mean' for col in useful_columns if col not in ['date', 'lat', 'lon']}\nagg_cols.update({'lat': 'mean', 'lon': 'mean'})\nif 'date' in useful_columns:\n    agg_cols['date'] = 'first'\nagg_cols['day_of_year'] = 'mean'\n\nmerged_nodes = data.groupby('grid_id').agg(agg_cols).reset_index()\n\n# ⏳ Temporal encoding\nif 'day_of_year' in merged_nodes.columns:\n    merged_nodes['sin_doy'] = np.sin(2 * np.pi * merged_nodes['day_of_year'] / 365)\n    merged_nodes['cos_doy'] = np.cos(2 * np.pi * merged_nodes['day_of_year'] / 365)\n\n# 🧪 Feature columns\nfeature_columns = [col for col in merged_nodes.columns if col not in ['grid_id', 'swe_value', 'lat', 'lon', 'date', 'day_of_year']]\n\n# ⚙️ Normalize\nscaler = StandardScaler()\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\nnode_features = torch.tensor(node_features, dtype=torch.float)\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float) if 'swe_value' in merged_nodes.columns else torch.zeros(len(merged_nodes))\n\n# 🌍 KDTree for edges\ncoordinates = merged_nodes[['lat', 'lon']].values\ntree = KDTree(coordinates)\nk = 6\nthreshold = 0.5\ndistances, neighbors = tree.query(coordinates, k=k+1)\n\nedge_list = []\nfor i in range(len(neighbors)):\n    for j in range(1, k+1):\n        if distances[i][j] < threshold:\n            edge_list.append((i, neighbors[i][j]))\n\nedges = np.array(edge_list).T\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.size > 0 else torch.empty((2, 0), dtype=torch.long)\n\n# 🧵 Graph Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\ngraph_data.dates = merged_nodes['date'].astype(str).tolist() if 'date' in merged_nodes.columns else []\n\n# 📊 Stats\nprint(\"\\n📊 Graph Summary Report\")\nprint(f\"🔹 Nodes: {graph_data.num_nodes}\")\nprint(f\"🔹 Edges: {graph_data.num_edges}\")\nprint(f\"🔹 Feature Shape: {graph_data.x.shape}\")\nprint(f\"🔹 Edge Shape: {graph_data.edge_index.shape}\")\n\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\nprint(f\"   • Min Degree : {deg.min().item()}\")\nprint(f\"   • Max Degree : {deg.max().item()}\")\nprint(f\"   • Mean Degree: {deg.float().mean():.2f}\")\nprint(f\"   • Std Degree : {deg.float().std():.2f}\")\nprint(f\"   • Isolated   : {(deg == 0).sum().item()} nodes ({(deg == 0).sum().item() / graph_data.num_nodes * 100:.2f}%)\")\n\n# 💾 Save\nsave_path = \"/media/volume1/gnn_graph_pinn_data_new.pt\"\ntorch.save(graph_data, save_path)\nprint(f\"\\n✅ Graph data saved at: {save_path}\")\n",
  "history_output" : "🔍 Checking available columns in CSV...\n⚠️ Missing columns skipped: ['snodas_mask']\n📥 Loading dataset...\n✅ Loaded shape: (4218762, 101)\n📊 Graph Summary Report\n🔹 Nodes: 68557\n🔹 Edges: 411292\n🔹 Feature Shape: torch.Size([68557, 98])\n🔹 Edge Shape: torch.Size([2, 411292])\n   • Min Degree : 0.0\n   • Max Degree : 6.0\n   • Mean Degree: 6.00\n   • Std Degree : 0.06\n   • Isolated   : 1 nodes (0.00%)\n✅ Graph data saved at: /media/volume1/gnn_graph_pinn_data_new.pt\n",
  "history_begin_time" : 1745348893227,
  "history_end_time" : 1745348987265,
  "history_notes" : null,
  "history_process" : "p7ar4i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "sPP67BDcDRCk",
  "history_input" : "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\nimport pandas as pd\nimport torch\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nfrom torch_geometric.utils import degree\n\n# 📍 File path\nfile_path = '/media/volume1/all_points_final_merged_training.csv'\nchunksize = 500000\n\n# 📋 Intended columns (used if they exist)\nbase_columns = [\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness',\n    'SWE', 'swe_value', 'fsca', 'air_temperature_tmmx', 'air_temperature_tmmn',\n    'potential_evapotranspiration', 'relative_humidity_rmax', 'relative_humidity_rmin',\n    'mean_vapor_pressure_deficit', 'wind_speed', 'snodas_mask', 'water_year'\n]\n\ntime_series_cols = [\n    f\"{col}_{i}\" for col in [\n        \"SWE\", \"air_temperature_tmmx\", \"air_temperature_tmmn\", \"potential_evapotranspiration\",\n        \"relative_humidity_rmax\", \"relative_humidity_rmin\", \"mean_vapor_pressure_deficit\",\n        \"precipitation_amount\", \"wind_speed\", \"fsca\"\n    ] for i in range(1, 8)\n]\n\ncumulative_cols = [\n    \"cumulative_SWE\", \"cumulative_air_temperature_tmmn\", \"cumulative_air_temperature_tmmx\",\n    \"cumulative_potential_evapotranspiration\", \"cumulative_mean_vapor_pressure_deficit\",\n    \"cumulative_precipitation_amount\", \"cumulative_relative_humidity_rmax\",\n    \"cumulative_relative_humidity_rmin\", \"cumulative_wind_speed\", \"cumulative_fsca\"\n]\n\nintended_columns = base_columns + time_series_cols + cumulative_cols\n\n# 🔍 Preview to detect available columns\nprint(\"🔍 Checking available columns in CSV...\")\npreview = pd.read_csv(file_path, nrows=1)\navailable_columns = set(preview.columns)\nuseful_columns = [col for col in intended_columns if col in available_columns]\n\nmissing_columns = [col for col in intended_columns if col not in available_columns]\nif missing_columns:\n    print(f\"⚠️ Missing columns skipped: {missing_columns}\")\n\n# 🧠 Load and clean data\nprint(\"\\n📥 Loading dataset...\")\ndf_list = []\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\n    if 'date' in chunk.columns:\n        chunk['date'] = pd.to_datetime(chunk['date'], errors='coerce')\n        chunk = chunk.dropna(subset=['date'])\n        chunk['day_of_year'] = chunk['date'].dt.dayofyear\n    else:\n        chunk['day_of_year'] = 1  # default value to avoid errors\n\n    if 'swe_value' in chunk.columns:\n        chunk = chunk[(chunk['swe_value'] >= 0) & (chunk['swe_value'] < 3000)]\n\n    df_list.append(chunk)\n\ndata = pd.concat(df_list, ignore_index=True)\ndata = data.drop_duplicates()\nprint(f\"✅ Loaded shape: {data.shape}\")\n\n# 🧮 Spatial & temporal binning\ngrid_size = 0.2\nnum_time_bins = 24\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\ndata['dayofyear_bin'] = (data['day_of_year'] // (365 / num_time_bins)).astype(int)\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['dayofyear_bin'].astype(str)\n\n# 📦 Aggregation\nagg_cols = {col: 'mean' for col in useful_columns if col not in ['date', 'lat', 'lon']}\nagg_cols.update({'lat': 'mean', 'lon': 'mean'})\nif 'date' in useful_columns:\n    agg_cols['date'] = 'first'\nagg_cols['day_of_year'] = 'mean'\n\nmerged_nodes = data.groupby('grid_id').agg(agg_cols).reset_index()\n\n# ⏳ Temporal encoding\nif 'day_of_year' in merged_nodes.columns:\n    merged_nodes['sin_doy'] = np.sin(2 * np.pi * merged_nodes['day_of_year'] / 365)\n    merged_nodes['cos_doy'] = np.cos(2 * np.pi * merged_nodes['day_of_year'] / 365)\n\n# 🧪 Feature columns\nfeature_columns = [col for col in merged_nodes.columns if col not in ['grid_id', 'swe_value', 'lat', 'lon', 'date', 'day_of_year']]\n\n# ⚙️ Normalize\nscaler = StandardScaler()\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\nnode_features = torch.tensor(node_features, dtype=torch.float)\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float) if 'swe_value' in merged_nodes.columns else torch.zeros(len(merged_nodes))\n\n# 🌍 KDTree for edges\ncoordinates = merged_nodes[['lat', 'lon']].values\ntree = KDTree(coordinates)\nk = 6\nthreshold = 0.5\ndistances, neighbors = tree.query(coordinates, k=k+1)\n\nedge_list = []\nfor i in range(len(neighbors)):\n    for j in range(1, k+1):\n        if distances[i][j] < threshold:\n            edge_list.append((i, neighbors[i][j]))\n\nedges = np.array(edge_list).T\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.size > 0 else torch.empty((2, 0), dtype=torch.long)\n\n# 🧵 Graph Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\ngraph_data.dates = merged_nodes['date'].astype(str).tolist() if 'date' in merged_nodes.columns else []\n\n# 📊 Stats\nprint(\"\\n📊 Graph Summary Report\")\nprint(f\"🔹 Nodes: {graph_data.num_nodes}\")\nprint(f\"🔹 Edges: {graph_data.num_edges}\")\nprint(f\"🔹 Feature Shape: {graph_data.x.shape}\")\nprint(f\"🔹 Edge Shape: {graph_data.edge_index.shape}\")\n\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\nprint(f\"   • Min Degree : {deg.min().item()}\")\nprint(f\"   • Max Degree : {deg.max().item()}\")\nprint(f\"   • Mean Degree: {deg.float().mean():.2f}\")\nprint(f\"   • Std Degree : {deg.float().std():.2f}\")\nprint(f\"   • Isolated   : {(deg == 0).sum().item()} nodes ({(deg == 0).sum().item() / graph_data.num_nodes * 100:.2f}%)\")\n\n# 💾 Save\n# save_path = \"/media/volume1/gnn_graph_time_all_states_1.pt\"\n# torch.save(graph_data, save_path)\n# print(f\"\\n✅ Graph data saved at: {save_path}\")\n",
  "history_output" : "🔍 Checking available columns in CSV...\n⚠️ Missing columns skipped: ['snodas_mask']\n📥 Loading dataset...\n✅ Loaded shape: (4218762, 101)\n📊 Graph Summary Report\n🔹 Nodes: 68557\n🔹 Edges: 411292\n🔹 Feature Shape: torch.Size([68557, 98])\n🔹 Edge Shape: torch.Size([2, 411292])\n   • Min Degree : 0.0\n   • Max Degree : 6.0\n   • Mean Degree: 6.00\n   • Std Degree : 0.06\n   • Isolated   : 1 nodes (0.00%)\n",
  "history_begin_time" : 1745348764176,
  "history_end_time" : 1745348858315,
  "history_notes" : null,
  "history_process" : "p7ar4i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "5gyozrYBEgMu",
  "history_input" : "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\nimport pandas as pd\nimport torch\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nfrom torch_geometric.utils import degree\n\n# 📍 File path\nfile_path = '/media/volume1/all_points_final_merged_training_snodas_mask_resnet_all_batch.csv'\nchunksize = 500000\n\n# 📋 Intended columns (used if they exist)\nbase_columns = [\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness',\n    'SWE', 'swe_value', 'fsca', 'air_temperature_tmmx', 'air_temperature_tmmn',\n    'potential_evapotranspiration', 'relative_humidity_rmax', 'relative_humidity_rmin',\n    'mean_vapor_pressure_deficit', 'wind_speed', 'snodas_mask', 'water_year'\n]\n\ntime_series_cols = [\n    f\"{col}_{i}\" for col in [\n        \"SWE\", \"air_temperature_tmmx\", \"air_temperature_tmmn\", \"potential_evapotranspiration\",\n        \"relative_humidity_rmax\", \"relative_humidity_rmin\", \"mean_vapor_pressure_deficit\",\n        \"precipitation_amount\", \"wind_speed\", \"fsca\"\n    ] for i in range(1, 8)\n]\n\ncumulative_cols = [\n    \"cumulative_SWE\", \"cumulative_air_temperature_tmmn\", \"cumulative_air_temperature_tmmx\",\n    \"cumulative_potential_evapotranspiration\", \"cumulative_mean_vapor_pressure_deficit\",\n    \"cumulative_precipitation_amount\", \"cumulative_relative_humidity_rmax\",\n    \"cumulative_relative_humidity_rmin\", \"cumulative_wind_speed\", \"cumulative_fsca\"\n]\n\nintended_columns = base_columns + time_series_cols + cumulative_cols\n\n# 🔍 Preview to detect available columns\nprint(\"🔍 Checking available columns in CSV...\")\npreview = pd.read_csv(file_path, nrows=1)\navailable_columns = set(preview.columns)\nuseful_columns = [col for col in intended_columns if col in available_columns]\n\nmissing_columns = [col for col in intended_columns if col not in available_columns]\nif missing_columns:\n    print(f\"⚠️ Missing columns skipped: {missing_columns}\")\n\n# 🧠 Load and clean data\nprint(\"\\n📥 Loading dataset...\")\ndf_list = []\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\n    if 'date' in chunk.columns:\n        chunk['date'] = pd.to_datetime(chunk['date'], errors='coerce')\n        chunk = chunk.dropna(subset=['date'])\n        chunk['day_of_year'] = chunk['date'].dt.dayofyear\n    else:\n        chunk['day_of_year'] = 1  # default value to avoid errors\n\n    if 'swe_value' in chunk.columns:\n        chunk = chunk[(chunk['swe_value'] >= 0) & (chunk['swe_value'] < 3000)]\n\n    df_list.append(chunk)\n\ndata = pd.concat(df_list, ignore_index=True)\ndata = data.drop_duplicates()\nprint(f\"✅ Loaded shape: {data.shape}\")\n\n# 🧮 Spatial & temporal binning\ngrid_size = 0.2\nnum_time_bins = 24\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\ndata['dayofyear_bin'] = (data['day_of_year'] // (365 / num_time_bins)).astype(int)\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['dayofyear_bin'].astype(str)\n\n# 📦 Aggregation\nagg_cols = {col: 'mean' for col in useful_columns if col not in ['date', 'lat', 'lon']}\nagg_cols.update({'lat': 'mean', 'lon': 'mean'})\nif 'date' in useful_columns:\n    agg_cols['date'] = 'first'\nagg_cols['day_of_year'] = 'mean'\n\nmerged_nodes = data.groupby('grid_id').agg(agg_cols).reset_index()\n\n# ⏳ Temporal encoding\nif 'day_of_year' in merged_nodes.columns:\n    merged_nodes['sin_doy'] = np.sin(2 * np.pi * merged_nodes['day_of_year'] / 365)\n    merged_nodes['cos_doy'] = np.cos(2 * np.pi * merged_nodes['day_of_year'] / 365)\n\n# 🧪 Feature columns\nfeature_columns = [col for col in merged_nodes.columns if col not in ['grid_id', 'swe_value', 'lat', 'lon', 'date', 'day_of_year']]\n\n# ⚙️ Normalize\nscaler = StandardScaler()\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\nnode_features = torch.tensor(node_features, dtype=torch.float)\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float) if 'swe_value' in merged_nodes.columns else torch.zeros(len(merged_nodes))\n\n# 🌍 KDTree for edges\ncoordinates = merged_nodes[['lat', 'lon']].values\ntree = KDTree(coordinates)\nk = 6\nthreshold = 0.5\ndistances, neighbors = tree.query(coordinates, k=k+1)\n\nedge_list = []\nfor i in range(len(neighbors)):\n    for j in range(1, k+1):\n        if distances[i][j] < threshold:\n            edge_list.append((i, neighbors[i][j]))\n\nedges = np.array(edge_list).T\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.size > 0 else torch.empty((2, 0), dtype=torch.long)\n\n# 🧵 Graph Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\ngraph_data.dates = merged_nodes['date'].astype(str).tolist() if 'date' in merged_nodes.columns else []\n\n# 📊 Stats\nprint(\"\\n📊 Graph Summary Report\")\nprint(f\"🔹 Nodes: {graph_data.num_nodes}\")\nprint(f\"🔹 Edges: {graph_data.num_edges}\")\nprint(f\"🔹 Feature Shape: {graph_data.x.shape}\")\nprint(f\"🔹 Edge Shape: {graph_data.edge_index.shape}\")\n\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\nprint(f\"   • Min Degree : {deg.min().item()}\")\nprint(f\"   • Max Degree : {deg.max().item()}\")\nprint(f\"   • Mean Degree: {deg.float().mean():.2f}\")\nprint(f\"   • Std Degree : {deg.float().std():.2f}\")\nprint(f\"   • Isolated   : {(deg == 0).sum().item()} nodes ({(deg == 0).sum().item() / graph_data.num_nodes * 100:.2f}%)\")\n\n# 💾 Save\n# save_path = \"/media/volume1/gnn_graph_time_all_states_1.pt\"\n# torch.save(graph_data, save_path)\n# print(f\"\\n✅ Graph data saved at: {save_path}\")\n",
  "history_output" : "🔍 Checking available columns in CSV...\n⚠️ Missing columns skipped: ['date', 'lat', 'lon', 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness', 'SWE', 'swe_value', 'fsca', 'air_temperature_tmmx', 'air_temperature_tmmn', 'potential_evapotranspiration', 'relative_humidity_rmax', 'relative_humidity_rmin', 'mean_vapor_pressure_deficit', 'wind_speed', 'snodas_mask', 'water_year', 'SWE_1', 'SWE_2', 'SWE_3', 'SWE_4', 'SWE_5', 'SWE_6', 'SWE_7', 'air_temperature_tmmx_1', 'air_temperature_tmmx_2', 'air_temperature_tmmx_3', 'air_temperature_tmmx_4', 'air_temperature_tmmx_5', 'air_temperature_tmmx_6', 'air_temperature_tmmx_7', 'air_temperature_tmmn_1', 'air_temperature_tmmn_2', 'air_temperature_tmmn_3', 'air_temperature_tmmn_4', 'air_temperature_tmmn_5', 'air_temperature_tmmn_6', 'air_temperature_tmmn_7', 'potential_evapotranspiration_1', 'potential_evapotranspiration_2', 'potential_evapotranspiration_3', 'potential_evapotranspiration_4', 'potential_evapotranspiration_5', 'potential_evapotranspiration_6', 'potential_evapotranspiration_7', 'relative_humidity_rmax_1', 'relative_humidity_rmax_2', 'relative_humidity_rmax_3', 'relative_humidity_rmax_4', 'relative_humidity_rmax_5', 'relative_humidity_rmax_6', 'relative_humidity_rmax_7', 'relative_humidity_rmin_1', 'relative_humidity_rmin_2', 'relative_humidity_rmin_3', 'relative_humidity_rmin_4', 'relative_humidity_rmin_5', 'relative_humidity_rmin_6', 'relative_humidity_rmin_7', 'mean_vapor_pressure_deficit_1', 'mean_vapor_pressure_deficit_2', 'mean_vapor_pressure_deficit_3', 'mean_vapor_pressure_deficit_4', 'mean_vapor_pressure_deficit_5', 'mean_vapor_pressure_deficit_6', 'mean_vapor_pressure_deficit_7', 'precipitation_amount_1', 'precipitation_amount_2', 'precipitation_amount_3', 'precipitation_amount_4', 'precipitation_amount_5', 'precipitation_amount_6', 'precipitation_amount_7', 'wind_speed_1', 'wind_speed_2', 'wind_speed_3', 'wind_speed_4', 'wind_speed_5', 'wind_speed_6', 'wind_speed_7', 'fsca_1', 'fsca_2', 'fsca_3', 'fsca_4', 'fsca_5', 'fsca_6', 'fsca_7', 'cumulative_SWE', 'cumulative_air_temperature_tmmn', 'cumulative_air_temperature_tmmx', 'cumulative_potential_evapotranspiration', 'cumulative_mean_vapor_pressure_deficit', 'cumulative_precipitation_amount', 'cumulative_relative_humidity_rmax', 'cumulative_relative_humidity_rmin', 'cumulative_wind_speed', 'cumulative_fsca']\n📥 Loading dataset...\n✅ Loaded shape: (0, 1)\nTraceback (most recent call last):\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py\", line 3805, in get_loc\n    return self._engine.get_loc(casted_key)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"index.pyx\", line 167, in pandas._libs.index.IndexEngine.get_loc\n  File \"index.pyx\", line 196, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7081, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7089, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'lat'\nThe above exception was the direct cause of the following exception:\nTraceback (most recent call last):\n  File \"/media/volume1/gw-workspace/5gyozrYBEgMu/graph_data_creation.py\", line 74, in <module>\n    data['lat_bin'] = (data['lat'] // grid_size).astype(int)\n                       ~~~~^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py\", line 4102, in __getitem__\n    indexer = self.columns.get_loc(key)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py\", line 3812, in get_loc\n    raise KeyError(key) from err\nKeyError: 'lat'\n",
  "history_begin_time" : 1745340546203,
  "history_end_time" : 1745340551453,
  "history_notes" : null,
  "history_process" : "p7ar4i",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "Dvfqixk7Ss3B",
  "history_input" : "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # Disable GPU (optional)\n\nimport pandas as pd\nimport torch\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nfrom torch_geometric.utils import degree\n\n# 1️⃣ Load Dataset\nfile_path = '/media/volume1/all_points_final_merged_training_snodas_mask_resnet_all_batch.csv'\nchunksize = 500000\n\n# Define intended useful columns\nuseful_columns = [\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness',\n    'SWE', 'swe_value', 'fsca', 'air_temperature_tmmx', 'air_temperature_tmmn',\n    'potential_evapotranspiration', 'relative_humidity_rmax', 'relative_humidity_rmin',\n    'mean_vapor_pressure_deficit', 'wind_speed', 'snodas_mask', 'water_year',\n] + [\n    f\"{col}_{i}\" for col in [\n        \"SWE\", \"air_temperature_tmmx\", \"air_temperature_tmmn\", \"potential_evapotranspiration\",\n        \"relative_humidity_rmax\", \"relative_humidity_rmin\", \"mean_vapor_pressure_deficit\",\n        \"precipitation_amount\", \"wind_speed\", \"fsca\"\n    ] for i in range(1, 8)\n] + [\n    \"cumulative_SWE\", \"cumulative_air_temperature_tmmn\", \"cumulative_air_temperature_tmmx\",\n    \"cumulative_potential_evapotranspiration\", \"cumulative_mean_vapor_pressure_deficit\",\n    \"cumulative_precipitation_amount\", \"cumulative_relative_humidity_rmax\",\n    \"cumulative_relative_humidity_rmin\", \"cumulative_wind_speed\", \"cumulative_fsca\"\n]\n\n# ✅ Step 1: Check available columns\nprint(\"\\n🔍 Checking available columns in CSV...\")\npreview = pd.read_csv(file_path, nrows=1)\navailable_columns = set(preview.columns)\n\n# ✅ Step 2: Filter only columns that exist\nfiltered_columns = [col for col in useful_columns if col in available_columns]\nmissing_columns = [col for col in useful_columns if col not in available_columns]\nif missing_columns:\n    print(f\"⚠️ Warning: Missing columns that will be skipped: {missing_columns}\")\n\n# ✅ Step 3: Load the data\nprint(\"\\n📥 Loading dataset...\")\ndf_list = []\nfor chunk in pd.read_csv(file_path, usecols=filtered_columns, chunksize=chunksize):\n    chunk['date'] = pd.to_datetime(chunk['date'], errors='coerce')\n    chunk = chunk.dropna(subset=['date'])\n    chunk['day_of_year'] = chunk['date'].dt.dayofyear\n\n    chunk = chunk[(chunk['swe_value'] >= 0) & (chunk['swe_value'] < 3000)]\n    df_list.append(chunk)\n\ndata = pd.concat(df_list, ignore_index=True)\ndata = data.drop_duplicates()\n\nprint(f\"✅ Finished loading. Dataset shape: {data.shape}\")\n\n# 2️⃣ Binning\nprint(\"\\n📦 Binning spatially and temporally...\")\ngrid_size = 0.2\nnum_time_bins = 24\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\ndata['dayofyear_bin'] = (data['day_of_year'] // (365 / num_time_bins)).astype(int)\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['dayofyear_bin'].astype(str)\n\n# 3️⃣ Aggregation\nprint(\"\\n🛠️ Aggregating per grid cell...\")\naggregation_cols = {col: 'mean' for col in filtered_columns if col not in ['date', 'lat', 'lon']}\naggregation_cols.update({'lat': 'mean', 'lon': 'mean', 'date': 'first', 'day_of_year': 'mean'})\n\nmerged_nodes = data.groupby('grid_id').agg(aggregation_cols).reset_index()\n\n# 4️⃣ Temporal Encoding\nprint(\"\\n⏳ Adding temporal features (sin/cos of day of year)...\")\nmerged_nodes['sin_doy'] = np.sin(2 * np.pi * merged_nodes['day_of_year'] / 365)\nmerged_nodes['cos_doy'] = np.cos(2 * np.pi * merged_nodes['day_of_year'] / 365)\n\n# 5️⃣ Feature Columns for GNN\nfeature_columns = [col for col in merged_nodes.columns if col not in ['grid_id', 'swe_value', 'lat', 'lon', 'date', 'day_of_year']]\n\n# 6️⃣ Normalize Features\nprint(\"\\n⚙️ Normalizing features...\")\nscaler = StandardScaler()\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\nnode_features = torch.tensor(node_features, dtype=torch.float)\n\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\n\n# 7️⃣ KDTree for Spatial Edges\nprint(\"\\n🌐 Building KDTree for edges...\")\ncoordinates = merged_nodes[['lat', 'lon']].values\ntree = KDTree(coordinates)\nk = 6\nthreshold = 0.5  # distance threshold in degrees\n\ndistances, neighbors = tree.query(coordinates, k=k+1)\nedge_list = []\nfor i in range(len(neighbors)):\n    for j in range(1, k+1):\n        if distances[i][j] < threshold:\n            edge_list.append((i, neighbors[i][j]))\n\nedges = np.array(edge_list).T\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\n\n# 8️⃣ Create PyTorch Geometric Graph Data\nprint(\"\\n🔗 Creating graph data object...\")\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\ngraph_data.dates = merged_nodes['date'].astype(str).tolist()\n\n# 9️⃣ Print Graph Stats\nprint(\"\\n📊 Graph Summary Report\")\nprint(f\"🔹 Nodes: {graph_data.num_nodes}\")\nprint(f\"🔹 Edges: {graph_data.num_edges}\")\nprint(f\"🔹 Feature Shape: {graph_data.x.shape}\")\nprint(f\"🔹 Edge Shape: {graph_data.edge_index.shape}\")\n\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\nprint(f\"   • Min Degree : {deg.min().item()}\")\nprint(f\"   • Max Degree : {deg.max().item()}\")\nprint(f\"   • Mean Degree: {deg.float().mean():.2f}\")\nprint(f\"   • Std Degree : {deg.float().std():.2f}\")\nprint(f\"   • Isolated   : {(deg == 0).sum().item()} nodes ({(deg == 0).sum().item() / graph_data.num_nodes * 100:.2f}%)\")\n\n# 🔟 Save Graph\n# save_path = \"/media/volume1/gnn_graph_time_all_states_1.pt\"\n# torch.save(graph_data, save_path)\n# print(f\"\\n✅ Graph data saved at: {save_path}\")\n",
  "history_output" : "🔍 Checking available columns in CSV...\n⚠️ Warning: Missing columns that will be skipped: ['date', 'lat', 'lon', 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness', 'SWE', 'swe_value', 'fsca', 'air_temperature_tmmx', 'air_temperature_tmmn', 'potential_evapotranspiration', 'relative_humidity_rmax', 'relative_humidity_rmin', 'mean_vapor_pressure_deficit', 'wind_speed', 'snodas_mask', 'water_year', 'SWE_1', 'SWE_2', 'SWE_3', 'SWE_4', 'SWE_5', 'SWE_6', 'SWE_7', 'air_temperature_tmmx_1', 'air_temperature_tmmx_2', 'air_temperature_tmmx_3', 'air_temperature_tmmx_4', 'air_temperature_tmmx_5', 'air_temperature_tmmx_6', 'air_temperature_tmmx_7', 'air_temperature_tmmn_1', 'air_temperature_tmmn_2', 'air_temperature_tmmn_3', 'air_temperature_tmmn_4', 'air_temperature_tmmn_5', 'air_temperature_tmmn_6', 'air_temperature_tmmn_7', 'potential_evapotranspiration_1', 'potential_evapotranspiration_2', 'potential_evapotranspiration_3', 'potential_evapotranspiration_4', 'potential_evapotranspiration_5', 'potential_evapotranspiration_6', 'potential_evapotranspiration_7', 'relative_humidity_rmax_1', 'relative_humidity_rmax_2', 'relative_humidity_rmax_3', 'relative_humidity_rmax_4', 'relative_humidity_rmax_5', 'relative_humidity_rmax_6', 'relative_humidity_rmax_7', 'relative_humidity_rmin_1', 'relative_humidity_rmin_2', 'relative_humidity_rmin_3', 'relative_humidity_rmin_4', 'relative_humidity_rmin_5', 'relative_humidity_rmin_6', 'relative_humidity_rmin_7', 'mean_vapor_pressure_deficit_1', 'mean_vapor_pressure_deficit_2', 'mean_vapor_pressure_deficit_3', 'mean_vapor_pressure_deficit_4', 'mean_vapor_pressure_deficit_5', 'mean_vapor_pressure_deficit_6', 'mean_vapor_pressure_deficit_7', 'precipitation_amount_1', 'precipitation_amount_2', 'precipitation_amount_3', 'precipitation_amount_4', 'precipitation_amount_5', 'precipitation_amount_6', 'precipitation_amount_7', 'wind_speed_1', 'wind_speed_2', 'wind_speed_3', 'wind_speed_4', 'wind_speed_5', 'wind_speed_6', 'wind_speed_7', 'fsca_1', 'fsca_2', 'fsca_3', 'fsca_4', 'fsca_5', 'fsca_6', 'fsca_7', 'cumulative_SWE', 'cumulative_air_temperature_tmmn', 'cumulative_air_temperature_tmmx', 'cumulative_potential_evapotranspiration', 'cumulative_mean_vapor_pressure_deficit', 'cumulative_precipitation_amount', 'cumulative_relative_humidity_rmax', 'cumulative_relative_humidity_rmin', 'cumulative_wind_speed', 'cumulative_fsca']\n📥 Loading dataset...\nTraceback (most recent call last):\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py\", line 3805, in get_loc\n    return self._engine.get_loc(casted_key)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"index.pyx\", line 167, in pandas._libs.index.IndexEngine.get_loc\n  File \"index.pyx\", line 196, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7081, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7089, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'date'\nThe above exception was the direct cause of the following exception:\nTraceback (most recent call last):\n  File \"/media/volume1/gw-workspace/Dvfqixk7Ss3B/graph_data_creation.py\", line 50, in <module>\n    chunk['date'] = pd.to_datetime(chunk['date'], errors='coerce')\n                                   ~~~~~^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py\", line 4102, in __getitem__\n    indexer = self.columns.get_loc(key)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py\", line 3812, in get_loc\n    raise KeyError(key) from err\nKeyError: 'date'\n",
  "history_begin_time" : 1745340438689,
  "history_end_time" : 1745340444163,
  "history_notes" : null,
  "history_process" : "p7ar4i",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "xoQxwvBYnqVN",
  "history_input" : "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\nimport pandas as pd\nimport torch\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nfrom torch_geometric.utils import degree\n\n# 1️⃣ Load Dataset\nfile_path = '/media/volume1/all_points_final_merged_training_snodas_mask_resnet_all_batch.csv'\nchunksize = 500000\n\n# ✅ Define useful columns based on the latest schema\nuseful_columns = [\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness',\n    'SWE', 'swe_value', 'fsca', 'air_temperature_tmmx', 'air_temperature_tmmn',\n    'potential_evapotranspiration', 'relative_humidity_rmax', 'relative_humidity_rmin',\n    'mean_vapor_pressure_deficit', 'wind_speed', 'snodas_mask', 'water_year',\n] + [\n    f\"{col}_{i}\" for col in [\n        \"SWE\", \"air_temperature_tmmx\", \"air_temperature_tmmn\", \"potential_evapotranspiration\",\n        \"relative_humidity_rmax\", \"relative_humidity_rmin\", \"mean_vapor_pressure_deficit\",\n        \"precipitation_amount\", \"wind_speed\", \"fsca\"\n    ] for i in range(1, 8)\n] + [\n    \"cumulative_SWE\", \"cumulative_air_temperature_tmmn\", \"cumulative_air_temperature_tmmx\",\n    \"cumulative_potential_evapotranspiration\", \"cumulative_mean_vapor_pressure_deficit\",\n    \"cumulative_precipitation_amount\", \"cumulative_relative_humidity_rmax\",\n    \"cumulative_relative_humidity_rmin\", \"cumulative_wind_speed\", \"cumulative_fsca\"\n]\n\ndf_list = []\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\n    chunk['date'] = pd.to_datetime(chunk['date'], errors='coerce')\n    chunk = chunk.dropna(subset=['date'])\n    chunk['day_of_year'] = chunk['date'].dt.dayofyear\n\n    chunk = chunk[(chunk['swe_value'] >= 0) & (chunk['swe_value'] < 3000)]\n    df_list.append(chunk)\n\ndata = pd.concat(df_list, ignore_index=True)\ndata = data.drop_duplicates()\n\n# 2️⃣ Binning\ngrid_size = 0.2\nnum_time_bins = 24\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\ndata['dayofyear_bin'] = (data['day_of_year'] // (365 / num_time_bins)).astype(int)\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str) + \"_\" + data['dayofyear_bin'].astype(str)\n\n# 3️⃣ Aggregation\naggregation_cols = {col: 'mean' for col in useful_columns if col not in ['date', 'lat', 'lon']}\naggregation_cols.update({'lat': 'mean', 'lon': 'mean', 'date': 'first', 'day_of_year': 'mean'})\n\nmerged_nodes = data.groupby('grid_id').agg(aggregation_cols).reset_index()\n\n# 4️⃣ Temporal Encoding\nmerged_nodes['sin_doy'] = np.sin(2 * np.pi * merged_nodes['day_of_year'] / 365)\nmerged_nodes['cos_doy'] = np.cos(2 * np.pi * merged_nodes['day_of_year'] / 365)\n\n# 5️⃣ Feature Columns for GNN\nfeature_columns = [col for col in merged_nodes.columns if col not in ['grid_id', 'swe_value', 'lat', 'lon', 'date', 'day_of_year']]\n\n# 6️⃣ Normalize\nscaler = StandardScaler()\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\nnode_features = torch.tensor(node_features, dtype=torch.float)\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\n\n# 7️⃣ KDTree for Edges\ncoordinates = merged_nodes[['lat', 'lon']].values\ntree = KDTree(coordinates)\nk = 6\nthreshold = 0.5\n\ndistances, neighbors = tree.query(coordinates, k=k+1)\nedge_list = []\nfor i in range(len(neighbors)):\n    for j in range(1, k+1):\n        if distances[i][j] < threshold:\n            edge_list.append((i, neighbors[i][j]))\n\nedges = np.array(edge_list).T\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\n\n# 8️⃣ Create Graph Data Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\ngraph_data.dates = merged_nodes['date'].astype(str).tolist()\n\n# 9️⃣ Graph Stats\nprint(\"\\n📊 Graph Summary Report\")\nprint(f\"🔹 Nodes: {graph_data.num_nodes}\")\nprint(f\"🔹 Edges: {graph_data.num_edges}\")\nprint(f\"🔹 Feature Shape: {graph_data.x.shape}\")\nprint(f\"🔹 Edge Shape: {graph_data.edge_index.shape}\")\n\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\nprint(f\"   • Min Degree : {deg.min().item()}\")\nprint(f\"   • Max Degree : {deg.max().item()}\")\nprint(f\"   • Mean Degree: {deg.float().mean():.2f}\")\nprint(f\"   • Std Degree : {deg.float().std():.2f}\")\nprint(f\"   • Isolated   : {(deg == 0).sum().item()} nodes ({(deg == 0).sum().item() / graph_data.num_nodes * 100:.2f}%)\")\n\n# 🔒 Save\nsave_path = \"/media/volume1/gnn_graph_time_all_states_1.pt\"\ntorch.save(graph_data, save_path)\nprint(f\"\\n✅ Graph data saved at: {save_path}\")\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/media/volume1/gw-workspace/xoQxwvBYnqVN/graph_data_creation.py\", line 36, in <module>\n    for chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 1026, in read_csv\n    return _read(filepath_or_buffer, kwds)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 620, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 1620, in __init__\n    self._engine = self._make_engine(f, self.engine)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 1898, in _make_engine\n    return mapping[engine](f, **self.options)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 140, in __init__\n    self._validate_usecols_names(usecols, self.orig_names)\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/base_parser.py\", line 979, in _validate_usecols_names\n    raise ValueError(\nValueError: Usecols do not match columns, columns expected but not found: ['mean_vapor_pressure_deficit_1', 'relative_humidity_rmax_2', 'cumulative_wind_speed', 'precipitation_amount_4', 'precipitation_amount_5', 'relative_humidity_rmin_2', 'Eastness', 'potential_evapotranspiration_7', 'wind_speed_5', 'Curvature', 'wind_speed_3', 'mean_vapor_pressure_deficit', 'cumulative_mean_vapor_pressure_deficit', 'air_temperature_tmmn_7', 'SWE_3', 'air_temperature_tmmn', 'SWE_4', 'cumulative_precipitation_amount', 'potential_evapotranspiration_1', 'cumulative_air_temperature_tmmx', 'cumulative_relative_humidity_rmax', 'air_temperature_tmmx_5', 'precipitation_amount_7', 'mean_vapor_pressure_deficit_5', 'wind_speed', 'Northness', 'air_temperature_tmmx_2', 'fsca_7', 'fsca_2', 'air_temperature_tmmn_1', 'fsca_4', 'mean_vapor_pressure_deficit_3', 'cumulative_relative_humidity_rmin', 'relative_humidity_rmin', 'precipitation_amount_1', 'Elevation', 'fsca_5', 'wind_speed_4', 'air_temperature_tmmx_6', 'wind_speed_1', 'relative_humidity_rmin_4', 'potential_evapotranspiration_4', 'relative_humidity_rmax_7', 'relative_humidity_rmax_4', 'cumulative_SWE', 'mean_vapor_pressure_deficit_4', 'precipitation_amount_3', 'swe_value', 'relative_humidity_rmin_1', 'fsca_3', 'SWE_2', 'relative_humidity_rmin_5', 'air_temperature_tmmn_4', 'SWE', 'air_temperature_tmmx_7', 'relative_humidity_rmax_6', 'cumulative_potential_evapotranspiration', 'fsca_1', 'relative_humidity_rmin_7', 'potential_evapotranspiration_2', 'air_temperature_tmmx_3', 'precipitation_amount_6', 'relative_humidity_rmax_5', 'wind_speed_2', 'relative_humidity_rmax_3', 'air_temperature_tmmx', 'SWE_6', 'potential_evapotranspiration_6', 'air_temperature_tmmn_5', 'fsca', 'air_temperature_tmmn_6', 'relative_humidity_rmin_6', 'Aspect', 'mean_vapor_pressure_deficit_6', 'lat', 'date', 'potential_evapotranspiration_3', 'relative_humidity_rmax_1', 'mean_vapor_pressure_deficit_2', 'air_temperature_tmmx_4', 'relative_humidity_rmax', 'cumulative_air_temperature_tmmn', 'snodas_mask', 'potential_evapotranspiration_5', 'SWE_5', 'precipitation_amount_2', 'fsca_6', 'potential_evapotranspiration', 'SWE_1', 'water_year', 'air_temperature_tmmn_3', 'wind_speed_6', 'air_temperature_tmmn_2', 'SWE_7', 'air_temperature_tmmx_1', 'mean_vapor_pressure_deficit_7', 'wind_speed_7', 'lon', 'Slope', 'relative_humidity_rmin_3', 'cumulative_fsca']\n",
  "history_begin_time" : 1745340007092,
  "history_end_time" : 1745340012292,
  "history_notes" : null,
  "history_process" : "p7ar4i",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "EVNFFgwsThRk",
  "history_input" : "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # 🔒 Disable GPU to avoid libnccl.so.2 errors\n\nimport pandas as pd\nimport torch\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nfrom torch_geometric.utils import degree\n\n# 1️⃣ Load Dataset (all spatial data, no region filter)\nfile_path = '/media/volume1/testing_data_pinn_model.csv'\nchunksize = 500000\n\n# useful_columns = [\n#     'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n#     'air_temperature_observed_f', 'precipitation_amount',\n#     'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n#     'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\n# ]\n\nuseful_columns = [\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'Aspect', 'Curvature',\n    'Northness', 'Eastness', 'pr', 'rmax', 'rmin', 'tmmx', 'tmmn', 'vs', 'etr',\n    'fsca', 'AMSR_SWE', 'water_year', 'snodas_mask'\n]\n\ndf_list = []\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\n    chunk['date'] = pd.to_datetime(chunk['date'])\n    chunk['day_of_year'] = chunk['date'].dt.dayofyear\n\n    filtered_chunk = chunk[\n        (chunk['swe_value'] >= 0) & (chunk['swe_value'] < 3000)\n    ]\n    df_list.append(filtered_chunk)\n\ndata = pd.concat(df_list, ignore_index=True)\ndata = data.drop_duplicates()\n\n# 2️⃣ Spatial & Temporal Grid Binning\ngrid_size = 0.2\nnum_time_bins = 24\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\ndata['dayofyear_bin'] = (data['day_of_year'] // (365 / num_time_bins)).astype(int)\ndata['grid_id'] = (\n    data['lat_bin'].astype(str) + \"_\" +\n    data['lon_bin'].astype(str) + \"_\" +\n    data['dayofyear_bin'].astype(str)\n)\n\n# 3️⃣ Aggregation per node\nmerged_nodes = data.groupby('grid_id').agg({\n    'Elevation': 'mean',\n    'Slope': 'mean',\n    'SWE': 'mean',\n    'snow_depth': 'mean',\n    'air_temperature_observed_f': 'mean',\n    'precipitation_amount': 'mean',\n    'relative_humidity_rmin': 'mean',\n    'relative_humidity_rmax': 'mean',\n    'wind_speed': 'mean',\n    'cumulative_SWE': 'mean',\n    'cumulative_precipitation_amount': 'mean',\n    'swe_value': 'mean',\n    'lat': 'mean',\n    'lon': 'mean',\n    'day_of_year': 'mean',\n    'date': 'first'\n}).reset_index()\n\n# 4️⃣ Temporal Features\nmerged_nodes['sin_doy'] = np.sin(2 * np.pi * merged_nodes['day_of_year'] / 365)\nmerged_nodes['cos_doy'] = np.cos(2 * np.pi * merged_nodes['day_of_year'] / 365)\n\n# 5️⃣ Feature Engineering & Reduction\nmerged_nodes['relative_humidity_avg'] = (\n    merged_nodes['relative_humidity_rmin'] + merged_nodes['relative_humidity_rmax']\n) / 2\nmerged_nodes['humidity_diff'] = merged_nodes['relative_humidity_rmax'] - merged_nodes['relative_humidity_rmin']\nmerged_nodes.drop(columns=['relative_humidity_rmin', 'relative_humidity_rmax', 'Slope', 'wind_speed'], inplace=True)\n\n# 6️⃣ Select Feature Columns\nfeature_columns = [\n    'SWE', 'swe_value', 'relative_humidity_rmin',\n    'potential_evapotranspiration', 'air_temperature_tmmx',\n    'relative_humidity_rmax', 'mean_vapor_pressure_deficit',\n    'air_temperature_tmmn', 'wind_speed', 'Elevation', 'Aspect',\n    'Curvature', 'Northness', 'Eastness', 'fsca', 'Slope', 'SWE_1',\n    'air_temperature_tmmn_1', 'potential_evapotranspiration_1',\n    'mean_vapor_pressure_deficit_1', 'relative_humidity_rmax_1',\n    'relative_humidity_rmin_1', 'air_temperature_tmmx_1', 'wind_speed_1',\n    'fsca_1', 'SWE_2', 'air_temperature_tmmn_2',\n    'potential_evapotranspiration_2', 'mean_vapor_pressure_deficit_2',\n    'relative_humidity_rmax_2', 'relative_humidity_rmin_2',\n    'air_temperature_tmmx_2', 'wind_speed_2', 'fsca_2', 'SWE_3',\n    'air_temperature_tmmn_3', 'potential_evapotranspiration_3',\n    'mean_vapor_pressure_deficit_3', 'relative_humidity_rmax_3',\n    'relative_humidity_rmin_3', 'air_temperature_tmmx_3', 'wind_speed_3',\n    'fsca_3', 'SWE_4', 'air_temperature_tmmn_4',\n    'potential_evapotranspiration_4', 'mean_vapor_pressure_deficit_4',\n    'relative_humidity_rmax_4', 'relative_humidity_rmin_4',\n    'air_temperature_tmmx_4', 'wind_speed_4', 'fsca_4', 'SWE_5',\n    'air_temperature_tmmn_5', 'potential_evapotranspiration_5',\n    'mean_vapor_pressure_deficit_5', 'relative_humidity_rmax_5',\n    'relative_humidity_rmin_5', 'air_temperature_tmmx_5', 'wind_speed_5',\n    'fsca_5', 'SWE_6', 'air_temperature_tmmn_6',\n    'potential_evapotranspiration_6', 'mean_vapor_pressure_deficit_6',\n    'relative_humidity_rmax_6', 'relative_humidity_rmin_6',\n    'air_temperature_tmmx_6', 'wind_speed_6', 'fsca_6', 'SWE_7',\n    'air_temperature_tmmn_7', 'potential_evapotranspiration_7',\n    'mean_vapor_pressure_deficit_7', 'relative_humidity_rmax_7',\n    'relative_humidity_rmin_7', 'air_temperature_tmmx_7', 'wind_speed_7',\n    'fsca_7', 'water_year', 'snodas_mask'\n]\n\n# 7️⃣ Normalize Features\nscaler = StandardScaler()\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\nnode_features = torch.tensor(node_features, dtype=torch.float)\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\n\n# 8️⃣ Build Graph Edges Using KDTree\ncoordinates = merged_nodes[['lat', 'lon']].values\ntree = KDTree(coordinates)\nk = 6\nthreshold = 0.5\n\ndistances, neighbors = tree.query(coordinates, k=k+1)\nedge_list = []\nfor i in range(len(neighbors)):\n    for j in range(1, k+1):\n        if distances[i][j] < threshold:\n            edge_list.append((i, neighbors[i][j]))\n\nedges = np.array(edge_list).T\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\n\n# 9️⃣ Build PyTorch Geometric Data Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\ngraph_data.dates = merged_nodes['date'].astype(str).tolist()\n\n# 🔟 Graph Summary\nprint(\"\\n📊 Graph Summary Report\")\nprint(f\"🔹 Number of nodes       : {graph_data.num_nodes}\")\nprint(f\"🔹 Number of edges       : {graph_data.num_edges}\")\nprint(f\"🔹 Node feature shape    : {graph_data.x.shape}\")\nprint(f\"🔹 Edge index shape      : {graph_data.edge_index.shape}\")\n\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\nprint(f\"\\n📈 Degree Statistics:\")\nprint(f\"   • Min degree          : {deg.min().item()}\")\nprint(f\"   • Max degree          : {deg.max().item()}\")\nprint(f\"   • Mean degree         : {deg.float().mean().item():.2f}\")\nprint(f\"   • Std deviation       : {deg.float().std().item():.2f}\")\n\nnum_isolated_nodes = (deg == 0).sum().item()\npercentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\nprint(f\"\\n🚨 Isolated Nodes:\")\nprint(f\"   • Count               : {num_isolated_nodes}\")\nprint(f\"   • Percent             : {percentage_isolated:.4f}%\")\n\nprint(\"\\n✅ Graph ready for PINN modeling!\")\n\n# 🔒 Save\ngraph_data_output_path = \"/media/volume1/gnn_graph_time_all_states_1.pt\"\ntorch.save(graph_data, graph_data_output_path)\nprint(f\"\\n✅ PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/media/volume1/gw-workspace/EVNFFgwsThRk/graph_data_creation.py\", line 30, in <module>\n    for chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 1026, in read_csv\n    return _read(filepath_or_buffer, kwds)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 620, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 1620, in __init__\n    self._engine = self._make_engine(f, self.engine)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 1898, in _make_engine\n    return mapping[engine](f, **self.options)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 140, in __init__\n    self._validate_usecols_names(usecols, self.orig_names)\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/base_parser.py\", line 979, in _validate_usecols_names\n    raise ValueError(\nValueError: Usecols do not match columns, columns expected but not found: ['vs', 'AMSR_SWE', 'rmax', 'lon', 'date', 'water_year', 'tmmn', 'Northness', 'tmmx', 'rmin', 'Curvature', 'Aspect', 'snodas_mask', 'Slope', 'pr', 'Eastness', 'Elevation', 'fsca', 'lat', 'etr']\n",
  "history_begin_time" : 1745333317291,
  "history_end_time" : 1745333322777,
  "history_notes" : null,
  "history_process" : "p7ar4i",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "hcK8oM9QDQBm",
  "history_input" : "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # 🔒 Disable GPU to avoid libnccl.so.2 errors\n\nimport pandas as pd\nimport torch\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nfrom torch_geometric.utils import degree\n\n# 1️⃣ Load Dataset (all spatial data, no region filter)\nfile_path = '/media/volume1/testing_data_pinn_model.csv'\nchunksize = 500000\n\n# useful_columns = [\n#     'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n#     'air_temperature_observed_f', 'precipitation_amount',\n#     'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n#     'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\n# ]\n\nuseful_columns = [\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'Aspect', 'Curvature',\n    'Northness', 'Eastness', 'pr', 'rmax', 'rmin', 'tmmx', 'tmmn', 'vs', 'etr',\n    'fsca', 'AMSR_SWE', 'water_year', 'snodas_mask'\n]\n\n\ndf_list = []\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\n    chunk['date'] = pd.to_datetime(chunk['date'])\n    chunk['day_of_year'] = chunk['date'].dt.dayofyear\n\n    filtered_chunk = chunk[\n        (chunk['swe_value'] >= 0) & (chunk['swe_value'] < 3000)\n    ]\n    df_list.append(filtered_chunk)\n\ndata = pd.concat(df_list, ignore_index=True)\ndata = data.drop_duplicates()\n\n# 2️⃣ Spatial & Temporal Grid Binning\ngrid_size = 0.2\nnum_time_bins = 24\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\ndata['dayofyear_bin'] = (data['day_of_year'] // (365 / num_time_bins)).astype(int)\ndata['grid_id'] = (\n    data['lat_bin'].astype(str) + \"_\" +\n    data['lon_bin'].astype(str) + \"_\" +\n    data['dayofyear_bin'].astype(str)\n)\n\n# 3️⃣ Aggregation per node\nmerged_nodes = data.groupby('grid_id').agg({\n    'Elevation': 'mean',\n    'Slope': 'mean',\n    'SWE': 'mean',\n    'snow_depth': 'mean',\n    'air_temperature_observed_f': 'mean',\n    'precipitation_amount': 'mean',\n    'relative_humidity_rmin': 'mean',\n    'relative_humidity_rmax': 'mean',\n    'wind_speed': 'mean',\n    'cumulative_SWE': 'mean',\n    'cumulative_precipitation_amount': 'mean',\n    'swe_value': 'mean',\n    'lat': 'mean',\n    'lon': 'mean',\n    'day_of_year': 'mean',\n    'date': 'first'\n}).reset_index()\n\n# 4️⃣ Temporal Features\nmerged_nodes['sin_doy'] = np.sin(2 * np.pi * merged_nodes['day_of_year'] / 365)\nmerged_nodes['cos_doy'] = np.cos(2 * np.pi * merged_nodes['day_of_year'] / 365)\n\n# 5️⃣ Feature Engineering & Reduction\nmerged_nodes['relative_humidity_avg'] = (\n    merged_nodes['relative_humidity_rmin'] + merged_nodes['relative_humidity_rmax']\n) / 2\nmerged_nodes['humidity_diff'] = merged_nodes['relative_humidity_rmax'] - merged_nodes['relative_humidity_rmin']\nmerged_nodes.drop(columns=['relative_humidity_rmin', 'relative_humidity_rmax', 'Slope', 'wind_speed'], inplace=True)\n\n# 6️⃣ Select Feature Columns\nfeature_columns = [\n    'SWE', 'swe_value', 'relative_humidity_rmin',\n    'potential_evapotranspiration', 'air_temperature_tmmx',\n    'relative_humidity_rmax', 'mean_vapor_pressure_deficit',\n    'air_temperature_tmmn', 'wind_speed', 'Elevation', 'Aspect',\n    'Curvature', 'Northness', 'Eastness', 'fsca', 'Slope', 'SWE_1',\n    'air_temperature_tmmn_1', 'potential_evapotranspiration_1',\n    'mean_vapor_pressure_deficit_1', 'relative_humidity_rmax_1',\n    'relative_humidity_rmin_1', 'air_temperature_tmmx_1', 'wind_speed_1',\n    'fsca_1', 'SWE_2', 'air_temperature_tmmn_2',\n    'potential_evapotranspiration_2', 'mean_vapor_pressure_deficit_2',\n    'relative_humidity_rmax_2', 'relative_humidity_rmin_2',\n    'air_temperature_tmmx_2', 'wind_speed_2', 'fsca_2', 'SWE_3',\n    'air_temperature_tmmn_3', 'potential_evapotranspiration_3',\n    'mean_vapor_pressure_deficit_3', 'relative_humidity_rmax_3',\n    'relative_humidity_rmin_3', 'air_temperature_tmmx_3', 'wind_speed_3',\n    'fsca_3', 'SWE_4', 'air_temperature_tmmn_4',\n    'potential_evapotranspiration_4', 'mean_vapor_pressure_deficit_4',\n    'relative_humidity_rmax_4', 'relative_humidity_rmin_4',\n    'air_temperature_tmmx_4', 'wind_speed_4', 'fsca_4', 'SWE_5',\n    'air_temperature_tmmn_5', 'potential_evapotranspiration_5',\n    'mean_vapor_pressure_deficit_5', 'relative_humidity_rmax_5',\n    'relative_humidity_rmin_5', 'air_temperature_tmmx_5', 'wind_speed_5',\n    'fsca_5', 'SWE_6', 'air_temperature_tmmn_6',\n    'potential_evapotranspiration_6', 'mean_vapor_pressure_deficit_6',\n    'relative_humidity_rmax_6', 'relative_humidity_rmin_6',\n    'air_temperature_tmmx_6', 'wind_speed_6', 'fsca_6', 'SWE_7',\n    'air_temperature_tmmn_7', 'potential_evapotranspiration_7',\n    'mean_vapor_pressure_deficit_7', 'relative_humidity_rmax_7',\n    'relative_humidity_rmin_7', 'air_temperature_tmmx_7', 'wind_speed_7',\n    'fsca_7', 'water_year', 'snodas_mask'\n]\n\n# 7️⃣ Normalize Features\nscaler = StandardScaler()\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\nnode_features = torch.tensor(node_features, dtype=torch.float)\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\n\n# 8️⃣ Build Graph Edges Using KDTree\ncoordinates = merged_nodes[['lat', 'lon']].values\ntree = KDTree(coordinates)\nk = 6\nthreshold = 0.5\n\ndistances, neighbors = tree.query(coordinates, k=k+1)\nedge_list = []\nfor i in range(len(neighbors)):\n    for j in range(1, k+1):\n        if distances[i][j] < threshold:\n            edge_list.append((i, neighbors[i][j]))\n\nedges = np.array(edge_list).T\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\n\n# 9️⃣ Build PyTorch Geometric Data Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\ngraph_data.dates = merged_nodes['date'].astype(str).tolist()\n\n# 🔟 Graph Summary\nprint(\"\\n📊 Graph Summary Report\")\nprint(f\"🔹 Number of nodes       : {graph_data.num_nodes}\")\nprint(f\"🔹 Number of edges       : {graph_data.num_edges}\")\nprint(f\"🔹 Node feature shape    : {graph_data.x.shape}\")\nprint(f\"🔹 Edge index shape      : {graph_data.edge_index.shape}\")\n\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\nprint(f\"\\n📈 Degree Statistics:\")\nprint(f\"   • Min degree          : {deg.min().item()}\")\nprint(f\"   • Max degree          : {deg.max().item()}\")\nprint(f\"   • Mean degree         : {deg.float().mean().item():.2f}\")\nprint(f\"   • Std deviation       : {deg.float().std().item():.2f}\")\n\nnum_isolated_nodes = (deg == 0).sum().item()\npercentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\nprint(f\"\\n🚨 Isolated Nodes:\")\nprint(f\"   • Count               : {num_isolated_nodes}\")\nprint(f\"   • Percent             : {percentage_isolated:.4f}%\")\n\nprint(\"\\n✅ Graph ready for PINN modeling!\")\n\n# 🔒 Save\ngraph_data_output_path = \"/media/volume1/gnn_graph_time_all_states_1.pt\"\ntorch.save(graph_data, graph_data_output_path)\nprint(f\"\\n✅ PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/media/volume1/gw-workspace/hcK8oM9QDQBm/graph_data_creation.py\", line 31, in <module>\n    for chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 1026, in read_csv\n    return _read(filepath_or_buffer, kwds)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 620, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 1620, in __init__\n    self._engine = self._make_engine(f, self.engine)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 1898, in _make_engine\n    return mapping[engine](f, **self.options)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 140, in __init__\n    self._validate_usecols_names(usecols, self.orig_names)\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/base_parser.py\", line 979, in _validate_usecols_names\n    raise ValueError(\nValueError: Usecols do not match columns, columns expected but not found: ['rmax', 'water_year', 'vs', 'snodas_mask', 'AMSR_SWE', 'rmin', 'tmmx', 'lat', 'Slope', 'Elevation', 'Eastness', 'etr', 'Curvature', 'pr', 'Aspect', 'fsca', 'tmmn', 'date', 'lon', 'Northness']\n",
  "history_begin_time" : 1745333158329,
  "history_end_time" : 1745333163694,
  "history_notes" : null,
  "history_process" : "p7ar4i",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "3hd1CGLfBRAu",
  "history_input" : "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # 🔒 Disable GPU to avoid libnccl.so.2 errors\n\nimport pandas as pd\nimport torch\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nfrom torch_geometric.utils import degree\n\n# 1️⃣ Load Dataset (all spatial data, no region filter)\nfile_path = '/media/volume1/testing_data_pinn_model.csv'\nchunksize = 500000\n\nuseful_columns = [\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\n]\n\ndf_list = []\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\n    chunk['date'] = pd.to_datetime(chunk['date'])\n    chunk['day_of_year'] = chunk['date'].dt.dayofyear\n\n    filtered_chunk = chunk[\n        (chunk['swe_value'] >= 0) & (chunk['swe_value'] < 3000)\n    ]\n    df_list.append(filtered_chunk)\n\ndata = pd.concat(df_list, ignore_index=True)\ndata = data.drop_duplicates()\n\n# 2️⃣ Spatial & Temporal Grid Binning\ngrid_size = 0.2\nnum_time_bins = 24\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\ndata['dayofyear_bin'] = (data['day_of_year'] // (365 / num_time_bins)).astype(int)\ndata['grid_id'] = (\n    data['lat_bin'].astype(str) + \"_\" +\n    data['lon_bin'].astype(str) + \"_\" +\n    data['dayofyear_bin'].astype(str)\n)\n\n# 3️⃣ Aggregation per node\nmerged_nodes = data.groupby('grid_id').agg({\n    'Elevation': 'mean',\n    'Slope': 'mean',\n    'SWE': 'mean',\n    'snow_depth': 'mean',\n    'air_temperature_observed_f': 'mean',\n    'precipitation_amount': 'mean',\n    'relative_humidity_rmin': 'mean',\n    'relative_humidity_rmax': 'mean',\n    'wind_speed': 'mean',\n    'cumulative_SWE': 'mean',\n    'cumulative_precipitation_amount': 'mean',\n    'swe_value': 'mean',\n    'lat': 'mean',\n    'lon': 'mean',\n    'day_of_year': 'mean',\n    'date': 'first'\n}).reset_index()\n\n# 4️⃣ Temporal Features\nmerged_nodes['sin_doy'] = np.sin(2 * np.pi * merged_nodes['day_of_year'] / 365)\nmerged_nodes['cos_doy'] = np.cos(2 * np.pi * merged_nodes['day_of_year'] / 365)\n\n# 5️⃣ Feature Engineering & Reduction\nmerged_nodes['relative_humidity_avg'] = (\n    merged_nodes['relative_humidity_rmin'] + merged_nodes['relative_humidity_rmax']\n) / 2\nmerged_nodes['humidity_diff'] = merged_nodes['relative_humidity_rmax'] - merged_nodes['relative_humidity_rmin']\nmerged_nodes.drop(columns=['relative_humidity_rmin', 'relative_humidity_rmax', 'Slope', 'wind_speed'], inplace=True)\n\n# 6️⃣ Select Feature Columns\nfeature_columns = [\n    'SWE', 'swe_value', 'relative_humidity_rmin',\n    'potential_evapotranspiration', 'air_temperature_tmmx',\n    'relative_humidity_rmax', 'mean_vapor_pressure_deficit',\n    'air_temperature_tmmn', 'wind_speed', 'Elevation', 'Aspect',\n    'Curvature', 'Northness', 'Eastness', 'fsca', 'Slope', 'SWE_1',\n    'air_temperature_tmmn_1', 'potential_evapotranspiration_1',\n    'mean_vapor_pressure_deficit_1', 'relative_humidity_rmax_1',\n    'relative_humidity_rmin_1', 'air_temperature_tmmx_1', 'wind_speed_1',\n    'fsca_1', 'SWE_2', 'air_temperature_tmmn_2',\n    'potential_evapotranspiration_2', 'mean_vapor_pressure_deficit_2',\n    'relative_humidity_rmax_2', 'relative_humidity_rmin_2',\n    'air_temperature_tmmx_2', 'wind_speed_2', 'fsca_2', 'SWE_3',\n    'air_temperature_tmmn_3', 'potential_evapotranspiration_3',\n    'mean_vapor_pressure_deficit_3', 'relative_humidity_rmax_3',\n    'relative_humidity_rmin_3', 'air_temperature_tmmx_3', 'wind_speed_3',\n    'fsca_3', 'SWE_4', 'air_temperature_tmmn_4',\n    'potential_evapotranspiration_4', 'mean_vapor_pressure_deficit_4',\n    'relative_humidity_rmax_4', 'relative_humidity_rmin_4',\n    'air_temperature_tmmx_4', 'wind_speed_4', 'fsca_4', 'SWE_5',\n    'air_temperature_tmmn_5', 'potential_evapotranspiration_5',\n    'mean_vapor_pressure_deficit_5', 'relative_humidity_rmax_5',\n    'relative_humidity_rmin_5', 'air_temperature_tmmx_5', 'wind_speed_5',\n    'fsca_5', 'SWE_6', 'air_temperature_tmmn_6',\n    'potential_evapotranspiration_6', 'mean_vapor_pressure_deficit_6',\n    'relative_humidity_rmax_6', 'relative_humidity_rmin_6',\n    'air_temperature_tmmx_6', 'wind_speed_6', 'fsca_6', 'SWE_7',\n    'air_temperature_tmmn_7', 'potential_evapotranspiration_7',\n    'mean_vapor_pressure_deficit_7', 'relative_humidity_rmax_7',\n    'relative_humidity_rmin_7', 'air_temperature_tmmx_7', 'wind_speed_7',\n    'fsca_7', 'water_year', 'snodas_mask'\n]\n\n# 7️⃣ Normalize Features\nscaler = StandardScaler()\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\nnode_features = torch.tensor(node_features, dtype=torch.float)\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\n\n# 8️⃣ Build Graph Edges Using KDTree\ncoordinates = merged_nodes[['lat', 'lon']].values\ntree = KDTree(coordinates)\nk = 6\nthreshold = 0.5\n\ndistances, neighbors = tree.query(coordinates, k=k+1)\nedge_list = []\nfor i in range(len(neighbors)):\n    for j in range(1, k+1):\n        if distances[i][j] < threshold:\n            edge_list.append((i, neighbors[i][j]))\n\nedges = np.array(edge_list).T\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\n\n# 9️⃣ Build PyTorch Geometric Data Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\ngraph_data.dates = merged_nodes['date'].astype(str).tolist()\n\n# 🔟 Graph Summary\nprint(\"\\n📊 Graph Summary Report\")\nprint(f\"🔹 Number of nodes       : {graph_data.num_nodes}\")\nprint(f\"🔹 Number of edges       : {graph_data.num_edges}\")\nprint(f\"🔹 Node feature shape    : {graph_data.x.shape}\")\nprint(f\"🔹 Edge index shape      : {graph_data.edge_index.shape}\")\n\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\nprint(f\"\\n📈 Degree Statistics:\")\nprint(f\"   • Min degree          : {deg.min().item()}\")\nprint(f\"   • Max degree          : {deg.max().item()}\")\nprint(f\"   • Mean degree         : {deg.float().mean().item():.2f}\")\nprint(f\"   • Std deviation       : {deg.float().std().item():.2f}\")\n\nnum_isolated_nodes = (deg == 0).sum().item()\npercentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\nprint(f\"\\n🚨 Isolated Nodes:\")\nprint(f\"   • Count               : {num_isolated_nodes}\")\nprint(f\"   • Percent             : {percentage_isolated:.4f}%\")\n\nprint(\"\\n✅ Graph ready for PINN modeling!\")\n\n# 🔒 Save\ngraph_data_output_path = \"/media/volume1/gnn_graph_time_all_states_1.pt\"\ntorch.save(graph_data, graph_data_output_path)\nprint(f\"\\n✅ PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/media/volume1/gw-workspace/3hd1CGLfBRAu/graph_data_creation.py\", line 24, in <module>\n    for chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 1026, in read_csv\n    return _read(filepath_or_buffer, kwds)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 620, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 1620, in __init__\n    self._engine = self._make_engine(f, self.engine)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 1898, in _make_engine\n    return mapping[engine](f, **self.options)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 140, in __init__\n    self._validate_usecols_names(usecols, self.orig_names)\n  File \"/home/geo2021/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/base_parser.py\", line 979, in _validate_usecols_names\n    raise ValueError(\nValueError: Usecols do not match columns, columns expected but not found: ['Elevation', 'air_temperature_observed_f', 'relative_humidity_rmax', 'lon', 'precipitation_amount', 'snow_depth', 'wind_speed', 'SWE', 'cumulative_SWE', 'swe_value', 'relative_humidity_rmin', 'date', 'cumulative_precipitation_amount', 'Slope', 'lat']\n",
  "history_begin_time" : 1745332940613,
  "history_end_time" : 1745332945786,
  "history_notes" : null,
  "history_process" : "p7ar4i",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "12uhJeCBqA0U",
  "history_input" : "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # 🔒 Disable GPU to avoid libnccl.so.2 errors\n\nimport pandas as pd\nimport torch\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nfrom torch_geometric.utils import degree\n\n# 1️⃣ Load Dataset (all spatial data, no region filter)\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\nchunksize = 500000\n\nuseful_columns = [\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\n]\n\ndf_list = []\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\n    chunk['date'] = pd.to_datetime(chunk['date'])\n    chunk['day_of_year'] = chunk['date'].dt.dayofyear\n\n    filtered_chunk = chunk[\n        (chunk['swe_value'] >= 0) & (chunk['swe_value'] < 3000)\n    ]\n    df_list.append(filtered_chunk)\n\ndata = pd.concat(df_list, ignore_index=True)\ndata = data.drop_duplicates()\n\n# 2️⃣ Spatial & Temporal Grid Binning\ngrid_size = 0.2\nnum_time_bins = 24\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\ndata['dayofyear_bin'] = (data['day_of_year'] // (365 / num_time_bins)).astype(int)\ndata['grid_id'] = (\n    data['lat_bin'].astype(str) + \"_\" +\n    data['lon_bin'].astype(str) + \"_\" +\n    data['dayofyear_bin'].astype(str)\n)\n\n# 3️⃣ Aggregation per node\nmerged_nodes = data.groupby('grid_id').agg({\n    'Elevation': 'mean',\n    'Slope': 'mean',\n    'SWE': 'mean',\n    'snow_depth': 'mean',\n    'air_temperature_observed_f': 'mean',\n    'precipitation_amount': 'mean',\n    'relative_humidity_rmin': 'mean',\n    'relative_humidity_rmax': 'mean',\n    'wind_speed': 'mean',\n    'cumulative_SWE': 'mean',\n    'cumulative_precipitation_amount': 'mean',\n    'swe_value': 'mean',\n    'lat': 'mean',\n    'lon': 'mean',\n    'day_of_year': 'mean',\n    'date': 'first'\n}).reset_index()\n\n# 4️⃣ Temporal Features\nmerged_nodes['sin_doy'] = np.sin(2 * np.pi * merged_nodes['day_of_year'] / 365)\nmerged_nodes['cos_doy'] = np.cos(2 * np.pi * merged_nodes['day_of_year'] / 365)\n\n# 5️⃣ Feature Engineering & Reduction\nmerged_nodes['relative_humidity_avg'] = (\n    merged_nodes['relative_humidity_rmin'] + merged_nodes['relative_humidity_rmax']\n) / 2\nmerged_nodes['humidity_diff'] = merged_nodes['relative_humidity_rmax'] - merged_nodes['relative_humidity_rmin']\nmerged_nodes.drop(columns=['relative_humidity_rmin', 'relative_humidity_rmax', 'Slope', 'wind_speed'], inplace=True)\n\n# 6️⃣ Select Feature Columns\nfeature_columns = [\n    'Elevation', 'SWE', 'snow_depth', 'air_temperature_observed_f',\n    'precipitation_amount', 'cumulative_SWE', 'cumulative_precipitation_amount',\n    'relative_humidity_avg', 'humidity_diff', 'sin_doy', 'cos_doy'\n]\n\n# 7️⃣ Normalize Features\nscaler = StandardScaler()\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\nnode_features = torch.tensor(node_features, dtype=torch.float)\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\n\n# 8️⃣ Build Graph Edges Using KDTree\ncoordinates = merged_nodes[['lat', 'lon']].values\ntree = KDTree(coordinates)\nk = 6\nthreshold = 0.5\n\ndistances, neighbors = tree.query(coordinates, k=k+1)\nedge_list = []\nfor i in range(len(neighbors)):\n    for j in range(1, k+1):\n        if distances[i][j] < threshold:\n            edge_list.append((i, neighbors[i][j]))\n\nedges = np.array(edge_list).T\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\n\n# 9️⃣ Build PyTorch Geometric Data Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\ngraph_data.dates = merged_nodes['date'].astype(str).tolist()\n\n# 🔟 Graph Summary\nprint(\"\\n📊 Graph Summary Report\")\nprint(f\"🔹 Number of nodes       : {graph_data.num_nodes}\")\nprint(f\"🔹 Number of edges       : {graph_data.num_edges}\")\nprint(f\"🔹 Node feature shape    : {graph_data.x.shape}\")\nprint(f\"🔹 Edge index shape      : {graph_data.edge_index.shape}\")\n\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\nprint(f\"\\n📈 Degree Statistics:\")\nprint(f\"   • Min degree          : {deg.min().item()}\")\nprint(f\"   • Max degree          : {deg.max().item()}\")\nprint(f\"   • Mean degree         : {deg.float().mean().item():.2f}\")\nprint(f\"   • Std deviation       : {deg.float().std().item():.2f}\")\n\nnum_isolated_nodes = (deg == 0).sum().item()\npercentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\nprint(f\"\\n🚨 Isolated Nodes:\")\nprint(f\"   • Count               : {num_isolated_nodes}\")\nprint(f\"   • Percent             : {percentage_isolated:.4f}%\")\n\nprint(\"\\n✅ Graph ready for PINN modeling!\")\n\n# 🔒 Save\ngraph_data_output_path = \"/media/volume1/gnn_graph_time_all_states_1.pt\"\ntorch.save(graph_data, graph_data_output_path)\nprint(f\"\\n✅ PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\n",
  "history_output" : "📊 Graph Summary Report\n🔹 Number of nodes       : 67694\n🔹 Number of edges       : 406108\n🔹 Node feature shape    : torch.Size([67694, 11])\n🔹 Edge index shape      : torch.Size([2, 406108])\n📈 Degree Statistics:\n   • Min degree          : 0.0\n   • Max degree          : 6.0\n   • Mean degree         : 6.00\n   • Std deviation       : 0.06\n🚨 Isolated Nodes:\n   • Count               : 1\n   • Percent             : 0.0015%\n✅ Graph ready for PINN modeling!\n✅ PyTorch Geometric Graph Data saved at: /media/volume1/gnn_graph_time_all_states_1.pt\n",
  "history_begin_time" : 1745274513303,
  "history_end_time" : 1745274554518,
  "history_notes" : "checkpoint-final-all-states",
  "history_process" : "p7ar4i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "eIjGBKTL25ny",
  "history_input" : "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # 🔒 Disable GPU to avoid libnccl.so.2 errors\n\nimport pandas as pd\nimport torch\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nfrom torch_geometric.utils import degree\n\n# 1️⃣ Load Dataset\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\nchunksize = 500000\n\nmin_lat, max_lat = 42.020065, 45.768982\nmin_lon, max_lon = -124.212205, -116.940392\n\nuseful_columns = [\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\n]\n\ndf_list = []\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\n    chunk['date'] = pd.to_datetime(chunk['date'])\n    chunk['day_of_year'] = chunk['date'].dt.dayofyear\n    filtered_chunk = chunk[\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon) &\n        (chunk['swe_value'] >= 0) & (chunk['swe_value'] < 3000)\n    ]\n    df_list.append(filtered_chunk)\n\ndata = pd.concat(df_list, ignore_index=True)\ndata = data.drop_duplicates()\n\n# 2️⃣ Merge Nodes Using Spatial & Temporal Grid Binning\ngrid_size = 0.2\nnum_time_bins = 24\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\ndata['dayofyear_bin'] = (data['day_of_year'] // (365 / num_time_bins)).astype(int)\ndata['grid_id'] = (\n    data['lat_bin'].astype(str) + \"_\" +\n    data['lon_bin'].astype(str) + \"_\" +\n    data['dayofyear_bin'].astype(str)\n)\n\n# 3️⃣ Aggregate with representative date\nmerged_nodes = data.groupby('grid_id').agg({\n    'Elevation': 'mean',\n    'Slope': 'mean',\n    'SWE': 'mean',\n    'snow_depth': 'mean',\n    'air_temperature_observed_f': 'mean',\n    'precipitation_amount': 'mean',\n    'relative_humidity_rmin': 'mean',\n    'relative_humidity_rmax': 'mean',\n    'wind_speed': 'mean',\n    'cumulative_SWE': 'mean',\n    'cumulative_precipitation_amount': 'mean',\n    'swe_value': 'mean',\n    'lat': 'mean',\n    'lon': 'mean',\n    'day_of_year': 'mean',\n    'date': 'first'\n}).reset_index()\n\n# 4️⃣ Temporal features\nmerged_nodes['sin_doy'] = np.sin(2 * np.pi * merged_nodes['day_of_year'] / 365)\nmerged_nodes['cos_doy'] = np.cos(2 * np.pi * merged_nodes['day_of_year'] / 365)\n\n# 5️⃣ Feature Engineering & Reduction\nmerged_nodes['relative_humidity_avg'] = (\n    merged_nodes['relative_humidity_rmin'] + merged_nodes['relative_humidity_rmax']\n) / 2\nmerged_nodes['humidity_diff'] = merged_nodes['relative_humidity_rmax'] - merged_nodes['relative_humidity_rmin']\nmerged_nodes.drop(columns=['relative_humidity_rmin', 'relative_humidity_rmax', 'Slope', 'wind_speed'], inplace=True)\n\n# 6️⃣ Select Features\nfeature_columns = [\n    'Elevation', 'SWE', 'snow_depth', 'air_temperature_observed_f',\n    'precipitation_amount', 'cumulative_SWE', 'cumulative_precipitation_amount',\n    'relative_humidity_avg', 'humidity_diff', 'sin_doy', 'cos_doy'\n]\n\n# 7️⃣ Normalize Features\nscaler = StandardScaler()\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\nnode_features = torch.tensor(node_features, dtype=torch.float)\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\n\n# 8️⃣ Graph edges using KDTree\ncoordinates = merged_nodes[['lat', 'lon']].values\ntree = KDTree(coordinates)\nk = 6\nthreshold = 0.5\n\ndistances, neighbors = tree.query(coordinates, k=k+1)\nedge_list = []\nfor i in range(len(neighbors)):\n    for j in range(1, k+1):\n        if distances[i][j] < threshold:\n            edge_list.append((i, neighbors[i][j]))\n\nedges = np.array(edge_list).T\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\n\n# 9️⃣ Create PyTorch Geometric Graph\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\ngraph_data.dates = merged_nodes['date'].astype(str).tolist()\n\n# 🔍 Stats\nprint(\"\\n📊 Graph Summary Report\")\nprint(f\"🔹 Number of nodes       : {graph_data.num_nodes}\")\nprint(f\"🔹 Number of edges       : {graph_data.num_edges}\")\nprint(f\"🔹 Node feature shape    : {graph_data.x.shape}\")\nprint(f\"🔹 Edge index shape      : {graph_data.edge_index.shape}\")\n\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\n\nprint(f\"\\n📈 Degree Statistics:\")\nprint(f\"   • Min degree          : {deg.min().item()}\")\nprint(f\"   • Max degree          : {deg.max().item()}\")\nprint(f\"   • Mean degree         : {deg.float().mean().item():.2f}\")\nprint(f\"   • Std deviation       : {deg.float().std().item():.2f}\")\n\nnum_isolated_nodes = (deg == 0).sum().item()\npercentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\nprint(f\"\\n🚨 Isolated Nodes:\")\nprint(f\"   • Count               : {num_isolated_nodes}\")\nprint(f\"   • Percent             : {percentage_isolated:.4f}%\")\n\nprint(\"\\n✅ Graph ready for PINN modeling!\")\n\n# 🔒 Save\ngraph_data_output_path = \"/media/volume1/gnn_graph_time_3.pt\"\ntorch.save(graph_data, graph_data_output_path)\nprint(f\"\\n✅ PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\n",
  "history_output" : "📊 Graph Summary Report\n🔹 Number of nodes       : 6748\n🔹 Number of edges       : 40488\n🔹 Node feature shape    : torch.Size([6748, 11])\n🔹 Edge index shape      : torch.Size([2, 40488])\n📈 Degree Statistics:\n   • Min degree          : 6.0\n   • Max degree          : 6.0\n   • Mean degree         : 6.00\n   • Std deviation       : 0.00\n🚨 Isolated Nodes:\n   • Count               : 0\n   • Percent             : 0.0000%\n✅ Graph ready for PINN modeling!\n✅ PyTorch Geometric Graph Data saved at: /media/volume1/gnn_graph_time_3.pt\n",
  "history_begin_time" : 1745274225850,
  "history_end_time" : 1745274256955,
  "history_notes" : null,
  "history_process" : "p7ar4i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "4oPcxs5v85Bl",
  "history_input" : "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # 🔒 Disable GPU to avoid libnccl.so.2 errors\n\nimport pandas as pd\nimport torch\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nfrom torch_geometric.utils import degree\n\n# 1️⃣ Load Dataset\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\nchunksize = 500000\n\nmin_lat, max_lat = 42.020065, 45.768982\nmin_lon, max_lon = -124.212205, -116.940392\n\nuseful_columns = [\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\n]\n\ndf_list = []\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\n    chunk['date'] = pd.to_datetime(chunk['date'])\n    chunk['day_of_year'] = chunk['date'].dt.dayofyear\n    filtered_chunk = chunk[\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon) &\n        (chunk['swe_value'] >= 0) & (chunk['swe_value'] < 3000)\n    ]\n    df_list.append(filtered_chunk)\n\ndata = pd.concat(df_list, ignore_index=True)\ndata = data.drop_duplicates()\n\n# 2️⃣ Merge Nodes Using Spatial & Temporal Grid Binning\ngrid_size = 0.2\nnum_time_bins = 24\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\ndata['dayofyear_bin'] = (data['day_of_year'] // (365 / num_time_bins)).astype(int)\ndata['grid_id'] = (\n    data['lat_bin'].astype(str) + \"_\" +\n    data['lon_bin'].astype(str) + \"_\" +\n    data['dayofyear_bin'].astype(str)\n)\n\n# 3️⃣ Aggregate with representative date\nmerged_nodes = data.groupby('grid_id').agg({\n    'Elevation': 'mean',\n    'Slope': 'mean',\n    'SWE': 'mean',\n    'snow_depth': 'mean',\n    'air_temperature_observed_f': 'mean',\n    'precipitation_amount': 'mean',\n    'relative_humidity_rmin': 'mean',\n    'relative_humidity_rmax': 'mean',\n    'wind_speed': 'mean',\n    'cumulative_SWE': 'mean',\n    'cumulative_precipitation_amount': 'mean',\n    'swe_value': 'mean',\n    'lat': 'mean',\n    'lon': 'mean',\n    'day_of_year': 'mean',\n    'date': 'first'\n}).reset_index()\n\n# 4️⃣ Temporal features\nmerged_nodes['sin_doy'] = np.sin(2 * np.pi * merged_nodes['day_of_year'] / 365)\nmerged_nodes['cos_doy'] = np.cos(2 * np.pi * merged_nodes['day_of_year'] / 365)\n\n# 5️⃣ Feature Engineering & Reduction\nmerged_nodes['relative_humidity_avg'] = (\n    merged_nodes['relative_humidity_rmin'] + merged_nodes['relative_humidity_rmax']\n) / 2\nmerged_nodes['humidity_diff'] = merged_nodes['relative_humidity_rmax'] - merged_nodes['relative_humidity_rmin']\nmerged_nodes.drop(columns=['relative_humidity_rmin', 'relative_humidity_rmax', 'Slope', 'wind_speed'], inplace=True)\n\n# 6️⃣ Select Features\nfeature_columns = [\n    'Elevation', 'SWE', 'snow_depth', 'air_temperature_observed_f',\n    'precipitation_amount', 'cumulative_SWE', 'cumulative_precipitation_amount',\n    'relative_humidity_avg', 'humidity_diff', 'sin_doy', 'cos_doy'\n]\n\n# 7️⃣ Normalize Features\nscaler = StandardScaler()\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\nnode_features = torch.tensor(node_features, dtype=torch.float)\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\n\n# 8️⃣ Graph edges using KDTree\ncoordinates = merged_nodes[['lat', 'lon']].values\ntree = KDTree(coordinates)\nk = 6\nthreshold = 0.5\n\ndistances, neighbors = tree.query(coordinates, k=k+1)\nedge_list = []\nfor i in range(len(neighbors)):\n    for j in range(1, k+1):\n        if distances[i][j] < threshold:\n            edge_list.append((i, neighbors[i][j]))\n\nedges = np.array(edge_list).T\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\n\n# 9️⃣ Create PyTorch Geometric Graph\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\ngraph_data.dates = merged_nodes['date'].astype(str).tolist()\n\n# 🔍 Stats\nprint(\"\\n📊 Graph Summary Report\")\nprint(f\"🔹 Number of nodes       : {graph_data.num_nodes}\")\nprint(f\"🔹 Number of edges       : {graph_data.num_edges}\")\nprint(f\"🔹 Node feature shape    : {graph_data.x.shape}\")\nprint(f\"🔹 Edge index shape      : {graph_data.edge_index.shape}\")\n\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\n\nprint(f\"\\n📈 Degree Statistics:\")\nprint(f\"   • Min degree          : {deg.min().item()}\")\nprint(f\"   • Max degree          : {deg.max().item()}\")\nprint(f\"   • Mean degree         : {deg.float().mean().item():.2f}\")\nprint(f\"   • Std deviation       : {deg.float().std().item():.2f}\")\n\nnum_isolated_nodes = (deg == 0).sum().item()\npercentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\nprint(f\"\\n🚨 Isolated Nodes:\")\nprint(f\"   • Count               : {num_isolated_nodes}\")\nprint(f\"   • Percent             : {percentage_isolated:.4f}%\")\n\nprint(\"\\n✅ Graph ready for PINN modeling!\")\n\n# 🔒 Save\n# graph_data_output_path = \"/media/volume1/gnn_graph_time_2.pt\"\n# torch.save(graph_data, graph_data_output_path)\n# print(f\"\\n✅ PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\n",
  "history_output" : "📊 Graph Summary Report\n🔹 Number of nodes       : 6748\n🔹 Number of edges       : 40488\n🔹 Node feature shape    : torch.Size([6748, 11])\n🔹 Edge index shape      : torch.Size([2, 40488])\n📈 Degree Statistics:\n   • Min degree          : 6.0\n   • Max degree          : 6.0\n   • Mean degree         : 6.00\n   • Std deviation       : 0.00\n🚨 Isolated Nodes:\n   • Count               : 0\n   • Percent             : 0.0000%\n✅ Graph ready for PINN modeling!\n",
  "history_begin_time" : 1745274133495,
  "history_end_time" : 1745274164197,
  "history_notes" : "k=6",
  "history_process" : "p7ar4i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "zLD9U3CJjedo",
  "history_input" : "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # 🔒 Disable GPU to avoid libnccl.so.2 errors\n\nimport pandas as pd\nimport torch\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nfrom torch_geometric.utils import degree\n\n# 1️⃣ Load Dataset\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\nchunksize = 500000\n\nmin_lat, max_lat = 42.020065, 45.768982\nmin_lon, max_lon = -124.212205, -116.940392\n\nuseful_columns = [\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\n]\n\ndf_list = []\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\n    chunk['date'] = pd.to_datetime(chunk['date'])\n    chunk['day_of_year'] = chunk['date'].dt.dayofyear\n    filtered_chunk = chunk[\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon) &\n        (chunk['swe_value'] >= 0) & (chunk['swe_value'] < 3000)\n    ]\n    df_list.append(filtered_chunk)\n\ndata = pd.concat(df_list, ignore_index=True)\ndata = data.drop_duplicates()\n\n# 2️⃣ Merge Nodes Using Spatial & Temporal Grid Binning\ngrid_size = 0.2\nnum_time_bins = 12\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\ndata['dayofyear_bin'] = (data['day_of_year'] // (365 / num_time_bins)).astype(int)\ndata['grid_id'] = (\n    data['lat_bin'].astype(str) + \"_\" +\n    data['lon_bin'].astype(str) + \"_\" +\n    data['dayofyear_bin'].astype(str)\n)\n\n# 3️⃣ Aggregate with representative date\nmerged_nodes = data.groupby('grid_id').agg({\n    'Elevation': 'mean',\n    'Slope': 'mean',\n    'SWE': 'mean',\n    'snow_depth': 'mean',\n    'air_temperature_observed_f': 'mean',\n    'precipitation_amount': 'mean',\n    'relative_humidity_rmin': 'mean',\n    'relative_humidity_rmax': 'mean',\n    'wind_speed': 'mean',\n    'cumulative_SWE': 'mean',\n    'cumulative_precipitation_amount': 'mean',\n    'swe_value': 'mean',\n    'lat': 'mean',\n    'lon': 'mean',\n    'day_of_year': 'mean',\n    'date': 'first'\n}).reset_index()\n\n# 4️⃣ Temporal features\nmerged_nodes['sin_doy'] = np.sin(2 * np.pi * merged_nodes['day_of_year'] / 365)\nmerged_nodes['cos_doy'] = np.cos(2 * np.pi * merged_nodes['day_of_year'] / 365)\n\n# 5️⃣ Feature reduction\nmerged_nodes['relative_humidity_avg'] = (\n    merged_nodes['relative_humidity_rmin'] + merged_nodes['relative_humidity_rmax']\n) / 2\nmerged_nodes.drop(columns=['relative_humidity_rmin', 'relative_humidity_rmax', 'Slope', 'wind_speed'], inplace=True)\n\n# 6️⃣ Normalize & construct features\nfeature_columns = [\n    'Elevation', 'SWE', 'snow_depth', 'air_temperature_observed_f',\n    'precipitation_amount', 'cumulative_SWE', 'cumulative_precipitation_amount',\n    'relative_humidity_avg', 'sin_doy', 'cos_doy', 'lat', 'lon'\n]\n\nscaler = StandardScaler()\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\nnode_features = torch.tensor(node_features, dtype=torch.float)\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\n\n# 7️⃣ Graph edges using KDTree\ncoordinates = merged_nodes[['lat', 'lon']].values\ntree = KDTree(coordinates)\nk = 4\nthreshold = 0.3\n\ndistances, neighbors = tree.query(coordinates, k=k+1)\nedge_list = []\nfor i in range(len(neighbors)):\n    for j in range(1, k+1):\n        if distances[i][j] < threshold:\n            edge_list.append((i, neighbors[i][j]))\n\nedges = np.array(edge_list).T\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\n\n# 8️⃣ Create PyTorch Geometric Graph\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\ngraph_data.dates = merged_nodes['date'].astype(str).tolist()\n\nprint(\"\\n📊 Graph Summary Report\")\nprint(f\"🔹 Number of nodes       : {graph_data.num_nodes}\")\nprint(f\"🔹 Number of edges       : {graph_data.num_edges}\")\nprint(f\"🔹 Node feature shape    : {graph_data.x.shape}\")\nprint(f\"🔹 Edge index shape      : {graph_data.edge_index.shape}\")\n\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\n\nprint(f\"\\n📈 Degree Statistics:\")\nprint(f\"   • Min degree          : {deg.min().item()}\")\nprint(f\"   • Max degree          : {deg.max().item()}\")\nprint(f\"   • Mean degree         : {deg.float().mean().item():.2f}\")\nprint(f\"   • Std deviation       : {deg.float().std().item():.2f}\")\n\nnum_isolated_nodes = (deg == 0).sum().item()\npercentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\nprint(f\"\\n🚨 Isolated Nodes:\")\nprint(f\"   • Count               : {num_isolated_nodes}\")\nprint(f\"   • Percent             : {percentage_isolated:.4f}%\")\n\nprint(\"\\n✅ Graph ready for PINN modeling!\")\n\n\n# 9️⃣ Save\ngraph_data_output_path = \"/media/volume1/gnn_graph_time_2.pt\"\ntorch.save(graph_data, graph_data_output_path)\n\nprint(f\"\\n✅ PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\n",
  "history_output" : "📊 Graph Summary Report\n🔹 Number of nodes       : 3496\n🔹 Number of edges       : 13984\n🔹 Node feature shape    : torch.Size([3496, 12])\n🔹 Edge index shape      : torch.Size([2, 13984])\n📈 Degree Statistics:\n   • Min degree          : 4.0\n   • Max degree          : 4.0\n   • Mean degree         : 4.00\n   • Std deviation       : 0.00\n🚨 Isolated Nodes:\n   • Count               : 0\n   • Percent             : 0.0000%\n✅ Graph ready for PINN modeling!\n✅ PyTorch Geometric Graph Data saved at: /media/volume1/gnn_graph_time_2.pt\n",
  "history_begin_time" : 1745272231206,
  "history_end_time" : 1745272261998,
  "history_notes" : null,
  "history_process" : "p7ar4i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "YRh9x82Cr6nh",
  "history_input" : "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # 🔒 Disable GPU to avoid libnccl.so.2 errors\n\nimport pandas as pd\nimport torch\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nfrom torch_geometric.utils import degree\n\n# 1️⃣ Load Dataset\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\nchunksize = 500000\n\nmin_lat, max_lat = 42.020065, 45.768982\nmin_lon, max_lon = -124.212205, -116.940392\n\nuseful_columns = [\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\n]\n\ndf_list = []\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\n    chunk['date'] = pd.to_datetime(chunk['date'])\n    chunk['day_of_year'] = chunk['date'].dt.dayofyear\n    filtered_chunk = chunk[\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon) &\n        (chunk['swe_value'] >= 0) & (chunk['swe_value'] < 3000)\n    ]\n    df_list.append(filtered_chunk)\n\ndata = pd.concat(df_list, ignore_index=True)\ndata = data.drop_duplicates()\n\n# 2️⃣ Merge Nodes Using Spatial & Temporal Grid Binning\ngrid_size = 0.2\nnum_time_bins = 12\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\ndata['dayofyear_bin'] = (data['day_of_year'] // (365 / num_time_bins)).astype(int)\ndata['grid_id'] = (\n    data['lat_bin'].astype(str) + \"_\" +\n    data['lon_bin'].astype(str) + \"_\" +\n    data['dayofyear_bin'].astype(str)\n)\n\n# 3️⃣ Aggregate with representative date\nmerged_nodes = data.groupby('grid_id').agg({\n    'Elevation': 'mean',\n    'Slope': 'mean',\n    'SWE': 'mean',\n    'snow_depth': 'mean',\n    'air_temperature_observed_f': 'mean',\n    'precipitation_amount': 'mean',\n    'relative_humidity_rmin': 'mean',\n    'relative_humidity_rmax': 'mean',\n    'wind_speed': 'mean',\n    'cumulative_SWE': 'mean',\n    'cumulative_precipitation_amount': 'mean',\n    'swe_value': 'mean',\n    'lat': 'mean',\n    'lon': 'mean',\n    'day_of_year': 'mean',\n    'date': 'first'\n}).reset_index()\n\n# 4️⃣ Temporal features\nmerged_nodes['sin_doy'] = np.sin(2 * np.pi * merged_nodes['day_of_year'] / 365)\nmerged_nodes['cos_doy'] = np.cos(2 * np.pi * merged_nodes['day_of_year'] / 365)\n\n# 5️⃣ Feature reduction\nmerged_nodes['relative_humidity_avg'] = (\n    merged_nodes['relative_humidity_rmin'] + merged_nodes['relative_humidity_rmax']\n) / 2\nmerged_nodes.drop(columns=['relative_humidity_rmin', 'relative_humidity_rmax', 'Slope', 'wind_speed'], inplace=True)\n\n# 6️⃣ Normalize & construct features\nfeature_columns = [\n    'Elevation', 'SWE', 'snow_depth', 'air_temperature_observed_f',\n    'precipitation_amount', 'cumulative_SWE', 'cumulative_precipitation_amount',\n    'relative_humidity_avg', 'sin_doy', 'cos_doy', 'lat', 'lon'\n]\n\nscaler = StandardScaler()\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\nnode_features = torch.tensor(node_features, dtype=torch.float)\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\n\n# 7️⃣ Graph edges using KDTree\ncoordinates = merged_nodes[['lat', 'lon']].values\ntree = KDTree(coordinates)\nk = 4\nthreshold = 0.3\n\ndistances, neighbors = tree.query(coordinates, k=k+1)\nedge_list = []\nfor i in range(len(neighbors)):\n    for j in range(1, k+1):\n        if distances[i][j] < threshold:\n            edge_list.append((i, neighbors[i][j]))\n\nedges = np.array(edge_list).T\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\n\n# 8️⃣ Create PyTorch Geometric Graph\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\ngraph_data.dates = merged_nodes['date'].astype(str).tolist()\n\nprint(\"\\n📊 Graph Summary Report\")\nprint(f\"🔹 Number of nodes       : {graph_data.num_nodes}\")\nprint(f\"🔹 Number of edges       : {graph_data.num_edges}\")\nprint(f\"🔹 Node feature shape    : {graph_data.x.shape}\")\nprint(f\"🔹 Edge index shape      : {graph_data.edge_index.shape}\")\n\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\n\nprint(f\"\\n📈 Degree Statistics:\")\nprint(f\"   • Min degree          : {deg.min().item()}\")\nprint(f\"   • Max degree          : {deg.max().item()}\")\nprint(f\"   • Mean degree         : {deg.float().mean().item():.2f}\")\nprint(f\"   • Std deviation       : {deg.float().std().item():.2f}\")\n\nnum_isolated_nodes = (deg == 0).sum().item()\npercentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\nprint(f\"\\n🚨 Isolated Nodes:\")\nprint(f\"   • Count               : {num_isolated_nodes}\")\nprint(f\"   • Percent             : {percentage_isolated:.4f}%\")\n\nprint(\"\\n✅ Graph ready for PINN modeling!\")\n\n\n# 9️⃣ Save\n# graph_data_output_path = \"/media/volume1/gnn_graph_time_enhanced.pt\"\n# torch.save(graph_data, graph_data_output_path)\n\n# print(f\"\\n✅ PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\n",
  "history_output" : "📊 Graph Summary Report\n🔹 Number of nodes       : 3496\n🔹 Number of edges       : 13984\n🔹 Node feature shape    : torch.Size([3496, 12])\n🔹 Edge index shape      : torch.Size([2, 13984])\n📈 Degree Statistics:\n   • Min degree          : 4.0\n   • Max degree          : 4.0\n   • Mean degree         : 4.00\n   • Std deviation       : 0.00\n🚨 Isolated Nodes:\n   • Count               : 0\n   • Percent             : 0.0000%\n✅ Graph ready for PINN modeling!\n",
  "history_begin_time" : 1745272176366,
  "history_end_time" : 1745272207557,
  "history_notes" : null,
  "history_process" : "p7ar4i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "t0qphunbnE6i",
  "history_input" : "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # 🔒 Disable GPU to avoid libnccl.so.2 errors\n\nimport pandas as pd\nimport torch\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nfrom torch_geometric.utils import degree\n\n# 1️⃣ Load Dataset\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\nchunksize = 500000\n\nmin_lat, max_lat = 42.020065, 45.768982\nmin_lon, max_lon = -124.212205, -116.940392\n\nuseful_columns = [\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\n]\n\ndf_list = []\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\n    chunk['date'] = pd.to_datetime(chunk['date'])\n    chunk['day_of_year'] = chunk['date'].dt.dayofyear\n    filtered_chunk = chunk[\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon) &\n        (chunk['swe_value'] >= 0) & (chunk['swe_value'] < 3000)\n    ]\n    df_list.append(filtered_chunk)\n\ndata = pd.concat(df_list, ignore_index=True)\ndata = data.drop_duplicates()\n\n# 2️⃣ Merge Nodes Using Spatial & Temporal Grid Binning\ngrid_size = 0.2\nnum_time_bins = 12\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\ndata['dayofyear_bin'] = (data['day_of_year'] // (365 / num_time_bins)).astype(int)\ndata['grid_id'] = (\n    data['lat_bin'].astype(str) + \"_\" +\n    data['lon_bin'].astype(str) + \"_\" +\n    data['dayofyear_bin'].astype(str)\n)\n\n# 3️⃣ Aggregate with representative date\nmerged_nodes = data.groupby('grid_id').agg({\n    'Elevation': 'mean',\n    'Slope': 'mean',\n    'SWE': 'mean',\n    'snow_depth': 'mean',\n    'air_temperature_observed_f': 'mean',\n    'precipitation_amount': 'mean',\n    'relative_humidity_rmin': 'mean',\n    'relative_humidity_rmax': 'mean',\n    'wind_speed': 'mean',\n    'cumulative_SWE': 'mean',\n    'cumulative_precipitation_amount': 'mean',\n    'swe_value': 'mean',\n    'lat': 'mean',\n    'lon': 'mean',\n    'day_of_year': 'mean',\n    'date': 'first'\n}).reset_index()\n\n# 4️⃣ Temporal features\nmerged_nodes['sin_doy'] = np.sin(2 * np.pi * merged_nodes['day_of_year'] / 365)\nmerged_nodes['cos_doy'] = np.cos(2 * np.pi * merged_nodes['day_of_year'] / 365)\n\n# 5️⃣ Feature reduction\nmerged_nodes['relative_humidity_avg'] = (\n    merged_nodes['relative_humidity_rmin'] + merged_nodes['relative_humidity_rmax']\n) / 2\nmerged_nodes.drop(columns=['relative_humidity_rmin', 'relative_humidity_rmax', 'Slope', 'wind_speed'], inplace=True)\n\n# 6️⃣ Normalize & construct features\nfeature_columns = [\n    'Elevation', 'SWE', 'snow_depth', 'air_temperature_observed_f',\n    'precipitation_amount', 'cumulative_SWE', 'cumulative_precipitation_amount',\n    'relative_humidity_avg', 'sin_doy', 'cos_doy', 'lat', 'lon'\n]\n\nscaler = StandardScaler()\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\nnode_features = torch.tensor(node_features, dtype=torch.float)\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\n\n# 7️⃣ Graph edges using KDTree\ncoordinates = merged_nodes[['lat', 'lon']].values\ntree = KDTree(coordinates)\nk = 4\nthreshold = 0.3\n\ndistances, neighbors = tree.query(coordinates, k=k+1)\nedge_list = []\nfor i in range(len(neighbors)):\n    for j in range(1, k+1):\n        if distances[i][j] < threshold:\n            edge_list.append((i, neighbors[i][j]))\n\nedges = np.array(edge_list).T\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\n\n# 8️⃣ Create PyTorch Geometric Graph\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\ngraph_data.dates = merged_nodes['date'].astype(str).tolist()\n\n# 9️⃣ Save\n# graph_data_output_path = \"/media/volume1/gnn_graph_time_enhanced.pt\"\n# torch.save(graph_data, graph_data_output_path)\n\n# print(f\"\\n✅ PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\n",
  "history_output" : "",
  "history_begin_time" : 1745272056200,
  "history_end_time" : 1745272087353,
  "history_notes" : null,
  "history_process" : "p7ar4i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "waWz2Ym7WY7s",
  "history_input" : "import pandas as pd\nimport torch\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nfrom torch_geometric.utils import degree\n\n# 1️⃣ Load Dataset\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\nchunksize = 500000\n\n# Define spatial bounds\nmin_lat, max_lat = 42.020065, 45.768982\nmin_lon, max_lon = -124.212205, -116.940392\n\nprint(\"Loading dataset...\")\n\nuseful_columns = [\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\n]\n\ndf_list = []\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\n    chunk['date'] = pd.to_datetime(chunk['date'])\n    chunk['day_of_year'] = chunk['date'].dt.dayofyear\n\n    filtered_chunk = chunk[\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\n    ]\n    df_list.append(filtered_chunk)\n\ndata = pd.concat(df_list, ignore_index=True)\nprint(f\"Filtered dataset loaded with {data.shape[0]} rows.\")\n\ndata = data.drop_duplicates()\nprint(f\"After removing duplicates: {data.shape[0]} rows.\")\n\n# 2️⃣ Merge Nodes Using Spatial & Temporal Grid Binning\ngrid_size = 0.4\nnum_time_bins = 12\n\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\ndata['dayofyear_bin'] = (data['day_of_year'] // (365 / num_time_bins)).astype(int)\n\ndata['grid_id'] = (\n    data['lat_bin'].astype(str) + \"_\" +\n    data['lon_bin'].astype(str) + \"_\" +\n    data['dayofyear_bin'].astype(str)\n)\n\n# 3️⃣ Aggregation with representative date retained\nmerged_nodes = data.groupby('grid_id').agg({\n    'Elevation': 'mean',\n    'Slope': 'mean',\n    'SWE': 'mean',\n    'snow_depth': 'mean',\n    'air_temperature_observed_f': 'mean',\n    'precipitation_amount': 'mean',\n    'relative_humidity_rmin': 'mean',\n    'relative_humidity_rmax': 'mean',\n    'wind_speed': 'mean',\n    'cumulative_SWE': 'mean',\n    'cumulative_precipitation_amount': 'mean',\n    'swe_value': 'mean',\n    'lat': 'mean',\n    'lon': 'mean',\n    'date': 'first'  # Get a representative date for each node\n}).reset_index()\n\nprint(f\"Merged nodes count: {merged_nodes.shape[0]} from {data.shape[0]} original nodes.\")\n\n# 4️⃣ Feature Merging & Reduction\nmerged_nodes['relative_humidity_avg'] = (merged_nodes['relative_humidity_rmin'] + merged_nodes['relative_humidity_rmax']) / 2\nmerged_nodes.drop(columns=['relative_humidity_rmin', 'relative_humidity_rmax'], inplace=True)\nmerged_nodes.drop(columns=['Slope', 'wind_speed'], inplace=True)\n\n# 5️⃣ Normalize Features\nscaler = StandardScaler()\nfeature_columns = [\n    'Elevation', 'SWE', 'snow_depth', 'air_temperature_observed_f',\n    'precipitation_amount', 'cumulative_SWE', 'cumulative_precipitation_amount',\n    'relative_humidity_avg'\n]\n\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\nnode_features = torch.tensor(node_features, dtype=torch.float)\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\n\n# 6️⃣ Build Graph Edges Using KDTree (k-NN with Distance Filtering)\ncoordinates = merged_nodes[['lat', 'lon']].values\ntree = KDTree(coordinates)\n\nk = 2\nthreshold = 0.2  # ~22 km\n\ndistances, neighbors = tree.query(coordinates, k=k+1)\n\nedge_list = []\nfor i in range(len(neighbors)):\n    for j in range(1, k+1):\n        if distances[i][j] < threshold:\n            edge_list.append((i, neighbors[i][j]))\n\nedges = np.array(edge_list).T\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\n\nprint(f\"New graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\n\n# 7️⃣ Create PyTorch Geometric Graph Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\ngraph_data.dates = merged_nodes['date'].astype(str).tolist()  # Keep as metadata\n\n# 8️⃣ Graph Summary and Statistics\nprint(\"\\nGraph Data Summary:\")\nprint(graph_data)\nprint(f\"Number of nodes: {graph_data.num_nodes}\")\nprint(f\"Number of edges: {graph_data.num_edges}\")\nprint(f\"Node feature shape: {graph_data.x.shape}\")\nprint(f\"Edge index shape: {graph_data.edge_index.shape}\")\n\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\nprint(f\"\\nDegree Stats:\")\nprint(f\"- Min degree: {deg.min().item()}\")\nprint(f\"- Max degree: {deg.max().item()}\")\nprint(f\"- Mean degree: {deg.float().mean().item()}\")\nprint(f\"- Std dev degree: {deg.float().std().item()}\")\n\nnum_isolated_nodes = (deg == 0).sum().item()\npercentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\nprint(f\"\\nIsolated nodes (degree=0): {num_isolated_nodes}\")\nprint(f\"Percentage of isolated nodes: {percentage_isolated:.4f}%\")\n\n# # 9️⃣ Save Graph\ngraph_data_output_path = \"/media/volume1/gnn_graph_time_1.pt\"\ntorch.save(graph_data, graph_data_output_path)\n\nprint(f\"\\n✅ PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\n",
  "history_output" : "Loading dataset...\nFiltered dataset loaded with 620662 rows.\nAfter removing duplicates: 618231 rows.\nMerged nodes count: 1791 from 618231 original nodes.\nNew graph constructed with 1791 nodes and 3582 edges.\nGraph Data Summary:\nData(x=[1791, 8], edge_index=[2, 3582], y=[1791], dates=[1791])\nNumber of nodes: 1791\nNumber of edges: 3582\nNode feature shape: torch.Size([1791, 8])\nEdge index shape: torch.Size([2, 3582])\nDegree Stats:\n- Min degree: 2.0\n- Max degree: 2.0\n- Mean degree: 2.0\n- Std dev degree: 0.0\nIsolated nodes (degree=0): 0\nPercentage of isolated nodes: 0.0000%\n✅ PyTorch Geometric Graph Data saved at: /media/volume1/gnn_graph_time_1.pt\n",
  "history_begin_time" : 1745254622707,
  "history_end_time" : 1745254653822,
  "history_notes" : null,
  "history_process" : "p7ar4i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "VZicMEecmkhI",
  "history_input" : "import pandas as pd\nimport torch\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nfrom torch_geometric.utils import degree\n\n# 1️⃣ Load Dataset\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\nchunksize = 500000\n\n# Define spatial bounds\nmin_lat, max_lat = 42.020065, 45.768982\nmin_lon, max_lon = -124.212205, -116.940392\n\nprint(\"Loading dataset...\")\n\nuseful_columns = [\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\n]\n\ndf_list = []\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\n    chunk['date'] = pd.to_datetime(chunk['date'])\n    chunk['day_of_year'] = chunk['date'].dt.dayofyear\n\n    filtered_chunk = chunk[\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) &\n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\n    ]\n    df_list.append(filtered_chunk)\n\ndata = pd.concat(df_list, ignore_index=True)\nprint(f\"Filtered dataset loaded with {data.shape[0]} rows.\")\n\ndata = data.drop_duplicates()\nprint(f\"After removing duplicates: {data.shape[0]} rows.\")\n\n# 2️⃣ Merge Nodes Using Spatial & Temporal Grid Binning\ngrid_size = 0.4\nnum_time_bins = 12\n\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\ndata['dayofyear_bin'] = (data['day_of_year'] // (365 / num_time_bins)).astype(int)\n\ndata['grid_id'] = (\n    data['lat_bin'].astype(str) + \"_\" +\n    data['lon_bin'].astype(str) + \"_\" +\n    data['dayofyear_bin'].astype(str)\n)\n\n# 3️⃣ Aggregation with representative date retained\nmerged_nodes = data.groupby('grid_id').agg({\n    'Elevation': 'mean',\n    'Slope': 'mean',\n    'SWE': 'mean',\n    'snow_depth': 'mean',\n    'air_temperature_observed_f': 'mean',\n    'precipitation_amount': 'mean',\n    'relative_humidity_rmin': 'mean',\n    'relative_humidity_rmax': 'mean',\n    'wind_speed': 'mean',\n    'cumulative_SWE': 'mean',\n    'cumulative_precipitation_amount': 'mean',\n    'swe_value': 'mean',\n    'lat': 'mean',\n    'lon': 'mean',\n    'date': 'first'  # Get a representative date for each node\n}).reset_index()\n\nprint(f\"Merged nodes count: {merged_nodes.shape[0]} from {data.shape[0]} original nodes.\")\n\n# 4️⃣ Feature Merging & Reduction\nmerged_nodes['relative_humidity_avg'] = (merged_nodes['relative_humidity_rmin'] + merged_nodes['relative_humidity_rmax']) / 2\nmerged_nodes.drop(columns=['relative_humidity_rmin', 'relative_humidity_rmax'], inplace=True)\nmerged_nodes.drop(columns=['Slope', 'wind_speed'], inplace=True)\n\n# 5️⃣ Normalize Features\nscaler = StandardScaler()\nfeature_columns = [\n    'Elevation', 'SWE', 'snow_depth', 'air_temperature_observed_f',\n    'precipitation_amount', 'cumulative_SWE', 'cumulative_precipitation_amount',\n    'relative_humidity_avg'\n]\n\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\nnode_features = torch.tensor(node_features, dtype=torch.float)\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\n\n# 6️⃣ Build Graph Edges Using KDTree (k-NN with Distance Filtering)\ncoordinates = merged_nodes[['lat', 'lon']].values\ntree = KDTree(coordinates)\n\nk = 2\nthreshold = 0.2  # ~22 km\n\ndistances, neighbors = tree.query(coordinates, k=k+1)\n\nedge_list = []\nfor i in range(len(neighbors)):\n    for j in range(1, k+1):\n        if distances[i][j] < threshold:\n            edge_list.append((i, neighbors[i][j]))\n\nedges = np.array(edge_list).T\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\n\nprint(f\"New graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\n\n# 7️⃣ Create PyTorch Geometric Graph Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\ngraph_data.dates = merged_nodes['date'].astype(str).tolist()  # Keep as metadata\n\n# 8️⃣ Graph Summary and Statistics\nprint(\"\\nGraph Data Summary:\")\nprint(graph_data)\nprint(f\"Number of nodes: {graph_data.num_nodes}\")\nprint(f\"Number of edges: {graph_data.num_edges}\")\nprint(f\"Node feature shape: {graph_data.x.shape}\")\nprint(f\"Edge index shape: {graph_data.edge_index.shape}\")\n\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\nprint(f\"\\nDegree Stats:\")\nprint(f\"- Min degree: {deg.min().item()}\")\nprint(f\"- Max degree: {deg.max().item()}\")\nprint(f\"- Mean degree: {deg.float().mean().item()}\")\nprint(f\"- Std dev degree: {deg.float().std().item()}\")\n\nnum_isolated_nodes = (deg == 0).sum().item()\npercentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\nprint(f\"\\nIsolated nodes (degree=0): {num_isolated_nodes}\")\nprint(f\"Percentage of isolated nodes: {percentage_isolated:.4f}%\")\n\n# # 9️⃣ Save Graph\n# graph_data_output_path = \"/media/volume1/gnn_graph_final_reduced_wa_4.pt\"\n# torch.save(graph_data, graph_data_output_path)\n\n# print(f\"\\n✅ PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\n",
  "history_output" : "Loading dataset...\nFiltered dataset loaded with 620662 rows.\nAfter removing duplicates: 618231 rows.\nMerged nodes count: 1791 from 618231 original nodes.\nNew graph constructed with 1791 nodes and 3582 edges.\nGraph Data Summary:\nData(x=[1791, 8], edge_index=[2, 3582], y=[1791], dates=[1791])\nNumber of nodes: 1791\nNumber of edges: 3582\nNode feature shape: torch.Size([1791, 8])\nEdge index shape: torch.Size([2, 3582])\nDegree Stats:\n- Min degree: 2.0\n- Max degree: 2.0\n- Mean degree: 2.0\n- Std dev degree: 0.0\nIsolated nodes (degree=0): 0\nPercentage of isolated nodes: 0.0000%\n",
  "history_begin_time" : 1745254455766,
  "history_end_time" : 1745254486977,
  "history_notes" : null,
  "history_process" : "p7ar4i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "4tAOaApASGNF",
  "history_input" : "import pandas as pd\nimport torch\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nfrom torch_geometric.utils import degree\n\n# 1️⃣ Load Dataset\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\nchunksize = 500000  \n\n# Define spatial bounds\nmin_lat, max_lat = 42.020065, 45.768982\nmin_lon, max_lon = -124.212205, -116.940392\n\nprint(\"Loading dataset...\")\n\nuseful_columns = [\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\n]\n\ndf_list = []\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\n    chunk['date'] = pd.to_datetime(chunk['date'])  \n    chunk['day_of_year'] = chunk['date'].dt.dayofyear  \n    \n    filtered_chunk = chunk[\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) & \n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\n    ]\n    df_list.append(filtered_chunk)\n\ndata = pd.concat(df_list, ignore_index=True)\nprint(f\"Filtered dataset loaded with {data.shape[0]} rows.\")\n\ndata = data.drop_duplicates()\nprint(f\"After removing duplicates: {data.shape[0]} rows.\")\n\n# 2️⃣ Merge Nodes Using Spatial & Temporal Grid Binning\ngrid_size = 0.4  # More aggressive merging (~44 km resolution)\nnum_time_bins = 12  # Merge data into ~90-day bins\n\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\n\ndata['dayofyear_bin'] = (data['day_of_year'] // (365 / num_time_bins)).astype(int)\n\ndata['grid_id'] = (\n    data['lat_bin'].astype(str) + \"_\" + \n    data['lon_bin'].astype(str) + \"_\" +\n    data['dayofyear_bin'].astype(str)\n)\n\nmerged_nodes = data.groupby('grid_id').agg({\n    'Elevation': 'mean',\n    'Slope': 'mean',\n    'SWE': 'mean',\n    'snow_depth': 'mean',\n    'air_temperature_observed_f': 'mean',\n    'precipitation_amount': 'mean',\n    'relative_humidity_rmin': 'mean',\n    'relative_humidity_rmax': 'mean',\n    'wind_speed': 'mean',\n    'cumulative_SWE': 'mean',\n    'cumulative_precipitation_amount': 'mean',\n    'swe_value': 'mean',  \n    'lat': 'mean',  \n    'lon': 'mean'\n}).reset_index()\n\nprint(f\"Merged nodes count: {merged_nodes.shape[0]} from {data.shape[0]} original nodes.\")\n\n# 3️⃣ Feature Merging & Reduction\nmerged_nodes['relative_humidity_avg'] = (merged_nodes['relative_humidity_rmin'] + merged_nodes['relative_humidity_rmax']) / 2\nmerged_nodes.drop(columns=['relative_humidity_rmin', 'relative_humidity_rmax'], inplace=True)\n\nmerged_nodes.drop(columns=['Slope', 'wind_speed'], inplace=True)\n\n# 4️⃣ Normalize Features\nscaler = StandardScaler()\nfeature_columns = [\n    'Elevation', 'SWE', 'snow_depth', 'air_temperature_observed_f', \n    'precipitation_amount', 'cumulative_SWE', 'cumulative_precipitation_amount', \n    'relative_humidity_avg'  \n]\n\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\nnode_features = torch.tensor(node_features, dtype=torch.float)\n\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\n\n# 5️⃣ Build Graph Edges Using KDTree (k-NN with Distance Filtering)\ncoordinates = merged_nodes[['lat', 'lon']].values\ntree = KDTree(coordinates)\n\nk = 2  # Reduce nearest neighbors to 2\nthreshold = 0.2  # Increase threshold to 0.2 (~22 km)\n\ndistances, neighbors = tree.query(coordinates, k=k+1)  \n\nedge_list = []\nfor i in range(len(neighbors)):\n    for j in range(1, k+1):  \n        if distances[i][j] < threshold:  \n            edge_list.append((i, neighbors[i][j]))\n\nedges = np.array(edge_list).T\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\n\nprint(f\"New graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\n\n# 6️⃣ Create PyTorch Geometric Graph Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# 7️⃣ Graph Summary and Statistics\nprint(\"\\nGraph Data Summary:\")\nprint(graph_data)\nprint(f\"Number of nodes: {graph_data.num_nodes}\")\nprint(f\"Number of edges: {graph_data.num_edges}\")\nprint(f\"Node feature shape: {graph_data.x.shape}\")\nprint(f\"Edge index shape: {graph_data.edge_index.shape}\")\n\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\nprint(f\"\\nDegree Stats:\")\nprint(f\"- Min degree: {deg.min().item()}\")\nprint(f\"- Max degree: {deg.max().item()}\")\nprint(f\"- Mean degree: {deg.float().mean().item()}\")\nprint(f\"- Standard deviation: {deg.float().std().item()}\")\n\nnum_isolated_nodes = (deg == 0).sum().item()\npercentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\nprint(f\"\\nNumber of isolated nodes (degree=0): {num_isolated_nodes}\")\nprint(f\"Percentage of isolated nodes: {percentage_isolated:.4f}%\")\n\n# 8️⃣ Save Graph\ngraph_data_output_path = \"/media/volume1/gnn_graph_final_reduced_wa_4.pt\"\ntorch.save(graph_data, graph_data_output_path)\n\nprint(f\"\\n✅ PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\n",
  "history_output" : "Loading dataset...\nFiltered dataset loaded with 620662 rows.\nAfter removing duplicates: 618231 rows.\nMerged nodes count: 1791 from 618231 original nodes.\nNew graph constructed with 1791 nodes and 3582 edges.\nGraph Data Summary:\nData(x=[1791, 8], edge_index=[2, 3582], y=[1791])\nNumber of nodes: 1791\nNumber of edges: 3582\nNode feature shape: torch.Size([1791, 8])\nEdge index shape: torch.Size([2, 3582])\nDegree Stats:\n- Min degree: 2.0\n- Max degree: 2.0\n- Mean degree: 2.0\n- Standard deviation: 0.0\nNumber of isolated nodes (degree=0): 0\nPercentage of isolated nodes: 0.0000%\n✅ PyTorch Geometric Graph Data saved at: /media/volume1/gnn_graph_final_reduced_wa_4.pt\n",
  "history_begin_time" : 1741792157463,
  "history_end_time" : 1741792194035,
  "history_notes" : null,
  "history_process" : "p7ar4i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "HIL6Bn0iEMNv",
  "history_input" : "import pandas as pd\nimport torch\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nfrom torch_geometric.utils import degree\nfrom sklearn.cluster import KMeans\n\n# 1️⃣ Load Dataset\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\nchunksize = 500000  # Read in chunks for efficiency\n\n# Define spatial bounds\nmin_lat, max_lat = 42.020065, 45.768982\nmin_lon, max_lon = -124.212205, -116.940392\n\nprint(\"Loading dataset...\")\n\nuseful_columns = [\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\n]\n\n# Load and filter dataset\ndf_list = []\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize, parse_dates=['date']):\n    chunk['day_of_year'] = chunk['date'].dt.dayofyear  # Convert date to day-of-year\n    filtered_chunk = chunk[\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) & \n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\n    ]\n    df_list.append(filtered_chunk)\n\n# Merge filtered chunks\ndata = pd.concat(df_list, ignore_index=True)\nprint(f\"Filtered dataset loaded with {data.shape[0]} rows.\")\n\n# Drop duplicates\ndata = data.drop_duplicates()\nprint(f\"After removing duplicates: {data.shape[0]} rows.\")\n\n# 2️⃣ Apply 15-Day Binning\nbin_size = 15  # Each bin is 15 days\ndata['time_bin'] = (data['day_of_year'] // bin_size).astype(int)\n\n# 3️⃣ Merge Nodes Using Spatial & Temporal Binning\ngrid_size = 0.1  # Larger spatial merging\nnum_time_bins = 24  # Using 15-day bins (~24 bins in a year)\n\n# Assign spatial bins\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\n\n# Create unique spatial-temporal grid ID\ndata['grid_id'] = (\n    data['lat_bin'].astype(str) + \"_\" + \n    data['lon_bin'].astype(str) + \"_\" +\n    data['time_bin'].astype(str)\n)\n\n# Aggregate nodes within each spatial-temporal grid cell (supernodes)\nmerged_nodes = data.groupby('grid_id').agg({\n    'Elevation': 'mean',\n    'Slope': 'mean',\n    'SWE': 'mean',\n    'snow_depth': 'mean',\n    'air_temperature_observed_f': 'mean',\n    'precipitation_amount': 'mean',\n    'relative_humidity_rmin': 'mean',\n    'relative_humidity_rmax': 'mean',\n    'wind_speed': 'mean',\n    'cumulative_SWE': 'mean',\n    'cumulative_precipitation_amount': 'mean',\n    'swe_value': 'mean',\n    'lat': 'mean',\n    'lon': 'mean'\n}).reset_index()\n\nprint(f\"Merged nodes count: {merged_nodes.shape[0]} from {data.shape[0]} original nodes.\")\n\n# 4️⃣ Reduce Nodes with K-Means Clustering (Fixed)\nnum_clusters = int(len(merged_nodes) * 0.98)  # **Keep 98% instead of 95%**\nprint(f\"Running K-Means with {num_clusters} clusters...\")\nkmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\nmerged_nodes['cluster'] = kmeans.fit_predict(merged_nodes[['lat', 'lon']])\n\n# Aggregate again based on clusters\nmerged_nodes = merged_nodes.groupby('cluster').agg({\n    'Elevation': 'mean',\n    'SWE': 'mean',\n    'snow_depth': 'mean',\n    'air_temperature_observed_f': 'mean',\n    'precipitation_amount': 'mean',\n    'relative_humidity_rmin': 'mean',\n    'relative_humidity_rmax': 'mean',\n    'wind_speed': 'mean',\n    'cumulative_SWE': 'mean',\n    'cumulative_precipitation_amount': 'mean',\n    'swe_value': 'mean',\n    'lat': 'mean',\n    'lon': 'mean'\n}).reset_index(drop=True)\n\nprint(f\"Nodes reduced to {merged_nodes.shape[0]} after clustering.\")\n\n# 5️⃣ Normalize Features\nscaler = StandardScaler()\nfeature_columns = [\n    'Elevation', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount'\n]\n\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\nnode_features = torch.tensor(node_features, dtype=torch.float)\n\n# Assign labels (target values)\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\n\n# 6️⃣ Build Graph Edges (Improved Connectivity)\ncoordinates = merged_nodes[['lat', 'lon']].values\ntree = KDTree(coordinates)\n\nk = 7  # **Increase to 7 for better connectivity**\nthreshold = 0.15  # **Increase threshold to connect more distant nodes**\n\ndistances, neighbors = tree.query(coordinates, k=k+1)\n\nedge_list = []\nfor i in range(len(neighbors)):\n    for j in range(1, k+1):\n        if 0.02 < distances[i][j] < threshold:\n            edge_list.append((i, neighbors[i][j]))\n\nedges = np.array(edge_list).T\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\n\nprint(f\"New graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\n\n# 7️⃣ Compute Graph Statistics\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\nprint(\"\\nGraph Data Summary:\")\nprint(graph_data)\nprint(f\"Number of nodes: {graph_data.num_nodes}\")\nprint(f\"Number of edges: {graph_data.num_edges}\")\nprint(f\"Node feature shape: {graph_data.x.shape}\")\nprint(f\"Edge index shape: {graph_data.edge_index.shape}\")\n\n# Compute degree statistics\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\nprint(f\"\\nDegree Stats:\")\nprint(f\"- Min degree: {deg.min().item()}\")\nprint(f\"- Max degree: {deg.max().item()}\")\nprint(f\"- Mean degree: {deg.float().mean().item()}\")\nprint(f\"- Standard deviation: {deg.float().std().item()}\")\n\n# Count isolated nodes\nnum_isolated_nodes = (deg == 0).sum().item()\npercentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\nprint(f\"\\nNumber of isolated nodes (degree=0): {num_isolated_nodes}\")\nprint(f\"Percentage of isolated nodes: {percentage_isolated:.4f}%\")\n\n# 8️⃣ Save Graph\n# graph_data_output_path = \"/media/volume1/gnn_graph_15day_bins_final.pt\"\n# torch.save(graph_data, graph_data_output_path)\n\n# print(f\"\\n✅ PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\n",
  "history_output" : "Loading dataset...\nFiltered dataset loaded with 620662 rows.\nAfter removing duplicates: 618231 rows.\nMerged nodes count: 10721 from 618231 original nodes.\nRunning K-Means with 10506 clusters...\n",
  "history_begin_time" : 1741792038131,
  "history_end_time" : 1741792163360,
  "history_notes" : null,
  "history_process" : "p7ar4i",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "hcwUE2tzAt2q",
  "history_input" : "import pandas as pd\nimport torch\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nfrom torch_geometric.utils import degree\nfrom sklearn.cluster import KMeans\n\n# 1️⃣ Load Dataset\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\nchunksize = 500000  # Read in chunks for efficiency\n\n# Define spatial bounds\nmin_lat, max_lat = 42.020065, 45.768982\nmin_lon, max_lon = -124.212205, -116.940392\n\nprint(\"Loading dataset...\")\n\nuseful_columns = [\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\n]\n\n# Load and filter dataset\ndf_list = []\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize, parse_dates=['date']):\n    chunk['day_of_year'] = chunk['date'].dt.dayofyear  # Convert date to day-of-year\n    filtered_chunk = chunk[\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) & \n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\n    ]\n    df_list.append(filtered_chunk)\n\n# Merge filtered chunks\ndata = pd.concat(df_list, ignore_index=True)\nprint(f\"Filtered dataset loaded with {data.shape[0]} rows.\")\n\n# Drop duplicates\ndata = data.drop_duplicates()\nprint(f\"After removing duplicates: {data.shape[0]} rows.\")\n\n# 2️⃣ Apply 15-Day Binning\nbin_size = 15  # Each bin is 15 days\ndata['time_bin'] = (data['day_of_year'] // bin_size).astype(int)\n\n# 3️⃣ Merge Nodes Using Spatial & Temporal Binning\ngrid_size = 0.1  # Larger spatial merging\nnum_time_bins = 24  # Using 15-day bins (~24 bins in a year)\n\n# Assign spatial bins\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\n\n# Create unique spatial-temporal grid ID\ndata['grid_id'] = (\n    data['lat_bin'].astype(str) + \"_\" + \n    data['lon_bin'].astype(str) + \"_\" +\n    data['time_bin'].astype(str)\n)\n\n# Aggregate nodes within each spatial-temporal grid cell (supernodes)\nmerged_nodes = data.groupby('grid_id').agg({\n    'Elevation': 'mean',\n    'Slope': 'mean',\n    'SWE': 'mean',\n    'snow_depth': 'mean',\n    'air_temperature_observed_f': 'mean',\n    'precipitation_amount': 'mean',\n    'relative_humidity_rmin': 'mean',\n    'relative_humidity_rmax': 'mean',\n    'wind_speed': 'mean',\n    'cumulative_SWE': 'mean',\n    'cumulative_precipitation_amount': 'mean',\n    'swe_value': 'mean',\n    'lat': 'mean',\n    'lon': 'mean'\n}).reset_index()\n\nprint(f\"Merged nodes count: {merged_nodes.shape[0]} from {data.shape[0]} original nodes.\")\n\n# 4️⃣ Reduce Nodes with K-Means Clustering (Fixed)\nnum_clusters = int(len(merged_nodes) * 0.98)  # **Keep 98% instead of 95%**\nkmeans = KMeans(n_clusters=num_clusters, random_state=42)\nmerged_nodes['cluster'] = kmeans.fit_predict(merged_nodes[['lat', 'lon']])\n\n# Aggregate again based on clusters\nmerged_nodes = merged_nodes.groupby('cluster').agg({\n    'Elevation': 'mean',\n    'SWE': 'mean',\n    'snow_depth': 'mean',\n    'air_temperature_observed_f': 'mean',\n    'precipitation_amount': 'mean',\n    'relative_humidity_rmin': 'mean',\n    'relative_humidity_rmax': 'mean',\n    'wind_speed': 'mean',\n    'cumulative_SWE': 'mean',\n    'cumulative_precipitation_amount': 'mean',\n    'swe_value': 'mean',\n    'lat': 'mean',\n    'lon': 'mean'\n}).reset_index(drop=True)\n\nprint(f\"Nodes reduced to {merged_nodes.shape[0]} after clustering.\")\n\n# 5️⃣ Normalize Features\nscaler = StandardScaler()\nfeature_columns = [\n    'Elevation', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount'\n]\n\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\nnode_features = torch.tensor(node_features, dtype=torch.float)\n\n# Assign labels (target values)\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\n\n# 6️⃣ Build Graph Edges (Improved Connectivity)\ncoordinates = merged_nodes[['lat', 'lon']].values\ntree = KDTree(coordinates)\n\nk = 7  # **Increase to 7 for much better connectivity**\nthreshold = 0.15  # **Increase threshold to connect more distant nodes**\n\ndistances, neighbors = tree.query(coordinates, k=k+1)\n\nedge_list = []\nfor i in range(len(neighbors)):\n    for j in range(1, k+1):\n        if 0.02 < distances[i][j] < threshold:\n            edge_list.append((i, neighbors[i][j]))\n\nedges = np.array(edge_list).T\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\n\nprint(f\"New graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\n\n# 7️⃣ Save Graph\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\ngraph_data_output_path = \"/media/volume1/gnn_graph_15day_bins_final.pt\"\ntorch.save(graph_data, graph_data_output_path)\n\n# print(f\"\\n✅ PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\n",
  "history_output" : "Loading dataset...\nFiltered dataset loaded with 620662 rows.\nAfter removing duplicates: 618231 rows.\nMerged nodes count: 10721 from 618231 original nodes.\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1474: ConvergenceWarning: Number of distinct clusters (2954) found smaller than n_clusters (10506). Possibly due to duplicate points in X.\n  return fit_method(estimator, *args, **kwargs)\nNodes reduced to 2954 after clustering.\nNew graph constructed with 2954 nodes and 1890 edges.\n",
  "history_begin_time" : 1741790477721,
  "history_end_time" : 1741790624311,
  "history_notes" : null,
  "history_process" : "p7ar4i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "RMqY1CmwQAj5",
  "history_input" : "import pandas as pd\nimport torch\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nfrom torch_geometric.utils import degree\nfrom sklearn.cluster import KMeans\n\n# 1️⃣ Load Dataset\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\nchunksize = 500000  # Read in chunks for efficiency\n\n# Define spatial bounds\nmin_lat, max_lat = 42.020065, 45.768982\nmin_lon, max_lon = -124.212205, -116.940392\n\nprint(\"Loading dataset...\")\n\nuseful_columns = [\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\n]\n\n# Load and filter dataset\ndf_list = []\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize, parse_dates=['date']):\n    chunk['day_of_year'] = chunk['date'].dt.dayofyear  # Convert date to day-of-year\n    filtered_chunk = chunk[\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) & \n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\n    ]\n    df_list.append(filtered_chunk)\n\n# Merge filtered chunks\ndata = pd.concat(df_list, ignore_index=True)\nprint(f\"Filtered dataset loaded with {data.shape[0]} rows.\")\n\n# Drop duplicates\ndata = data.drop_duplicates()\nprint(f\"After removing duplicates: {data.shape[0]} rows.\")\n\n# 2️⃣ Apply 15-Day Binning\nbin_size = 15  # Each bin is 15 days\ndata['time_bin'] = (data['day_of_year'] // bin_size).astype(int)\n\n# 3️⃣ Merge Nodes Using Spatial & Temporal Binning\ngrid_size = 0.1  # Larger spatial merging\nnum_time_bins = 24  # Using 15-day bins (~24 bins in a year)\n\n# Assign spatial bins\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\n\n# Create unique spatial-temporal grid ID\ndata['grid_id'] = (\n    data['lat_bin'].astype(str) + \"_\" + \n    data['lon_bin'].astype(str) + \"_\" +\n    data['time_bin'].astype(str)\n)\n\n# Aggregate nodes within each spatial-temporal grid cell (supernodes)\nmerged_nodes = data.groupby('grid_id').agg({\n    'Elevation': 'mean',\n    'Slope': 'mean',\n    'SWE': 'mean',\n    'snow_depth': 'mean',\n    'air_temperature_observed_f': 'mean',\n    'precipitation_amount': 'mean',\n    'relative_humidity_rmin': 'mean',\n    'relative_humidity_rmax': 'mean',\n    'wind_speed': 'mean',\n    'cumulative_SWE': 'mean',\n    'cumulative_precipitation_amount': 'mean',\n    'swe_value': 'mean',\n    'lat': 'mean',\n    'lon': 'mean'\n}).reset_index()\n\nprint(f\"Merged nodes count: {merged_nodes.shape[0]} from {data.shape[0]} original nodes.\")\n\n# 4️⃣ Reduce Nodes with K-Means Clustering (Fixed)\nnum_clusters = int(len(merged_nodes) * 0.98)  # **Keep 98% instead of 95%**\nkmeans = KMeans(n_clusters=num_clusters, random_state=42)\nmerged_nodes['cluster'] = kmeans.fit_predict(merged_nodes[['lat', 'lon']])\n\n# Aggregate again based on clusters\nmerged_nodes = merged_nodes.groupby('cluster').agg({\n    'Elevation': 'mean',\n    'SWE': 'mean',\n    'snow_depth': 'mean',\n    'air_temperature_observed_f': 'mean',\n    'precipitation_amount': 'mean',\n    'relative_humidity_rmin': 'mean',\n    'relative_humidity_rmax': 'mean',\n    'wind_speed': 'mean',\n    'cumulative_SWE': 'mean',\n    'cumulative_precipitation_amount': 'mean',\n    'swe_value': 'mean',\n    'lat': 'mean',\n    'lon': 'mean'\n}).reset_index(drop=True)\n\nprint(f\"Nodes reduced to {merged_nodes.shape[0]} after clustering.\")\n\n# 5️⃣ Normalize Features\nscaler = StandardScaler()\nfeature_columns = [\n    'Elevation', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount'\n]\n\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\nnode_features = torch.tensor(node_features, dtype=torch.float)\n\n# Assign labels (target values)\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\n\n# 6️⃣ Build Graph Edges (Improved Connectivity)\ncoordinates = merged_nodes[['lat', 'lon']].values\ntree = KDTree(coordinates)\n\nk = 7  # **Increase to 7 for much better connectivity**\nthreshold = 0.15  # **Increase threshold to connect more distant nodes**\n\ndistances, neighbors = tree.query(coordinates, k=k+1)\n\nedge_list = []\nfor i in range(len(neighbors)):\n    for j in range(1, k+1):\n        if 0.02 < distances[i][j] < threshold:\n            edge_list.append((i, neighbors[i][j]))\n\nedges = np.array(edge_list).T\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\n\nprint(f\"New graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\n\n# 7️⃣ Save Graph\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\ngraph_data_output_path = \"/media/volume1/gnn_graph_15day_bins_final.pt\"\ntorch.save(graph_data, graph_data_output_path)\n\n# print(f\"\\n✅ PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\n",
  "history_output" : "Loading dataset...\nFiltered dataset loaded with 620662 rows.\nAfter removing duplicates: 618231 rows.\nMerged nodes count: 10721 from 618231 original nodes.\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1474: ConvergenceWarning: Number of distinct clusters (2954) found smaller than n_clusters (10506). Possibly due to duplicate points in X.\n  return fit_method(estimator, *args, **kwargs)\nNodes reduced to 2954 after clustering.\nNew graph constructed with 2954 nodes and 1890 edges.\n",
  "history_begin_time" : 1741790215071,
  "history_end_time" : 1741790311449,
  "history_notes" : null,
  "history_process" : "p7ar4i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "zQhJnAro3yEy",
  "history_input" : "import pandas as pd\nimport torch\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nfrom torch_geometric.utils import degree\nfrom sklearn.cluster import KMeans\n\n# 1️⃣ Load Dataset\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\nchunksize = 500000  # Read in chunks for efficiency\n\n# Define spatial bounds\nmin_lat, max_lat = 42.020065, 45.768982\nmin_lon, max_lon = -124.212205, -116.940392\n\nprint(\"Loading dataset...\")\n\nuseful_columns = [\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\n]\n\n# Load and filter dataset\ndf_list = []\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize, parse_dates=['date']):\n    chunk['day_of_year'] = chunk['date'].dt.dayofyear  # Convert date to day-of-year\n    filtered_chunk = chunk[\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) & \n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\n    ]\n    df_list.append(filtered_chunk)\n\n# Merge filtered chunks\ndata = pd.concat(df_list, ignore_index=True)\nprint(f\"Filtered dataset loaded with {data.shape[0]} rows.\")\n\n# Drop duplicates\ndata = data.drop_duplicates()\nprint(f\"After removing duplicates: {data.shape[0]} rows.\")\n\n# 2️⃣ Apply 15-Day Binning\nbin_size = 15  # Each bin is 15 days\ndata['time_bin'] = (data['day_of_year'] // bin_size).astype(int)\n\n# 3️⃣ Merge Nodes Using Spatial & Temporal Binning\ngrid_size = 0.1  # Larger spatial merging\nnum_time_bins = 24  # Using 15-day bins (~24 bins in a year)\n\n# Assign spatial bins\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\n\n# Create unique spatial-temporal grid ID\ndata['grid_id'] = (\n    data['lat_bin'].astype(str) + \"_\" + \n    data['lon_bin'].astype(str) + \"_\" +\n    data['time_bin'].astype(str)\n)\n\n# Aggregate nodes within each spatial-temporal grid cell (supernodes)\nmerged_nodes = data.groupby('grid_id').agg({\n    'Elevation': 'mean',\n    'Slope': 'mean',\n    'SWE': 'mean',\n    'snow_depth': 'mean',\n    'air_temperature_observed_f': 'mean',\n    'precipitation_amount': 'mean',\n    'relative_humidity_rmin': 'mean',\n    'relative_humidity_rmax': 'mean',\n    'wind_speed': 'mean',\n    'cumulative_SWE': 'mean',\n    'cumulative_precipitation_amount': 'mean',\n    'swe_value': 'mean',\n    'lat': 'mean',\n    'lon': 'mean'\n}).reset_index()\n\nprint(f\"Merged nodes count: {merged_nodes.shape[0]} from {data.shape[0]} original nodes.\")\n\n# 4️⃣ Reduce Nodes with K-Means Clustering (Fixed)\nnum_clusters = int(len(merged_nodes) * 0.98)  # **Keep 98% instead of 95%**\nkmeans = KMeans(n_clusters=num_clusters, random_state=42)\nmerged_nodes['cluster'] = kmeans.fit_predict(merged_nodes[['lat', 'lon']])\n\n# Aggregate again based on clusters\nmerged_nodes = merged_nodes.groupby('cluster').agg({\n    'Elevation': 'mean',\n    'SWE': 'mean',\n    'snow_depth': 'mean',\n    'air_temperature_observed_f': 'mean',\n    'precipitation_amount': 'mean',\n    'relative_humidity_rmin': 'mean',\n    'relative_humidity_rmax': 'mean',\n    'wind_speed': 'mean',\n    'cumulative_SWE': 'mean',\n    'cumulative_precipitation_amount': 'mean',\n    'swe_value': 'mean',\n    'lat': 'mean',\n    'lon': 'mean'\n}).reset_index(drop=True)\n\nprint(f\"Nodes reduced to {merged_nodes.shape[0]} after clustering.\")\n\n# 5️⃣ Normalize Features\nscaler = StandardScaler()\nfeature_columns = [\n    'Elevation', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount'\n]\n\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\nnode_features = torch.tensor(node_features, dtype=torch.float)\n\n# Assign labels (target values)\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\n\n# 6️⃣ Build Graph Edges (Improved Connectivity)\ncoordinates = merged_nodes[['lat', 'lon']].values\ntree = KDTree(coordinates)\n\nk = 7  # **Increase to 7 for much better connectivity**\nthreshold = 0.15  # **Increase threshold to connect more distant nodes**\n\ndistances, neighbors = tree.query(coordinates, k=k+1)\n\nedge_list = []\nfor i in range(len(neighbors)):\n    for j in range(1, k+1):\n        if 0.02 < distances[i][j] < threshold:\n            edge_list.append((i, neighbors[i][j]))\n\nedges = np.array(edge_list).T\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\n\nprint(f\"New graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\n\n# 7️⃣ Save Graph\n# graph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n# graph_data_output_path = \"/media/volume1/gnn_graph_15day_bins_final.pt\"\n# torch.save(graph_data, graph_data_output_path)\n\n# print(f\"\\n✅ PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\n",
  "history_output" : "Loading dataset...\nFiltered dataset loaded with 620662 rows.\nAfter removing duplicates: 618231 rows.\nMerged nodes count: 10721 from 618231 original nodes.\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1474: ConvergenceWarning: Number of distinct clusters (2954) found smaller than n_clusters (10506). Possibly due to duplicate points in X.\n  return fit_method(estimator, *args, **kwargs)\nNodes reduced to 2954 after clustering.\nNew graph constructed with 2954 nodes and 1890 edges.\n",
  "history_begin_time" : 1741790039059,
  "history_end_time" : 1741790136691,
  "history_notes" : null,
  "history_process" : "p7ar4i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "nX6quJeOQS2f",
  "history_input" : "import pandas as pd\nimport torch\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nfrom torch_geometric.utils import degree\nfrom sklearn.cluster import KMeans\n\n# 1️⃣ Load Dataset\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\nchunksize = 500000  # Read in chunks for efficiency\n\n# Define spatial bounds\nmin_lat, max_lat = 42.020065, 45.768982\nmin_lon, max_lon = -124.212205, -116.940392\n\nprint(\"Loading dataset...\")\n\nuseful_columns = [\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\n]\n\n# Load and filter dataset\ndf_list = []\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize, parse_dates=['date']):\n    chunk['day_of_year'] = chunk['date'].dt.dayofyear  # Convert date to day-of-year\n    filtered_chunk = chunk[\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) & \n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\n    ]\n    df_list.append(filtered_chunk)\n\n# Merge filtered chunks\ndata = pd.concat(df_list, ignore_index=True)\nprint(f\"Filtered dataset loaded with {data.shape[0]} rows.\")\n\n# Drop duplicates\ndata = data.drop_duplicates()\nprint(f\"After removing duplicates: {data.shape[0]} rows.\")\n\n# 2️⃣ Apply 15-Day Binning\nbin_size = 15  # Each bin is 15 days\ndata['time_bin'] = (data['day_of_year'] // bin_size).astype(int)\n\n# 3️⃣ Merge Nodes Using Spatial & Temporal Binning\ngrid_size = 0.1  # Larger spatial merging\nnum_time_bins = 24  # Using 15-day bins (~24 bins in a year)\n\n# Assign spatial bins\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\n\n# Create unique spatial-temporal grid ID\ndata['grid_id'] = (\n    data['lat_bin'].astype(str) + \"_\" + \n    data['lon_bin'].astype(str) + \"_\" +\n    data['time_bin'].astype(str)\n)\n\n# Aggregate nodes within each spatial-temporal grid cell (supernodes)\nmerged_nodes = data.groupby('grid_id').agg({\n    'Elevation': 'mean',\n    'Slope': 'mean',\n    'SWE': 'mean',\n    'snow_depth': 'mean',\n    'air_temperature_observed_f': 'mean',\n    'precipitation_amount': 'mean',\n    'relative_humidity_rmin': 'mean',\n    'relative_humidity_rmax': 'mean',\n    'wind_speed': 'mean',\n    'cumulative_SWE': 'mean',\n    'cumulative_precipitation_amount': 'mean',\n    'swe_value': 'mean',\n    'lat': 'mean',\n    'lon': 'mean'\n}).reset_index()\n\nprint(f\"Merged nodes count: {merged_nodes.shape[0]} from {data.shape[0]} original nodes.\")\n\n# 4️⃣ Reduce Nodes with K-Means Clustering (Fixed)\nnum_clusters = int(len(merged_nodes) * 0.95)  # **Keep 95% instead of 90%**\nkmeans = KMeans(n_clusters=num_clusters, random_state=42)\nmerged_nodes['cluster'] = kmeans.fit_predict(merged_nodes[['lat', 'lon']])\n\n# Aggregate again based on clusters\nmerged_nodes = merged_nodes.groupby('cluster').agg({\n    'Elevation': 'mean',\n    'SWE': 'mean',\n    'snow_depth': 'mean',\n    'air_temperature_observed_f': 'mean',\n    'precipitation_amount': 'mean',\n    'relative_humidity_rmin': 'mean',\n    'relative_humidity_rmax': 'mean',\n    'wind_speed': 'mean',\n    'cumulative_SWE': 'mean',\n    'cumulative_precipitation_amount': 'mean',\n    'swe_value': 'mean',\n    'lat': 'mean',\n    'lon': 'mean'\n}).reset_index(drop=True)\n\nprint(f\"Nodes reduced to {merged_nodes.shape[0]} after clustering.\")\n\n# 5️⃣ Normalize Features\nscaler = StandardScaler()\nfeature_columns = [\n    'Elevation', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount'\n]\n\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\nnode_features = torch.tensor(node_features, dtype=torch.float)\n\n# Assign labels (target values)\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\n\n# 6️⃣ Build Graph Edges (Fixed)\ncoordinates = merged_nodes[['lat', 'lon']].values\ntree = KDTree(coordinates)\n\nk = 5  # **Increase to 5 for better connectivity**\nthreshold = 0.1  # **Increase threshold to keep more edges**\n\ndistances, neighbors = tree.query(coordinates, k=k+1)\n\nedge_list = []\nfor i in range(len(neighbors)):\n    for j in range(1, k+1):\n        if 0.02 < distances[i][j] < threshold:\n            edge_list.append((i, neighbors[i][j]))\n\nedges = np.array(edge_list).T\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\n\nprint(f\"New graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\n\n# 7️⃣ Save Graph\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n# graph_data_output_path = \"/media/volume1/gnn_graph_15day_bins_fixed.pt\"\n# torch.save(graph_data, graph_data_output_path)\n\n# print(f\"\\n✅ PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\n",
  "history_output" : "Loading dataset...\nFiltered dataset loaded with 620662 rows.\nAfter removing duplicates: 618231 rows.\nMerged nodes count: 10721 from 618231 original nodes.\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1474: ConvergenceWarning: Number of distinct clusters (2948) found smaller than n_clusters (10184). Possibly due to duplicate points in X.\n  return fit_method(estimator, *args, **kwargs)\nNodes reduced to 2948 after clustering.\nNew graph constructed with 2948 nodes and 669 edges.\n",
  "history_begin_time" : 1741789737794,
  "history_end_time" : 1741789827557,
  "history_notes" : null,
  "history_process" : "p7ar4i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "3e19zgpqiNWY",
  "history_input" : "import pandas as pd\nimport torch\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nfrom torch_geometric.utils import degree\nfrom sklearn.cluster import KMeans\n\n# 1️⃣ Load Dataset\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\nchunksize = 500000  # Read in chunks for efficiency\n\n# Define spatial bounds\nmin_lat, max_lat = 42.020065, 45.768982\nmin_lon, max_lon = -124.212205, -116.940392\n\nprint(\"Loading dataset...\")\n\nuseful_columns = [\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\n]\n\n# Load and filter dataset\ndf_list = []\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize, parse_dates=['date']):\n    chunk['day_of_year'] = chunk['date'].dt.dayofyear  # Convert date to day-of-year\n    filtered_chunk = chunk[\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) & \n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\n    ]\n    df_list.append(filtered_chunk)\n\n# Merge filtered chunks\ndata = pd.concat(df_list, ignore_index=True)\nprint(f\"Filtered dataset loaded with {data.shape[0]} rows.\")\n\n# Drop duplicates\ndata = data.drop_duplicates()\nprint(f\"After removing duplicates: {data.shape[0]} rows.\")\n\n# 2️⃣ Apply 15-Day Binning\nbin_size = 15  # Each bin is 15 days\ndata['time_bin'] = (data['day_of_year'] // bin_size).astype(int)\n\n# 3️⃣ Merge Nodes Using Spatial & Temporal Binning\ngrid_size = 0.1  # Larger spatial merging\nnum_time_bins = 24  # Using 15-day bins (~24 bins in a year)\n\n# Assign spatial bins\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\n\n# Create unique spatial-temporal grid ID\ndata['grid_id'] = (\n    data['lat_bin'].astype(str) + \"_\" + \n    data['lon_bin'].astype(str) + \"_\" +\n    data['time_bin'].astype(str)\n)\n\n# Aggregate nodes within each spatial-temporal grid cell (supernodes)\nmerged_nodes = data.groupby('grid_id').agg({\n    'Elevation': 'mean',\n    'Slope': 'mean',\n    'SWE': 'mean',\n    'snow_depth': 'mean',\n    'air_temperature_observed_f': 'mean',\n    'precipitation_amount': 'mean',\n    'relative_humidity_rmin': 'mean',\n    'relative_humidity_rmax': 'mean',\n    'wind_speed': 'mean',\n    'cumulative_SWE': 'mean',\n    'cumulative_precipitation_amount': 'mean',\n    'swe_value': 'mean',  # Target variable\n    'lat': 'mean',  # Representative lat/lon\n    'lon': 'mean',\n    'date': 'min'  # Keep earliest date for each bin\n}).reset_index()\n\nprint(f\"Merged nodes count: {merged_nodes.shape[0]} from {data.shape[0]} original nodes.\")\n\n# 4️⃣ Reduce Nodes Further Using K-Means Clustering\nnum_clusters = int(len(merged_nodes) * 0.9)  # Reduce by 25%\nkmeans = KMeans(n_clusters=num_clusters, random_state=42)\nmerged_nodes['cluster'] = kmeans.fit_predict(merged_nodes[['lat', 'lon']])\n\n# Aggregate again based on clusters\nmerged_nodes = merged_nodes.groupby('cluster').agg({\n    'Elevation': 'mean',\n    'Slope': 'mean',\n    'SWE': 'mean',\n    'snow_depth': 'mean',\n    'air_temperature_observed_f': 'mean',\n    'precipitation_amount': 'mean',\n    'relative_humidity_rmin': 'mean',\n    'relative_humidity_rmax': 'mean',\n    'wind_speed': 'mean',\n    'cumulative_SWE': 'mean',\n    'cumulative_precipitation_amount': 'mean',\n    'swe_value': 'mean',\n    'lat': 'mean',\n    'lon': 'mean'\n}).reset_index(drop=True)\n\nprint(f\"Nodes reduced to {merged_nodes.shape[0]} after clustering.\")\n\n# 5️⃣ Normalize Features\nscaler = StandardScaler()\nfeature_columns = [\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount'\n]\n\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\nnode_features = torch.tensor(node_features, dtype=torch.float)\n\n# Assign labels (target values)\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\n\n# 6️⃣ Build Graph Edges Using KDTree (Edge Sparsification)\ncoordinates = merged_nodes[['lat', 'lon']].values\ntree = KDTree(coordinates)\n\nk = 3  # Reduce nearest neighbors to limit edges\nthreshold = 0.05  # Larger distance threshold to remove weak edges\n\n# Query k-NN\ndistances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\n\n# Apply Edge Sparsification (Keep Only Strong Connections)\nedge_list = []\nfor i in range(len(neighbors)):\n    for j in range(1, k+1):  # Skip self-connection\n        if 0.02 < distances[i][j] < threshold:  # Keep only strong connections\n            edge_list.append((i, neighbors[i][j]))\n\n# Convert edges to tensor\nedges = np.array(edge_list).T\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\n\nprint(f\"New graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\n\n# 7️⃣ Create PyTorch Geometric Graph Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# 8️⃣ Graph Summary and Statistics\nprint(\"\\nGraph Data Summary:\")\nprint(graph_data)\nprint(f\"Number of nodes: {graph_data.num_nodes}\")\nprint(f\"Number of edges: {graph_data.num_edges}\")\nprint(f\"Node feature shape: {graph_data.x.shape}\")\nprint(f\"Edge index shape: {graph_data.edge_index.shape}\")\n\n# Compute degree statistics\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\nprint(f\"\\nDegree Stats:\")\nprint(f\"- Min degree: {deg.min().item()}\")\nprint(f\"- Max degree: {deg.max().item()}\")\nprint(f\"- Mean degree: {deg.float().mean().item()}\")\nprint(f\"- Standard deviation: {deg.float().std().item()}\")\n\n# Count isolated nodes\nnum_isolated_nodes = (deg == 0).sum().item()\npercentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\nprint(f\"\\nNumber of isolated nodes (degree=0): {num_isolated_nodes}\")\nprint(f\"Percentage of isolated nodes: {percentage_isolated:.4f}%\")\n\n# # 9️⃣ Save Graph for Later Use\n# graph_data_output_path = \"/media/volume1/gnn_graph_15day_bins_optimized.pt\"\n# torch.save(graph_data, graph_data_output_path)\n\n# print\n",
  "history_output" : "Loading dataset...\nFiltered dataset loaded with 620662 rows.\nAfter removing duplicates: 618231 rows.\nMerged nodes count: 10721 from 618231 original nodes.\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1474: ConvergenceWarning: Number of distinct clusters (2947) found smaller than n_clusters (9648). Possibly due to duplicate points in X.\n  return fit_method(estimator, *args, **kwargs)\nNodes reduced to 2947 after clustering.\nNew graph constructed with 2947 nodes and 88 edges.\nGraph Data Summary:\nData(x=[2947, 11], edge_index=[2, 88], y=[2947])\nNumber of nodes: 2947\nNumber of edges: 88\nNode feature shape: torch.Size([2947, 11])\nEdge index shape: torch.Size([2, 88])\nDegree Stats:\n- Min degree: 0.0\n- Max degree: 3.0\n- Mean degree: 0.029860874637961388\n- Standard deviation: 0.24537281692028046\nNumber of isolated nodes (degree=0): 2897\nPercentage of isolated nodes: 98.3034%\n",
  "history_begin_time" : 1741789367273,
  "history_end_time" : 1741789458349,
  "history_notes" : null,
  "history_process" : "p7ar4i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "10ob7xbqaNt8",
  "history_input" : "import pandas as pd\nimport torch\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nfrom torch_geometric.utils import degree\nfrom sklearn.cluster import KMeans\n\n# 1️⃣ Load Dataset\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\nchunksize = 500000  # Read in chunks for efficiency\n\n# Define spatial bounds\nmin_lat, max_lat = 42.020065, 45.768982\nmin_lon, max_lon = -124.212205, -116.940392\n\nprint(\"Loading dataset...\")\n\nuseful_columns = [\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\n]\n\n# Load and filter dataset\ndf_list = []\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize, parse_dates=['date']):\n    chunk['day_of_year'] = chunk['date'].dt.dayofyear  # Convert date to day-of-year\n    filtered_chunk = chunk[\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) & \n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\n    ]\n    df_list.append(filtered_chunk)\n\n# Merge filtered chunks\ndata = pd.concat(df_list, ignore_index=True)\nprint(f\"Filtered dataset loaded with {data.shape[0]} rows.\")\n\n# Drop duplicates\ndata = data.drop_duplicates()\nprint(f\"After removing duplicates: {data.shape[0]} rows.\")\n\n# 2️⃣ Apply 15-Day Binning\nbin_size = 15  # Each bin is 15 days\ndata['time_bin'] = (data['day_of_year'] // bin_size).astype(int)\n\n# 3️⃣ Merge Nodes Using Spatial & Temporal Binning\ngrid_size = 0.1  # Larger spatial merging\nnum_time_bins = 24  # Using 15-day bins (~24 bins in a year)\n\n# Assign spatial bins\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\n\n# Create unique spatial-temporal grid ID\ndata['grid_id'] = (\n    data['lat_bin'].astype(str) + \"_\" + \n    data['lon_bin'].astype(str) + \"_\" +\n    data['time_bin'].astype(str)\n)\n\n# Aggregate nodes within each spatial-temporal grid cell (supernodes)\nmerged_nodes = data.groupby('grid_id').agg({\n    'Elevation': 'mean',\n    'Slope': 'mean',\n    'SWE': 'mean',\n    'snow_depth': 'mean',\n    'air_temperature_observed_f': 'mean',\n    'precipitation_amount': 'mean',\n    'relative_humidity_rmin': 'mean',\n    'relative_humidity_rmax': 'mean',\n    'wind_speed': 'mean',\n    'cumulative_SWE': 'mean',\n    'cumulative_precipitation_amount': 'mean',\n    'swe_value': 'mean',  # Target variable\n    'lat': 'mean',  # Representative lat/lon\n    'lon': 'mean',\n    'date': 'min'  # Keep earliest date for each bin\n}).reset_index()\n\nprint(f\"Merged nodes count: {merged_nodes.shape[0]} from {data.shape[0]} original nodes.\")\n\n# 4️⃣ Reduce Nodes Further Using K-Means Clustering\nnum_clusters = int(len(merged_nodes) * 0.75)  # Reduce by 25%\nkmeans = KMeans(n_clusters=num_clusters, random_state=42)\nmerged_nodes['cluster'] = kmeans.fit_predict(merged_nodes[['lat', 'lon']])\n\n# Aggregate again based on clusters\nmerged_nodes = merged_nodes.groupby('cluster').agg({\n    'Elevation': 'mean',\n    'Slope': 'mean',\n    'SWE': 'mean',\n    'snow_depth': 'mean',\n    'air_temperature_observed_f': 'mean',\n    'precipitation_amount': 'mean',\n    'relative_humidity_rmin': 'mean',\n    'relative_humidity_rmax': 'mean',\n    'wind_speed': 'mean',\n    'cumulative_SWE': 'mean',\n    'cumulative_precipitation_amount': 'mean',\n    'swe_value': 'mean',\n    'lat': 'mean',\n    'lon': 'mean'\n}).reset_index(drop=True)\n\nprint(f\"Nodes reduced to {merged_nodes.shape[0]} after clustering.\")\n\n# 5️⃣ Normalize Features\nscaler = StandardScaler()\nfeature_columns = [\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount'\n]\n\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\nnode_features = torch.tensor(node_features, dtype=torch.float)\n\n# Assign labels (target values)\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\n\n# 6️⃣ Build Graph Edges Using KDTree (Edge Sparsification)\ncoordinates = merged_nodes[['lat', 'lon']].values\ntree = KDTree(coordinates)\n\nk = 2  # Reduce nearest neighbors to limit edges\nthreshold = 0.1  # Larger distance threshold to remove weak edges\n\n# Query k-NN\ndistances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\n\n# Apply Edge Sparsification (Keep Only Strong Connections)\nedge_list = []\nfor i in range(len(neighbors)):\n    for j in range(1, k+1):  # Skip self-connection\n        if 0.02 < distances[i][j] < threshold:  # Keep only strong connections\n            edge_list.append((i, neighbors[i][j]))\n\n# Convert edges to tensor\nedges = np.array(edge_list).T\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\n\nprint(f\"New graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\n\n# 7️⃣ Create PyTorch Geometric Graph Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# 8️⃣ Graph Summary and Statistics\nprint(\"\\nGraph Data Summary:\")\nprint(graph_data)\nprint(f\"Number of nodes: {graph_data.num_nodes}\")\nprint(f\"Number of edges: {graph_data.num_edges}\")\nprint(f\"Node feature shape: {graph_data.x.shape}\")\nprint(f\"Edge index shape: {graph_data.edge_index.shape}\")\n\n# Compute degree statistics\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\nprint(f\"\\nDegree Stats:\")\nprint(f\"- Min degree: {deg.min().item()}\")\nprint(f\"- Max degree: {deg.max().item()}\")\nprint(f\"- Mean degree: {deg.float().mean().item()}\")\nprint(f\"- Standard deviation: {deg.float().std().item()}\")\n\n# Count isolated nodes\nnum_isolated_nodes = (deg == 0).sum().item()\npercentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\nprint(f\"\\nNumber of isolated nodes (degree=0): {num_isolated_nodes}\")\nprint(f\"Percentage of isolated nodes: {percentage_isolated:.4f}%\")\n\n# # 9️⃣ Save Graph for Later Use\n# graph_data_output_path = \"/media/volume1/gnn_graph_15day_bins_optimized.pt\"\n# torch.save(graph_data, graph_data_output_path)\n\n# print\n",
  "history_output" : "Loading dataset...\nFiltered dataset loaded with 620662 rows.\nAfter removing duplicates: 618231 rows.\nMerged nodes count: 10721 from 618231 original nodes.\n/home/geo2021/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1474: ConvergenceWarning: Number of distinct clusters (2942) found smaller than n_clusters (8040). Possibly due to duplicate points in X.\n  return fit_method(estimator, *args, **kwargs)\nNodes reduced to 2942 after clustering.\nNew graph constructed with 2942 nodes and 228 edges.\nGraph Data Summary:\nData(x=[2942, 11], edge_index=[2, 228], y=[2942])\nNumber of nodes: 2942\nNumber of edges: 228\nNode feature shape: torch.Size([2942, 11])\nEdge index shape: torch.Size([2, 228])\nDegree Stats:\n- Min degree: 0.0\n- Max degree: 2.0\n- Mean degree: 0.07749830186367035\n- Standard deviation: 0.3258814215660095\nNumber of isolated nodes (degree=0): 2765\nPercentage of isolated nodes: 93.9837%\n",
  "history_begin_time" : 1741789135524,
  "history_end_time" : 1741789216294,
  "history_notes" : null,
  "history_process" : "p7ar4i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "C9pEgffQJDEH",
  "history_input" : "import pandas as pd\nimport torch\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nfrom torch_geometric.utils import degree\n\n# 1️⃣ Load Dataset\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\nchunksize = 500000  # Read in chunks for efficiency\n\n# Define spatial bounds\nmin_lat, max_lat = 42.020065, 45.768982\nmin_lon, max_lon = -124.212205, -116.940392\n\nprint(\"Loading dataset...\")\n\nuseful_columns = [\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\n]\n\n# Load and filter dataset\ndf_list = []\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize, parse_dates=['date']):\n    chunk['day_of_year'] = chunk['date'].dt.dayofyear  # Convert date to day-of-year\n    filtered_chunk = chunk[\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) & \n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\n    ]\n    df_list.append(filtered_chunk)\n\n# Merge filtered chunks\ndata = pd.concat(df_list, ignore_index=True)\nprint(f\"Filtered dataset loaded with {data.shape[0]} rows.\")\n\n# Drop duplicates\ndata = data.drop_duplicates()\nprint(f\"After removing duplicates: {data.shape[0]} rows.\")\n\n# 2️⃣ Merge Nodes Using Spatial & 12-Bin Temporal Binning\ngrid_size = 0.05  # Spatial resolution (degrees)\nnum_time_bins = 12  # 12 time bins (~30-day intervals)\n\n# Assign spatial bins\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\n\n# Assign time bin\ndata['time_bin'] = (data['day_of_year'] // (365 / num_time_bins)).astype(int)\n\n# Create unique spatial-temporal grid ID\ndata['grid_id'] = (\n    data['lat_bin'].astype(str) + \"_\" + \n    data['lon_bin'].astype(str) + \"_\" +\n    data['time_bin'].astype(str)\n)\n\n# Aggregate nodes within each spatial-temporal grid cell (supernodes)\nmerged_nodes = data.groupby('grid_id').agg({\n    'Elevation': 'mean',\n    'Slope': 'mean',\n    'SWE': 'mean',\n    'snow_depth': 'mean',\n    'air_temperature_observed_f': 'mean',\n    'precipitation_amount': 'mean',\n    'relative_humidity_rmin': 'mean',\n    'relative_humidity_rmax': 'mean',\n    'wind_speed': 'mean',\n    'cumulative_SWE': 'mean',\n    'cumulative_precipitation_amount': 'mean',\n    'swe_value': 'mean',  # Target variable\n    'lat': 'mean',  # Representative lat/lon\n    'lon': 'mean',\n    'date': 'min'  # Keep earliest date for each bin\n}).reset_index()\n\nprint(f\"Merged nodes count: {merged_nodes.shape[0]} from {data.shape[0]} original nodes.\")\n\n# 3️⃣ Normalize Features\nscaler = StandardScaler()\nfeature_columns = [\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount'\n]\n\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\nnode_features = torch.tensor(node_features, dtype=torch.float)\n\n# Assign labels (target values)\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\n\n# 4️⃣ Build Graph Edges Using KDTree (k-NN with Distance Filtering)\ncoordinates = merged_nodes[['lat', 'lon']].values\ntree = KDTree(coordinates)\n\nk = 3  # Reduce nearest neighbors to limit edges\nthreshold = 0.05  # Increase distance threshold to reduce unnecessary connections\n\n# Query k-NN\ndistances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\n\n# Construct edge list\nedge_list = []\nfor i in range(len(neighbors)):\n    for j in range(1, k+1):  # Skip first entry (self-node)\n        if distances[i][j] < threshold:  # Apply distance filter\n            edge_list.append((i, neighbors[i][j]))\n\n# Convert edges to tensor\nedges = np.array(edge_list).T\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\n\nprint(f\"New graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\n\n# 5️⃣ Create PyTorch Geometric Graph Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# 6️⃣ Graph Summary and Statistics\nprint(\"\\nGraph Data Summary:\")\nprint(graph_data)\nprint(f\"Number of nodes: {graph_data.num_nodes}\")\nprint(f\"Number of edges: {graph_data.num_edges}\")\nprint(f\"Node feature shape: {graph_data.x.shape}\")\nprint(f\"Edge index shape: {graph_data.edge_index.shape}\")\n\n# Compute degree statistics\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\nprint(f\"\\nDegree Stats:\")\nprint(f\"- Min degree: {deg.min().item()}\")\nprint(f\"- Max degree: {deg.max().item()}\")\nprint(f\"- Mean degree: {deg.float().mean().item()}\")\nprint(f\"- Standard deviation: {deg.float().std().item()}\")\n\n# Count isolated nodes\nnum_isolated_nodes = (deg == 0).sum().item()\npercentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\nprint(f\"\\nNumber of isolated nodes (degree=0): {num_isolated_nodes}\")\nprint(f\"Percentage of isolated nodes: {percentage_isolated:.4f}%\")\n\n# 7️⃣ Save Graph for Later Use\n# graph_data_output_path = \"/media/volume1/gnn_graph_12_bins.pt\"\n# torch.save(graph_data, graph_data_output_path)\n\n# print(f\"\\n✅ PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\n",
  "history_output" : "Loading dataset...\nFiltered dataset loaded with 620662 rows.\nAfter removing duplicates: 618231 rows.\nMerged nodes count: 6978 from 618231 original nodes.\nNew graph constructed with 6978 nodes and 20918 edges.\nGraph Data Summary:\nData(x=[6978, 11], edge_index=[2, 20918], y=[6978])\nNumber of nodes: 6978\nNumber of edges: 20918\nNode feature shape: torch.Size([6978, 11])\nEdge index shape: torch.Size([2, 20918])\nDegree Stats:\n- Min degree: 0.0\n- Max degree: 3.0\n- Mean degree: 2.9977071285247803\n- Standard deviation: 0.06768485903739929\nNumber of isolated nodes (degree=0): 2\nPercentage of isolated nodes: 0.0287%\n",
  "history_begin_time" : 1741788096643,
  "history_end_time" : 1741788130201,
  "history_notes" : "12bins",
  "history_process" : "p7ar4i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "ld5IIw0jVXKA",
  "history_input" : "import pandas as pd\nimport torch\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nfrom torch_geometric.utils import degree\n\n# 1️⃣ Load Dataset\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\nchunksize = 500000  # Read in chunks for efficiency\n\n# Define spatial bounds\nmin_lat, max_lat = 42.020065, 45.768982\nmin_lon, max_lon = -124.212205, -116.940392\n\nprint(\"Loading dataset...\")\n\nuseful_columns = [\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\n]\n\n# Load and filter dataset\ndf_list = []\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize, parse_dates=['date']):\n    filtered_chunk = chunk[\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) & \n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\n    ]\n    df_list.append(filtered_chunk)\n\n# Merge filtered chunks\ndata = pd.concat(df_list, ignore_index=True)\nprint(f\"Filtered dataset loaded with {data.shape[0]} rows.\")\n\n# Drop duplicates\ndata = data.drop_duplicates()\nprint(f\"After removing duplicates: {data.shape[0]} rows.\")\n\n# 2️⃣ Merge Nodes Using Spatial Grid Binning (Without Exploding Dates)\ngrid_size = 0.05  # Increase grid resolution to merge more nodes\n\n# Assign each node to a grid cell\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\n\n# Create unique grid cell ID (without separating by `date`)\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str)\n\n# Aggregate nodes within each grid cell (supernodes), keeping the **representative date**\nmerged_nodes = data.groupby('grid_id').agg({\n    'Elevation': 'mean',\n    'Slope': 'mean',\n    'SWE': 'mean',\n    'snow_depth': 'mean',\n    'air_temperature_observed_f': 'mean',\n    'precipitation_amount': 'mean',\n    'relative_humidity_rmin': 'mean',\n    'relative_humidity_rmax': 'mean',\n    'wind_speed': 'mean',\n    'cumulative_SWE': 'mean',\n    'cumulative_precipitation_amount': 'mean',\n    'swe_value': 'mean',  # Target variable\n    'lat': 'mean',  # Representative lat/lon\n    'lon': 'mean',\n    'date': 'min'  # Keep only the earliest date per grid cell\n}).reset_index()\n\nprint(f\"Merged nodes count: {merged_nodes.shape[0]} from {data.shape[0]} original nodes.\")\n\n# 3️⃣ Normalize Features\nscaler = StandardScaler()\nfeature_columns = [\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount'\n]\n\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\nnode_features = torch.tensor(node_features, dtype=torch.float)\n\n# Assign labels (target values)\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\n\n# 4️⃣ Build Graph Edges Using KDTree (k-NN with Distance Filtering)\ncoordinates = merged_nodes[['lat', 'lon']].values\ntree = KDTree(coordinates)\n\nk = 3  # Reduce nearest neighbors to limit edges\nthreshold = 0.05  # Increase distance threshold to reduce unnecessary connections\n\n# Query k-NN\ndistances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\n\n# Construct edge list\nedge_list = []\nfor i in range(len(neighbors)):\n    for j in range(1, k+1):  # Skip first entry (self-node)\n        if distances[i][j] < threshold:  # Apply distance filter\n            edge_list.append((i, neighbors[i][j]))\n\n# Convert edges to tensor\nedges = np.array(edge_list).T\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\n\nprint(f\"New graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\n\n# 5️⃣ Create PyTorch Geometric Graph Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# 6️⃣ Graph Summary and Statistics\nprint(\"\\nGraph Data Summary:\")\nprint(graph_data)\nprint(f\"Number of nodes: {graph_data.num_nodes}\")\nprint(f\"Number of edges: {graph_data.num_edges}\")\nprint(f\"Node feature shape: {graph_data.x.shape}\")\nprint(f\"Edge index shape: {graph_data.edge_index.shape}\")\n\n# Compute degree statistics\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\nprint(f\"\\nDegree Stats:\")\nprint(f\"- Min degree: {deg.min().item()}\")\nprint(f\"- Max degree: {deg.max().item()}\")\nprint(f\"- Mean degree: {deg.float().mean().item()}\")\nprint(f\"- Standard deviation: {deg.float().std().item()}\")\n\n# Count isolated nodes\nnum_isolated_nodes = (deg == 0).sum().item()\npercentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\nprint(f\"\\nNumber of isolated nodes (degree=0): {num_isolated_nodes}\")\nprint(f\"Percentage of isolated nodes: {percentage_isolated:.4f}%\")\n\n# # 7️⃣ Save Graph for Later Use\ngraph_data_output_path = \"/media/volume1/gnn_graph_wa_wout_bins.pt\"\ntorch.save(graph_data, graph_data_output_path)\n\nprint(f\"\\n✅ PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\n",
  "history_output" : "Loading dataset...\nFiltered dataset loaded with 620662 rows.\nAfter removing duplicates: 618231 rows.\nMerged nodes count: 574 from 618231 original nodes.\nNew graph constructed with 574 nodes and 262 edges.\nGraph Data Summary:\nData(x=[574, 11], edge_index=[2, 262], y=[574])\nNumber of nodes: 574\nNumber of edges: 262\nNode feature shape: torch.Size([574, 11])\nEdge index shape: torch.Size([2, 262])\nDegree Stats:\n- Min degree: 0.0\n- Max degree: 3.0\n- Mean degree: 0.45644599199295044\n- Standard deviation: 0.6504276394844055\nNumber of isolated nodes (degree=0): 357\nPercentage of isolated nodes: 62.1951%\n✅ PyTorch Geometric Graph Data saved at: /media/volume1/gnn_graph_wa_wout_bins.pt\n",
  "history_begin_time" : 1741787962608,
  "history_end_time" : 1741787995225,
  "history_notes" : "no bins",
  "history_process" : "p7ar4i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "yPyQQPsiORKG",
  "history_input" : "import pandas as pd\nimport torch\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nfrom torch_geometric.utils import degree\n\n# 1️⃣ Load Dataset\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\nchunksize = 500000  # Read in chunks for efficiency\n\n# Define spatial bounds\nmin_lat, max_lat = 42.020065, 45.768982\nmin_lon, max_lon = -124.212205, -116.940392\n\nprint(\"Loading dataset...\")\n\nuseful_columns = [\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\n]\n\n# Load and filter dataset\ndf_list = []\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize, parse_dates=['date']):\n    chunk['day_of_year'] = chunk['date'].dt.dayofyear  # Convert date to day-of-year\n    filtered_chunk = chunk[\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) & \n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\n    ]\n    df_list.append(filtered_chunk)\n\n# Merge filtered chunks\ndata = pd.concat(df_list, ignore_index=True)\nprint(f\"Filtered dataset loaded with {data.shape[0]} rows.\")\n\n# Drop duplicates\ndata = data.drop_duplicates()\nprint(f\"After removing duplicates: {data.shape[0]} rows.\")\n\n# 2️⃣ Merge Nodes Using Spatial & Temporal Binning\ngrid_size = 0.05  # Spatial resolution (degrees)\nnum_time_bins = 24  # 24 time bins (~15-day intervals)\n\n# Assign spatial bins\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\n\n# Assign time bin\ndata['time_bin'] = (data['day_of_year'] // (365 / num_time_bins)).astype(int)\n\n# Create unique spatial-temporal grid ID\ndata['grid_id'] = (\n    data['lat_bin'].astype(str) + \"_\" + \n    data['lon_bin'].astype(str) + \"_\" +\n    data['time_bin'].astype(str)\n)\n\n# Aggregate nodes within each spatial-temporal grid cell (supernodes)\nmerged_nodes = data.groupby('grid_id').agg({\n    'Elevation': 'mean',\n    'Slope': 'mean',\n    'SWE': 'mean',\n    'snow_depth': 'mean',\n    'air_temperature_observed_f': 'mean',\n    'precipitation_amount': 'mean',\n    'relative_humidity_rmin': 'mean',\n    'relative_humidity_rmax': 'mean',\n    'wind_speed': 'mean',\n    'cumulative_SWE': 'mean',\n    'cumulative_precipitation_amount': 'mean',\n    'swe_value': 'mean',  # Target variable\n    'lat': 'mean',  # Representative lat/lon\n    'lon': 'mean',\n    'date': 'min'  # Keep earliest date for each bin\n}).reset_index()\n\nprint(f\"Merged nodes count: {merged_nodes.shape[0]} from {data.shape[0]} original nodes.\")\n\n# 3️⃣ Normalize Features\nscaler = StandardScaler()\nfeature_columns = [\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount'\n]\n\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\nnode_features = torch.tensor(node_features, dtype=torch.float)\n\n# Assign labels (target values)\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\n\n# 4️⃣ Build Graph Edges Using KDTree (k-NN with Distance Filtering)\ncoordinates = merged_nodes[['lat', 'lon']].values\ntree = KDTree(coordinates)\n\nk = 3  # Reduce nearest neighbors to limit edges\nthreshold = 0.05  # Increase distance threshold to reduce unnecessary connections\n\n# Query k-NN\ndistances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\n\n# Construct edge list\nedge_list = []\nfor i in range(len(neighbors)):\n    for j in range(1, k+1):  # Skip first entry (self-node)\n        if distances[i][j] < threshold:  # Apply distance filter\n            edge_list.append((i, neighbors[i][j]))\n\n# Convert edges to tensor\nedges = np.array(edge_list).T\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\n\nprint(f\"New graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\n\n# 5️⃣ Create PyTorch Geometric Graph Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# 6️⃣ Graph Summary and Statistics\nprint(\"\\nGraph Data Summary:\")\nprint(graph_data)\nprint(f\"Number of nodes: {graph_data.num_nodes}\")\nprint(f\"Number of edges: {graph_data.num_edges}\")\nprint(f\"Node feature shape: {graph_data.x.shape}\")\nprint(f\"Edge index shape: {graph_data.edge_index.shape}\")\n\n# Compute degree statistics\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\nprint(f\"\\nDegree Stats:\")\nprint(f\"- Min degree: {deg.min().item()}\")\nprint(f\"- Max degree: {deg.max().item()}\")\nprint(f\"- Mean degree: {deg.float().mean().item()}\")\nprint(f\"- Standard deviation: {deg.float().std().item()}\")\n\n# Count isolated nodes\nnum_isolated_nodes = (deg == 0).sum().item()\npercentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\nprint(f\"\\nNumber of isolated nodes (degree=0): {num_isolated_nodes}\")\nprint(f\"Percentage of isolated nodes: {percentage_isolated:.4f}%\")\n\n# 7️⃣ Save Graph for Later Use\n# graph_data_output_path = \"/media/volume1/gnn_graph_24_bins.pt\"\n# torch.save(graph_data, graph_data_output_path)\n\n# print(f\"\\n✅ PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\n",
  "history_output" : "Loading dataset...\nFiltered dataset loaded with 620662 rows.\nAfter removing duplicates: 618231 rows.\nMerged nodes count: 13501 from 618231 original nodes.\nNew graph constructed with 13501 nodes and 40497 edges.\nGraph Data Summary:\nData(x=[13501, 11], edge_index=[2, 40497], y=[13501])\nNumber of nodes: 13501\nNumber of edges: 40497\nNode feature shape: torch.Size([13501, 11])\nEdge index shape: torch.Size([2, 40497])\nDegree Stats:\n- Min degree: 0.0\n- Max degree: 3.0\n- Mean degree: 2.9995555877685547\n- Standard deviation: 0.03651213273406029\nNumber of isolated nodes (degree=0): 2\nPercentage of isolated nodes: 0.0148%\n",
  "history_begin_time" : 1741787705597,
  "history_end_time" : 1741787738822,
  "history_notes" : "24bins",
  "history_process" : "p7ar4i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "jHKFpEqEUpvU",
  "history_input" : "import pandas as pd\nimport torch\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nfrom torch_geometric.utils import degree\n\n# 1️⃣ Load Dataset\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\nchunksize = 500000  \n\n# Define spatial bounds\nmin_lat, max_lat = 42.020065, 45.768982\nmin_lon, max_lon = -124.212205, -116.940392\n\nprint(\"Loading dataset...\")\n\nuseful_columns = [\n    'date', 'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\n]\n\ndf_list = []\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\n    chunk['date'] = pd.to_datetime(chunk['date'])  \n    chunk['day_of_year'] = chunk['date'].dt.dayofyear  \n    \n    filtered_chunk = chunk[\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) & \n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\n    ]\n    df_list.append(filtered_chunk)\n\ndata = pd.concat(df_list, ignore_index=True)\nprint(f\"Filtered dataset loaded with {data.shape[0]} rows.\")\n\ndata = data.drop_duplicates()\nprint(f\"After removing duplicates: {data.shape[0]} rows.\")\n\n# 2️⃣ Merge Nodes Using Spatial & Temporal Grid Binning\ngrid_size = 0.4  # More aggressive merging (~44 km resolution)\nnum_time_bins = 4  # Merge data into ~90-day bins\n\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\n\ndata['dayofyear_bin'] = (data['day_of_year'] // (365 / num_time_bins)).astype(int)\n\ndata['grid_id'] = (\n    data['lat_bin'].astype(str) + \"_\" + \n    data['lon_bin'].astype(str) + \"_\" +\n    data['dayofyear_bin'].astype(str)\n)\n\nmerged_nodes = data.groupby('grid_id').agg({\n    'Elevation': 'mean',\n    'Slope': 'mean',\n    'SWE': 'mean',\n    'snow_depth': 'mean',\n    'air_temperature_observed_f': 'mean',\n    'precipitation_amount': 'mean',\n    'relative_humidity_rmin': 'mean',\n    'relative_humidity_rmax': 'mean',\n    'wind_speed': 'mean',\n    'cumulative_SWE': 'mean',\n    'cumulative_precipitation_amount': 'mean',\n    'swe_value': 'mean',  \n    'lat': 'mean',  \n    'lon': 'mean'\n}).reset_index()\n\nprint(f\"Merged nodes count: {merged_nodes.shape[0]} from {data.shape[0]} original nodes.\")\n\n# 3️⃣ Feature Merging & Reduction\nmerged_nodes['relative_humidity_avg'] = (merged_nodes['relative_humidity_rmin'] + merged_nodes['relative_humidity_rmax']) / 2\nmerged_nodes.drop(columns=['relative_humidity_rmin', 'relative_humidity_rmax'], inplace=True)\n\nmerged_nodes.drop(columns=['Slope', 'wind_speed'], inplace=True)\n\n# 4️⃣ Normalize Features\nscaler = StandardScaler()\nfeature_columns = [\n    'Elevation', 'SWE', 'snow_depth', 'air_temperature_observed_f', \n    'precipitation_amount', 'cumulative_SWE', 'cumulative_precipitation_amount', \n    'relative_humidity_avg'  \n]\n\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\nnode_features = torch.tensor(node_features, dtype=torch.float)\n\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\n\n# 5️⃣ Build Graph Edges Using KDTree (k-NN with Distance Filtering)\ncoordinates = merged_nodes[['lat', 'lon']].values\ntree = KDTree(coordinates)\n\nk = 2  # Reduce nearest neighbors to 2\nthreshold = 0.2  # Increase threshold to 0.2 (~22 km)\n\ndistances, neighbors = tree.query(coordinates, k=k+1)  \n\nedge_list = []\nfor i in range(len(neighbors)):\n    for j in range(1, k+1):  \n        if distances[i][j] < threshold:  \n            edge_list.append((i, neighbors[i][j]))\n\nedges = np.array(edge_list).T\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\n\nprint(f\"New graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\n\n# 6️⃣ Create PyTorch Geometric Graph Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# 7️⃣ Graph Summary and Statistics\nprint(\"\\nGraph Data Summary:\")\nprint(graph_data)\nprint(f\"Number of nodes: {graph_data.num_nodes}\")\nprint(f\"Number of edges: {graph_data.num_edges}\")\nprint(f\"Node feature shape: {graph_data.x.shape}\")\nprint(f\"Edge index shape: {graph_data.edge_index.shape}\")\n\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\nprint(f\"\\nDegree Stats:\")\nprint(f\"- Min degree: {deg.min().item()}\")\nprint(f\"- Max degree: {deg.max().item()}\")\nprint(f\"- Mean degree: {deg.float().mean().item()}\")\nprint(f\"- Standard deviation: {deg.float().std().item()}\")\n\nnum_isolated_nodes = (deg == 0).sum().item()\npercentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\nprint(f\"\\nNumber of isolated nodes (degree=0): {num_isolated_nodes}\")\nprint(f\"Percentage of isolated nodes: {percentage_isolated:.4f}%\")\n\n# 8️⃣ Save Graph\ngraph_data_output_path = \"/media/volume1/gnn_graph_final_reduced_wa_4.pt\"\ntorch.save(graph_data, graph_data_output_path)\n\nprint(f\"\\n✅ PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\n",
  "history_output" : "Loading dataset...\nFiltered dataset loaded with 620662 rows.\nAfter removing duplicates: 618231 rows.\nMerged nodes count: 685 from 618231 original nodes.\nNew graph constructed with 685 nodes and 1370 edges.\nGraph Data Summary:\nData(x=[685, 8], edge_index=[2, 1370], y=[685])\nNumber of nodes: 685\nNumber of edges: 1370\nNode feature shape: torch.Size([685, 8])\nEdge index shape: torch.Size([2, 1370])\nDegree Stats:\n- Min degree: 2.0\n- Max degree: 2.0\n- Mean degree: 2.0\n- Standard deviation: 0.0\nNumber of isolated nodes (degree=0): 0\nPercentage of isolated nodes: 0.0000%\n✅ PyTorch Geometric Graph Data saved at: /media/volume1/gnn_graph_final_reduced_wa_4.pt\n",
  "history_begin_time" : 1741743136547,
  "history_end_time" : 1741743170795,
  "history_notes" : "checkpoint",
  "history_process" : "p7ar4i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "cUQDBSYlvtz7",
  "history_input" : "import pandas as pd\nimport torch\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nfrom torch_geometric.utils import degree\n\n# 1️⃣ Load Dataset\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\nchunksize = 500000  # Read in chunks for efficiency\n\nprint(\"Loading dataset...\")\n\nuseful_columns = [\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount', 'swe_value'\n]\n\n# Load dataset without spatial filtering\ndf_list = []\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\n    df_list.append(chunk)\n\n# Merge chunks\ndata = pd.concat(df_list, ignore_index=True)\nprint(f\"Dataset loaded with {data.shape[0]} rows.\")\n\n# Drop duplicates\ndata = data.drop_duplicates()\nprint(f\"After removing duplicates: {data.shape[0]} rows.\")\n\n# 2️⃣ Merge Nodes Using Spatial Grid Binning\ngrid_size = 0.01  # Grid resolution in degrees\n\n# Assign each node to a grid cell\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\n\n# Create unique grid cell ID\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str)\n\n# Aggregate nodes within each grid cell (supernodes)\nmerged_nodes = data.groupby('grid_id').agg({\n    'Elevation': 'mean',\n    'Slope': 'mean',\n    'SWE': 'mean',\n    'snow_depth': 'mean',\n    'air_temperature_observed_f': 'mean',\n    'precipitation_amount': 'mean',\n    'relative_humidity_rmin': 'mean',\n    'relative_humidity_rmax': 'mean',\n    'wind_speed': 'mean',\n    'cumulative_SWE': 'mean',\n    'cumulative_precipitation_amount': 'mean',\n    'swe_value': 'mean',  # Target variable\n    'lat': 'mean',  # Representative lat/lon\n    'lon': 'mean'\n}).reset_index()\n\nprint(f\"Merged nodes count: {merged_nodes.shape[0]} from {data.shape[0]} original nodes.\")\n\n# 3️⃣ Normalize Features\nscaler = StandardScaler()\nfeature_columns = [\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount'\n]\n\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\nnode_features = torch.tensor(node_features, dtype=torch.float)\n\n# Assign labels (target values)\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\n\n# 4️⃣ Build Graph Edges Using KDTree (k-NN with Distance Filtering)\ncoordinates = merged_nodes[['lat', 'lon']].values\ntree = KDTree(coordinates)\n\nk = 5  # Number of nearest neighbors\nthreshold = 0.02  # Maximum connection distance\n\n# Query k-NN\ndistances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\n\n# Construct edge list\nedge_list = []\nfor i in range(len(neighbors)):\n    for j in range(1, k+1):  # Skip first entry (self-node)\n        if distances[i][j] < threshold:  # Apply distance filter\n            edge_list.append((i, neighbors[i][j]))\n\n# Convert edges to tensor\nedges = np.array(edge_list).T\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\n\nprint(f\"New graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\n\n# 5️⃣ Create PyTorch Geometric Graph Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# 6️⃣ Graph Summary and Statistics\nprint(\"\\nGraph Data Summary:\")\nprint(graph_data)\nprint(f\"Number of nodes: {graph_data.num_nodes}\")\nprint(f\"Number of edges: {graph_data.num_edges}\")\nprint(f\"Node feature shape: {graph_data.x.shape}\")\nprint(f\"Edge index shape: {graph_data.edge_index.shape}\")\n\n# Compute degree statistics\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\nprint(f\"\\nDegree Stats:\")\nprint(f\"- Min degree: {deg.min().item()}\")\nprint(f\"- Max degree: {deg.max().item()}\")\nprint(f\"- Mean degree: {deg.float().mean().item()}\")\nprint(f\"- Standard deviation: {deg.float().std().item()}\")\n\n# Count isolated nodes\nnum_isolated_nodes = (deg == 0).sum().item()\npercentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\nprint(f\"\\nNumber of isolated nodes (degree=0): {num_isolated_nodes}\")\nprint(f\"Percentage of isolated nodes: {percentage_isolated:.4f}%\")\n\n# 7️⃣ Save Graph for Later Use\ngraph_data_output_path = \"/media/volume1/gnn_graph_final_all_data.pt\"\ntorch.save(graph_data, graph_data_output_path)\n\nprint(f\"\\n✅ PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\n",
  "history_output" : "Loading dataset...\nDataset loaded with 6155644 rows.\nAfter removing duplicates: 3980781 rows.\nMerged nodes count: 7029 from 3980781 original nodes.\nNew graph constructed with 7029 nodes and 3387 edges.\nGraph Data Summary:\nData(x=[7029, 11], edge_index=[2, 3387], y=[7029])\nNumber of nodes: 7029\nNumber of edges: 3387\nNode feature shape: torch.Size([7029, 11])\nEdge index shape: torch.Size([2, 3387])\nDegree Stats:\n- Min degree: 0.0\n- Max degree: 5.0\n- Mean degree: 0.481860876083374\n- Standard deviation: 0.681796133518219\nNumber of isolated nodes (degree=0): 4230\nPercentage of isolated nodes: 60.1793%\n✅ PyTorch Geometric Graph Data saved at: /media/volume1/gnn_graph_final_all_data.pt\n",
  "history_begin_time" : 1741226512631,
  "history_end_time" : 1741226549992,
  "history_notes" : "without time final all states",
  "history_process" : "p7ar4i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "rfD5ipxBnyiR",
  "history_input" : "import pandas as pd\nimport torch\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nfrom torch_geometric.utils import degree\n\n# 1️⃣ Load Dataset\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\nchunksize = 500000  # Read in chunks for efficiency\n\n# Define spatial bounds\nmin_lat, max_lat = 42.195969, 48.908059\nmin_lon, max_lon = -123.793945, -111.423340\n\nprint(\"Loading dataset...\")\n\nuseful_columns = [\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount','swe_value'\n]\n\n# Load and filter dataset\ndf_list = []\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\n    filtered_chunk = chunk[\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) & \n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\n    ]\n    df_list.append(filtered_chunk)\n\n# Merge filtered chunks\ndata = pd.concat(df_list, ignore_index=True)\nprint(f\"Filtered dataset loaded with {data.shape[0]} rows.\")\n\n# Drop duplicates\ndata = data.drop_duplicates()\nprint(f\"After removing duplicates: {data.shape[0]} rows.\")\n\n# 2️⃣ Merge Nodes Using Spatial Grid Binning\ngrid_size = 0.01  # Grid resolution in degrees\n\n# Assign each node to a grid cell\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\n\n# Create unique grid cell ID\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str)\n\n# Aggregate nodes within each grid cell (supernodes)\nmerged_nodes = data.groupby('grid_id').agg({\n    'Elevation': 'mean',\n    'Slope': 'mean',\n    'SWE': 'mean',\n    'snow_depth': 'mean',\n    'air_temperature_observed_f': 'mean',\n    'precipitation_amount': 'mean',\n    'relative_humidity_rmin': 'mean',\n    'relative_humidity_rmax': 'mean',\n    'wind_speed': 'mean',\n    'cumulative_SWE': 'mean',\n    'cumulative_precipitation_amount': 'mean',\n    'swe_value': 'mean',  # Target variable\n    'lat': 'mean',  # Representative lat/lon\n    'lon': 'mean'\n}).reset_index()\n\nprint(f\"Merged nodes count: {merged_nodes.shape[0]} from {data.shape[0]} original nodes.\")\n\n# 3️⃣ Normalize Features\nscaler = StandardScaler()\nfeature_columns = [\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount'\n]\n\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\nnode_features = torch.tensor(node_features, dtype=torch.float)\n\n# Assign labels (target values)\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\n\n# 4️⃣ Build Graph Edges Using KDTree (k-NN with Distance Filtering)\ncoordinates = merged_nodes[['lat', 'lon']].values\ntree = KDTree(coordinates)\n\nk = 5  # Number of nearest neighbors\nthreshold = 0.02  # Maximum connection distance\n\n# Query k-NN\ndistances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\n\n# Construct edge list\nedge_list = []\nfor i in range(len(neighbors)):\n    for j in range(1, k+1):  # Skip first entry (self-node)\n        if distances[i][j] < threshold:  # Apply distance filter\n            edge_list.append((i, neighbors[i][j]))\n\n# Convert edges to tensor\nedges = np.array(edge_list).T\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\n\nprint(f\"New graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\n\n# 5️⃣ Create PyTorch Geometric Graph Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# 6️⃣ Graph Summary and Statistics\nprint(\"\\nGraph Data Summary:\")\nprint(graph_data)\nprint(f\"Number of nodes: {graph_data.num_nodes}\")\nprint(f\"Number of edges: {graph_data.num_edges}\")\nprint(f\"Node feature shape: {graph_data.x.shape}\")\nprint(f\"Edge index shape: {graph_data.edge_index.shape}\")\n\n# Compute degree statistics\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\nprint(f\"\\nDegree Stats:\")\nprint(f\"- Min degree: {deg.min().item()}\")\nprint(f\"- Max degree: {deg.max().item()}\")\nprint(f\"- Mean degree: {deg.float().mean().item()}\")\nprint(f\"- Standard deviation: {deg.float().std().item()}\")\n\n# Count isolated nodes\nnum_isolated_nodes = (deg == 0).sum().item()\npercentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\nprint(f\"\\nNumber of isolated nodes (degree=0): {num_isolated_nodes}\")\nprint(f\"Percentage of isolated nodes: {percentage_isolated:.4f}%\")\n\n# 7️⃣ Save Graph for Later Use\ngraph_data_output_path = \"/media/volume1/gnn_graph_final_2_2.pt\"\ntorch.save(graph_data, graph_data_output_path)\n\nprint(f\"\\n✅ PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\n",
  "history_output" : "Loading dataset...\nFiltered dataset loaded with 1723955 rows.\nAfter removing duplicates: 1217018 rows.\nMerged nodes count: 2064 from 1217018 original nodes.\nNew graph constructed with 2064 nodes and 1010 edges.\nGraph Data Summary:\nData(x=[2064, 11], edge_index=[2, 1010], y=[2064])\nNumber of nodes: 2064\nNumber of edges: 1010\nNode feature shape: torch.Size([2064, 11])\nEdge index shape: torch.Size([2, 1010])\nDegree Stats:\n- Min degree: 0.0\n- Max degree: 3.0\n- Mean degree: 0.4893410801887512\n- Standard deviation: 0.6147979497909546\nNumber of isolated nodes (degree=0): 1176\nPercentage of isolated nodes: 56.9767%\n✅ PyTorch Geometric Graph Data saved at: /media/volume1/gnn_graph_final_2_2.pt\n",
  "history_begin_time" : 1741225202200,
  "history_end_time" : 1741225235918,
  "history_notes" : "3 states New graph constructed with 2064 : nodes and 1010 edges.",
  "history_process" : "p7ar4i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "7yhhp0c4juu",
  "history_input" : "import pandas as pd\nimport torch\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import KDTree\nfrom torch_geometric.utils import degree\n\n# 1️⃣ Load Dataset\nfile_path = '/media/volume1/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\nchunksize = 500000  # Read in chunks for efficiency\n\n# Define spatial bounds\nmin_lat, max_lat = 42.020065, 45.768982\nmin_lon, max_lon = -124.212205, -116.940392\n\nprint(\"Loading dataset...\")\n\nuseful_columns = [\n    'lat', 'lon', 'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount','swe_value'\n]\n\n# Load and filter dataset\ndf_list = []\nfor chunk in pd.read_csv(file_path, usecols=useful_columns, chunksize=chunksize):\n    filtered_chunk = chunk[\n        (chunk[\"lat\"] >= min_lat) & (chunk[\"lat\"] <= max_lat) & \n        (chunk[\"lon\"] >= min_lon) & (chunk[\"lon\"] <= max_lon)\n    ]\n    df_list.append(filtered_chunk)\n\n# Merge filtered chunks\ndata = pd.concat(df_list, ignore_index=True)\nprint(f\"Filtered dataset loaded with {data.shape[0]} rows.\")\n\n# Drop duplicates\ndata = data.drop_duplicates()\nprint(f\"After removing duplicates: {data.shape[0]} rows.\")\n\n# 2️⃣ Merge Nodes Using Spatial Grid Binning\ngrid_size = 0.01  # Grid resolution in degrees\n\n# Assign each node to a grid cell\ndata['lat_bin'] = (data['lat'] // grid_size).astype(int)\ndata['lon_bin'] = (data['lon'] // grid_size).astype(int)\n\n# Create unique grid cell ID\ndata['grid_id'] = data['lat_bin'].astype(str) + \"_\" + data['lon_bin'].astype(str)\n\n# Aggregate nodes within each grid cell (supernodes)\nmerged_nodes = data.groupby('grid_id').agg({\n    'Elevation': 'mean',\n    'Slope': 'mean',\n    'SWE': 'mean',\n    'snow_depth': 'mean',\n    'air_temperature_observed_f': 'mean',\n    'precipitation_amount': 'mean',\n    'relative_humidity_rmin': 'mean',\n    'relative_humidity_rmax': 'mean',\n    'wind_speed': 'mean',\n    'cumulative_SWE': 'mean',\n    'cumulative_precipitation_amount': 'mean',\n    'swe_value': 'mean',  # Target variable\n    'lat': 'mean',  # Representative lat/lon\n    'lon': 'mean'\n}).reset_index()\n\nprint(f\"Merged nodes count: {merged_nodes.shape[0]} from {data.shape[0]} original nodes.\")\n\n# 3️⃣ Normalize Features\nscaler = StandardScaler()\nfeature_columns = [\n    'Elevation', 'Slope', 'SWE', 'snow_depth',\n    'air_temperature_observed_f', 'precipitation_amount',\n    'relative_humidity_rmin', 'relative_humidity_rmax', 'wind_speed',\n    'cumulative_SWE', 'cumulative_precipitation_amount'\n]\n\nnode_features = scaler.fit_transform(merged_nodes[feature_columns])\nnode_features = torch.tensor(node_features, dtype=torch.float)\n\n# Assign labels (target values)\nlabels = torch.tensor(merged_nodes['swe_value'].values, dtype=torch.float)\n\n# 4️⃣ Build Graph Edges Using KDTree (k-NN with Distance Filtering)\ncoordinates = merged_nodes[['lat', 'lon']].values\ntree = KDTree(coordinates)\n\nk = 5  # Number of nearest neighbors\nthreshold = 0.02  # Maximum connection distance\n\n# Query k-NN\ndistances, neighbors = tree.query(coordinates, k=k+1)  # k+1 to exclude self-connections\n\n# Construct edge list\nedge_list = []\nfor i in range(len(neighbors)):\n    for j in range(1, k+1):  # Skip first entry (self-node)\n        if distances[i][j] < threshold:  # Apply distance filter\n            edge_list.append((i, neighbors[i][j]))\n\n# Convert edges to tensor\nedges = np.array(edge_list).T\nedge_index = torch.tensor(edges, dtype=torch.long) if edges.shape[1] > 0 else torch.empty((2, 0), dtype=torch.long)\n\nprint(f\"New graph constructed with {merged_nodes.shape[0]} nodes and {edge_index.shape[1]} edges.\")\n\n# 5️⃣ Create PyTorch Geometric Graph Object\ngraph_data = Data(x=node_features, edge_index=edge_index, y=labels)\n\n# 6️⃣ Graph Summary and Statistics\nprint(\"\\nGraph Data Summary:\")\nprint(graph_data)\nprint(f\"Number of nodes: {graph_data.num_nodes}\")\nprint(f\"Number of edges: {graph_data.num_edges}\")\nprint(f\"Node feature shape: {graph_data.x.shape}\")\nprint(f\"Edge index shape: {graph_data.edge_index.shape}\")\n\n# Compute degree statistics\ndeg = degree(edge_index[0], num_nodes=graph_data.num_nodes)\nprint(f\"\\nDegree Stats:\")\nprint(f\"- Min degree: {deg.min().item()}\")\nprint(f\"- Max degree: {deg.max().item()}\")\nprint(f\"- Mean degree: {deg.float().mean().item()}\")\nprint(f\"- Standard deviation: {deg.float().std().item()}\")\n\n# Count isolated nodes\nnum_isolated_nodes = (deg == 0).sum().item()\npercentage_isolated = (num_isolated_nodes / graph_data.num_nodes) * 100\nprint(f\"\\nNumber of isolated nodes (degree=0): {num_isolated_nodes}\")\nprint(f\"Percentage of isolated nodes: {percentage_isolated:.4f}%\")\n\n# 7️⃣ Save Graph for Later Use\ngraph_data_output_path = \"/media/volume1/gnn_graph_final_1_1.pt\"\ntorch.save(graph_data, graph_data_output_path)\n\nprint(f\"\\n✅ PyTorch Geometric Graph Data saved at: {graph_data_output_path}\")\n",
  "history_output" : "Loading dataset...\nFiltered dataset loaded with 620662 rows.\nAfter removing duplicates: 434795 rows.\nMerged nodes count: 703 from 434795 original nodes.\nNew graph constructed with 703 nodes and 348 edges.\nGraph Data Summary:\nData(x=[703, 11], edge_index=[2, 348], y=[703])\nNumber of nodes: 703\nNumber of edges: 348\nNode feature shape: torch.Size([703, 11])\nEdge index shape: torch.Size([2, 348])\nDegree Stats:\n- Min degree: 0.0\n- Max degree: 3.0\n- Mean degree: 0.49502134323120117\n- Standard deviation: 0.6490542888641357\nNumber of isolated nodes (degree=0): 411\nPercentage of isolated nodes: 58.4637%\n✅ PyTorch Geometric Graph Data saved at: /media/volume1/gnn_graph_final_1_1.pt\n",
  "history_begin_time" : 1741212681464,
  "history_end_time" : 1741212717001,
  "history_notes" : "wash state without date",
  "history_process" : "p7ar4i",
  "host_id" : "100001",
  "indicator" : "Done"
}]